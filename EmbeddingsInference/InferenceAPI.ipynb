{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral on my own ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "\n",
    "try:\n",
    "\t# role = sagemaker.get_execution_role()\n",
    "\trole = 'arn:aws:iam::<REPLACE WITH ACCOUNT ID>:role/service-role/AmazonSageMaker-ExecutionRole-20231220T180882'\n",
    "except ValueError:\n",
    "\tiam = boto3.client('iam')\n",
    "\trole = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "\t'HF_MODEL_ID': 'mistralai/Mistral-7B-Instruct-v0.1',\n",
    "\t'SM_NUM_GPUS': json.dumps(1),\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "\t# image_uri=get_huggingface_llm_image_uri(\"huggingface\",version=\"1.1.0\"),\n",
    "\timage_uri='public.ecr.aws/p4k4c4t9/huggingface-pytorch-inference-extended',\n",
    "\t# transformers_version='4.10.2',\n",
    "\t# pytorch_version='1.8.1',\n",
    "\t# py_version='py36',\n",
    "\tenv=hub,\n",
    "\trole=role, \n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "\tinitial_instance_count=1,\n",
    "\tinstance_type=\"ml.g4dn.2xlarge\",\n",
    "\tcontainer_startup_health_check_timeout=300,\n",
    "  )\n",
    "  \n",
    "# send request\n",
    "predictor.predict({\n",
    "\t\"inputs\": \"My name is Julien and I like to\",\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "\n",
    "try:\n",
    "\t# role = sagemaker.get_execution_role()\n",
    "\trole = 'arn:aws:iam::<REPLACE WITH ACCOUNT ID>:role/service-role/AmazonSageMaker-ExecutionRole-20231220T180882'\n",
    "except ValueError:\n",
    "\tiam = boto3.client('iam')\n",
    "\trole = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "\t'HF_MODEL_ID':'meta-llama/Llama-2-7b-chat-hf',\n",
    "\t'SM_NUM_GPUS': json.dumps(1),\n",
    "\t'HUGGING_FACE_HUB_TOKEN': '<REPLACE WITH YOUR TOKEN>'\n",
    "}\n",
    "\n",
    "assert hub['HUGGING_FACE_HUB_TOKEN'] != '<REPLACE WITH YOUR TOKEN>', \"You have to provide a token.\"\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "\timage_uri=get_huggingface_llm_image_uri(\"huggingface\",version=\"1.1.0\"),\n",
    "\tenv=hub,\n",
    "\trole=role, \n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "\tinitial_instance_count=1,\n",
    "\tinstance_type=\"ml.g4dn.2xlarge\",\n",
    "\tcontainer_startup_health_check_timeout=300,\n",
    "  )\n",
    "  \n",
    "# send request\n",
    "predictor.predict({\n",
    "\t\"inputs\": \"My name is Julien and I like to\",\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace Inference API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '2019 trip to Japan?\\nWe are planning a trip to Japan in 2019 and would love to hear more about your experience.\\nWe are a family of 4 (2 adults and 2 children aged 10 and 12) and are interested in visiting Tokyo, Kyoto, Osaka and Hiroshima.\\nWe would appreciate any information you can share about your itinerary, accommodation, transportation and any tips or'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "api_key = os.environ[\"INFERENCE_API_KEY\"]\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/meta-llama/Llama-2-70b-chat-hf\"\n",
    "headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "output = query({\n",
    "\t\"inputs\": \"Can you please let us know more details about your \",\n",
    "})\n",
    "\n",
    "output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2018-2019 season?\\n\\nWe are a small, but very active club. We have 15 members, 10 of which are active. We have 2 meetings per month, one is a business meeting and the other is a social meeting. We have a lot of fun and we are very active in the community. We have a lot of fundraisers and we donate to many different organizations. We have a lot of fun and we are very active in the community.\\n\\nWhat are some of the highlights of your 2018-2019 season?\\n\\nWe had a lot of fun and we are very active in the community. We have a lot of fundraisers and we donate to many different organizations. We have a lot of fun and we are very active in the community.\\n\\nWhat are some of the challenges you faced this year?\\n\\nWe had a lot of fun and we are very active in the community. We have a lot of fundraisers and we donate to many different organizations. We have a lot of fun and we are very active in the community.\\n\\nWhat are some of the things you are looking'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with parameters\n",
    "output = query({\n",
    "\t\"inputs\": \"Can you please let us know more details about your \",\n",
    "    \"parameters\": {\"max_new_tokens\": 250, \"max_time\": 60.0, \"return_full_text\": False},\n",
    "    \"options\": {\"wait_for_model\": True}\n",
    "})\n",
    "\n",
    "output[0]['generated_text']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': 'The model mistralai/Mixtral-8x7B-v0.1 is too large to be loaded automatically (93GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints).'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-v0.1\"\n",
    "headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "output = query({\n",
    "\t\"inputs\": \"Can you please let us know more details about your \",\n",
    "})\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
