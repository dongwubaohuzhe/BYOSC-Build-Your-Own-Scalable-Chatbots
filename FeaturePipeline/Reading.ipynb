{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?client_id=745312273193-mloneconglkdeh457jl05jm6bg6b1hdn.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&access_type=offline&response_type=code\n",
      "\n",
      "Authentication successful.\n"
     ]
    }
   ],
   "source": [
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "\n",
    "gauth = GoogleAuth()\n",
    "gauth.LocalWebserverAuth()\n",
    "\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leggi File da GDrive nella cartella \"slides\" (con id 1u2jMMh-hxSb93sL-BGsX9tYVeuA2wdcq). \n",
    "import os\n",
    "\n",
    "file_list = drive.ListFile({'q': \"'1u2jMMh-hxSb93sL-BGsX9tYVeuA2wdcq' in parents and trashed=false\"}).GetList() \n",
    "new_files = []\n",
    "for file in file_list:\n",
    "    if not os.path.isfile(f\"{file['title']}\"): \n",
    "        #print(\"title: %s, id: %s\" % (file[\"title\"],file[\"id\"]))\n",
    "        file.GetContentFile(file[\"title\"])\n",
    "        new_files.append(file)\n",
    "\n",
    "#new_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "transcriptions=[]\n",
    "for file in new_files:\n",
    "    pdfReader = PyPDF2.PdfReader(file[\"title\"])\n",
    "\n",
    "    count = len(pdfReader.pages)\n",
    "    output = \"\"\n",
    "    for i in range(count):\n",
    "        pageObj = pdfReader.pages[i]\n",
    "        output+= \"\\n\" + pageObj.extract_text()\n",
    "    transcriptions.append(output)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nServerless Machine Learning\\nJim Dowling\\njdowling@kth.se\\n2022-11-04\\nEnterprise AI Value Chain\\n1 / 54\\nModern Enterprise Data and ML Infrastructure\\n2 / 54\\nMonolithic ML Pipeline\\n3 / 54\\nProblems with Monolithic ML Pipelines\\n▶They are often not modular - their components are not modular and cannot be\\nindependently scaled or deployed on different hardware (e.g., CPUs for feature engi-\\nneering, GPUs for model training).\\n▶They are difficult to test - production software needs automated tests to ensure\\nfeatures and models are of high quality.\\n▶They tightly couple the execution of feature engineering, model training, and infer-\\nence steps - running them in the same pipeline program at the same time.\\n▶They do not promote reuse of features/models/code. The code for computing fea-\\ntures (feature logic) cannot be easily disentangled from its pipeline jungle.\\n4 / 54\\nModularity enables more Robust and Scalable Systems\\nModular water pipes in a Google Datacenter. Instead of one giant water pipe (our\\nmonolithic notebook), separate water pipes reduce the blast radius if one fails. Color\\ncoding makes it easier to debug problems in a damaged water pipe.\\n5 / 54\\nPipelines as Modular Programs\\n▶Modularity involves structuring your code such that its functionality is separated into\\nindependent classes and/or functions that can be more easily reused and tested.\\n▶Modules should be placed in accessible classes or functions, keeping them small and\\neasy to understand and document.\\n▶Modules enable code to be more easily reused in different pipelines.\\n▶Modules enable code to be more easily independently tested, enabling the easier and\\nearlier discovery of bugs.\\n6 / 54\\nSupervised ML Pipeline Stages\\ntrain (features ,labels )−>model\\nmodel (features )−>predictions\\n7 / 54\\nML Pipeline Stages in a Serverless Machine Learning System\\n8 / 54\\nML Pipeline Stages - Data Sources\\n9 / 54\\nConnect to Data Sources and Read Raw Data\\n▶Discover data sources, securely connect to heterogeneous data sources\\n▶Manage dependencies such as connectors and drivers\\n▶Manage connection information securely: network endpoint, database/table names,\\nauthentication credentials such as API keys or credentials (username/password)\\n10 / 54\\nHeterogeneous Data Sources\\n11 / 54\\nFile Formats for different Data Sources\\n12 / 54\\nML Pipeline Stages - Feature Pipelines\\n13 / 54\\nFeature Pipelines\\n14 / 54\\nFeature Pipelines\\n▶A feature pipeline is a program that orchestrates the execution of feature engineering\\nsteps on input data to create feature values.\\nExamples of feature engineering steps:\\n▶Clean, validate, data\\n▶Data de-duplication, pseudononymization, data wrangling\\n▶Feature extraction, aggregations, dimensionality reduction, feature binning, feature\\ncrosses\\n15 / 54\\nTabular Data\\n16 / 54\\nTabular Data as Features, Labels, Entity (or Primary) Keys,\\nEvent Time\\n17 / 54\\nTabular Data in Pandas\\n18 / 54\\nExploratory Data Analysis in Pandas\\n19 / 54\\nAggregations in Pandas\\n20 / 54\\nRolling Windows in Pandas\\n21 / 54\\nFeature binning\\n22 / 54\\nFeature Crosses\\n▶A feature cross is a synthetic feature formed by multiplying (crossing) two or more\\nfeatures. By multiplying features together, you encode nonlinearity in the feature\\nspace.\\n▶For example, imagine we are looking for credit card fraud activity within a geographic\\nregion (e.g., a city district), how would we capture that as a feature?\\n▶We could cross to a geographic area (binned latitude and binned longitude - a grid\\nidentifying a city district) with the level of credit card activity within that geographic\\narea.\\n23 / 54\\nEmbeddings as Features\\n▶An embedding is a lower dimension representation of a sparse input that retains some\\nof the semantics of the input.\\n▶An embedding store (vector database) stores semantically similar inputs close to-\\ngether in the embedding space. You can implement “similarity search” by finding\\nembeddings close in embedding space. You can even apply arithmetic on embeddings\\nto discover semantic relationships.\\n24 / 54\\nML Pipeline Stages - Feature Store\\n25 / 54\\nStore Features\\nThere are two general ways people manage features and labels for both training and\\nserving:\\n▶(1) Compute features on-demand as part of the model training or batch inference\\npipeline.\\n▶(2) Use a feature store to store the features so that they can be reused across\\ndifferent models for both training and inference. For online models that require\\nfeatures with either historical or contextual information , feature stores are typically\\nused.\\n26 / 54\\nML Pipeline Stages - Training Pipelines\\n27 / 54\\nFeature Types\\nReference: https://www.hopsworks.ai/post/feature-types-for-machine-learning\\n28 / 54\\nFeature Types Taxonomy\\n29 / 54\\nModel Training Pipelines\\n30 / 54\\nModel-Dependent Transformations\\nReference: https://developers.google.com/machine-learning/data-\\nprep/transform/introduction\\n31 / 54\\nTransformations in Pandas\\n32 / 54\\nDifferent types of Transformations\\n33 / 54\\nModel Training with Train and Test Sets\\n34 / 54\\nModel Training with Train and Test Sets in Scikti-Learn\\n35 / 54\\nModel Training is an Iterative Process\\n36 / 54\\nModel-Centric Iteration to Improve Model Performance\\nPossible steps to improve your model performance:\\n▶Try out a different supervised ML learning algorithm (e.g., random forest, feedforward\\ndeep neural network, Gradient-boosted decision tree)\\n▶Try out new combinations of hyperparameters (e.g., number of training epochs, the\\nlearning rate, number of layers in a deep neural network, adjust regularizations such\\nas Dropout or BatchNorm)\\n▶Evaluate your model on a validation set (keeping a separate holdout test set for final\\nmodel performance evaluation)\\n37 / 54\\nData-Centric Iteration to Improve Model Performance\\nSteps to improve your model\\n▶Add or remove features to or from your model (feature selection)\\n▶Add more training data\\n▶Remove poor quality training samples\\n▶Improve the quality of existing training samples (e.g., using Cleanlab or Snorkel)\\n▶Rank the importance of the training samples (Active Learning)\\n38 / 54\\nTrain, Validation, and Test Sets\\n▶Random splits of the training data when the data is not time-series data\\n▶Time-series splits of the training data when the data is time-series data\\n39 / 54\\nModel Training is an Iterative Process\\n40 / 54\\nML Pipeline Stages - Inference Pipelines\\n41 / 54\\nBatch Inference Pipeline\\n42 / 54\\nOnline Inference Pipeline\\n43 / 54\\nServerless ML with Python\\n▶Write Feature, Training, and Inference Pipelines in Python\\n▶Orchestrate the execution of Pipelines using Serverless Compute Platforms\\n▶Store features and models in a serverless feature/model store\\n▶Run a User Interface (UI), written in Python, on serverless infrastructure\\n44 / 54\\nServerless Compute Platforms\\n45 / 54\\nServerless Feature Stores and Model Registry/Serving\\nFeature Stores\\n▶Hopsworks\\nModel Registry and Serving\\n▶Hopsworks\\n▶AWS Sagemaker\\n▶Databricks\\n▶Google Vertex\\n46 / 54\\nServerless User Interfaces\\n▶Hugging Faces Spaces\\n▶Streamlit Cloud\\n47 / 54\\nIris Flower Dataset\\nhttps://github.com/ID2223KTH/id2223kth.github.io/tree/master/src/serverless-ml-\\nintro\\n▶4 input features: sepal length, sepal width, petal length, petal width\\n▶label (target): Iris Flower Type (one of Setosa, Versicolor, Virginica)\\n▶Only 150 samples in the dataset\\n48 / 54\\nServerless Iris with Modal, Hopsworks, and Hugging Face\\n49 / 54\\nIris Flowers: Feature Pipeline with Modal and Hopsworks\\nimport os\\nimport modal\\nstub = modal.Stub()\\nhopsworks_image = modal.Image.debian_slim().pip_install([\"hopsworks\"])\\n@stub.function(image=hopsworks_image, schedule=modal.Period(days=1), \\\\\\nsecret=modal.Secret.from_name(\"jim-hopsworks-ai\"))\\ndef f():\\nimport hopsworks\\nimport pandas as pd\\nproject = hopsworks.login()\\nfs = project.get_feature_store()\\niris_df = pd.read_csv(\"https://repo.hops.works/master/hopsworks-tutorials/data/iris.csv\")\\niris_fg = fs.get_or_create_feature_group( name=\"iris_modal\", version=1,\\nprimary_key=[\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\"],\\ndescription=\"Iris flower dataset\")\\niris_fg.insert(iris_df)\\nif __name__ == \"__main__\":\\nwith stub.run():\\nf()\\n50 / 54\\nTraining Pipeline with Modal and Hopsworks\\n@stub.function(image=hopsworks_image, schedule=modal.Period(days=1),\\\\\\nsecret=modal.Secret.from_name(\"jim-hopsworks-ai\"))\\ndef f():\\n# lots of imports\\nproject = hopsworks.login()\\nfs = project.get_feature_store()\\ntry:\\nfeature_view = fs.get_feature_view(name=\"iris_modal\", version=1)\\nexcept:\\niris_fg = fs.get_feature_group(name=\"iris_modal\", version=1)\\nquery = iris_fg.select_all()\\nfeature_view = fs.create_feature_view(name=\"iris_modal\",\\nversion=1,\\ndescription=\"Read from Iris flower dataset\",\\nlabels=[\"variety\"],\\nquery=query)\\nX_train, X_test, y_train, y_test = feature_view.train_test_split(0.2)\\nmodel = KNeighborsClassifier(n_neighbors=2)\\nmodel.fit(X_train, y_train.values.ravel())\\n51 / 54\\nTraining Pipeline (ctd)\\ny_pred = model.predict(X_test)\\nmetrics = classification_report(y_test, y_pred, output_dict=True)\\nresults = confusion_matrix(y_test, y_pred)\\ndf_cm = pd.DataFrame(results, [’True Setosa’, ’True Versicolor’, ’True Virginica’],\\n[’Pred Setosa’, ’Pred Versicolor’, ’Pred Virginica’])\\ncm = sns.heatmap(df_cm, annot=True)\\nfig = cm.get_figure()\\njoblib.dump(model, \"iris_model/iris_model.pkl\")\\nfig.savefig(\"iris_model/confusion_matrix.png\")\\ninput_schema = Schema(X_train)\\noutput_schema = Schema(y_train)\\nmodel_schema = ModelSchema(input_schema, output_schema)\\nmr = project.get_model_registry()\\niris_model = mr.python.create_model(\\nname=\"iris_modal\",\\nmetrics={\"accuracy\" : metrics[’accuracy’]},\\nmodel_schema=model_schema,\\ndescription=\"Iris Flower Predictor\")\\niris_model.save(\"iris_model\")\\n52 / 54\\nInteractive Inference Pipeline with Hugging Face/Hopsworks\\nmodel = mr.get_model(\"iris_modal\", version=1)\\nmodel_dir = model.download()\\nmodel = joblib.load(model_dir + \"/iris_model.pkl\")\\ndef iris(sepal_length, sepal_width, petal_length, petal_width):\\ninput_list = []\\ninput_list.append(sepal_length)\\ninput_list.append(sepal_width)\\ninput_list.append(petal_length)\\ninput_list.append(petal_width)\\nres = model.predict(np.asarray(input_list).reshape(1, -1))\\nflower_url = \"https://raw.githubusercontent.com/.../assets/\" + res[0] + \".png\"\\nreturn Image.open(requests.get(flower_url, stream=True).raw)\\ndemo = gr.Interface(\\nfn=iris, title=\"Iris Flower Predictive Analytics\", allow_flagging=\"never\",\\ndescription=\"Experiment with sepal/petal lengths/widths to predict which flower it is.\",\\ninputs=[ gr.inputs.Number(default=1.0, label=\"sepal length (cm)\"),\\ngr.inputs.Number(default=1.0, label=\"sepal width (cm)\"),\\ngr.inputs.Number(default=1.0, label=\"petal length (cm)\"),\\ngr.inputs.Number(default=1.0, label=\"petal width (cm)\"),],\\noutputs=gr.Image(type=\"pil\"))\\ndemo.launch()\\n53 / 54\\nQuestions?\\nAcknowledgements\\nSome of the images are used with permission from Hopsworks AB.\\n54 / 54', '\\nIntroduction\\nJim Dowling\\njdowling@kth.se\\nCourse Assistants: Fabien Schmidt, Javier Ron, Ahmad Al-Shishtawy\\n2023-10-30\\nSlides by Amir H. Payberah and Jim Dowling\\nCourse Information\\n1 / 79\\nCourse Objective\\n▶This course has a system-based focus.\\n▶Learn the theory of machine learning and deep learning.\\n▶Learn the practical aspects of building machine learning and deep learning algorithms\\nusing data parallel programming platforms, such as TensorFlow.\\n2 / 79\\n————————————————\\n3 / 79\\nTopics Covered in the Course\\n▶Part 1: large scale deep learning\\n•TensorFlow\\n•Deep Neural Networks (DNN)\\n•Different DNN architectures, e.g., CNNs, RNNs, Autoencoders, GAN\\n•Distributed learning\\n▶Part 2: large scale machine learning\\n•Serverless Machine Learning\\n•MLOps (machine learning operations)\\n•Feature Stores for Machine Learning\\n•Distributed Training of DNNs\\n3 / 79\\nIntended Learning Outcomes (ILOs)\\n▶ILO1: explain the principles of ML/DL algorithms and apply their techniques to solve\\nproblems.\\n▶ILO2: demonstrate an ability to design DNN architectures and explain challenges to\\nscaling their training and inference with increasing compute and data.\\n▶ILO3: explain the principles of scaling machine learning systems.\\n▶ILO4: implement scalable ML/DL systems.\\n4 / 79\\nThe Course Assessment\\n▶Task1: the Examination on 8th January 2023(A-F)\\n▶Task2: the lab assignments (A-F)\\n▶Task3: the final project (P/F)\\n5 / 79\\nHow Each ILO is Assessed?\\nTask1 Task2 Task3\\nILO1 x\\nILO2 x x x\\nILO3 x x\\nILO4 x x\\n6 / 79\\nTask1: The Exam (A-F)\\n▶Questions about the lectures and the labs.\\n▶Some sample questions for the exam will be distributed in mid December.\\n▶The examination is graded (A-F).\\n7 / 79\\nTask2: The Lab Assignments (A-F)\\n▶Two lab assignments: source code and oral presentation.\\n▶E: source code\\n▶D: source code + half questions (basic)\\n▶C: source code + all questions (basic)\\n▶B: source code + half questions (basic and advanced)\\n▶A: source code + all questions (basic and advanced)\\n8 / 79\\nTask3: The Final Project (A-F)\\n▶One final project: source code and oral presentation.\\n▶Proposed by students and confirmed by the teacher:\\n▶Source code and documentation as a README.md by the students of the project\\n▶5 minute Presentation of the project by the students.\\n▶20% of total grade bonus for projects graded as excellent.\\n9 / 79\\nThe Final Grade\\n▶The final grade is the weighted average of the Exam (0.3), two labs (0.15 each). A\\nbonus of .09 points will be added for projects graded ’excellent’.\\n▶To compute it, map A-E to 5-1, and take the average.\\n▶The floating values are rounded up, if they are more than half, otherwise they are\\nrounded down.\\n•E.g., 3.6 will be rounded to 4, and 4.5 will be rounded to 4.\\n▶A late submission will reduce your grade level by one. That is, A will become B, B\\nwill become C, and so on.\\n▶To pass the course, you need to take at least E in all the assignments.\\n10 / 79\\nHow do you submit the Assignments?\\n▶Through the Canvas site.\\n▶Students will work in groups of two on Task 2 and Task 3.\\n11 / 79\\nThe Course Material\\n▶Hands-on machine learning with Scikit-Learn and TensorFlow, 2nd Edition, A. Geron,\\nO’Reilly Media, 2019\\n▶Deep learning, I. Goodfellow et al., Cambridge: MIT press, 2016\\n12 / 79\\nThe Course Web Page\\nhttps://id2223kth.github.io\\n13 / 79\\nSupervised Machine Learning 101\\n14 / 79\\nFeatures in Machine Learning\\n15 / 79\\nSupervised Machine Learning (ML)\\n16 / 79\\nA Linear Model can classify the Fruit (Classifier)\\n17 / 79\\nThe Decision Boundary\\n18 / 79\\nSource code for a small sample of data with a decision tree\\nclassifier\\n19 / 79\\nBut wait, apples can also be red!\\n20 / 79\\nIt’s harder to separate Green Apples, Oranges, and Red Apples\\nwith just 2 colors (red and green)\\n21 / 79\\nFrom two dimensions to millions of dimensions\\n▶Can we just add more features? Yes. If you add weight and smoothness, you can\\nseparate apples and oranges. We could plot our fruit in 3d or even 4d and find a\\nplane that separates apples and oranges\\n▶You can add many more features (dimensions). With the caveat that too many\\ndimensions can lead to overfitting. Overfitting means your model is not good at\\ngeneralizing to correctly predict new fruit examples (it would work well for the training\\ndata, but not unseen (new) examples)\\n▶In image classification, each pixel is a feature. That’s millions of features for a single\\nHD image. Deep learning can be used to train models with millions of features.\\n22 / 79\\nNot all properties with predictive power should be features\\n23 / 79\\nLots of features: deep learning for apple classification works\\n24 / 79\\nA model can also be trained to predict a number (regression)\\n25 / 79\\nPredicting Surf Height at a beach – Classification or\\nRegression?\\n26 / 79\\nWhat is Supervised Machine Learning, then?\\n▶With our Apple/Orange classifier, we used (features, label) examples to train a model\\nto find a decision boundary.\\n▶Then when a new fruit arrived, we could use the model to predict if the new piece\\nof fruit is an apple or an orange.\\n▶We can generalize to say that supervised machine learning is concerned with:\\n▶extracting a pattern from labeled data (features) to a model\\n▶using that model to make predictions for new unlabeled data (features)\\n27 / 79\\nTradtional ML courses vs ID2223\\n▶Static Datasets, where Features for ML are correct and unbiased\\n▶The goal is to optimize your model with a model evaluation metric (accuracy) to\\ncommunicate the value of your model\\n▶Data never stops coming and it comes from heterogeneous data sources\\n▶Communicate the value of your model as a Prediction Service - one that can be scaled\\nand deployed using MLOps (Machine Learning Operations) best practices (versioning,\\nautomated testing/deployment)\\n28 / 79\\nFeature Engineering is often treated like this “helpful” guide to\\ndrawing a Barn Owl\\n29 / 79\\nExtract the features from the input data. We will study feature\\nengineering at scale.\\nRaw Data Extracted Feature Method\\nHotel room bookings Weekly vacancy level Aggregation\\nUser’s web session history Session history embedding Dimensionality Reduction\\nUser’s date of birth Binning\\nHourly spot electricity prices Scale into range [0, 1] Normalization\\nUser’s home country Binary number of country One Hot Encoding\\n30 / 79\\nMany Enterprises have walled gardens between teams building\\nproduction ML Systems\\n31 / 79\\nWith infrastructure support (Serverless Machine Learning),\\ndevelopers need less infrastructural skills to deploy ML Systems\\n32 / 79\\nMonolithic ML Pipeline\\n33 / 79\\nRefactor the Monolithic ML Pipeline to Scale your ML Systems\\n34 / 79\\nMachine Learning and Deep Learning\\n35 / 79\\nLearning Algorithms\\n▶A ML algorithm is an algorithm that is able to learn from data.\\n▶What is learning?\\n▶A computer program is said to learn from experience E with respect to some class of\\ntasks T and performance measure P, if its performance at tasks in T, as measured\\nby P, improves with experience E. (Tom M. Mitchell)\\n36 / 79\\nLearning Algorithms - Example 1\\n▶A spam filter that can learn to flag spam given examples of spam emails and examples\\nof regular emails.\\n▶Task T: flag spam for new emails\\n▶Experience E: the training data\\n▶Performance measure P: the ratio of correctly classified emails\\n[https://bit.ly/2oiplYM]\\n37 / 79\\nLearning Algorithms - Example 2\\n▶Given dataset of prices of 500 houses, how can we learn to predict the prices of other\\nhouses, as a function of the size of their living areas?\\n▶Task T: predict the price\\n▶Experience E: the dataset of living areas and prices\\n▶Performance measure P: the difference between the predicted price and the real price\\n[https://bit.ly/2MyiJUy]\\n38 / 79\\nTypes of Machine Learning Algorithms\\n▶Supervised learning\\n•Input data is labeled, e.g., spam/not-spam or a stock price at a time.\\n•Regression vs. classification\\n▶Unsupervised learning\\n•Input data is unlabeled.\\n•Find hidden structures in data.\\n39 / 79\\nAI Generations - Deep Learning\\n▶For many tasks, it is difficult to know what features should be extracted\\n▶Use machine learning to discover the mapping from representation to output\\n[https://bit.ly/2woLEzs]\\n40 / 79\\nImage Classification with Deep Learning\\n▶For image classification, where each pixel is a feature, Deep Learning can do the\\nfeature extraction as part of the learning algorithm.\\n41 / 79\\nSheepdog or Mop\\n42 / 79\\nChihuahua or Muffin\\n43 / 79\\nBarn Owl or Apple\\n44 / 79\\nTraining Deep Neural Networks\\n▶Computationally intensive\\n▶Time consuming\\n[https://cloud.google.com/tpu/docs/images/inceptionv3onc--oview.png]\\n45 / 79\\nWhy?\\n▶Massive amount of training dataset\\n▶Large number of parameters\\n46 / 79\\nAccuracy vs. Data/Model Size\\n[Jeff Dean at AI Frontiers: Trends and Developments in Deep Learning Research]\\n47 / 79\\nAccuracy vs. Data/Model Size\\n[Jeff Dean at AI Frontiers: Trends and Developments in Deep Learning Research]\\n48 / 79\\nAccuracy vs. Data/Model Size\\n[Jeff Dean at AI Frontiers: Trends and Developments in Deep Learning Research]\\n49 / 79\\nWhy Does Deep Learning Work Now?\\n▶Huge quantity of data\\n▶Tremendous increase in computing power\\n▶Better training algorithms\\n50 / 79\\nLinear Algebra Review\\n51 / 79\\nVector\\n▶A vector is an array of numbers.\\n▶Notation:\\n•Denoted by bold lowercase letters, e.g., x.\\n•xidenotes the ith entry.\\nx=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0x1\\nx2\\n...\\nxn\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n52 / 79\\nMatrix and Tensor\\n▶A matrix is a 2-D array of numbers.\\n▶A tensor is an array with more than two axes.\\n▶Notation:\\n•Denoted by bold uppercase letters, e.g., A.\\n•aijdenotes the entry in ith row and jth column.\\n•IfAism×n, it has mrows and ncolumns.\\nA=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0a1,1a1,2a1,3. . . a1,n\\na2,1a2,2a2,3. . . a2,n\\n...............\\nam,1am,2am,3. . . am,n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n53 / 79\\nMatrix Addition and Subtraction\\n▶The matrices must have the same dimensions.\\nA=\\x14a b\\nc d\\x15\\n+\\x14e f\\ng h\\x15\\n=\\x14a+e b +f\\nc+g d +h\\x15\\n54 / 79\\nMatrix Product\\n▶The matrix product of matrices AandBis a third matrix C, where C=AB.\\n▶IfAis of shape m×nandBis of shape n×p, then Cis of shape m×p.\\ncij=X\\nkaikbkj\\n▶Properties\\n•Associative: ( AB)C=A(BC)\\n•Not commutative: AB̸=BA\\n[https://en.wikipedia.org/wiki/Matrix multiplication]\\n55 / 79\\nMatrix Transpose\\n▶Swap the rows and columns of a matrix.\\nA=\\uf8ee\\n\\uf8f0a b\\nc d\\ne f\\uf8f9\\n\\uf8fb⇒A⊺=\\x14a c e\\nb d f\\x15\\n▶Properties\\n•Aij=A⊺\\nji\\n•IfAism×n, then A⊺isn×m\\n•(A+B)⊺=A⊺+B⊺\\n•(AB)⊺=B⊺A⊺\\n56 / 79\\nInverse of a Matrix\\n▶IfAis a square matrix, its inverse is called A−1.\\nAA−1=A−1A=I\\n▶Where I, the identity matrix, is a diagonal matrix with all 1’s on the diagonal.\\nI2=\\x141 0\\n0 1\\x15\\nI3=\\uf8ee\\n\\uf8f01 0 0\\n0 1 0\\n0 0 1\\uf8f9\\n\\uf8fb\\n57 / 79\\nLpNorm for Vectors\\n▶We can measure the size of vectors using a norm function.\\n▶Norms are functions mapping vectors to non-negative values.\\n▶L1norm\\n||x||1=X\\ni|xi|\\n▶L2norm\\n||x||2= (X\\ni|xi|2)1\\n2=q\\nx2\\n1+x2\\n2+···+x2n\\n▶Lpnorm\\n||x||p= (X\\ni|xi|p)1\\np\\n58 / 79\\nProbability Review\\n59 / 79\\nRandom Variables\\n▶Random variable: a variable that can take on different values randomly.\\n▶Random variables may be discrete or continuous.\\n•Discrete random variable: finite or countably infinite number of states\\n•Continuous random variable: real value\\n▶Notation:\\n•Denoted by an upper case letter, e.g., X\\n•Values of a random variable Xare denoted by lower case letters, e.g., xandy.\\n60 / 79\\nProbability Distributions\\n▶Probability distribution: how likely a random variable is to take on each of its possible\\nstates.\\n•E.g., the random variable Xdenotes the outcome of a coin toss.\\n•The probability distribution of Xwould take the value 0.5forX=head , and 0.5for\\nY=tail (assuming the coin is fair).\\n▶The way we describe probability distributions depends on whether the variables are\\ndiscrete or continuous.\\n61 / 79\\nDiscrete Variables\\n▶Probability mass function (PMF): the probability distribution of a discrete random\\nvariable X.\\n▶Notation: denoted by a lowercase p.\\n•E.g., p(x) =1indicates that X=xis certain\\n•E.g., p(x) =0indicates that X=xis impossible\\n▶Properties:\\n•The domain Dofpmust be the set of all possible states of X\\n•∀x∈D(X),0≤p(x)≤1\\n•P\\nx∈D(X)p(x) =1\\n62 / 79\\nIndependence\\n▶Two random variables XandYare independent, if their probability distribution can\\nbe expressed as their products.\\n∀x∈D(X),y∈D(Y),p(X=x,Y=y) =p(X=x)p(Y=y)\\n▶E.g., if a coin is tossed and a single 6-sided die is rolled, then the probability of\\nlanding on the head side of the coin and rolling a 3 on the die is:\\np(X=head ,Y=3) =p(X=head )p(Y=3) =1\\n2×1\\n6=1\\n12\\n63 / 79\\nConditional Probability\\n▶Conditional probability: the probability of an event given that another event has\\noccurred.\\np(Y=y|X=x) =p(Y=y,X=x)\\np(X=x)\\n▶E.g., if 60% of the class passed both labs and 80% of the class passed the first labs,\\nthen what percent of those who passed the first lab also passed the second lab?\\n•E.g., XandYrandom variables for the first and the second labs, respectively.\\np(Y=lab2|X=lab1 ) =p(Y=lab2 ,X=lab1 )\\np(X=lab1 )=0.6\\n0.8=3\\n4\\n64 / 79\\nExpectation\\n▶The expected value of a random variable Xwith respect to a probability distribution\\np(X) is the average value that Xtakes on when it is drawn from p(X).\\nEx∼p[X] =X\\nxp(x)x\\n▶E.g., If X:{1,2,3}, and p(X=1) =0.3,p(X=2) =0.5,p(X=3) =0.2\\n•E[X] =0.3×1+0.5×2+0.2×3=1.9\\n65 / 79\\nVariance and Standard Deviation\\n▶The variance gives a measure of how much the values of a random variable Xvary\\nas we sample it from its probability distribution p(X).\\nVar(X) =E[(X−E[X])2]\\nVar(X) =X\\nxp(x)(x−E[X])2\\n▶E.g., If X:{1,2,3}, and p(X=1) =0.3,p(X=2) =0.5,p(X=3) =0.2\\n•E[X] =0.3×1+0.5×2+0.2×3=1.9\\n•Var(X) =0.3(1−1.9)2+0.5(2−1.9)2+0.2(3−1.9)2=0.49\\n▶The standard deviation, shown by σ, is the square root of the variance.\\n66 / 79\\nCovariance (1/2)\\n▶The covariance gives some sense of how much two values are linearly related to each\\nother.\\nCov(X,Y) =E[(X−E[X])(Y−E[Y])]\\nCov(X,Y) =X X\\n(x,y)p(x,y)(x−E[X])(y−E[Y])\\n67 / 79\\nCovariance (2/2)\\nY\\np(X, Y) 1 2 3 p(X)\\n1 1/4 1/4 0 1/2\\nX 2 0 1/4 1/4 1/2\\np(Y) 1/4 1/2 1/4 1\\nE[X] =1\\n2×1+1\\n2×2=3\\n2E[Y] =1\\n4×1+1\\n2×2+1\\n4×3=2\\nCov(X,Y) =X X\\n(x,y)p(x,y)(x−E[X])(y−E[Y])\\n=1\\n4(1−3\\n2)(1−2) +1\\n4(1−3\\n2)(2−2) +0(1−3\\n2)(3−2)\\n+0(2−3\\n2)(1−2) +1\\n4(2−3\\n2)(2−2) +1\\n4(2−3\\n2)(3−2) =1\\n4\\n68 / 79\\nCorrelation Coefficient\\n▶The Correlation coefficient is a quantity that measures the strength of the association\\n(or dependence) between two random variables, e.g., XandY.\\nρ(X,Y) =Cov(X,Y)\\nσ(X)σ(Y)\\n69 / 79\\nProbability and Likelihood (1/2)\\n▶LetX:{x(1),x(2),···,x(m)}be a discrete random variable drawn independently from\\na distribution probability pdepending on a parameter θ.\\n•For six tosses of a coin, X:{h,t,t,t,h,t},h: head, and t: tail.\\n•Suppose you have a coin with probability θto land heads and ( 1−θ) to land tails.\\n▶p(X|θ=2\\n3) is the probability of Xgiven θ=2\\n3.\\n▶p(X=h|θ) is the likelihood of θgiven X=h.\\n▶Likelihood ( L): a function of the parameters ( θ) of a probability model, given specific\\nobserved data, e.g., X=h.\\nL(θ|X) =p(X|θ)\\n70 / 79\\nProbability and Likelihood (2/2)\\n▶The likelihood differs from that of a probability.\\n▶A probability p(X|θ) refers to the occurrence of future events.\\n▶A likelihood L(θ|X) refers to past events with known outcomes.\\n71 / 79\\nMaximum Likelihood Estimator\\n▶If samples in Xare independent we have:\\nL(θ|X) =p(X|θ) =p(x(1),x(2),···,x(m)|θ)\\n=p(x(1)|θ)p(x(2)|θ)···p(x(m)|θ) =mY\\ni=1p(x(i)|θ)\\n▶The maximum likelihood estimator (MLE): what is the most likely value of θgiven\\nthe training set?\\n^θMLE= arg max\\nθL(θ|X) = arg max\\nθmY\\ni=1p(x(i)|θ)\\n72 / 79\\nMaximum Likelihood Estimator - Example\\n▶Six tosses of a coin, with the following model:\\n•Possible outcomes: h with probability of θ, and t with probability ( 1−θ).\\n•Results of coin tosses are independent of one another.\\n▶Data: X:{h,t,t,t,h,t}\\n▶The likelihood is\\nL(θ|X) =p(X|θ)\\n=p(X=h|θ)p(X=t|θ)p(X=t|θ)p(X=t|θ)p(X=h|θ)p(X=t|θ)\\n=θ(1−θ)(1−θ)(1−θ)θ(1−θ)\\n=θ2(1−θ)4\\n▶^θis the value of θthat maximizes the likelihood:\\n^θMLE= arg max\\nθL(θ|X) =2\\n2+4\\n73 / 79\\nLog-Likelihood\\n▶The MLE product is prone to numerical underflow.\\n^θMLE= arg max\\nθL(θ|X) = arg max\\nθmY\\ni=1p(x(i)|θ)\\n▶To overcome this problem we can use the logarithm of the likelihood.\\n•It does not change its arg max, but transforms a product into a sum.\\n^θMLE= arg max\\nθmX\\ni=1logp (x(i)|θ)\\n74 / 79\\nNegative Log-Likelihood\\n▶Likelihood: L(θ|X) =Qm\\ni=1p(x(i)|θ)\\n▶Log-Likelihood: logL (θ|X) =logQm\\ni=1p(x(i)|θ) =Pm\\ni=1logp (x(i)|θ)\\n▶Negative Log-Likelihood: −logL (θ|X) =−Pm\\ni=1logp (x(i)|θ)\\n▶Negative log-likelihood is also called the cross-entropy\\n75 / 79\\nCross-Entropy\\n▶Coss-entropy: quantify the difference (error) between two probability distributions.\\n▶How close is the predicted distribution to the true distribution?\\nH(p,q) =−X\\nxp(x)log(q(x))\\n▶Where pis the true distribution, and qthe predicted distribution.\\n76 / 79\\nCross-Entropy - Example\\n▶Six tosses of a coin: X:{h,t,t,t,h,t}\\n▶The true distribution p:p(h) =2\\n6andp(t) =4\\n6\\n▶The predicted distribution q: h with probability of θ, and t with probability ( 1−θ).\\n▶Cross entropy: H(p,q) =−P\\nxp(x)log(q(x))\\n=−p(h)log(q(h))−p(t)log(q(t)) =−2\\n6log(θ)−4\\n6log(1−θ)\\n▶Likelihood: θ2(1−θ)4\\n▶Negative log likelihood: −log(θ2(1−θ)4) =−2log (θ)−4log (1−θ)\\n77 / 79\\nReferences\\n▶Ian Goodfellow et al., Deep Learning (Ch. 1, 2, 3)\\n78 / 79\\nQuestions?\\nAcknowledgements\\nSome of the pictures were copied from the book Hands-On Machine Learning\\nwith Scikit-Learn and TensorFlow, Aurelien Geron, O’Reilly Media, 2017.\\n79 / 79']\n"
     ]
    }
   ],
   "source": [
    "print(transcriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"text\": \"Serverless Machine Learning\\n\\nJim Dowling\\njdowling@kth.se\\n2022-11-04\\n\\nProblems with Monolithic ML Pipelines\\n- Lack of modularity, scalability, and reusability\\n- Difficulty in testing\\n- Tight coupling of feature engineering, model training, and inference steps\\n\\nModularity enables more robust and scalable systems by separating functionality into independent classes and functions that can be reused and tested. It allows for easier debugging and promotes code reuse. \\n\\nA supervised ML pipeline typically consists of two stages: training and prediction. In a serverless machine learning system, there are additional stages such as data sources and feature pipelines. \\n\\nThe data sources stage involves connecting to heterogeneous data sources, managing dependencies and connection information securely. Different file formats may be used for different data sources.\\n\\nThe feature pipelines stage orchestrates the execution of feature engineering steps on input data, including tasks such as cleaning, deduplication, extraction, aggregations, and dimensionality reduction. It also includes advanced techniques like feature binning and feature crosses, which involve combining multiple features to create synthetic features. Embeddings are lower-dimensional representations of sparse input that retain some semantic information.\\n\\nA feature store is used to store and manage features and labels for both training and prediction. It allows for easy retrieval and reuse of features.\\n\\n\",\n",
      "    \"questions\": [\n",
      "        \"What are the problems with monolithic ML pipelines?\",\n",
      "        \"Why is modularity important in machine learning systems?\",\n",
      "        \"What are some of the tasks involved in feature pipelines?\"\n",
      "    ],\n",
      "    \"answers\": [\n",
      "        \"The problems with monolithic ML pipelines include lack of modularity, scalability, and reusability, difficulty in testing, and tight coupling of feature engineering, model training, and inference steps.\",\n",
      "        \"Modularity is important in machine learning systems because it enables more robust and scalable systems. It allows for easier code reuse, testing, and debugging.\",\n",
      "        \"Some tasks involved in feature pipelines include data cleaning, deduplication, extraction, aggregations, dimensionality reduction, feature binning, feature crosses, and creating embeddings.\"\n",
      "    ]\n",
      "}\n",
      "{\n",
      "  \"text\": \"Feature types for machine learning can be computed on-demand as part of the model training or batch inference pipeline, or they can be stored in a feature store for reuse across different models for both training and inference. Feature stores are typically used for online models that require features with historical or contextual information. Model training pipelines involve various stages such as data preprocessing, transformations, and iterative processes to improve model performance. Model-centric iterations involve trying different supervised ML learning algorithms, adjusting hyperparameters, and evaluating the model on a validation set. Data-centric iterations involve feature selection, adding/removing training data, removing poor quality samples, improving the quality of existing samples, and ranking the importance of training samples. Inference pipelines include batch inference pipelines and online inference pipelines. Serverless ML allows writing feature, training, and inference pipelines in Python and orchestrating their execution using serverless compute platforms. Features and models can be stored in a serverless feature/model store, and serverless user interfaces written in Python can be run on serverless infrastructure. There are various serverless compute platforms, feature stores, model registries, and serving platforms available. An example of a dataset used for serverless ML is the Iris Flower dataset, which includes input features such as sepal length, sepal width, petal length, and petal width, along with the target label of the Iris Flower Type. Modal, Hopsworks, and Hugging Face are used in the context of serverless ML for the Iris dataset.\",\n",
      "  \"questions\": [\n",
      "    \"What are the different stages involved in model training pipelines?\",\n",
      "    \"How can model performance be improved through model-centric iterations?\",\n",
      "    \"What is the significance of serverless compute platforms in serverless ML?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The different stages involved in model training pipelines include data preprocessing, transformations, and iterative processes.\",\n",
      "    \"Model performance can be improved through model-centric iterations by trying different supervised ML learning algorithms, adjusting hyperparameters, and evaluating the model on a validation set.\",\n",
      "    \"Serverless compute platforms are significant in serverless ML as they allow writing and executing feature, training, and inference pipelines, storing features and models, and running serverless user interfaces.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"text\": \"This code snippet shows an example of a machine learning pipeline for training and deploying a model to predict the type of Iris flower based on its sepal and petal measurements. The pipeline involves creating a feature view from a feature group containing the Iris flower dataset, splitting the data into training and testing sets, training a K-nearest neighbors classifier model, evaluating the model's performance using classification metrics and a confusion matrix, and saving the trained model. The code also includes an interactive inference pipeline for making predictions using the trained model. The predicted flower type is used to generate a URL pointing to an image of the predicted flower.\\n\",\n",
      "  \"questions\": [\n",
      "    \"What is the purpose of the machine learning pipeline?\",\n",
      "    \"How is the model's performance evaluated?\",\n",
      "    \"What is the purpose of the interactive inference pipeline?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The purpose of the machine learning pipeline is to train and deploy a model that can predict the type of Iris flower based on its sepal and petal measurements.\",\n",
      "    \"The model's performance is evaluated using classification metrics such as accuracy and a confusion matrix.\",\n",
      "    \"The purpose of the interactive inference pipeline is to allow users to input sepal and petal measurements and get predictions on the type of Iris flower.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"text\": \"This course has a system-based focus. The objective of this course is to learn the theory of machine learning and deep learning, as well as the practical aspects of building machine learning and deep learning algorithms using data parallel programming platforms like TensorFlow. The course covers topics such as TensorFlow, Deep Neural Networks (DNN), different DNN architectures (CNNs, RNNs, Autoencoders, GAN), distributed learning, serverless machine learning, MLOps (machine learning operations), feature stores for machine learning, and distributed training of DNNs. The intended learning outcomes of the course include the ability to explain the principles of ML/DL algorithms, design DNN architectures, scale machine learning systems, and implement scalable ML/DL systems. The course assessment consists of an exam, lab assignments, and a final project. The final grade is determined based on the weighted average of the exam and labs, with a bonus for excellent projects. Assignments can be submitted through the Canvas site. The course material includes books such as 'Hands-on machine learning with Scikit-Learn and TensorFlow' and 'Deep learning'. The course web page is available at https://id2223kth.github.io.\",\n",
      "  \"questions\": [\n",
      "    \"What is the objective of this course?\",\n",
      "    \"What are some of the topics covered in the course?\",\n",
      "    \"How is the final grade calculated?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The objective of this course is to learn the theory of machine learning and deep learning, as well as the practical aspects of building machine learning and deep learning algorithms using data parallel programming platforms like TensorFlow.\",\n",
      "    \"Some of the topics covered in the course include TensorFlow, Deep Neural Networks (DNN), different DNN architectures (CNNs, RNNs, Autoencoders, GAN), distributed learning, serverless machine learning, MLOps (machine learning operations), feature stores for machine learning, and distributed training of DNNs.\",\n",
      "    \"The final grade is calculated based on the weighted average of the exam and labs, with a bonus for excellent projects. Assignments can be submitted through the Canvas site.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"text\": \"Deep learning is capable of training models with millions of features, making it suitable for image classification where each pixel is treated as a feature. In supervised machine learning, a model is trained using labeled data to find a decision boundary and then used to make predictions for new unlabeled data. Feature engineering involves extracting relevant features from raw data, such as aggregating hotel room bookings to determine weekly vacancy level or using one hot encoding to represent a user's home country. Machine learning systems can be deployed using MLOps best practices for scalability. Learning algorithms improve their performance at specific tasks with experience and can be applied to various domains, such as spam filtering or predicting house prices. There are different types of machine learning algorithms, including supervised learning where input data is labeled, and unsupervised learning where input data is unlabeled. Deep learning, a subset of machine learning, is particularly useful for tasks where the optimal features are difficult to determine.\",\n",
      "  \"questions\": [\n",
      "    \"How does deep learning handle features in image classification?\",\n",
      "    \"What is the purpose of feature engineering in machine learning?\",\n",
      "    \"What are the main types of machine learning algorithms?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Deep learning can treat each pixel as a feature in image classification, allowing it to train models with millions of features.\",\n",
      "    \"Feature engineering involves extracting relevant features from raw data to improve the performance of machine learning models.\",\n",
      "    \"The main types of machine learning algorithms are supervised learning, where input data is labeled, and unsupervised learning, where input data is unlabeled.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"text\": \"Deep learning has become computationally intensive and time-consuming due to the massive amount of training data and large number of parameters. The increase in computing power, better training algorithms, and huge quantities of data are some of the reasons why deep learning works now. Linear algebra is an important tool for understanding deep learning, with vectors and matrices being fundamental components. Vectors are arrays of numbers denoted by bold lowercase letters, while matrices are 2-D arrays of numbers denoted by bold uppercase letters. Matrix addition, subtraction, and multiplication have specific rules, and the transpose of a matrix involves swapping its rows and columns. The inverse of a matrix exists if it is square, and the Lp norm measures the size of vectors using a norm function. Probability is also integral to deep learning, with random variables and probability distributions playing a key role. Discrete random variables have probability mass functions (PMFs), which describe the probability distribution of the variable. Independence between random variables can be determined by whether their probability distributions can be expressed as products. Conditional probability looks at the probability of an event given that another event has occurred.\",\n",
      "  \"questions\": [\n",
      "    \"What are some reasons why deep learning has become computationally intensive and time-consuming?\",\n",
      "    \"What is the importance of linear algebra in understanding deep learning?\",\n",
      "    \"How is independence between random variables determined?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Deep learning has become computationally intensive and time-consuming due to the massive amount of training data and large number of parameters.\",\n",
      "    \"Linear algebra is important in understanding deep learning as vectors and matrices are fundamental components.\",\n",
      "    \"Independence between random variables can be determined by whether their probability distributions can be expressed as products.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"text\": \"Probability and likelihood are concepts used in statistics to measure the likelihood of events occurring or to estimate the parameters of a probability distribution. Probability refers to the occurrence of future events, while likelihood refers to past events with known outcomes. Maximum likelihood estimation is a method used to estimate the most likely value of a parameter given a set of observed data. The log-likelihood is often used to avoid numerical underflow when calculating the likelihood, and the negative log-likelihood is used as a measure of error or difference between two probability distributions.\",\n",
      "  \"questions\": [\n",
      "    \"What is the difference between probability and likelihood?\",\n",
      "    \"What is maximum likelihood estimation?\",\n",
      "    \"How is the negative log-likelihood used?\",\n",
      "    \"What does the log-likelihood help avoid?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Probability refers to the occurrence of future events, while likelihood refers to past events with known outcomes.\",\n",
      "    \"Maximum likelihood estimation is a method used to estimate the most likely value of a parameter given a set of observed data.\",\n",
      "    \"The negative log-likelihood is used as a measure of error or difference between two probability distributions.\",\n",
      "    \"The log-likelihood helps avoid numerical underflow when calculating the likelihood.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"text\": \"Entropy is a concept used to measure the uncertainty or disorder in a set of data. In this example, we have six tosses of a coin, resulting in the sequence: h, t, t, t, h, t. The true distribution of the coin tosses is given as p(h) = 2/6 and p(t) = 4/6. The predicted distribution, denoted as q, assumes that the outcome of a coin toss could be heads (h) with probability θ and tails (t) with probability (1-θ). The cross entropy between the true distribution and predicted distribution is calculated using the formula: H(p,q) = -p(h)log(q(h)) - p(t)log(q(t)). Finding the likelihood involves multiplying the probabilities of the observed outcomes, which in this case is θ^2(1-θ)^4. The negative log likelihood is then determined by taking the negative logarithm of the likelihood. The given references for further reading are Ian Goodfellow et al.'s book 'Deep Learning' and Aurelien Geron's book 'Hands-On Machine Learning with Scikit-Learn and TensorFlow'.\",\n",
      "  \"questions\": [\n",
      "    \"What is entropy and how is it used to measure uncertainty in data?\",\n",
      "    \"What is the true distribution of the coin tosses in this example?\",\n",
      "    \"What is the negative log likelihood and how is it calculated?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Entropy is a measure of uncertainty or disorder in a set of data. It is used to quantify how unpredictable the data is.\",\n",
      "    \"The true distribution of the coin tosses in this example is p(h) = 2/6 and p(t) = 4/6.\",\n",
      "    \"The negative log likelihood is a measure used in statistical modeling. It is calculated by taking the negative logarithm of the likelihood function, which is the probability of observing the given data.\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "from math import ceil\n",
    "\n",
    "api_key = \"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\"\n",
    "client = OpenAI(api_key=api_key)\n",
    "for t in transcriptions:\n",
    "    context = t\n",
    "    i = 0\n",
    "    chunks = []\n",
    "    for i in range(ceil(len(t)/4097)):\n",
    "        chunks.append(t[i*4097:i*4097+4097])\n",
    "    for c in chunks:\n",
    "        context = c\n",
    "        question = \"The text above is the result of the transcription of slides in the PDF file format. Remove chapter names and slides numbers and rephrase the sentences. Once you do that generate 2 to 3 meaningful questions on the text and the respective answers. Plese reply in the JSON format {'text':<full elaborated text>,'questions':<questions generated>,'answers':<answers generated>}. DO NOT write anything else than the requested JSON and remember to write the full elaborated content and not just one part.\"\n",
    "        # response = openai.Completion.create(\n",
    "        # engine=\"gpt-3.5-turbo\",\n",
    "        prompt=f\"\\nContext: {context}\\nQuestion: {question}\"\n",
    "        # )\n",
    "    # answer = response.choices[0].text.strip()\n",
    "    # print(answer)\n",
    "        response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt },\n",
    "            \n",
    "        ]\n",
    "\n",
    "        )\n",
    "        print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.6 64-bit",
   "name": "python3116jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}