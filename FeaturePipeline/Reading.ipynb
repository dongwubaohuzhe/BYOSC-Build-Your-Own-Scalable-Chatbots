{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "- https://pythonhosted.org/PyDrive/quickstart.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-12 00:39:47,757 INFO: file_cache is only supported with oauth2client<4.0.0\n"
     ]
    }
   ],
   "source": [
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "\n",
    "\n",
    "# gauth = GoogleAuth()\n",
    "# gauth.CommandLineAuth()\n",
    "\n",
    "# drive = GoogleDrive(gauth)\n",
    "gauth = GoogleAuth()\n",
    "# Try to load saved client credentials\n",
    "gauth.LoadCredentialsFile(\"credentials.json\")\n",
    "if gauth.credentials is None:\n",
    "    # Authenticate if they're not there\n",
    "    gauth.LocalWebserverAuth()\n",
    "elif gauth.access_token_expired:\n",
    "    # Refresh them if expired\n",
    "    gauth.Refresh()\n",
    "else:\n",
    "    # Initialize the saved creds\n",
    "    gauth.Authorize()\n",
    "# Save the current credentials to a file\n",
    "gauth.SaveCredentialsFile(\"credentials.json\")\n",
    "\n",
    "drive = GoogleDrive(gauth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file from GDrive in the folder \"slides\" (with id folder_id)\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "folder_id = os.environ[\"FOLDER_ID\"]\n",
    "\n",
    "file_list = drive.ListFile({'q': f\"'{folder_id}' in parents and trashed=false\"}).GetList() \n",
    "new_files = []\n",
    "for file in file_list:\n",
    "    if not os.path.isfile(f\"{file['title']}\"): \n",
    "        # print(\"title: %s, id: %s\" % (file[\"title\"],file[\"id\"]))\n",
    "        file.GetContentFile(file[\"title\"])\n",
    "        new_files.append(file)\n",
    "\n",
    "# new_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DeepLearning-chapter-10.pdf': {'text': ['C h a p t e r 1 0\\nS e qu e n ce Mo d e l i n g: Recurren t\\nan d Recursiv e N e t s\\nRecurrentneuralnetworksorRNNs( ,)areafamilyof Rumelhart e t a l .1986a\\nneuralnetworksforprocessingsequentialdata.Muchasaconvolutionalnetwork\\nisaneuralnetworkthatisspecializedforprocessingagridofvalues Xsuchas\\nanimage,arecurrentneuralnetworkisaneuralnetworkthatisspecializedfor\\nprocessingasequenceofvaluesx( 1 ), . . . ,x( ) τ.Justasconvolutionalnetworks\\ncanreadilyscaletoimageswithlargewidthandheight,andsomeconvolutional\\nnetworkscanprocessimagesofvariablesize,recurrentnetworkscanscaletomuch\\nlongersequencesthanwouldbepracticalfornetworkswithoutsequence-based\\nspecialization.Mostrecurrentnetworkscanalsoprocesssequencesofvariable\\nlength.\\nTogofrommulti-layernetworkstorecurrentnetworks,weneedtotakeadvan-\\ntageofoneoftheearlyideasfoundinmachinelearningandstatisticalmodelsof\\nthe1980s:sharingparametersacrossdiﬀerentpartsofamodel.Parametersharing\\nmakesitpossibletoextendandapplythemodeltoexamplesofdiﬀerentforms\\n(diﬀerentlengths,here)andgeneralizeacrossthem.Ifwehadseparateparameters\\nforeachvalueofthetimeindex,wecouldnotgeneralizetosequencelengthsnot\\nseenduringtraining,norsharestatisticalstrengthacrossdiﬀerentsequencelengths\\nandacrossdiﬀerentpositionsintime.Suchsharingisparticularlyimportantwhen\\naspeciﬁcpieceofinformationcanoccuratmultiplepositionswithinthesequence.\\nForexample,considerthetwosentences“IwenttoNepalin2009”and“In2009,\\nIwenttoNepal.”Ifweaskamachinelearningmodeltoreadeachsentenceand\\nextracttheyearinwhichthenarratorwenttoNepal,wewouldlikeittorecognize\\ntheyear2009astherelevantpieceofinformation,whetheritappearsinthesixth\\n373', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nwordorthesecondwordofthesentence.Supposethatwetrainedafeedforward\\nnetworkthatprocessessentencesofﬁxedlength.Atraditionalfullyconnected\\nfeedforwardnetworkwouldhaveseparateparametersforeachinputfeature,soit\\nwouldneedtolearnalloftherulesofthelanguageseparatelyateachpositionin\\nthesentence.Bycomparison,arecurrentneuralnetworksharesthesameweights\\nacrossseveraltimesteps.\\nArelatedideaistheuseofconvolutionacrossa1-Dtemporalsequence.This\\nconvolutionalapproachisthebasisfortime-delayneuralnetworks(Langand\\nHinton1988Waibel1989Lang1990 ,; e t a l .,; e t a l .,).Theconvolutionoperation\\nallowsanetworktoshareparametersacrosstime,butisshallow.Theoutput\\nofconvolutionisasequencewhereeachmemberoftheoutputisafunctionof\\nasmallnumberofneighboringmembersoftheinput.Theideaofparameter\\nsharingmanifestsintheapplicationofthesameconvolutionkernelateachtime\\nstep.Recurrentnetworksshareparametersinadiﬀerentway.Eachmemberofthe\\noutputisafunctionofthepreviousmembersoftheoutput.Eachmemberofthe\\noutputisproducedusingthesameupdateruleappliedtothepreviousoutputs.\\nThisrecurrentformulationresultsinthesharingofparametersthroughavery\\ndeepcomputational graph.\\nForthesimplicityofexposition,werefertoRNNsasoperatingonasequence\\nthatcontainsvectorsx( ) twiththetimestepindex trangingfromto1 τ.In\\npractice,recurrentnetworksusuallyoperateonminibatchesofsuchsequences,\\nwithadiﬀerentsequencelength τforeachmemberoftheminibatch.Wehave\\nomittedtheminibatchindicestosimplifynotation.Moreover,thetimestepindex\\nneednotliterallyrefertothepassageoftimeintherealworld.Sometimesitrefers\\nonlytothepositioninthesequence.RNNsmayalsobeappliedintwodimensions\\nacrossspatialdatasuchasimages,andevenwhenappliedtodatainvolvingtime,\\nthenetworkmayhaveconnectionsthatgobackwardsintime,providedthatthe\\nentiresequenceisobservedbeforeitisprovidedtothenetwork.\\nThischapterextendstheideaofacomputational graphtoincludecycles.These\\ncyclesrepresenttheinﬂuenceofthepresentvalueofavariableonitsownvalue\\natafuturetimestep.Suchcomputational graphsallowustodeﬁnerecurrent\\nneuralnetworks.Wethendescribemanydiﬀerentwaystoconstruct,train,and\\nuserecurrentneuralnetworks.\\nFormoreinformationonrecurrentneuralnetworksthanisavailableinthis\\nchapter,wereferthereadertothetextbookofGraves2012().\\n3 7 4', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\n10.1UnfoldingComputationalGraphs\\nAcomputational graphisawaytoformalizethestructureofasetofcomputations,\\nsuchasthoseinvolvedinmappinginputsandparameterstooutputsandloss.\\nPleaserefertosectionforageneralintroduction. Inthissectionweexplain 6.5.1\\ntheideaofunfoldingarecursiveorrecurrentcomputationintoacomputational\\ngraphthathasarepetitivestructure,typicallycorrespondingtoachainofevents.\\nUnfoldingthisgraphresultsinthesharingofparametersacrossadeepnetwork\\nstructure.\\nForexample,considertheclassicalformofadynamicalsystem:\\ns( ) t= ( fs( 1 ) t −;)θ , (10.1)\\nwheres( ) tiscalledthestateofthesystem.\\nEquationisrecurrentbecausethedeﬁnitionof 10.1 sattime trefersbackto\\nthesamedeﬁnitionattime. t−1\\nForaﬁnitenumberoftimesteps τ,thegraphcanbeunfoldedbyapplying\\nthedeﬁnition τ−1times.Forexample,ifweunfoldequationfor10.1 τ= 3time\\nsteps,weobtain\\ns( 3 )=( fs( 2 );)θ (10.2)\\n=(( f fs( 1 ););)θθ (10.3)\\nUnfoldingtheequationbyrepeatedlyapplyingthedeﬁnitioninthiswayhas\\nyieldedanexpressionthatdoesnotinvolverecurrence.Suchanexpressioncan\\nnowberepresentedbyatraditionaldirectedacycliccomputational graph.\\xa0The\\nunfoldedcomputational graphofequationandequationisillustratedin 10.1 10.3\\nﬁgure.10.1\\ns( t − 1 )s( t − 1 )s( ) ts( ) ts( + 1 ) ts( + 1 ) t\\nf fs( ) . . .s( ) . . .s( ) . . .s( ) . . .\\nf f f f f f\\nFigure10.1:Theclassicaldynamicalsystemdescribedbyequation,illustratedasan 10.1\\nunfoldedcomputationalgraph.\\xa0Eachnoderepresentsthestateatsometime tandthe\\nfunction fmapsthestateat ttothestateat t+1.Thesameparameters(thesamevalue\\nofusedtoparametrize)areusedforalltimesteps. θ f\\nAsanotherexample,letusconsideradynamicalsystemdrivenbyanexternal\\nsignalx( ) t,\\ns( ) t= ( fs( 1 ) t −,x( ) t;)θ , (10.4)\\n3 7 5', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nwhereweseethatthestatenowcontainsinformationaboutthewholepastsequence.\\nRecurrentneuralnetworkscanbebuiltinmanydiﬀerentways.Muchas\\nalmostanyfunctioncanbeconsideredafeedforwardneuralnetwork,essentially\\nanyfunctioninvolvingrecurrencecanbeconsideredarecurrentneuralnetwork.\\nManyrecurrentneuralnetworksuseequationorasimilarequationto 10.5\\ndeﬁnethevaluesoftheirhiddenunits.\\xa0Toindicatethatthestateisthehidden\\nunitsofthenetwork,wenowrewriteequationusingthevariable 10.4 htorepresent\\nthestate:\\nh( ) t= ( fh( 1 ) t −,x( ) t;)θ , (10.5)\\nillustratedinﬁgure,typicalRNNswilladdextraarchitecturalfeaturessuch 10.2\\nasoutputlayersthatreadinformationoutofthestatetomakepredictions.h\\nWhentherecurrentnetworkistrainedtoperformataskthatrequirespredicting\\nthefuturefromthepast,thenetworktypicallylearnstouseh( ) tasakindoflossy\\nsummaryofthetask-relevantaspectsofthepastsequenceofinputsupto t.This\\nsummaryisingeneralnecessarilylossy,sinceitmapsanarbitrarylengthsequence\\n(x( ) t,x( 1 ) t −,x( 2 ) t −, . . . ,x( 2 ),x( 1 ))toaﬁxedlengthvectorh( ) t.Dependingonthe\\ntrainingcriterion,thissummarymightselectivelykeepsomeaspectsofthepast\\nsequencewithmoreprecisionthanotheraspects.Forexample,iftheRNNisused\\ninstatisticallanguagemodeling,typicallytopredictthenextwordgivenprevious\\nwords,itmaynotbenecessarytostorealloftheinformationintheinputsequence\\nuptotime t,butratheronlyenoughinformationtopredicttherestofthesentence.\\nThemostdemandingsituationiswhenweaskh( ) ttoberichenoughtoallow\\nonetoapproximately recovertheinputsequence,asinautoencoderframeworks\\n(chapter).14\\nf fhh\\nx xh( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) t\\nx( t − 1 )x( t − 1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) th( ) . . .h( ) . . .h( ) . . .h( ) . . .\\nf f\\nU nf ol df f f f f\\nFigure10.2:Arecurrentnetworkwithnooutputs.Thisrecurrentnetworkjustprocesses\\ninformationfromtheinputxbyincorporatingitintothestatehthatispassedforward\\nthroughtime. ( L e f t )Circuitdiagram.Theblacksquareindicatesadelayofasingletime\\nstep.Thesamenetworkseenasanunfoldedcomputationalgraph,whereeach ( R i g h t )\\nnodeisnowassociatedwithoneparticulartimeinstance.\\nEquationcanbedrawnintwodiﬀerentways.OnewaytodrawtheRNN 10.5\\niswithadiagramcontainingonenodeforeverycomponentthatmightexistina\\n3 7 6', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nphysicalimplementationofthemodel,suchasabiologicalneuralnetwork.Inthis\\nview,thenetworkdeﬁnesacircuitthatoperatesinrealtime,withphysicalparts\\nwhosecurrentstatecaninﬂuencetheirfuturestate,asintheleftofﬁgure.10.2\\nThroughoutthischapter,weuseablacksquareinacircuitdiagramtoindicate\\nthataninteractiontakesplacewithadelayofasingletimestep,fromthestate\\nattime ttothestateattime t+1.TheotherwaytodrawtheRNNisasan\\nunfoldedcomputational graph,inwhicheachcomponentisrepresentedbymany\\ndiﬀerentvariables,withonevariablepertimestep,representingthestateofthe\\ncomponentatthatpointintime.Eachvariableforeachtimestepisdrawnasa\\nseparatenodeofthecomputational graph,asintherightofﬁgure.Whatwe10.2\\ncallunfoldingistheoperationthatmapsacircuitasintheleftsideoftheﬁgure\\ntoacomputational graphwithrepeatedpiecesasintherightside.Theunfolded\\ngraphnowhasasizethatdependsonthesequencelength.\\nWecanrepresenttheunfoldedrecurrenceafterstepswithafunction t g( ) t:\\nh( ) t= g( ) t(x( ) t,x( 1 ) t −,x( 2 ) t −, . . . ,x( 2 ),x( 1 )) (10.6)\\n=( fh( 1 ) t −,x( ) t;)θ (10.7)\\nThefunction g( ) ttakesthewholepastsequence (x( ) t,x( 1 ) t −,x( 2 ) t −, . . . ,x( 2 ),x( 1 ))\\nasinputandproducesthecurrentstate,buttheunfoldedrecurrentstructure\\nallowsustofactorize g( ) tintorepeatedapplicationofafunction f.Theunfolding\\nprocessthusintroducestwomajoradvantages:\\n1.Regardlessofthesequencelength,thelearnedmodelalwayshasthesame\\ninputsize,becauseitisspeciﬁedintermsoftransitionfromonestateto\\nanotherstate,ratherthanspeciﬁedintermsofavariable-length historyof\\nstates.\\n2.Itispossibletousethetransitionfunction s a m e fwiththesameparameters\\nateverytimestep.\\nThesetwofactorsmakeitpossibletolearnasinglemodel fthatoperateson\\nalltimestepsandallsequencelengths,ratherthanneedingtolearnaseparate\\nmodel g( ) tforallpossibletimesteps.Learningasingle,sharedmodelallows\\ngeneralization tosequencelengthsthatdidnotappearinthetrainingset,and\\nallowsthemodeltobeestimatedwithfarfewertrainingexamplesthanwouldbe\\nrequiredwithoutparametersharing.\\nBoththerecurrentgraphandtheunrolledgraphhavetheiruses.Therecurrent\\ngraphissuccinct.Theunfoldedgraphprovidesanexplicitdescriptionofwhich\\ncomputations toperform.Theunfoldedgraphalsohelpstoillustratetheideaof\\n3 7 7', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\ninformationﬂowforwardintime(computingoutputsandlosses)andbackward\\nintime(computinggradients)byexplicitlyshowingthepathalongwhichthis\\ninformationﬂows.\\n10.2RecurrentNeuralNetworks\\nArmedwiththegraphunrollingandparametersharingideasofsection,we10.1\\ncandesignawidevarietyofrecurrentneuralnetworks.\\nUUV V\\nWWo( t − 1 )o( t − 1 )\\nhhooy y\\nLL\\nx xo( ) to( ) to( + 1 ) to( + 1 ) tL( t − 1 )L( t − 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t − 1 )y( t − 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t\\nh( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) t\\nx( t − 1 )x( t − 1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) tWW WW WW WW\\nh( ) . . .h( ) . . .h( ) . . .h( ) . . .V V V V V V\\nUU UU UUU nf ol d\\nFigure10.3:Thecomputationalgraphtocomputethetraininglossofarecurrentnetwork\\nthatmapsaninputsequenceofxvaluestoacorrespondingsequenceofoutputovalues.\\nAloss Lmeasureshowfareachoisfromthecorrespondingtrainingtargety.Whenusing\\nsoftmaxoutputs,weassumeoistheunnormalizedlogprobabilities.Theloss Linternally\\ncomputesˆy=softmax(o) andcomparesthistothetargety.TheRNNhasinputtohidden\\nconnectionsparametrizedbyaweightmatrixU,hidden-to-hiddenrecurrentconnections\\nparametrizedbyaweightmatrixW,andhidden-to-outputconnectionsparametrizedby\\naweightmatrixV.Equationdeﬁnesforwardpropagationinthismodel. 10.8 ( L e f t )The\\nRNNanditslossdrawnwithrecurrentconnections. ( R i g h t )Thesameseenasantime-\\nunfoldedcomputationalgraph,whereeachnodeisnowassociatedwithoneparticular\\ntimeinstance.\\nSomeexamplesofimportantdesignpatternsforrecurrentneuralnetworks\\nincludethefollowing:\\n3 7 8', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\n•Recurrentnetworksthatproduceanoutputateachtimestepandhave\\nrecurrentconnectionsbetweenhiddenunits,illustratedinﬁgure.10.3\\n•Recurrentnetworksthatproduceanoutputateachtimestepandhave\\nrecurrentconnectionsonlyfromtheoutputatonetimesteptothehidden\\nunitsatthenexttimestep,illustratedinﬁgure10.4\\n•Recurrentnetworkswithrecurrentconnectionsbetweenhiddenunits,that\\nreadanentiresequenceandthenproduceasingleoutput,illustratedin\\nﬁgure.10.5\\nﬁgureisareasonablyrepresentativeexamplethatwereturntothroughout 10.3\\nmostofthechapter.\\nTherecurrentneuralnetworkofﬁgureandequationisuniversalinthe 10.3 10.8\\nsensethatanyfunctioncomputablebyaTuringmachinecanbecomputedbysuch\\narecurrentnetworkofaﬁnitesize.TheoutputcanbereadfromtheRNNafter\\nanumberoftimestepsthatisasymptoticallylinearinthenumberoftimesteps\\nusedbytheTuringmachineandasymptoticallylinearinthelengthoftheinput\\n(SiegelmannandSontag1991Siegelmann1995SiegelmannandSontag1995 ,;,; ,;\\nHyotyniemi1996,).ThefunctionscomputablebyaTuringmachinearediscrete,\\nsotheseresultsregardexactimplementation ofthefunction,notapproximations .\\nTheRNN,whenusedasaTuringmachine,takesabinarysequenceasinputandits\\noutputsmustbediscretizedtoprovideabinaryoutput.Itispossibletocomputeall\\nfunctionsinthissettingusingasinglespeciﬁcRNNofﬁnitesize(Siegelmannand\\nSontag1995()use886units).The“input”oftheTuringmachineisaspeciﬁcation\\nofthefunctiontobecomputed,sothesamenetworkthatsimulatesthisTuring\\nmachineissuﬃcientforallproblems.ThetheoreticalRNNusedfortheproof\\ncansimulateanunboundedstackbyrepresentingitsactivationsandweightswith\\nrationalnumbersofunboundedprecision.\\nWenowdeveloptheforwardpropagationequationsfortheRNNdepictedin\\nﬁgure.Theﬁguredoesnotspecifythechoiceofactivationfunctionforthe 10.3\\nhiddenunits.Hereweassumethehyperbolictangentactivationfunction.Also,\\ntheﬁguredoesnotspecifyexactlywhatformtheoutputandlossfunctiontake.\\nHereweassumethattheoutputisdiscrete,asiftheRNNisusedtopredictwords\\norcharacters.Anaturalwaytorepresentdiscretevariablesistoregardtheoutput\\noasgivingtheunnormalized logprobabilitiesofeachpossiblevalueofthediscrete\\nvariable.Wecanthenapplythesoftmaxoperationasapost-processingstepto\\nobtainavectorˆyofnormalizedprobabilitiesovertheoutput.Forwardpropagation\\nbeginswithaspeciﬁcationoftheinitialstateh( 0 ).Then,foreachtimestepfrom\\n3 7 9', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nUV\\nWo( t − 1 )o( t − 1 )\\nhhooy y\\nLL\\nx xo( ) to( ) to( + 1 ) to( + 1 ) tL( t − 1 )L( t − 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t − 1 )y( t − 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t\\nh( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) t\\nx( t − 1 )x( t − 1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) tW W W Wo( ) . . .o( ) . . .\\nh( ) . . .h( ) . . .V V V\\nU U UU nf ol d\\nFigure10.4:AnRNNwhoseonlyrecurrenceisthefeedbackconnectionfromtheoutput\\ntothehiddenlayer.Ateachtimestep t,theinputisxt,thehiddenlayeractivationsare\\nh( ) t,theoutputsareo( ) t,thetargetsarey( ) tandthelossis L( ) t. ( L e f t )Circuitdiagram.\\n( R i g h t )Unfoldedcomputationalgraph.SuchanRNNislesspowerful(canexpressa\\nsmallersetoffunctions)thanthoseinthefamilyrepresentedbyﬁgure.TheRNN 10.3\\ninﬁgurecanchoosetoputanyinformationitwantsaboutthepastintoitshidden 10.3\\nrepresentationhandtransmithtothefuture.TheRNNinthisﬁgureistrainedto\\nputaspeciﬁcoutputvalueintoo,andoistheonlyinformationitisallowedtosend\\ntothefuture.Therearenodirectconnectionsfromhgoingforward.Theprevioush\\nisconnectedtothepresentonlyindirectly,viathepredictionsitwasusedtoproduce.\\nUnlessoisveryhigh-dimensionalandrich,itwillusuallylackimportantinformation\\nfromthepast.ThismakestheRNNinthisﬁgurelesspowerful,butitmaybeeasierto\\ntrainbecauseeachtimestepcanbetrainedinisolationfromtheothers,allowinggreater\\nparallelizationduringtraining,asdescribedinsection.10.2.1\\n3 8 0', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nt t τ = 1to= ,weapplythefollowingupdateequations:\\na( ) t= +bWh( 1 ) t −+Ux( ) t(10.8)\\nh( ) t=tanh(a( ) t) (10.9)\\no( ) t= +cVh( ) t(10.10)\\nˆy( ) t=softmax(o( ) t) (10.11)\\nwheretheparametersarethebiasvectorsbandcalongwiththeweightmatrices\\nU,VandW,respectivelyforinput-to-hidden, hidden-to-output andhidden-to-\\nhiddenconnections.Thisisanexampleofarecurrentnetworkthatmapsan\\ninputsequencetoanoutputsequenceofthesamelength.Thetotallossfora\\ngivensequenceofvaluespairedwithasequenceofvalueswouldthenbejust x y\\nthesumofthelossesoverallthetimesteps.Forexample,if L( ) tisthenegative\\nlog-likelihoodof y( ) tgivenx( 1 ), . . . ,x( ) t,then\\nL\\ue010\\n{x( 1 ), . . . ,x( ) τ}{ ,y( 1 ), . . . ,y( ) τ}\\ue011\\n(10.12)\\n=\\ue058\\ntL( ) t(10.13)\\n=−\\ue058\\ntlog p m o de l\\ue010\\ny( ) t|{x( 1 ), . . . ,x( ) t}\\ue011\\n, (10.14)\\nwhere p m o de l\\ue000\\ny( ) t|{x( 1 ), . . . ,x( ) t}\\ue001\\nisgivenbyreadingtheentryfor y( ) tfromthe\\nmodel’soutputvectorˆy( ) t.Computingthegradientofthislossfunctionwithrespect\\ntotheparametersisanexpensiveoperation.Thegradientcomputationinvolves\\nperformingaforwardpropagationpassmovinglefttorightthroughourillustration\\noftheunrolledgraphinﬁgure,followedbyabackwardpropagationpass 10.3\\nmovingrighttoleftthroughthegraph.Theruntimeis O( τ) andcannotbereduced\\nbyparallelization becausetheforwardpropagationgraphisinherentlysequential;\\neachtimestepmayonlybecomputedafterthepreviousone.\\xa0Statescomputed\\nintheforwardpassmustbestoreduntiltheyarereusedduringthebackward\\npass,sothememorycostisalso O( τ).Theback-propagation algorithmapplied\\ntotheunrolledgraphwith O( τ)costiscalledback-propagationthroughtime\\norBPTTandisdiscussedfurtherinsection.Thenetworkwithrecurrence 10.2.2\\nbetweenhiddenunitsisthusverypowerfulbutalsoexpensivetotrain.Istherean\\nalternative?\\n10.2.1TeacherForcingandNetworkswithOutputRecurrence\\nThenetworkwithrecurrentconnectionsonlyfromtheoutputatonetimestepto\\nthehiddenunitsatthenexttimestep(showninﬁgure)isstrictlylesspowerful 10.4\\n3 8 1', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nbecauseitlackshidden-to-hidden recurrentconnections.Forexample,itcannot\\nsimulateauniversalTuringmachine.Becausethisnetworklackshidden-to-hidden\\nrecurrence,itrequiresthattheoutputunitscapturealloftheinformationabout\\nthepastthatthenetworkwillusetopredictthefuture.Becausetheoutputunits\\nareexplicitlytrainedtomatchthetrainingsettargets,theyareunlikelytocapture\\nthenecessaryinformationaboutthepasthistoryoftheinput,unlesstheuser\\nknowshowtodescribethefullstateofthesystemandprovidesitaspartofthe\\ntrainingsettargets.Theadvantageofeliminatinghidden-to-hidden recurrence\\nisthat,foranylossfunctionbasedoncomparingthepredictionattime ttothe\\ntrainingtargetattime t,allthetimestepsaredecoupled.Trainingcanthusbe\\nparallelized,withthegradientforeachstep tcomputedinisolation.Thereisno\\nneedtocomputetheoutputfortheprevioustimestepﬁrst,becausethetraining\\nsetprovidestheidealvalueofthatoutput.\\nh( t − 1 )h( t − 1 )\\nWh( ) th( ) t . . . . . .\\nx( t − 1 )x( t − 1 )x( ) tx( ) tx( ) . . .x( ) . . .W W\\nU U Uh( ) τh( ) τ\\nx( ) τx( ) τW\\nUo( ) τo( ) τy( ) τy( ) τL( ) τL( ) τ\\nV\\n. . . . . .\\nFigure10.5:Time-unfoldedrecurrentneuralnetworkwithasingleoutputattheend\\nofthesequence.Suchanetworkcanbeusedtosummarizeasequenceandproducea\\nﬁxed-sizerepresentationusedasinputforfurtherprocessing.\\xa0Theremightbeatarget\\nrightattheend(asdepictedhere)orthegradientontheoutputo( ) tcanbeobtainedby\\nback-propagatingfromfurtherdownstreammodules.\\nModelsthathaverecurrentconnectionsfromtheiroutputsleadingbackinto\\nthemodelmaybetrainedwithteacherforcing.Teacherforcingisaprocedure\\nthatemergesfromthemaximumlikelihoodcriterion,inwhichduringtrainingthe\\nmodelreceivesthegroundtruthoutput y( ) tasinputattime t+1.\\xa0Wecansee\\nthisbyexaminingasequencewithtwotimesteps.Theconditionalmaximum\\n3 8 2', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\no( t − 1 )o( t − 1 )o( ) to( ) t\\nh( t − 1 )h( t − 1 )h( ) th( ) t\\nx( t − 1 )x( t − 1 )x( ) tx( ) tW\\nV V\\nU Uo( t − 1 )o( t − 1 )o( ) to( ) tL( t − 1 )L( t − 1 )L( ) tL( ) ty( t − 1 )y( t − 1 )y( ) ty( ) t\\nh( t − 1 )h( t − 1 )h( ) th( ) t\\nx( t − 1 )x( t − 1 )x( ) tx( ) tW\\nV V\\nU U\\nT r ai n\\xa0 t i m e T e s t \\xa0 t i m e\\nFigure10.6:Illustrationofteacherforcing.Teacherforcingisatrainingtechniquethatis\\napplicabletoRNNsthathaveconnectionsfromtheiroutputtotheirhiddenstatesatthe\\nnexttimestep. ( L e f t )Attraintime,wefeedthe c o r r e c t o u t p u ty( ) tdrawnfromthetrain\\nsetasinputtoh( + 1 ) t.Whenthemodelisdeployed,thetrueoutputisgenerally ( R i g h t )\\nnotknown.Inthiscase,weapproximatethecorrectoutputy( ) twiththemodel’soutput\\no( ) t,andfeedtheoutputbackintothemodel.\\n3 8 3', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nlikelihoodcriterionis\\nlog p\\ue010\\ny( 1 ),y( 2 )|x( 1 ),x( 2 )\\ue011\\n(10.15)\\n=log p\\ue010\\ny( 2 )|y( 1 ),x( 1 ),x( 2 )\\ue011\\n+log p\\ue010\\ny( 1 )|x( 1 ),x( 2 )\\ue011\\n(10.16)\\nInthisexample,weseethatattime t= 2,themodelistrainedtomaximizethe\\nconditionalprobabilityofy( 2 )given b o t hthexsequencesofarandthepreviousy\\nvaluefromthetrainingset.Maximumlikelihoodthusspeciﬁesthatduringtraining,\\nratherthanfeedingthemodel’sownoutputbackintoitself,theseconnections\\nshouldbefedwiththetargetvaluesspecifyingwhatthecorrectoutputshouldbe.\\nThisisillustratedinﬁgure.10.6\\nWeoriginallymotivatedteacherforcingasallowingustoavoidback-propagation\\nthroughtimeinmodelsthatlackhidden-to-hidden connections.Teacherforcing\\nmaystillbeappliedtomodelsthathavehidden-to-hidden connectionssolongas\\ntheyhaveconnectionsfromtheoutputatonetimesteptovaluescomputedinthe\\nnexttimestep.However,assoonasthehiddenunitsbecomeafunctionofearlier\\ntimesteps,theBPTTalgorithmisnecessary.Somemodelsmaythusbetrained\\nwithbothteacherforcingandBPTT.\\nThedisadvantageofstrictteacherforcingarisesifthenetworkisgoingtobe\\nlaterusedinanopen-loopmode,withthenetworkoutputs(orsamplesfrom\\ntheoutputdistribution)fedbackasinput.\\xa0Inthiscase,thekindofinputsthat\\nthenetworkseesduringtrainingcouldbequitediﬀerentfromthekindofinputs\\nthatitwillseeattesttime.\\xa0Onewaytomitigatethisproblemistotrainwith\\nbothteacher-forcedinputsandwithfree-runninginputs,forexamplebypredicting\\nthecorrecttargetanumberofstepsinthefuturethroughtheunfoldedrecurrent\\noutput-to-input paths.Inthisway,thenetworkcanlearntotakeintoaccount\\ninputconditions(suchasthoseitgeneratesitselfinthefree-runningmode)not\\nseenduringtrainingandhowtomapthestatebacktowardsonethatwillmake\\nthenetworkgenerateproperoutputsafterafewsteps.Anotherapproach(Bengio\\ne t a l .,)tomitigatethegapbetweentheinputsseenattraintimeandthe 2015b\\ninputsseenattesttimerandomlychoosestousegeneratedvaluesoractualdata\\nvaluesasinput.Thisapproachexploitsacurriculumlearningstrategytogradually\\nusemoreofthegeneratedvaluesasinput.\\n10.2.2ComputingtheGradientinaRecurrentNeuralNetwork\\nComputingthegradientthrougharecurrentneuralnetworkisstraightforward.\\nOnesimplyappliesthegeneralizedback-propagationalgorithmofsection6.5.6\\n3 8 4', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\ntotheunrolledcomputational graph.Nospecializedalgorithmsarenecessary.\\nGradientsobtainedbyback-propagation maythenbeusedwithanygeneral-purpose\\ngradient-basedtechniquestotrainanRNN.\\nTogainsomeintuitionforhowtheBPTTalgorithmbehaves,weprovidean\\nexampleofhowtocomputegradientsbyBPTTfortheRNNequationsabove\\n(equationandequation).Thenodesofourcomputational graphinclude 10.8 10.12\\ntheparametersU,V,W,bandcaswellasthesequenceofnodesindexedby\\ntforx( ) t,h( ) t,o( ) tand L( ) t.\\xa0Foreachnode Nweneedtocomputethegradient\\n∇ N Lrecursively,basedonthegradientcomputedatnodesthatfollowitinthe\\ngraph.Westarttherecursionwiththenodesimmediatelyprecedingtheﬁnalloss\\n∂ L\\n∂ L( ) t= 1 . (10.17)\\nInthisderivationweassumethattheoutputso( ) tareusedastheargumenttothe\\nsoftmaxfunctiontoobtainthevectorˆyofprobabilitiesovertheoutput.Wealso\\nassumethatthelossisthenegativelog-likelihoodofthetruetarget y( ) tgiventhe\\ninputsofar.Thegradient∇o( ) t Lontheoutputsattimestep t,forall i , t,isas\\nfollows:\\n(∇o( ) t L)i=∂ L\\n∂ o( ) t\\ni=∂ L\\n∂ L( ) t∂ L( ) t\\n∂ o( ) t\\ni=ˆ y( ) t\\ni− 1i , y( ) t .(10.18)\\nWeworkourwaybackwards,startingfromtheendofthesequence.Attheﬁnal\\ntimestep, τh( ) τonlyhaso( ) τasadescendent,soitsgradientissimple:\\n∇h( ) τ L= V\\ue03e∇o( ) τ L. (10.19)\\nWecantheniteratebackwardsintimetoback-propagate gradientsthroughtime,\\nfrom t= τ−1downto t= 1,notingthath( ) t(for t < τ)hasasdescendentsboth\\no( ) tandh( + 1 ) t.Itsgradientisthusgivenby\\n∇h( ) t L=\\ue020\\n∂h( + 1 ) t\\n∂h( ) t\\ue021\\ue03e\\n(∇h( +1) t L)+\\ue020\\n∂o( ) t\\n∂h( ) t\\ue021\\ue03e\\n(∇o( ) t L) (10.20)\\n= W\\ue03e(∇h( +1) t L)diag\\ue012\\n1−\\ue010\\nh( + 1 ) t\\ue0112\\ue013\\n+V\\ue03e(∇o( ) t L)(10.21)\\nwhere diag\\ue010\\n1−\\ue000\\nh( + 1 ) t\\ue0012\\ue011\\nindicatesthediagonalmatrixcontainingtheelements\\n1−( h( + 1 ) t\\ni)2.ThisistheJacobianofthehyperbolictangentassociatedwiththe\\nhiddenunitattime. i t+1\\n3 8 5', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nOncethegradientsonthe\\xa0internalnodesofthe\\xa0computational graphare\\nobtained,\\xa0wecanobtainthegradientsontheparameternodes.Becausethe\\nparametersaresharedacrossmanytimesteps,wemusttakesomecarewhen\\ndenotingcalculusoperationsinvolvingthesevariables.Theequationswewishto\\nimplementusethebpropmethodofsection,thatcomputesthecontribution 6.5.6\\nofasingleedgeinthecomputational graphtothegradient.However,the∇ W f\\noperatorusedincalculustakesintoaccountthecontributionofWtothevalue\\nof fduetoedgesinthecomputational graph.Toresolvethisambiguity,we a l l\\nintroducedummyvariablesW( ) tthataredeﬁnedtobecopiesofWbutwitheach\\nW( ) tusedonlyattimestep t.Wemaythenuse∇W( ) ttodenotethecontribution\\noftheweightsattimesteptothegradient. t\\nUsingthisnotation,thegradientontheremainingparametersisgivenby:\\n∇ c L=\\ue058\\nt\\ue020\\n∂o( ) t\\n∂c\\ue021\\ue03e\\n∇o( ) t L=\\ue058\\nt∇o( ) t L (10.22)\\n∇ b L=\\ue058\\nt\\ue020\\n∂h( ) t\\n∂b( ) t\\ue021\\ue03e\\n∇h( ) t L=\\ue058\\ntdiag\\ue012\\n1−\\ue010\\nh( ) t\\ue0112\\ue013\\n∇h( ) t L(10.23)\\n∇ V L=\\ue058\\nt\\ue058\\ni\\ue020\\n∂ L\\n∂ o( ) t\\ni\\ue021\\n∇ V o( ) t\\ni=\\ue058\\nt(∇o( ) t L)h( ) t\\ue03e(10.24)\\n∇ W L=\\ue058\\nt\\ue058\\ni\\ue020\\n∂ L\\n∂ h( ) t\\ni\\ue021\\n∇W( ) t h( ) t\\ni (10.25)\\n=\\ue058\\ntdiag\\ue012\\n1−\\ue010\\nh( ) t\\ue0112\\ue013\\n(∇h( ) t L)h( 1 ) t −\\ue03e(10.26)\\n∇ U L=\\ue058\\nt\\ue058\\ni\\ue020\\n∂ L\\n∂ h( ) t\\ni\\ue021\\n∇U( ) t h( ) t\\ni (10.27)\\n=\\ue058\\ntdiag\\ue012\\n1−\\ue010\\nh( ) t\\ue0112\\ue013\\n(∇h( ) t L)x( ) t\\ue03e(10.28)\\nWedonotneedtocomputethegradientwithrespecttox( ) tfortrainingbecause\\nitdoesnothaveanyparametersasancestorsinthecomputational graphdeﬁning\\ntheloss.\\n3 8 6', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\n10.2.3RecurrentNetworksasDirectedGraphicalModels\\nIntheexamplerecurrentnetworkwehavedevelopedsofar,thelosses L( ) twere\\ncross-entropiesbetweentrainingtargetsy( ) tandoutputso( ) t.Aswithafeedforward\\nnetwork,itisinprinciplepossibletousealmostanylosswitharecurrentnetwork.\\nThelossshouldbechosenbasedonthetask.Aswithafeedforwardnetwork,we\\nusuallywishtointerprettheoutputoftheRNNasaprobabilitydistribution,and\\nweusuallyusethecross-entropyassociatedwiththatdistributiontodeﬁnetheloss.\\nMeansquarederroristhecross-entropylossassociatedwithanoutputdistribution\\nthatisaunitGaussian,forexample,justaswithafeedforwardnetwork.\\nWhen\\xa0we\\xa0use\\xa0apredictivelog-likelihood\\xa0trainingobjective,such\\xa0asequa-\\ntion,wetraintheRNNtoestimatetheconditionaldistributionofthenext 10.12\\nsequenceelementy( ) tgiventhepastinputs.Thismaymeanthatwemaximize\\nthelog-likelihood\\nlog( py( ) t|x( 1 ), . . . ,x( ) t) , (10.29)\\nor,ifthemodelincludesconnectionsfromtheoutputatonetimesteptothenext\\ntimestep,\\nlog( py( ) t|x( 1 ), . . . ,x( ) t,y( 1 ), . . . ,y( 1 ) t −) . (10.30)\\nDecomposingthejointprobabilityoverthesequenceofyvaluesasaseriesof\\none-stepprobabilisticpredictionsisonewaytocapturethefulljointdistribution\\nacrossthewholesequence.Whenwedonotfeedpastyvaluesasinputsthat\\nconditionthenextstepprediction,thedirectedgraphicalmodelcontainsnoedges\\nfromanyy( ) iinthepasttothecurrenty( ) t.Inthiscase,theoutputsyare\\nconditionallyindependentgiventhesequenceofxvalues.Whenwedofeedthe\\nactualyvalues(nottheirprediction,buttheactualobservedorgeneratedvalues)\\nbackintothenetwork,thedirectedgraphicalmodelcontainsedgesfromally( ) i\\nvaluesinthepasttothecurrent y( ) tvalue.\\nAsasimpleexample,letusconsiderthecasewheretheRNNmodelsonlya\\nsequenceofscalarrandomvariables Y={y( 1 ), . . . ,y( ) τ},withnoadditionalinputs\\nx.Theinputattimestep tissimplytheoutputattimestep t−1.TheRNNthen\\ndeﬁnesadirectedgraphicalmodelovertheyvariables.Weparametrizethejoint\\ndistributionoftheseobservationsusingthechainrule(equation)forconditional3.6\\nprobabilities:\\nP P () = Y ( y( 1 ), . . . , y( ) τ) =τ\\ue059\\nt = 1P( y( ) t| y( 1 ) t −, y( 2 ) t −, . . . , y( 1 ))(10.31)\\nwheretheright-handsideofthebarisemptyfor t=1,ofcourse.Hencethe\\nnegativelog-likelihoodofasetofvalues { y( 1 ), . . . , y( ) τ}accordingtosuchamodel\\n3 8 7', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\ny( 1 )y( 1 )y( 2 )y( 2 )y( 3 )y( 3 )y( 4 )y( 4 )y( 5 )y( 5 )y( ) . . .y( ) . . .\\nFigure10.7:Fullyconnectedgraphicalmodelforasequence y( 1 ), y( 2 ), . . . , y( ) t, . . .:every\\npastobservation y( ) imayinﬂuencetheconditionaldistributionofsome y( ) t(for t > i),\\ngiventhepreviousvalues.Parametrizingthegraphicalmodeldirectlyaccordingtothis\\ngraph(asinequation)mightbeveryineﬃcient,withanevergrowingnumberof 10.6\\ninputsandparametersforeachelementofthesequence.RNNsobtainthesamefull\\nconnectivitybuteﬃcientparametrization,asillustratedinﬁgure.10.8\\nis\\nL=\\ue058\\ntL( ) t(10.32)\\nwhere\\nL( ) t= log( − Py( ) t= y( ) t| y( 1 ) t −, y( 2 ) t −, . . . , y( 1 )) .(10.33)\\ny( 1 )y( 1 )y( 2 )y( 2 )y( 3 )y( 3 )y( 4 )y( 4 )y( 5 )y( 5 )y( ) . . .y( ) . . .h( 1 )h( 1 )h( 2 )h( 2 )h( 3 )h( 3 )h( 4 )h( 4 )h( 5 )h( 5 )h( ) . . .h( ) . . .\\nFigure10.8:IntroducingthestatevariableinthegraphicalmodeloftheRNN,even\\nthoughitisadeterministicfunctionofitsinputs,helpstoseehowwecanobtainavery\\neﬃcientparametrization,basedonequation.Everystageinthesequence(for 10.5 h( ) t\\nandy( ) t)involvesthesamestructure(thesamenumberofinputsforeachnode)andcan\\nsharethesameparameterswiththeotherstages.\\nTheedgesinagraphicalmodelindicatewhichvariablesdependdirectlyonother\\nvariables.Manygraphicalmodelsaimtoachievestatisticalandcomputational\\neﬃciencybyomittingedgesthatdonotcorrespondtostronginteractions.For\\n3 8 8', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nexample,itiscommontomaketheMarkovassumptionthatthegraphicalmodel\\nshouldonlycontainedgesfrom{y( ) t k −, . . . ,y( 1 ) t −}toy( ) t,ratherthancontaining\\nedgesfromtheentirepasthistory.However,insomecases,webelievethatallpast\\ninputsshouldhaveaninﬂuenceonthenextelementofthesequence.RNNsare\\nusefulwhenwebelievethatthedistributionovery( ) tmaydependonavalueofy( ) i\\nfromthedistantpastinawaythatisnotcapturedbytheeﬀectofy( ) iony( 1 ) t −.\\nOnewaytointerpretanRNNasagraphicalmodelistoviewtheRNNas\\ndeﬁningagraphicalmodelwhosestructureisthecompletegraph,abletorepresent\\ndirectdependenciesbetweenanypairofyvalues.Thegraphicalmodeloverthey\\nvalueswiththecompletegraphstructureisshowninﬁgure.Thecomplete10.7\\ngraphinterpretationoftheRNNisbasedonignoringthehiddenunitsh( ) tby\\nmarginalizing themoutofthemodel.\\nItismoreinterestingtoconsiderthegraphicalmodelstructureofRNNsthat\\nresultsfromregardingthehiddenunitsh( ) tasrandomvariables.1Includingthe\\nhiddenunitsinthegraphicalmodelrevealsthattheRNNprovidesaveryeﬃcient\\nparametrization ofthejointdistributionovertheobservations.Supposethatwe\\nrepresentedanarbitraryjointdistributionoverdiscretevalueswithatabular\\nrepresentation—anarraycontainingaseparateentryforeachpossibleassignment\\nofvalues,withthevalueofthatentrygivingtheprobabilityofthatassignment\\noccurring.\\xa0If ycantakeon kdiﬀerentvalues,thetabularrepresentationwould\\nhave O( kτ)parameters.Bycomparison,duetoparametersharing,thenumberof\\nparametersintheRNNis O(1)asafunctionofsequencelength.Thenumberof\\nparametersintheRNNmaybeadjustedtocontrolmodelcapacitybutisnotforced\\ntoscalewithsequencelength.EquationshowsthattheRNNparametrizes 10.5\\nlong-termrelationshipsbetweenvariableseﬃciently,usingrecurrentapplications\\nofthesamefunction fandsameparametersθateachtimestep.Figure10.8\\nillustratesthegraphicalmodelinterpretation.Incorporating theh( ) tnodesin\\nthegraphicalmodeldecouplesthepastandthefuture,actingasanintermediate\\nquantitybetweenthem.Avariable y( ) iinthedistantpastmayinﬂuenceavariable\\ny( ) tviaitseﬀectonh.Thestructureofthisgraphshowsthatthemodelcanbe\\neﬃcientlyparametrized byusingthesameconditionalprobabilitydistributionsat\\neachtimestep,andthatwhenthevariablesareallobserved,theprobabilityofthe\\njointassignmentofallvariablescanbeevaluatedeﬃciently.\\nEvenwiththeeﬃcientparametrization ofthegraphicalmodel,someoperations\\nremaincomputationally challenging.Forexample,itisdiﬃculttopredictmissing\\n1Th e c o n d i t i o n a l d i s t rib u t i o n o v e r t h e s e v a ria b l e s g i v e n t h e i r p a re n t s i s d e t e rm i n i s t i c . Th i s i s\\np e rfe c t l y l e g i t i m a t e , t h o u g h i t i s s o m e wh a t ra re t o d e s i g n a g ra p h i c a l m o d e l with s u c h d e t e rm i n i s t i c\\nh i d d e n u n i t s .\\n3 8 9', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nvaluesinthemiddleofthesequence.\\nThepricerecurrentnetworkspayfortheirreducednumberofparametersis\\nthat theparametersmaybediﬃcult. o p t i m i z i ng\\nTheparametersharingusedinrecurrentnetworksreliesontheassumption\\nthatthesameparameterscanbeusedfordiﬀerenttimesteps.Equivalently,the\\nassumptionisthattheconditionalprobabilitydistributionoverthevariablesat\\ntime t+1 giventhevariablesattime tisstationary,meaningthattherelationship\\nbetweentheprevioustimestepandthenexttimestepdoesnotdependon t.In\\nprinciple,itwouldbepossibletouse tasanextrainputateachtimestepandlet\\nthelearnerdiscoveranytime-dependencewhilesharingasmuchasitcanbetween\\ndiﬀerenttimesteps.Thiswouldalreadybemuchbetterthanusingadiﬀerent\\nconditionalprobabilitydistributionforeach t,butthenetworkwouldthenhaveto\\nextrapolatewhenfacedwithnewvaluesof. t\\nTocompleteourviewofanRNNasagraphicalmodel,wemustdescribehow\\ntodrawsamplesfromthemodel.Themainoperationthatweneedtoperformis\\nsimplytosamplefromtheconditionaldistributionateachtimestep.\\xa0However,\\nthereisoneadditionalcomplication.\\xa0The RNNmusthavesomemechanismfor\\ndeterminingthelengthofthesequence.Thiscanbeachievedinvariousways.\\nInthecasewhentheoutputisasymboltakenfromavocabulary,onecan\\naddaspecialsymbolcorrespondingtotheendofasequence(Schmidhuber2012,).\\nWhenthatsymbolisgenerated,thesamplingprocessstops.Inthetrainingset,\\nweinsertthissymbolasanextramemberofthesequence,immediatelyafterx( ) τ\\nineachtrainingexample.\\nAnotheroptionistointroduceanextraBernoullioutputtothemodelthat\\nrepresentsthedecisiontoeithercontinuegenerationorhaltgenerationateach\\ntimestep.Thisapproachismoregeneralthantheapproachofaddinganextra\\nsymboltothevocabulary,becauseitmaybeappliedtoanyRNN,ratherthan\\nonlyRNNsthatoutputasequenceofsymbols.Forexample,itmaybeappliedto\\nanRNNthatemitsasequenceofrealnumbers.Thenewoutputunitisusuallya\\nsigmoidunittrainedwiththecross-entropyloss.Inthisapproachthesigmoidis\\ntrainedtomaximizethelog-probabilit yofthecorrectpredictionastowhetherthe\\nsequenceendsorcontinuesateachtimestep.\\nAnotherwaytodeterminethesequencelength τistoaddanextraoutputto\\nthemodelthatpredictstheinteger τitself.Themodelcansampleavalueof τ\\nandthensample τstepsworthofdata.Thisapproachrequiresaddinganextra\\ninputtotherecurrentupdateateachtimestepsothattherecurrentupdateis\\nawareofwhetheritisneartheendofthegeneratedsequence.Thisextrainput\\ncaneitherconsistofthevalueof τorcanconsistof τ t−,thenumberofremaining\\n3 9 0', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\ntimesteps.Withoutthisextrainput,theRNNmightgeneratesequencesthat\\nendabruptly,suchasasentencethatendsbeforeitiscomplete.Thisapproachis\\nbasedonthedecomposition\\nP(x( 1 ), . . . ,x( ) τ) = ()( P τ Px( 1 ), . . . ,x( ) τ| τ .)(10.34)\\nThestrategyofpredicting τdirectlyisusedforexamplebyGoodfellow e t a l .\\n().2014d\\n10.2.4ModelingSequencesConditionedonContextwithRNNs\\nIntheprevioussectionwedescribedhowanRNNcouldcorrespondtoadirected\\ngraphicalmodeloverasequenceofrandomvariables y( ) twithnoinputsx.Of\\ncourse,ourdevelopmentofRNNsasinequationincludedasequenceof 10.8\\ninputsx( 1 ),x( 2 ), . . . ,x( ) τ.Ingeneral,RNNsallowtheextensionofthegraphical\\nmodelviewtorepresentnotonlyajointdistributionoverthe yvariablesbut\\nalsoaconditionaldistributionover ygivenx.Asdiscussedinthecontextof\\nfeedforwardnetworksinsection,anymodelrepresentingavariable 6.2.1.1 P(y;θ)\\ncanbereinterpretedasamodelrepresentingaconditionaldistribution P(yω|)\\nwithω=θ.Wecanextendsuchamodeltorepresentadistribution P(yx|)by\\nusingthesame P(yω|)asbefore,butmakingωafunctionofx.Inthecaseof\\nanRNN,thiscanbeachievedindiﬀerentways.Wereviewherethemostcommon\\nandobviouschoices.\\nPreviously,wehavediscussedRNNsthattakeasequenceofvectorsx( ) tfor\\nt=1 , . . . , τasinput.\\xa0Anotheroptionistotakeonlyasinglevectorxasinput.\\nWhenxisaﬁxed-sizevector,wecansimplymakeitanextrainputoftheRNN\\nthatgeneratesthe ysequence.Somecommonwaysofprovidinganextrainputto\\nanRNNare:\\n1.\\xa0asanextrainputateachtimestep,or\\n2.\\xa0astheinitialstateh( 0 ),or\\n3.\\xa0both.\\nTheﬁrstandmostcommonapproachisillustratedinﬁgure.Theinteraction10.9\\nbetweentheinputxandeachhiddenunitvectorh( ) tisparametrized byanewly\\nintroducedweightmatrixRthatwasabsentfromthemodelofonlythesequence\\nof yvalues.\\xa0Thesameproductx\\ue03eRisaddedasadditionalinputtothehidden\\nunitsateverytimestep.Wecanthinkofthechoiceofxasdeterminingthevalue\\n3 9 1', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nofx\\ue03eRthatiseﬀectivelyanewbiasparameterusedforeachofthehiddenunits.\\nTheweightsremainindependentoftheinput.Wecanthinkofthismodelastaking\\ntheparametersθofthenon-conditional modelandturningthemintoω,where\\nthebiasparameterswithinarenowafunctionoftheinput. ω\\no( t − 1 )o( t − 1 )o( ) to( ) to( + 1 ) to( + 1 ) tL( t − 1 )L( t − 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t − 1 )y( t − 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t\\nh( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) tW W W W\\ns( ) . . .s( ) . . .h( ) . . .h( ) . . .V V VU U U\\nx xy( ) . . .y( ) . . .\\nR R R R R\\nFigure10.9:AnRNNthatmapsaﬁxed-lengthvectorxintoadistributionoversequences\\nY.ThisRNNisappropriatefortaskssuchasimagecaptioning,whereasingleimageis\\nusedasinputtoamodelthatthenproducesasequenceofwordsdescribingtheimage.\\nEachelementy( ) toftheobservedoutputsequenceservesbothasinput(forthecurrent\\ntimestep)and,duringtraining,astarget(fortheprevioustimestep).\\nRatherthanreceivingonlyasinglevectorxasinput,theRNNmayreceive\\nasequenceofvectorsx( ) tasinput.TheRNNdescribedinequationcorre-10.8\\nspondstoaconditionaldistribution P(y( 1 ), . . . ,y( ) τ|x( 1 ), . . . ,x( ) τ)thatmakesa\\nconditionalindependence assumptionthatthisdistributionfactorizesas\\n\\ue059\\ntP(y( ) t|x( 1 ), . . . ,x( ) t) . (10.35)\\nToremovetheconditionalindependenceassumption,wecanaddconnectionsfrom\\ntheoutputattime ttothehiddenunitattime t+1,asshowninﬁgure.The10.10\\nmodelcanthenrepresentarbitraryprobabilitydistributionsovertheysequence.\\nThiskindofmodelrepresentingadistributionoverasequencegivenanother\\n3 9 2', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\no( t − 1 )o( t − 1 )o( ) to( ) to( + 1 ) to( + 1 ) tL( t − 1 )L( t − 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t − 1 )y( t − 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t\\nh( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) tW W W W\\nh( ) . . .h( ) . . .h( ) . . .h( ) . . .V V V\\nU U U\\nx( t − 1 )x( t − 1 )R\\nx( ) tx( ) tx( + 1 ) tx( + 1 ) tR R\\nFigure10.10:\\xa0Aconditionalrecurrentneuralnetworkmappingavariable-lengthsequence\\nofxvaluesintoadistributionoversequencesofyvaluesofthesamelength.Comparedto\\nﬁgure,thisRNNcontainsconnectionsfromthepreviousoutputtothecurrentstate. 10.3\\nTheseconnectionsallowthisRNNtomodelanarbitrarydistributionoversequencesofy\\ngivensequencesofxofthesamelength.TheRNNofﬁgureisonlyabletorepresent 10.3\\ndistributionsinwhichtheyvaluesareconditionallyindependentfromeachothergiven\\nthevalues.x\\n3 9 3', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nsequencestillhasonerestriction,whichisthatthelengthofbothsequencesmust\\nbethesame.Wedescribehowtoremovethisrestrictioninsection.10.4\\no( t − 1 )o( t − 1 )o( ) to( ) to( + 1 ) to( + 1 ) tL( t − 1 )L( t − 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t − 1 )y( t − 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t\\nh( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) t\\nx( t − 1 )x( t − 1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) tg( t − 1 )g( t − 1 )g( ) tg( ) tg( +1 ) tg( +1 ) t\\nFigure10.11:\\xa0Computation ofatypicalbidirectionalrecurrentneuralnetwork,meant\\ntolearntomapinputsequencesxtotargetsequencesy,withloss L( ) tateachstep t.\\nThehrecurrencepropagatesinformationforwardintime(towardstheright)whilethe\\ngrecurrencepropagatesinformationbackwardintime(towardstheleft).Thusateach\\npoint t,theoutputunitso( ) tcanbeneﬁtfromarelevantsummaryofthepastinitsh( ) t\\ninputandfromarelevantsummaryofthefutureinitsg( ) tinput.\\n10.3BidirectionalRNNs\\nAlloftherecurrentnetworkswehaveconsidereduptonowhavea“causal”struc-\\nture,meaningthatthestateattime tonlycapturesinformationfromthepast,\\nx( 1 ), . . . ,x( 1 ) t −,andthepresentinputx( ) t.Someofthemodelswehavediscussed\\nalsoallowinformationfrompastyvaluestoaﬀectthecurrentstatewhenthey\\nvaluesareavailable.\\nHowever,inmanyapplicationswewanttooutputapredictionofy( ) twhichmay\\n3 9 4', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\ndependon t h e w h o l e i npu t s e q u e nc e.Forexample,inspeechrecognition,thecorrect\\ninterpretationofthecurrentsoundasaphonememaydependonthenextfew\\nphonemesbecauseofco-articulationandpotentiallymayevendependonthenext\\nfewwordsbecauseofthelinguisticdependenciesbetweennearbywords:ifthere\\naretwointerpretationsofthecurrentwordthatarebothacousticallyplausible,we\\nmayhavetolookfarintothefuture(andthepast)todisambiguatethem.Thisis\\nalsotrueofhandwritingrecognitionandmanyothersequence-to-sequencelearning\\ntasks,describedinthenextsection.\\nBidirectionalrecurrentneuralnetworks(orbidirectional RNNs)wereinvented\\ntoaddressthatneed(SchusterandPaliwal1997,).Theyhavebeenextremelysuc-\\ncessful(Graves2012,)inapplicationswherethatneedarises,suchashandwriting\\nrecognition(Graves2008GravesandSchmidhuber2009 e t a l .,; ,),speechrecogni-\\ntion(GravesandSchmidhuber2005Graves2013 Baldi ,; e t a l .,)andbioinformatics (\\ne t a l .,).1999\\nAsthenamesuggests,bidirectionalRNNscombineanRNNthatmovesforward\\nthroughtimebeginningfromthestartofthesequencewithanotherRNNthat\\nmovesbackwardthroughtimebeginningfromtheendofthesequence.Figure10.11\\nillustratesthetypicalbidirectional RNN,withh( ) tstandingforthestateofthe\\nsub-RNNthatmovesforwardthroughtimeandg( ) tstandingforthestateofthe\\nsub-RNNthatmovesbackwardthroughtime.\\xa0Thisallowstheoutputunitso( ) t\\ntocomputearepresentationthatdependson b o t h t h e p a s t a nd t h e f u t u r ebut\\nismostsensitivetotheinputvaluesaroundtime t,withouthavingtospecifya\\nﬁxed-sizewindowaround t(asonewouldhavetodowithafeedforwardnetwork,\\naconvolutionalnetwork,oraregularRNNwithaﬁxed-sizelook-aheadbuﬀer).\\nThisideacanbenaturallyextendedto2-dimensionalinput,suchasimages,by\\nhavingRNNs,eachonegoinginoneofthefourdirections:\\xa0up, down,left, f o u r\\nright.Ateachpoint ( i , j)ofa2-Dgrid,anoutput O i , jcouldthencomputea\\nrepresentationthatwouldcapturemostlylocalinformationbutcouldalsodepend\\non\\xa0long-range inputs,ifthe\\xa0RNN\\xa0isable\\xa0tolearn\\xa0tocarry\\xa0that\\xa0information.\\nComparedtoaconvolutionalnetwork,RNNsappliedtoimagesaretypicallymore\\nexpensivebutallowforlong-rangelateralinteractionsbetweenfeaturesinthe\\nsamefeaturemap(,; Visin e t a l .2015Kalchbrenner 2015 e t a l .,).Indeed,the\\nforwardpropagationequationsforsuchRNNsmaybewritteninaformthatshows\\ntheyuseaconvolutionthatcomputesthebottom-upinputtoeachlayer,prior\\ntotherecurrentpropagationacrossthefeaturemapthatincorporatesthelateral\\ninteractions.\\n3 9 5', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\n10.4Encoder-DecoderSequence-to-SequenceArchitec-\\ntures\\nWehaveseeninﬁgurehowanRNNcanmapaninputsequencetoaﬁxed-size 10.5\\nvector.WehaveseeninﬁgurehowanRNNcanmapaﬁxed-sizevectortoa 10.9\\nsequence.\\xa0Wehaveseeninﬁgures,,andhowanRNNcan 10.310.410.1010.11\\nmapaninputsequencetoanoutputsequenceofthesamelength.\\nE nc ode r\\n…\\nx( 1 )x( 1 )x( 2 )x( 2 )x( ) . . .x( ) . . .x( n x )x( n x )\\nD e c ode r\\n…\\ny( 1 )y( 1 )y( 2 )y( 2 )y( ) . . .y( ) . . .y( n y )y( n y )CC\\nFigure10.12:\\xa0Exam pleofanencoder-decoderorsequence-to-sequenceRNNarchitecture,\\nforlearningtogenerateanoutputsequence( y( 1 ), . . . , y( n y ))givenaninputsequence\\n( x( 1 ), x( 2 ), . . . , x( n x )).ItiscomposedofanencoderRNNthatreadstheinputsequence\\nandadecoderRNNthatgeneratestheoutputsequence(orcomputestheprobabilityofa\\ngivenoutputsequence).TheﬁnalhiddenstateoftheencoderRNNisusedtocomputea\\ngenerallyﬁxed-sizecontextvariable Cwhichrepresentsasemanticsummaryoftheinput\\nsequenceandisgivenasinputtothedecoderRNN.\\nHerewediscusshowanRNNcanbetrainedtomapaninputsequencetoan\\noutputsequencewhichisnotnecessarilyofthesamelength.\\xa0This comesupin\\nmanyapplications,suchasspeechrecognition,machinetranslationorquestion\\n3 9 6', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nanswering,wheretheinputandoutputsequencesinthetrainingsetaregenerally\\nnotofthesamelength(althoughtheirlengthsmightberelated).\\nWeoftencalltheinputtotheRNNthe“context.”Wewanttoproducea\\nrepresentationofthiscontext, C.Thecontext Cmightbeavectororsequenceof\\nvectorsthatsummarizetheinputsequenceXx= (( 1 ), . . . ,x( n x )).\\nThesimplestRNNarchitectureformappingavariable-length sequenceto\\nanothervariable-length sequencewasﬁrstproposedby ()and Cho e t a l .2014a\\nshortlyafterbySutskever2014 e t a l .(),whoindependentlydevelopedthatarchi-\\ntectureandweretheﬁrsttoobtainstate-of-the-art translationusingthisapproach.\\nTheformersystemisbasedonscoringproposalsgeneratedbyanothermachine\\ntranslationsystem,whilethelatterusesastandalonerecurrentnetworktogenerate\\nthetranslations.\\xa0Theseauthorsrespectivelycalledthisarchitecture, illustrated\\ninﬁgure,theencoder-decoderorsequence-to-sequencearchitecture.The 10.12\\nideaisverysimple:(1)anencoderorreaderorinputRNNprocessestheinput\\nsequence.Theencoderemitsthecontext C,usuallyasasimplefunctionofits\\nﬁnalhiddenstate.\\xa0(2)adecoderorwriteroroutputRNNisconditionedon\\nthatﬁxed-lengthvector(justlikeinﬁgure)togeneratetheoutputsequence 10.9\\nY=(y( 1 ), . . . ,y( n y )).Theinnovationofthiskindofarchitectureoverthose\\npresentedinearliersectionsofthischapteristhatthelengths n xand n ycan\\nvaryfromeachother,whilepreviousarchitectures constrained n x= n y= τ.Ina\\nsequence-to-sequencearchitecture,thetwoRNNsaretrainedjointlytomaximize\\ntheaverageoflog P(y( 1 ), . . . ,y( n y )|x( 1 ), . . . ,x( n x ))overallthepairsofxandy\\nsequencesinthetrainingset.Thelaststateh n xoftheencoderRNNistypically\\nusedasarepresentation Coftheinputsequencethatisprovidedasinputtothe\\ndecoderRNN.\\nIfthecontext Cisavector,thenthedecoderRNNissimplyavector-to-\\nsequenceRNNasdescribedinsection.Aswehaveseen,thereareatleast 10.2.4\\ntwowaysforavector-to-sequenceRNNtoreceiveinput.Theinputcanbeprovided\\nastheinitialstateoftheRNN,ortheinputcanbeconnectedtothehiddenunits\\nateachtimestep.Thesetwowayscanalsobecombined.\\nThereisnoconstraintthattheencodermusthavethesamesizeofhiddenlayer\\nasthedecoder.\\nOneclearlimitationofthisarchitectureiswhenthecontext Coutputbythe\\nencoderRNNhasadimensionthatistoosmalltoproperlysummarizealong\\nsequence.Thisphenomenon wasobservedby ()inthecontext Bahdanau e t a l .2015\\nofmachinetranslation.Theyproposedtomake Cavariable-length sequencerather\\nthanaﬁxed-sizevector.Additionally,theyintroducedanattentionmechanism\\nthatlearnstoassociateelementsofthesequence Ctoelementsoftheoutput\\n3 9 7', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nsequence.Seesectionformoredetails. 12.4.5.1\\n10.5DeepRecurrentNetworks\\nThecomputationinmostRNNscanbedecomposedintothreeblocksofparameters\\nandassociatedtransformations:\\n1.\\xa0fromtheinputtothehiddenstate,\\n2.\\xa0fromtheprevioushiddenstatetothenexthiddenstate,and\\n3.\\xa0fromthehiddenstatetotheoutput.\\nWiththeRNNarchitectureofﬁgure,eachofthesethreeblocksisassociated 10.3\\nwithasingleweightmatrix.Inotherwords,whenthenetworkisunfolded,each\\nofthesecorrespondstoashallowtransformation.\\xa0Byashallowtransformation,\\nwemeanatransformationthatwouldberepresentedbyasinglelayerwithin\\nadeepMLP.Typicallythisisatransformationrepresentedbyalearnedaﬃne\\ntransformationfollowedbyaﬁxednonlinearity.\\nWoulditbeadvantageoustointroducedepthineachoftheseoperations?\\nExperimentalevidence(Graves2013Pascanu2014a e t a l .,; e t a l .,)stronglysuggests\\nso.Theexperimentalevidenceisinagreementwiththeideathatweneedenough\\ndepthinordertoperformtherequiredmappings.SeealsoSchmidhuber1992(),\\nElHihiandBengio1996Jaeger2007a (),or()forearlierworkondeepRNNs.\\nGraves2013 e t a l .()weretheﬁrsttoshowasigniﬁcantbeneﬁtofdecomposing\\nthestateofanRNNintomultiplelayersasinﬁgure(left).Wecanthink 10.13\\nofthelowerlayersinthehierarchydepictedinﬁgureaasplayingarole 10.13\\nintransformingtherawinputintoarepresentationthatismoreappropriate,at\\nthehigherlevelsofthehiddenstate.Pascanu2014a e t a l .()goastepfurther\\nandproposetohaveaseparateMLP(possiblydeep)foreachofthethreeblocks\\nenumeratedabove,asillustratedinﬁgureb.Considerationsofrepresentational 10.13\\ncapacitysuggesttoallocateenoughcapacityineachofthesethreesteps,butdoing\\nsobyaddingdepthmayhurtlearningbymakingoptimization diﬃcult.Ingeneral,\\nitiseasiertooptimizeshallowerarchitectures,andaddingtheextradepthof\\nﬁgurebmakestheshortestpathfromavariableintimestep 10.13 ttoavariable\\nintimestep t+1becomelonger.Forexample,ifanMLPwithasinglehidden\\nlayerisusedforthestate-to-statetransition,wehavedoubledthelengthofthe\\nshortestpathbetweenvariablesinanytwodiﬀerenttimesteps,comparedwiththe\\nordinaryRNNofﬁgure.However,asarguedby 10.3 Pascanu2014a e t a l .(),this\\n3 9 8', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nhy\\nxz\\n( a) ( b) ( c )xhy\\nxhy\\nFigure10.13:Arecurrentneuralnetworkcanbemadedeepinmanyways(Pascanu\\ne t a l .,).Thehiddenrecurrentstatecanbebrokendownintogroupsorganized 2014a ( a )\\nhierarchically.Deepercomputation(e.g.,anMLP)canbeintroducedintheinput-to- ( b )\\nhidden,hidden-to-hiddenandhidden-to-outputparts.\\xa0Thismaylengthentheshortest\\npathlinkingdiﬀerenttimesteps.Thepath-lengtheningeﬀectcanbemitigatedby ( c )\\nintroducingskipconnections.\\n3 9 9', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\ncanbemitigatedbyintroducingskipconnectionsinthehidden-to-hidden path,as\\nillustratedinﬁgurec.10.13\\n10.6RecursiveNeuralNetworks\\nx( 1 )x( 1 )x( 2 )x( 2 )x( 3 )x( 3 )V V Vy yL L\\nx( 4 )x( 4 )Voo\\nU W U WUW\\nFigure10.14:Arecursivenetworkhasacomputationalgraphthatgeneralizesthatofthe\\nrecurrentnetworkfromachaintoatree.Avariable-sizesequencex( 1 ),x( 2 ), . . . ,x( ) tcan\\nbemappedtoaﬁxed-sizerepresentation(theoutputo),withaﬁxedsetofparameters\\n(theweightmatricesU,V,W).Theﬁgureillustratesasupervisedlearningcaseinwhich\\nsometargetisprovidedwhichisassociatedwiththewholesequence. y\\nRecursiveneuralnetworks2representyetanothergeneralization ofrecurrent\\nnetworks,withadiﬀerentkindofcomputational graph,whichisstructuredasa\\ndeeptree,ratherthanthechain-likestructureofRNNs.Thetypicalcomputational\\ngraphforarecursivenetworkisillustratedinﬁgure.Recursiveneural 10.14\\n2W e s u g g e s t t o n o t a b b re v i a t e “ re c u rs i v e n e u ra l n e t w o rk ” a s “ R NN” t o a v o i d c o n f u s i o n with\\n“ re c u rre n t n e u ra l n e t w o rk . ”\\n4 0 0', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nnetworkswereintroducedbyPollack1990()andtheirpotentialuseforlearningto\\nreasonwasdescribedby().Recursivenetworkshavebeensuccessfully Bottou2011\\nappliedtoprocessing d a t a s t r u c t u r e sasinputtoneuralnets(Frasconi1997 e t a l .,,\\n1998 Socher2011ac2013a ),innaturallanguageprocessing( e t a l .,,,)aswellasin\\ncomputervision( ,). Socher e t a l .2011b\\nOneclearadvantageofrecursivenetsoverrecurrentnetsisthatforasequence\\nofthesamelength τ,thedepth(measuredasthenumberofcompositionsof\\nnonlinearoperations)canbedrasticallyreducedfrom τto O(log τ),whichmight\\nhelpdealwithlong-termdependencies.Anopenquestionishowtobeststructure\\nthetree.Oneoptionistohaveatreestructurewhichdoesnotdependonthedata,\\nsuchasabalancedbinarytree.Insomeapplicationdomains,externalmethods\\ncansuggesttheappropriatetreestructure.Forexample,whenprocessingnatural\\nlanguagesentences,thetreestructurefortherecursivenetworkcanbeﬁxedto\\nthestructureoftheparsetreeofthesentenceprovidedbyanaturallanguage\\nparser( ,,).\\xa0Ideally,onewouldlikethelearneritselfto Socher e t a l .2011a2013a\\ndiscoverandinferthetreestructurethatisappropriateforanygiveninput,as\\nsuggestedby(). Bottou2011\\nManyvariantsoftherecursivenetideaarepossible.Forexample,Frasconi\\ne t a l .()and1997Frasconi1998 e t a l .()associatethedatawithatreestructure,\\nandassociatethe\\xa0inputsandtargetswith\\xa0individualnodesofthe\\xa0tree.The\\ncomputationperformedbyeachnodedoesnothavetobethetraditionalartiﬁcial\\nneuroncomputation(aﬃnetransformationofallinputsfollowedbyamonotone\\nnonlinearity).Forexample, ()proposeusingtensoroperations Socher e t a l .2013a\\nandbilinearforms,whichhavepreviouslybeenfoundusefultomodelrelationships\\nbetweenconcepts(Weston2010Bordes2012 e t a l .,; e t a l .,)whentheconceptsare\\nrepresentedbycontinuousvectors(embeddings).\\n10.7TheChallengeofLong-TermDependencies\\nThemathematical challengeoflearninglong-termdependenciesinrecurrentnet-\\nworkswasintroducedinsection.Thebasicproblemisthatgradientsprop- 8.2.5\\nagatedovermanystagestendtoeithervanish(mostofthetime)orexplode\\n(rarely,butwithmuchdamagetotheoptimization). Evenifweassumethatthe\\nparametersaresuchthattherecurrentnetworkisstable(canstorememories,\\nwithgradientsnotexploding),thediﬃcultywithlong-termdependenciesarises\\nfromtheexponentiallysmallerweightsgiventolong-terminteractions(involving\\nthemultiplicationofmanyJacobians)comparedtoshort-termones.Manyother\\nsourcesprovideadeepertreatment(,; Hochreiter1991Doya1993Bengio,; e t a l .,\\n4 0 1', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\n− − − 6 0 4 0 2 0 0 2 0 4 0 6 0\\nI nput c o o r di na t e− 4− 3− 2− 101234P r o j e c t i o n o f o utput0\\n1\\n2\\n3\\n4\\n5\\nFigure10.15:Whencomposingmanynonlinearfunctions(likethelinear-tanhlayershown\\nhere),theresultishighlynonlinear,typicallywithmostofthevaluesassociatedwithatiny\\nderivative,somevalueswithalargederivative,andmanyalternationsbetweenincreasing\\nanddecreasing.Inthisplot,weplotalinearprojectionofa100-dimensionalhiddenstate\\ndowntoasingledimension,plottedonthe y-axis.\\xa0The x-axisisthecoordinateofthe\\ninitialstatealongarandomdirectioninthe100-dimensionalspace.Wecanthusviewthis\\nplotasalinearcross-sectionofahigh-dimensionalfunction.Theplotsshowthefunction\\naftereachtimestep,orequivalently,aftereachnumberoftimesthetransitionfunction\\nhasbeencomposed.\\n1994Pascanu2013 ; e t a l .,).Inthissection,wedescribetheprobleminmore\\ndetail.Theremainingsectionsdescribeapproachestoovercomingtheproblem.\\nRecurrentnetworksinvolvethecompositionofthesamefunctionmultiple\\ntimes,oncepertimestep.Thesecompositionscanresultinextremelynonlinear\\nbehavior,asillustratedinﬁgure.10.15\\nInparticular,thefunctioncompositionemployedbyrecurrentneuralnetworks\\nsomewhatresemblesmatrixmultiplication. Wecanthinkoftherecurrencerelation\\nh( ) t= W\\ue03eh( 1 ) t −(10.36)\\nasaverysimplerecurrentneuralnetworklackinganonlinearactivationfunction,\\nandlackinginputsx.As\\xa0described\\xa0insection\\xa0,\\xa0thisrecurrencerelation 8.2.5\\nessentiallydescribesthepowermethod.Itmaybesimpliﬁedto\\nh( ) t=\\ue000\\nWt\\ue001\\ue03eh( 0 ), (10.37)\\nandifadmitsaneigendecompositionoftheform W\\nWQQ = Λ\\ue03e, (10.38)\\n4 0 2', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nwithorthogonal ,therecurrencemaybesimpliﬁedfurtherto Q\\nh( ) t= Q\\ue03eΛtQh( 0 ). (10.39)\\nTheeigenvaluesareraisedtothepowerof tcausingeigenvalueswithmagnitude\\nlessthanonetodecaytozeroandeigenvalueswithmagnitudegreaterthanoneto\\nexplode.Anycomponentofh( 0 )thatisnotalignedwiththelargesteigenvector\\nwilleventuallybediscarded.\\nThisproblemisparticulartorecurrentnetworks.Inthescalarcase,imagine\\nmultiplyingaweight wbyitselfmanytimes.Theproduct wtwilleithervanishor\\nexplodedependingonthemagnitudeof w.However,ifwemakeanon-recurrent\\nnetworkthathasadiﬀerentweight w( ) tateachtimestep,thesituationisdiﬀerent.\\nIftheinitialstateisgivenby,thenthestateattime 1 tisgivenby\\ue051\\nt w( ) t.Suppose\\nthatthe w( ) tvaluesaregeneratedrandomly,independentlyfromoneanother,with\\nzeromeanandvariance v.Thevarianceoftheproductis O( vn).Toobtainsome\\ndesiredvariance v∗wemaychoosetheindividualweightswithvariance v=n√\\nv∗.\\nVerydeepfeedforwardnetworkswithcarefullychosenscalingcanthusavoidthe\\nvanishingandexplodinggradientproblem,asarguedby(). Sussillo2014\\nThevanishingandexplodinggradientproblemforRNNswasindependently\\ndiscoveredbyseparateresearchers(,; ,,). Hochreiter1991Bengio e t a l .19931994\\nOnemayhopethattheproblemcanbeavoidedsimplybystayinginaregionof\\nparameterspacewherethegradientsdonotvanishorexplode.Unfortunately,in\\nordertostorememoriesinawaythatisrobusttosmallperturbations,theRNN\\nmustenteraregionofparameterspacewheregradientsvanish( ,, Bengio e t a l .1993\\n1994).Speciﬁcally,wheneverthemodelisabletorepresentlongtermdependencies,\\nthegradientofalongterminteractionhasexponentiallysmallermagnitudethan\\nthegradientofashortterminteraction.\\xa0It doesnotmeanthatitisimpossible\\ntolearn,butthatitmighttakeaverylongtimetolearnlong-termdependencies,\\nbecausethesignalaboutthesedependencieswilltendtobehiddenbythesmallest\\nﬂuctuationsarisingfromshort-termdependencies.Inpractice,theexperiments\\nin ()showthatasweincreasethespanofthedependenciesthat Bengio e t a l .1994\\nneedtobecaptured,gradient-basedoptimization becomesincreasinglydiﬃcult,\\nwiththeprobabilityofsuccessfultrainingofatraditionalRNNviaSGDrapidly\\nreaching0forsequencesofonlylength10or20.\\nForadeepertreatmentofrecurrentnetworksasdynamicalsystems,seeDoya\\n(), ()and (),withareview 1993Bengio e t a l .1994SiegelmannandSontag1995\\ninPascanu2013 e t a l .().Theremainingsectionsofthischapterdiscussvarious\\napproachesthathavebeenproposedtoreducethediﬃcultyoflearninglong-\\ntermdependencies(insomecasesallowinganRNNtolearndependenciesacross\\n4 0 3', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nhundredsofsteps),buttheproblemoflearninglong-termdependenciesremains\\noneofthemainchallengesindeeplearning.\\n10.8EchoStateNetworks\\nTherecurrentweightsmappingfromh( 1 ) t −toh( ) tandtheinputweightsmapping\\nfromx( ) ttoh( ) taresomeofthemostdiﬃcultparameterstolearninarecurrent\\nnetwork.Oneproposed(,; ,; ,; Jaeger2003Maass e t a l .2002JaegerandHaas2004\\nJaeger2007b,)approachtoavoidingthisdiﬃcultyistosettherecurrentweights\\nsuchthattherecurrenthiddenunitsdoagoodjobofcapturingthehistoryofpast\\ninputs,and l e a r n o nl y t h e o u t p u t w e i g h t s.Thisistheideathatwasindependently\\nproposedforechostatenetworksorESNs( ,;,) JaegerandHaas2004Jaeger2007b\\nandliquidstatemachines(,).Thelatterissimilar,except Maass e t a l .2002\\nthatitusesspikingneurons(withbinaryoutputs)insteadofthecontinuous-valued\\nhiddenunitsusedforESNs.BothESNsandliquidstatemachinesaretermed\\nreservoircomputing(LukoševičiusandJaeger2009,)todenotethefactthat\\nthehiddenunitsformofreservoiroftemporalfeatureswhichmaycapturediﬀerent\\naspectsofthehistoryofinputs.\\nOnewaytothinkaboutthesereservoircomputingrecurrentnetworksisthat\\ntheyaresimilartokernelmachines:theymapanarbitrarylengthsequence(the\\nhistoryofinputsuptotime t)intoaﬁxed-lengthvector(therecurrentstateh( ) t),\\nonwhichalinearpredictor(typicallyalinearregression)canbeappliedtosolve\\ntheproblemofinterest.Thetrainingcriterionmaythenbeeasilydesignedtobe\\nconvexasafunctionoftheoutputweights.Forexample,iftheoutputconsists\\noflinearregressionfromthehiddenunitstotheoutputtargets,andthetraining\\ncriterionismeansquarederror,thenitisconvexandmaybesolvedreliablywith\\nsimplelearningalgorithms(,). Jaeger2003\\nTheimportantquestionistherefore:howdowesettheinputandrecurrent\\nweightssothatarichsetofhistoriescanberepresentedintherecurrentneural\\nnetworkstate?\\xa0Theanswerproposedinthereservoircomputingliteratureisto\\nviewtherecurrentnetasadynamicalsystem,andsettheinputandrecurrent\\nweightssuchthatthedynamicalsystemisneartheedgeofstability.\\nTheoriginalideawastomaketheeigenvaluesoftheJacobianofthestate-to-\\nstatetransitionfunctionbecloseto.Asexplainedinsection,animportant 1 8.2.5\\ncharacteristicofarecurrentnetworkistheeigenvaluespectrumoftheJacobians\\nJ( ) t=∂ s( ) t\\n∂ s( 1 ) t −.OfparticularimportanceisthespectralradiusofJ( ) t,deﬁnedto\\nbethemaximumoftheabsolutevaluesofitseigenvalues.\\n4 0 4', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nTounderstandtheeﬀectofthespectralradius,considerthesimplecaseof\\nback-propagationwithaJacobianmatrixJthatdoesnotchangewith t.This\\ncasehappens,forexample,whenthenetworkispurelylinear.SupposethatJhas\\naneigenvectorvwithcorrespondingeigenvalue λ.Considerwhathappensaswe\\npropagateagradientvectorbackwardsthroughtime.Ifwebeginwithagradient\\nvectorg,thenafteronestepofback-propagation,wewillhaveJg,andafter n\\nstepswewillhaveJng.Nowconsiderwhathappensifweinsteadback-propagate\\naperturbedversionofg.Ifwebeginwithg+ δv,thenafteronestep,wewill\\nhaveJ(g+ δv).After nsteps,wewillhaveJn(g+ δv).Fromthiswecansee\\nthatback-propagationstartingfromgandback-propagationstartingfromg+ δv\\ndivergeby δJnvafter nstepsofback-propagation.Ifvischosentobeaunit\\neigenvectorofJwitheigenvalue λ,thenmultiplicationbytheJacobiansimply\\nscalesthediﬀerenceateachstep.Thetwoexecutionsofback-propagationare\\nseparatedbyadistanceof δ λ||n.Whenvcorrespondstothelargestvalueof|| λ,\\nthisperturbationachievesthewidestpossibleseparationofaninitialperturbation\\nofsize. δ\\nWhen || λ >1,thedeviationsize δ λ||ngrowsexponentiallylarge.When || λ <1,\\nthedeviationsizebecomesexponentiallysmall.\\nOfcourse,thisexampleassumedthattheJacobianwasthesameatevery\\ntimestep,correspondingtoarecurrentnetworkwithnononlinearity.Whena\\nnonlinearityispresent,thederivativeofthenonlinearitywillapproachzeroon\\nmanytimesteps,andhelptopreventtheexplosionresultingfromalargespectral\\nradius.\\xa0Indeed,themostrecentworkonechostatenetworksadvocatesusinga\\nspectralradiusmuchlargerthanunity(,;,). Yildiz e t a l .2012Jaeger2012\\nEverythingwehavesaidaboutback-propagation viarepeatedmatrixmultipli-\\ncationappliesequallytoforwardpropagationinanetworkwithnononlinearity,\\nwherethestateh( + 1 ) t= h( ) t \\ue03eW.\\nWhenalinearmapW\\ue03ealwaysshrinkshasmeasuredbythe L2norm,then\\nwesaythatthemapiscontractive.Whenthespectralradiusislessthanone,\\nthemappingfromh( ) ttoh( + 1 ) tiscontractive,soasmallchangebecomessmaller\\naftereachtimestep.Thisnecessarilymakesthenetworkforgetinformationabout\\nthepastwhenweuseaﬁnitelevelofprecision(suchas32bitintegers)tostore\\nthestatevector.\\nTheJacobianmatrixtellsushowasmallchangeofh( ) tpropagatesonestep\\nforward,orequivalently,howthegradientonh( + 1 ) tpropagatesonestepbackward,\\nduringback-propagation. NotethatneitherWnorJneedtobesymmetric(al-\\nthoughtheyaresquareandreal),sotheycanhavecomplex-valuedeigenvaluesand\\neigenvectors,withimaginarycomponentscorrespondingtopotentiallyoscillatory\\n4 0 5', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nbehavior(ifthesameJacobianwasappliediteratively).Eventhoughh( ) tora\\nsmallvariationofh( ) tofinterestinback-propagation arereal-valued,theycan\\nbeexpressedinsuchacomplex-valuedbasis.Whatmattersiswhathappensto\\nthemagnitude(complexabsolutevalue)ofthesepossiblycomplex-valuedbasis\\ncoeﬃcients,\\xa0whenwemultiplythematrixbythevector.Aneigenvaluewith\\nmagnitudegreaterthanonecorrespondstomagniﬁcation (exponentialgrowth,if\\nappliediteratively)orshrinking(exponentialdecay,ifappliediteratively).\\nWithanonlinearmap,\\xa0theJacobianisfreetochangeateachstep.The\\ndynamicsthereforebecomemorecomplicated.However,itremainstruethata\\nsmallinitialvariationcanturnintoalargevariationafterseveralsteps.One\\ndiﬀerencebetweenthepurelylinearcaseandthenonlinearcaseisthattheuseof\\nasquashingnonlinearitysuchastanhcancausetherecurrentdynamicstobecome\\nbounded.Notethat\\xa0itispossible\\xa0forback-propagation to\\xa0retainunbounded\\ndynamicsevenwhenforwardpropagationhasboundeddynamics,forexample,\\nwhenasequenceoftanhunitsareallinthemiddleoftheirlinearregimeandare\\nconnectedbyweightmatriceswithspectralradiusgreaterthan.However,itis 1\\nrareforalloftheunitstosimultaneouslylieattheirlinearactivationpoint. tanh\\nThestrategyofechostatenetworksissimplytoﬁxtheweightstohavesome\\nspectralradiussuchas,whereinformationiscarriedforwardthroughtimebut 3\\ndoesnotexplodeduetothestabilizingeﬀectofsaturatingnonlinearities liketanh.\\nMorerecently,ithasbeenshownthatthetechniquesusedtosettheweights\\ninESNscouldbeusedtotheweightsinafullytrainablerecurrentnet- i nit i a l i z e\\nwork(withthehidden-to-hidden recurrentweightstrainedusingback-propagation\\nthroughtime),helpingtolearnlong-termdependencies(Sutskever2012Sutskever ,;\\ne t a l .,).Inthissetting,aninitialspectralradiusof1.2performswell,combined 2013\\nwiththesparseinitialization schemedescribedinsection.8.4\\n10.9LeakyUnitsandOtherStrategiesforMultiple\\nTimeScales\\nOnewaytodealwithlong-termdependencies istodesignamodelthatoperates\\natmultipletimescales,sothatsomepartsofthemodeloperateatﬁne-grained\\ntimescalesandcanhandlesmalldetails,whileotherpartsoperateatcoarsetime\\nscalesandtransferinformationfromthedistantpasttothepresentmoreeﬃciently.\\nVariousstrategiesforbuildingbothﬁneandcoarsetimescalesarepossible.These\\nincludetheadditionofskipconnectionsacrosstime,“leakyunits”thatintegrate\\nsignalswithdiﬀerenttimeconstants,andtheremovalofsomeoftheconnections\\n4 0 6', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nusedtomodelﬁne-grainedtimescales.\\n10.9.1AddingSkipConnectionsthroughTime\\nOnewaytoobtaincoarsetimescalesistoadddirectconnectionsfromvariablesin\\nthedistantpasttovariablesinthepresent.Theideaofusingsuchskipconnections\\ndatesbackto()andfollowsfromtheideaofincorporatingdelaysin Lin e t a l .1996\\nfeedforwardneuralnetworks( ,).Inanordinaryrecurrent LangandHinton1988\\nnetwork,arecurrentconnectiongoesfromaunitattime ttoaunitattime t+1.\\nItispossibletoconstructrecurrentnetworkswithlongerdelays(,). Bengio1991\\nAswehaveseeninsection,gradientsmayvanishorexplodeexponentially 8.2.5\\nw i t h r e s p e c t t o t h e nu m b e r o f t i m e s t e p s.()introducedrecurrent Lin e t a l .1996\\nconnectionswithatime-delayof dtomitigatethisproblem.Gradientsnow\\ndiminishexponentiallyasafunctionofτ\\ndratherthan τ.Sincethereareboth\\ndelayedandsinglestepconnections,gradientsmaystillexplodeexponentiallyin τ.\\nThisallowsthelearningalgorithmtocapturelongerdependenciesalthoughnotall\\nlong-termdependencies mayberepresentedwellinthisway.\\n10.9.2LeakyUnitsandaSpectrumofDiﬀerentTimeScales\\nAnotherwaytoobtainpathsonwhichtheproductofderivativesisclosetooneisto\\nhaveunitswith l i ne a rself-connectionsandaweightnearoneontheseconnections.\\nWhenweaccumulatearunningaverage µ( ) tofsomevalue v( ) tbyapplyingthe\\nupdate µ( ) t← α µ( 1 ) t −+(1− α) v( ) tthe αparameterisanexampleofalinearself-\\nconnectionfrom µ( 1 ) t −to µ( ) t.When αisnearone,therunningaverageremembers\\ninformationaboutthepastforalongtime,andwhen αisnearzero,information\\naboutthepastisrapidlydiscarded.Hiddenunitswithlinearself-connectionscan\\nbehavesimilarlytosuchrunningaverages.Suchhiddenunitsarecalledleaky\\nunits.\\nSkipconnectionsthrough dtimestepsareawayofensuringthataunitcan\\nalwayslearntobeinﬂuencedbyavaluefrom dtimestepsearlier.Theuseofa\\nlinearself-connectionwithaweightnearoneisadiﬀerentwayofensuringthatthe\\nunitcanaccessvaluesfromthepast.Thelinearself-connectionapproachallows\\nthiseﬀecttobeadaptedmoresmoothlyandﬂexiblybyadjustingthereal-valued\\nαratherthanbyadjustingtheinteger-valuedskiplength.\\nTheseideaswereproposedby()andby (). Mozer1992 ElHihiandBengio1996\\nLeakyunitswerealsofoundtobeusefulinthecontextofechostatenetworks\\n(,). Jaeger e t a l .2007\\n4 0 7', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nTherearetwobasicstrategiesforsettingthetimeconstantsusedbyleaky\\nunits.\\xa0Onestrategyistomanuallyﬁxthemtovaluesthatremainconstant,for\\nexamplebysamplingtheirvaluesfromsomedistributiononceatinitialization time.\\nAnotherstrategyistomakethetimeconstantsfreeparametersandlearnthem.\\nHavingsuchleakyunitsatdiﬀerenttimescalesappearstohelpwithlong-term\\ndependencies(,;Mozer1992Pascanu2013 e t a l .,).\\n10.9.3RemovingConnections\\nAnotherapproachtohandlelong-termdependenciesistheideaoforganizing\\nthestateoftheRNNatmultipletime-scales( ,),with ElHihiandBengio1996\\ninformationﬂowingmoreeasilythroughlongdistancesattheslowertimescales.\\nThisideadiﬀersfromtheskipconnectionsthroughtimediscussedearlier\\nbecauseitinvolvesactively r e m o v i nglength-oneconnectionsandreplacingthem\\nwithlongerconnections.Unitsmodiﬁedinsuchawayareforcedtooperateona\\nlongtimescale.Skipconnectionsthroughtimeedges.Unitsreceivingsuch a d d\\nnewconnectionsmaylearntooperateonalongtimescalebutmayalsochooseto\\nfocusontheirothershort-termconnections.\\nTherearediﬀerentwaysinwhichagroupofrecurrentunitscanbeforcedto\\noperateatdiﬀerenttimescales.Oneoptionistomaketherecurrentunitsleaky,\\nbuttohavediﬀerentgroupsofunitsassociatedwithdiﬀerentﬁxedtimescales.\\nThiswastheproposalin()andhasbeensuccessfullyusedin Mozer1992 Pascanu\\ne t a l .().Anotheroptionistohaveexplicitanddiscreteupdatestakingplace 2013\\natdiﬀerenttimes,withadiﬀerentfrequencyfordiﬀerentgroupsofunits.Thisis\\ntheapproachof ()and ElHihiandBengio1996Koutnik 2014 e t a l .().Itworked\\nwellonanumberofbenchmarkdatasets.\\n10.10TheLongShort-TermMemoryandOtherGated\\nRNNs\\nAsofthiswriting,themosteﬀectivesequencemodelsusedinpracticalapplications\\narecalledgatedRNNs.Theseincludethelongshort-termmemoryand\\nnetworksbasedonthe . gatedrecurrentunit\\nLikeleakyunits,gatedRNNsarebasedontheideaofcreatingpathsthrough\\ntimethathavederivativesthatneithervanishnorexplode.Leakyunits\\xa0did\\nthiswithconnectionweightsthatwereeithermanuallychosenconstantsorwere\\nparameters.GatedRNNsgeneralizethistoconnectionweightsthatmaychange\\n4 0 8', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nateachtimestep.\\n×\\ni nput i nput\\xa0gate f or ge t \\xa0 gate output\\xa0gateoutput\\ns t at es e l f - l oop×\\n+ ×\\nFigure10.16:BlockdiagramoftheLSTMrecurrentnetwork“cell.”Cellsareconnected\\nrecurrentlytoeachother,replacingtheusualhiddenunitsofordinaryrecurrentnetworks.\\nAninputfeatureiscomputedwitharegularartiﬁcialneuronunit.Itsvaluecanbe\\naccumulatedintothestateifthesigmoidalinputgateallowsit.Thestateunithasa\\nlinearself-loopwhoseweightiscontrolledbytheforgetgate.Theoutputofthecellcan\\nbeshutoﬀbytheoutputgate.Allthegatingunitshaveasigmoidnonlinearity,whilethe\\ninputunitcanhaveanysquashingnonlinearity.\\xa0Thestateunitcanalsobeusedasan\\nextrainputtothegatingunits.Theblacksquareindicatesadelayofasingletimestep.\\nLeakyunitsallowthenetworkto a c c u m u l a t einformation(suchasevidence\\nforaparticularfeatureorcategory)overalongduration.However,oncethat\\ninformationhasbeenused,itmightbeusefulfortheneuralnetworkto f o r g e tthe\\noldstate.Forexample,ifasequenceismadeofsub-sequencesandwewantaleaky\\nunittoaccumulateevidenceinsideeachsub-subsequence,weneedamechanismto\\nforgettheoldstatebysettingittozero.Insteadofmanuallydecidingwhento\\nclearthestate,wewanttheneuralnetworktolearntodecidewhentodoit.This\\n4 0 9', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\niswhatgatedRNNsdo.\\n10.10.1LSTM\\nThecleverideaofintroducingself-loopstoproducepathswherethegradient\\ncanﬂowforlongdurationsisacorecontributionoftheinitiallongshort-term\\nmemory(LSTM)model(HochreiterandSchmidhuber1997,).Acrucialaddition\\nhasbeentomaketheweightonthisself-loopconditionedonthecontext,ratherthan\\nﬁxed(,).Bymakingtheweightofthisself-loopgated(controlled Gers e t a l .2000\\nbyanotherhiddenunit),thetimescaleofintegrationcanbechangeddynamically.\\nInthiscase,wemeanthatevenforanLSTMwithﬁxedparameters,thetimescale\\nofintegrationcanchangebasedontheinputsequence,becausethetimeconstants\\nareoutputbythemodelitself.TheLSTMhasbeenfoundextremelysuccessful\\ninmanyapplications,\\xa0suchasunconstrainedhandwriting recognition(Graves\\ne t a l .,),speechrecognition( 2009 Graves2013GravesandJaitly2014 e t a l .,; ,),\\nhandwritinggeneration(Graves2013,),machinetranslation(Sutskever2014 e t a l .,),\\nimagecaptioning(,; Kiros e t a l .2014bVinyals2014bXu2015 e t a l .,; e t a l .,)and\\nparsing(Vinyals2014a e t a l .,).\\nTheLSTMblockdiagramisillustratedinﬁgure.Thecorresponding 10.16\\nforwardpropagationequationsaregivenbelow,inthecaseofashallowrecurrent\\nnetworkarchitecture. Deeperarchitectures havealsobeensuccessfullyused(Graves\\ne t a l .,;2013Pascanu2014a e t a l .,).Insteadofaunitthatsimplyappliesanelement-\\nwisenonlinearitytotheaﬃnetransformationofinputsandrecurrentunits,LSTM\\nrecurrentnetworkshave“LSTMcells”thathaveaninternalrecurrence(aself-loop),\\ninadditiontotheouterrecurrenceoftheRNN.Eachcellhasthesameinputs\\nandoutputsasanordinaryrecurrentnetwork,buthasmoreparametersanda\\nsystemofgatingunitsthatcontrolstheﬂowofinformation. Themostimportant\\ncomponentisthestateunit s( ) t\\nithathasalinearself-loopsimilartotheleaky\\nunitsdescribedintheprevioussection.However,here,theself-loopweight(orthe\\nassociatedtimeconstant)iscontrolledbyaforgetgateunit f( ) t\\ni(fortimestep t\\nandcell),thatsetsthisweighttoavaluebetween0and1viaasigmoidunit: i\\nf( ) t\\ni= σ\\uf8eb\\n\\uf8ed bf\\ni+\\ue058\\njUf\\ni , j x( ) t\\nj+\\ue058\\njWf\\ni , j h( 1 ) t −\\nj\\uf8f6\\n\\uf8f8 ,(10.40)\\nwherex( ) tisthecurrentinputvectorandh( ) tisthecurrenthiddenlayervector,\\ncontainingtheoutputsofalltheLSTMcells,andbf,Uf,Wfarerespectively\\nbiases,inputweightsandrecurrentweightsfortheforgetgates.TheLSTMcell\\n4 1 0', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\ninternalstateisthusupdatedasfollows,butwithaconditionalself-loopweight\\nf( ) t\\ni:\\ns( ) t\\ni= f( ) t\\ni s( 1 ) t −\\ni + g( ) t\\ni σ\\uf8eb\\n\\uf8ed b i+\\ue058\\njU i , j x( ) t\\nj+\\ue058\\njW i , j h( 1 ) t −\\nj\\uf8f6\\n\\uf8f8 ,(10.41)\\nwhereb,UandWrespectivelydenotethebiases,inputweightsandrecurrent\\nweightsintotheLSTMcell.Theexternalinputgateunit g( ) t\\niiscomputed\\nsimilarlytotheforgetgate(withasigmoidunittoobtainagatingvaluebetween\\n0and1),butwithitsownparameters:\\ng( ) t\\ni= σ\\uf8eb\\n\\uf8ed bg\\ni+\\ue058\\njUg\\ni , j x( ) t\\nj+\\ue058\\njWg\\ni , j h( 1 ) t −\\nj\\uf8f6\\n\\uf8f8 .(10.42)\\nTheoutput h( ) t\\nioftheLSTMcellcanalsobeshutoﬀ,viatheoutputgate q( ) t\\ni,\\nwhichalsousesasigmoidunitforgating:\\nh( ) t\\ni= tanh\\ue010\\ns( ) t\\ni\\ue011\\nq( ) t\\ni (10.43)\\nq( ) t\\ni= σ\\uf8eb\\n\\uf8ed bo\\ni+\\ue058\\njUo\\ni , j x( ) t\\nj+\\ue058\\njWo\\ni , j h( 1 ) t −\\nj\\uf8f6\\n\\uf8f8 (10.44)\\nwhichhasparametersbo,Uo,Woforitsbiases,inputweightsandrecurrent\\nweights,respectively.Amongthevariants,onecanchoosetousethecellstate s( ) t\\ni\\nasanextrainput(withitsweight)intothethreegatesofthe i-thunit,asshown\\ninﬁgure.Thiswouldrequirethreeadditionalparameters. 10.16\\nLSTMnetworkshavebeenshowntolearnlong-termdependenciesmoreeasily\\nthanthesimplerecurrentarchitectures,ﬁrstonartiﬁcialdatasetsdesignedfor\\ntestingtheabilitytolearnlong-termdependencies( ,; Bengio e t a l .1994Hochreiter\\nandSchmidhuber1997Hochreiter 2001 ,; e t a l .,),thenonchallengingsequence\\nprocessingtaskswherestate-of-the-art performance wasobtained(Graves2012,;\\nGraves2013Sutskever2014 e t a l .,; e t a l .,).VariantsandalternativestotheLSTM\\nhavebeenstudiedandusedandarediscussednext.\\n10.10.2OtherGatedRNNs\\nWhichpieces\\xa0ofthe\\xa0LSTMarchitecture are\\xa0actually necessary?Whatother\\nsuccessfularchitecturescouldbedesignedthatallowthenetworktodynamically\\ncontrolthetimescaleandforgettingbehaviorofdiﬀerentunits?\\n4 1 1', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nSomeanswerstothesequestionsaregivenwiththerecentworkongatedRNNs,\\nwhoseunitsarealsoknownasgatedrecurrentunitsorGRUs(,; Cho e t a l .2014b\\nChung20142015aJozefowicz2015Chrupala 2015 e t a l .,,; e t a l .,; e t a l .,).Themain\\ndiﬀerencewiththeLSTMisthatasinglegatingunitsimultaneouslycontrolsthe\\nforgettingfactorandthedecisiontoupdatethestateunit.Theupdateequations\\narethefollowing:\\nh( ) t\\ni= u( 1 ) t −\\ni h( 1 ) t −\\ni+(1− u( 1 ) t −\\ni) σ\\uf8eb\\n\\uf8ed b i+\\ue058\\njU i , j x( 1 ) t −\\nj +\\ue058\\njW i , j r( 1 ) t −\\nj h( 1 ) t −\\nj\\uf8f6\\n\\uf8f8 ,\\n(10.45)\\nwhereustandsfor“update”gateandrfor“reset”gate.Theirvalueisdeﬁnedas\\nusual:\\nu( ) t\\ni= σ\\uf8eb\\n\\uf8ed bu\\ni+\\ue058\\njUu\\ni , j x( ) t\\nj+\\ue058\\njWu\\ni , j h( ) t\\nj\\uf8f6\\n\\uf8f8 (10.46)\\nand\\nr( ) t\\ni= σ\\uf8eb\\n\\uf8ed br\\ni+\\ue058\\njUr\\ni , j x( ) t\\nj+\\ue058\\njWr\\ni , j h( ) t\\nj\\uf8f6\\n\\uf8f8 .(10.47)\\nTheresetandupdatesgatescanindividually“ignore”partsofthestatevector.\\nTheupdategatesactlikeconditionalleakyintegratorsthatcanlinearlygateany\\ndimension,thuschoosingtocopyit(atoneextremeofthesigmoid)orcompletely\\nignoreit(attheotherextreme)byreplacingitbythenew“targetstate”value\\n(towardswhichtheleakyintegratorwantstoconverge).Theresetgatescontrol\\nwhichpartsofthestategetusedtocomputethenexttargetstate,introducingan\\nadditionalnonlineareﬀectintherelationshipbetweenpaststateandfuturestate.\\nManymorevariantsaroundthisthemecanbedesigned.Forexamplethe\\nresetgate(orforgetgate)outputcouldbesharedacrossmultiplehiddenunits.\\nAlternately,theproductofaglobalgate(coveringawholegroupofunits,suchas\\nanentirelayer)andalocalgate(perunit)couldbeusedtocombineglobalcontrol\\nandlocalcontrol.However,severalinvestigationsoverarchitectural variations\\noftheLSTMandGRUfoundnovariantthatwouldclearlybeatbothofthese\\nacrossawiderangeoftasks(,; Greﬀ e t a l .2015Jozefowicz2015Greﬀ e t a l .,).\\ne t a l .()foundthatacrucialingredientistheforgetgate,while 2015 Jozefowicz\\ne t a l .()foundthataddingabiasof1totheLSTMforgetgate,apractice 2015\\nadvocatedby (),makestheLSTMasstrongasthebestofthe Gers e t a l .2000\\nexploredarchitecturalvariants.\\n4 1 2', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\n10.11OptimizationforLong-TermDependencies\\nSection andsectionhavedescribedthevanishingandexplodinggradient 8.2.5 10.7\\nproblemsthatoccurwhenoptimizingRNNsovermanytimesteps.\\nAninterestingideaproposedbyMartensandSutskever2011()isthatsecond\\nderivativesmayvanishatthesametimethatﬁrstderivativesvanish.Second-order\\noptimization algorithmsmayroughlybeunderstoodasdividingtheﬁrstderivative\\nbythesecondderivative(inhigherdimension,multiplyingthegradientbythe\\ninverseHessian).Ifthesecondderivativeshrinksatasimilarratetotheﬁrst\\nderivative,thentheratioofﬁrstandsecondderivativesmayremainrelatively\\nconstant.Unfortunately,second-ordermethodshavemanydrawbacks,including\\nhighcomputational cost,theneedforalargeminibatch,andatendencytobe\\nattractedtosaddlepoints.MartensandSutskever2011()foundpromisingresults\\nusingsecond-ordermethods.Later,Sutskever2013 e t a l .()foundthatsimpler\\nmethodssuchasNesterovmomentumwithcarefulinitialization couldachieve\\nsimilarresults.SeeSutskever2012()formoredetail.\\xa0Bothoftheseapproaches\\nhavelargelybeenreplacedbysimplyusingSGD(evenwithoutmomentum)applied\\ntoLSTMs.Thisispartofacontinuingthemeinmachinelearningthatitisoften\\nmucheasiertodesignamodelthatiseasytooptimizethanitistodesignamore\\npowerfuloptimization algorithm.\\n10.11.1ClippingGradients\\nAsdiscussedinsection,stronglynonlinearfunctionssuchasthosecomputed 8.2.4\\nbyarecurrentnetovermanytimestepstendtohavederivativesthatcanbe\\neitherverylargeorverysmallinmagnitude.Thisisillustratedinﬁgureand8.3\\nﬁgure,inwhichweseethattheobjectivefunction(asafunctionofthe 10.17\\nparameters)hasa“landscape”\\xa0inwhichoneﬁnds“cliﬀs”:wideandratherﬂat\\nregionsseparatedbytinyregionswheretheobjectivefunctionchangesquickly,\\nformingakindofcliﬀ.\\nThediﬃcultythatarisesisthatwhentheparametergradientisverylarge,a\\ngradientdescentparameterupdatecouldthrowtheparametersveryfar,intoa\\nregionwheretheobjectivefunctionislarger,undoingmuchoftheworkthathad\\nbeendonetoreachthecurrentsolution.Thegradienttellsusthedirectionthat\\ncorrespondstothesteepestdescentwithinaninﬁnitesimalregionsurroundingthe\\ncurrentparameters.Outsideofthisinﬁnitesimalregion,thecostfunctionmay\\nbegintocurvebackupwards.Theupdatemustbechosentobesmallenoughto\\navoidtraversingtoomuchupwardcurvature.Wetypicallyuselearningratesthat\\n4 1 3', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\ndecayslowlyenoughthatconsecutivestepshaveapproximatelythesamelearning\\nrate.Astepsizethatisappropriateforarelativelylinearpartofthelandscapeis\\nofteninappropriate andcausesuphillmotionifweenteramorecurvedpartofthe\\nlandscapeonthenextstep.\\n\\ue077\\n\\ue062\\ue04a\\ue077\\ue03b \\ue062\\n\\ue028\\ue029\\ue057 \\ue069 \\ue074 \\ue068 \\ue06f \\ue075 \\ue074 \\ue020 \\ue063 \\ue06c \\ue069 \\ue070 \\ue070 \\ue069 \\ue06e \\ue067\\n\\ue077\\n\\ue062\\ue04a\\ue077\\ue03b \\ue062\\n\\ue028\\ue029\\ue057 \\ue069 \\ue074 \\ue068 \\ue020 \\ue063 \\ue06c \\ue069 \\ue070 \\ue070 \\ue069 \\ue06e \\ue067\\nFigure10.17:Exampleoftheeﬀectofgradientclippinginarecurrentnetworkwith\\ntwoparameterswandb.Gradientclippingcanmakegradientdescentperformmore\\nreasonablyinthevicinityofextremelysteepcliﬀs.Thesesteepcliﬀscommonlyoccur\\ninrecurrentnetworksnearwherearecurrentnetworkbehavesapproximatelylinearly.\\nThecliﬀisexponentiallysteepinthenumberoftimestepsbecausetheweightmatrix\\nismultipliedbyitselfonceforeachtimestep. ( L e f t )Gradientdescentwithoutgradient\\nclippingovershootsthebottomofthissmallravine,thenreceivesaverylargegradient\\nfromthecliﬀface.Thelargegradientcatastrophicallypropelstheparametersoutsidethe\\naxesoftheplot.Gradientdescentwithgradientclippinghasamoremoderate ( R i g h t )\\nreactiontothecliﬀ.Whileitdoesascendthecliﬀface,thestepsizeisrestrictedsothat\\nitcannotbepropelledawayfromsteepregionnearthesolution.Figureadaptedwith\\npermissionfromPascanu2013 e t a l .().\\nAsimpletypeofsolutionhasbeeninusebypractitioners formanyyears:\\nclippingthegradient.Therearediﬀerentinstancesofthisidea(Mikolov2012,;\\nPascanu2013 e t a l .,).Oneoptionistocliptheparametergradientfromaminibatch\\ne l e m e nt - w i s e(Mikolov2012,)justbeforetheparameterupdate.Anotheristo c l i p\\nt h e norm ||||g o f t h e g r a d i e ntg(Pascanu2013 e t a l .,)justbeforetheparameter\\nupdate:\\nif||||g > v (10.48)\\ng←g v\\n||||g(10.49)\\n4 1 4', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nwhere visthenormthresholdandgisusedtoupdateparameters.Becausethe\\ngradientofalltheparameters(includingdiﬀerentgroupsofparameters,suchas\\nweightsandbiases)isrenormalizedjointlywithasinglescalingfactor,thelatter\\nmethodhastheadvantagethatitguaranteesthateachstepisstillinthegradient\\ndirection,butexperimentssuggestthatbothformsworksimilarly.Although\\ntheparameterupdatehasthesamedirectionasthetruegradient,withgradient\\nnormclipping,theparameterupdatevectornormisnowbounded.Thisbounded\\ngradientavoidsperformingadetrimentalstepwhenthegradientexplodes.In\\nfact,evensimplytakinga r a ndom s t e pwhenthegradientmagnitudeisabove\\nathresholdtendstoworkalmostaswell.Iftheexplosionissoseverethatthe\\ngradientisnumerically InforNan(consideredinﬁniteornot-a-number),then\\narandomstepofsize vcanbetakenandwilltypicallymoveawayfromthe\\nnumericallyunstableconﬁguration. Clippingthegradientnormper-minibatchwill\\nnotchangethedirectionofthegradientforanindividualminibatch.However,\\ntakingtheaverageofthenorm-clippedgradientfrommanyminibatchesisnot\\nequivalenttoclippingthenormofthetruegradient(thegradientformedfrom\\nusingallexamples).Examplesthathavelargegradientnorm,aswellasexamples\\nthatappearinthesameminibatchassuchexamples,willhavetheircontribution\\ntotheﬁnaldirectiondiminished.Thisstandsincontrasttotraditionalminibatch\\ngradientdescent,wherethetruegradientdirectionisequaltotheaverageoverall\\nminibatchgradients.Putanotherway,traditionalstochasticgradientdescentuses\\nanunbiasedestimateofthegradient,whilegradientdescentwithnormclipping\\nintroducesaheuristicbiasthatweknowempiricallytobeuseful.Withelement-\\nwiseclipping,thedirectionoftheupdateisnotalignedwiththetruegradient\\northeminibatchgradient,butitisstilladescentdirection.Ithasalsobeen\\nproposed(Graves2013,)tocliptheback-propagatedgradient(withrespectto\\nhiddenunits)butnocomparisonhasbeenpublishedbetweenthesevariants;we\\nconjecturethatallthesemethodsbehavesimilarly.\\n10.11.2RegularizingtoEncourageInformationFlow\\nGradientclippinghelpstodealwithexplodinggradients,butitdoesnothelpwith\\nvanishinggradients.Toaddressvanishinggradientsandbettercapturelong-term\\ndependencies,wediscussedtheideaofcreatingpathsinthecomputational graphof\\ntheunfoldedrecurrentarchitecturealongwhichtheproductofgradientsassociated\\nwitharcsisnear1.OneapproachtoachievethisiswithLSTMsandotherself-\\nloopsandgatingmechanisms,describedaboveinsection.Anotherideais 10.10\\ntoregularizeorconstraintheparameterssoastoencourage“informationﬂow.”\\nInparticular,wewouldlikethegradientvector∇h( ) t Lbeingback-propagatedto\\n4 1 5', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nmaintainitsmagnitude,evenifthelossfunctiononlypenalizestheoutputatthe\\nendofthesequence.Formally,wewant\\n(∇h( ) t L)∂h( ) t\\n∂h( 1 ) t −(10.50)\\ntobeaslargeas\\n∇h( ) t L. (10.51)\\nWiththisobjective,Pascanu2013 e t a l .()proposethefollowingregularizer:\\nΩ =\\ue058\\nt\\uf8eb\\n\\uf8ed\\ue00c\\ue00c\\ue00c|∇(h( ) t L)∂ h( ) t\\n∂ h( 1 ) t −\\ue00c\\ue00c\\ue00c|\\n||∇h( ) t L||−1\\uf8f6\\n\\uf8f82\\n. (10.52)\\nComputingthegradientofthisregularizermayappeardiﬃcult,butPascanu\\ne t a l .()proposeanapproximation inwhichweconsidertheback-propagated 2013\\nvectors∇h( ) t Lasiftheywereconstants(forthepurposeofthisregularizer,so\\nthatthereisnoneedtoback-propagatethroughthem).Theexperimentswith\\nthisregularizersuggestthat,ifcombinedwiththenormclippingheuristic(which\\nhandlesgradientexplosion),theregularizercanconsiderablyincreasethespanof\\nthedependenciesthatanRNNcanlearn.\\xa0BecauseitkeepstheRNNdynamics\\nontheedgeofexplosivegradients,thegradientclippingisparticularlyimportant.\\nWithoutgradientclipping,gradientexplosionpreventslearningfromsucceeding.\\nAkeyweaknessofthisapproachisthatitisnotaseﬀectiveastheLSTMfor\\ntaskswheredataisabundant,suchaslanguagemodeling.\\n10.12ExplicitMemory\\nIntelligencerequiresknowledgeandacquiringknowledgecanbedonevialearning,\\nwhichhasmotivatedthedevelopmentoflarge-scaledeeparchitectures.However,\\ntherearediﬀerentkindsofknowledge.Someknowledgecanbeimplicit,sub-\\nconscious,anddiﬃculttoverbalize—suchashowtowalk,orhowadoglooks\\ndiﬀerentfromacat.Otherknowledgecanbeexplicit,declarative,andrelatively\\nstraightforwardtoputintowords—everydaycommonsense knowledge,like“acat\\nisakindofanimal,”orveryspeciﬁcfactsthatyouneedtoknowtoaccomplish\\nyourcurrentgoals,like“themeetingwiththesalesteamisat3:00PMinroom\\n141.”\\nNeuralnetworksexcelatstoringimplicitknowledge.However,theystruggleto\\nmemorizefacts.\\xa0Stochasticgradientdescentrequiresmanypresentationsofthe\\n4 1 6', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nT ask \\xa0 ne t w or k ,\\nc ontrol l i ng\\xa0th e \\xa0 m e m o r yMe m or y \\xa0 c e l l s\\nW r i t i ng\\nm e c hani s mR e adi ng\\nm e c hani s m\\nFigure10.18:Aschematicofanexampleofanetworkwithanexplicitmemory,capturing\\nsomeofthekeydesignelementsoftheneuralTuringmachine.Inthisdiagramwe\\ndistinguishthe“representation”partofthemodel(the“tasknetwork,”herearecurrent\\nnetinthebottom)fromthe“memory”partofthemodel(thesetofcells),whichcan\\nstorefacts.Thetasknetworklearnsto“control”thememory,decidingwheretoreadfrom\\nandwheretowritetowithinthememory(throughthereadingandwritingmechanisms,\\nindicatedbyboldarrowspointingatthereadingandwritingaddresses).\\n4 1 7', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nsameinputbeforeitcanbestoredinaneuralnetworkparameters,andeventhen,\\nthatinputwillnotbestoredespeciallyprecisely.Graves2014b e t a l .()hypothesized\\nthatthisisbecauseneuralnetworkslacktheequivalentoftheworkingmemory\\nsystemthatallowshumanbeingstoexplicitlyholdandmanipulatepiecesof\\ninformationthat\\xa0arerelevantto\\xa0achieving\\xa0some goal.Suchexplicit\\xa0memory\\ncomponentswouldallowoursystemsnotonlytorapidlyand“intentionally”store\\nandretrievespeciﬁcfactsbutalsotosequentiallyreasonwiththem.Theneed\\nforneuralnetworksthatcanprocessinformationinasequenceofsteps,changing\\nthewaytheinputisfedintothenetworkateachstep,haslongbeenrecognized\\nasimportantfortheabilitytoreasonratherthantomakeautomatic,intuitive\\nresponsestotheinput(,). Hinton1990\\nToresolvethisdiﬃculty,Weston2014 e t a l .()introducedmemorynetworks\\nthatincludeasetofmemorycellsthatcanbeaccessedviaanaddressingmecha-\\nnism.Memorynetworksoriginallyrequiredasupervisionsignalinstructingthem\\nhowtousetheirmemorycells.Graves2014b e t a l .()introducedtheneural\\nTuringmachine,whichisabletolearntoreadfromandwritearbitrarycontent\\ntomemorycellswithoutexplicitsupervisionaboutwhichactionstoundertake,\\nandallowedend-to-endtrainingwithoutthissupervisionsignal,viatheuseof\\nacontent-basedsoftattentionmechanism(see ()andsec- Bahdanau e t a l .2015\\ntion).Thissoftaddressingmechanismhasbecomestandardwithother 12.4.5.1\\nrelatedarchitecturesemulatingalgorithmicmechanismsinawaythatstillallows\\ngradient-basedoptimization ( ,; Sukhbaatar e t a l .2015JoulinandMikolov2015,;\\nKumar 2015Vinyals2015aGrefenstette2015 e t a l .,; e t a l .,; e t a l .,).\\nEachmemorycellcanbethoughtofasanextensionofthememorycellsin\\nLSTMsandGRUs.Thediﬀerenceisthatthenetworkoutputsaninternalstate\\nthatchooseswhichcelltoreadfromorwriteto,justasmemoryaccessesina\\ndigitalcomputerreadfromorwritetoaspeciﬁcaddress.\\nItisdiﬃculttooptimizefunctionsthatproduceexact,integeraddresses.To\\nalleviatethisproblem,NTMsactuallyreadtoorwritefrommanymemorycells\\nsimultaneously.Toread,theytakeaweightedaverageofmanycells.Towrite,they\\nmodifymultiplecellsbydiﬀerentamounts.Thecoeﬃcientsfortheseoperations\\narechosentobefocusedonasmallnumberofcells,forexample,byproducing\\nthemviaasoftmaxfunction.Usingtheseweightswithnon-zeroderivativesallows\\nthefunctionscontrollingaccesstothememorytobeoptimizedusinggradient\\ndescent.Thegradientonthesecoeﬃcientsindicateswhethereachofthemshould\\nbeincreasedordecreased,butthegradientwilltypicallybelargeonlyforthose\\nmemoryaddressesreceivingalargecoeﬃcient.\\nThesememorycellsaretypicallyaugmentedtocontainavector,ratherthan\\n4 1 8', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nthesinglescalarstoredbyanLSTMorGRUmemorycell.Therearetworeasons\\ntoincreasethesizeofthememorycell.Onereasonisthatwehaveincreasedthe\\ncostofaccessingamemorycell.\\xa0Wepaythecomputational costofproducinga\\ncoeﬃcientformanycells,butweexpectthesecoeﬃcientstoclusteraroundasmall\\nnumberofcells.Byreadingavectorvalue,ratherthanascalarvalue,wecan\\noﬀsetsomeofthiscost.Anotherreasontousevector-valuedmemorycellsisthat\\ntheyallowforcontent-basedaddressing,wheretheweightusedtoreadtoor\\nwritefromacellisafunctionofthatcell.Vector-valuedcellsallowustoretrievea\\ncompletevector-valuedmemoryifweareabletoproduceapatternthatmatches\\nsomebutnotallofitselements.Thisisanalogoustothewaythatpeoplecan\\nrecallthelyricsofasongbasedonafewwords.Wecanthinkofacontent-based\\nreadinstructionassaying,“Retrievethelyricsofthesongthathasthechorus‘We\\nallliveinayellowsubmarine.’”Content-basedaddressingismoreusefulwhenwe\\nmaketheobjectstoberetrievedlarge—ifeveryletterofthesongwasstoredina\\nseparatememorycell,wewouldnotbeabletoﬁndthemthisway.Bycomparison,\\nlocation-basedaddressingisnotallowedtorefertothecontentofthememory.\\nWecanthinkofalocation-basedreadinstructionassaying“Retrievethelyricsof\\nthesonginslot347.”Location-basedaddressingcanoftenbeaperfectlysensible\\nmechanismevenwhenthememorycellsaresmall.\\nIfthecontentofamemorycelliscopied(notforgotten)atmosttimesteps,then\\ntheinformationitcontainscanbepropagatedforwardintimeandthegradients\\npropagatedbackwardintimewithouteithervanishingorexploding.\\nTheexplicitmemoryapproachisillustratedinﬁgure,whereweseethat 10.18\\na“taskneuralnetwork”\\xa0iscoupledwithamemory.Althoughthattaskneural\\nnetworkcouldbefeedforwardorrecurrent,theoverallsystemisarecurrentnetwork.\\nThetasknetworkcanchoosetoreadfromorwritetospeciﬁcmemoryaddresses.\\nExplicitmemoryseemstoallowmodelstolearntasksthatordinaryRNNsorLSTM\\nRNNscannotlearn.Onereasonforthisadvantagemaybebecauseinformationand\\ngradientscanbepropagated(forwardintimeorbackwardsintime,respectively)\\nforverylongdurations.\\nAsanalternativetoback-propagationthroughweightedaveragesofmemory\\ncells,wecaninterpretthememoryaddressingcoeﬃcientsasprobabilities and\\nstochasticallyreadjustonecell(ZarembaandSutskever2015,).Optimizingmodels\\nthatmakediscretedecisionsrequiresspecializedoptimization algorithms,described\\ninsection.Sofar,trainingthesestochasticarchitectures thatmakediscrete 20.9.1\\ndecisionsremainsharderthantrainingdeterministicalgorithmsthatmakesoft\\ndecisions.\\nWhetheritissoft(allowingback-propagation) orstochasticandhard,the\\n4 1 9', 'CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nmechanism\\xa0forchoosing\\xa0anaddress\\xa0isin\\xa0itsform\\xa0identical\\xa0totheattention\\nmechanismwhichhadbeenpreviouslyintroducedinthecontextofmachine\\ntranslation( ,)anddiscussedinsection.\\xa0Theidea Bahdanau e t a l .2015 12.4.5.1\\nofattentionmechanismsforneuralnetworkswasintroducedevenearlier,inthe\\ncontextofhandwritinggeneration(Graves2013,),withanattentionmechanism\\nthatwasconstrainedtomoveonlyforwardintimethroughthesequence.In\\nthecaseofmachinetranslationandmemorynetworks,ateachstep,thefocusof\\nattentioncanmovetoacompletelydiﬀerentplace,comparedtothepreviousstep.\\nRecurrentneuralnetworksprovideawaytoextenddeeplearningtosequential\\ndata.Theyarethelastmajortoolinourdeeplearningtoolbox.Ourdiscussionnow\\nmovestohowtochooseandusethesetoolsandhowtoapplythemtoreal-world\\ntasks.\\n4 2 0']}, 'DeepLearning-chapter-9.pdf': {'text': ['C h a p t e r 9\\nC on v ol u t i on al N e t w orks\\nCon v o l ut i o na l net w o r k s(,),alsoknownas LeCun1989 c o n v o l ut i o na l neur al\\nnet w o r k sorCNNs,areaspecializedkindofneuralnetworkforprocessingdata\\nthathasaknown,grid-liketopology.Examplesincludetime-seriesdata,whichcan\\nbethoughtofasa1Dgridtakingsamplesatregulartimeintervals,andimagedata,\\nwhichcanbethoughtofasa2Dgridofpixels.Convolutionalnetworkshavebeen\\ntremendouslysuccessfulinpracticalapplications.Thename“convolutionalneural\\nnetwork”indicatesthatthenetworkemploysamathematical operationcalled\\nc o n v o l ut i o n.Convolutionisaspecializedkindoflinearoperation.Convolutional\\nnetworksaresimplyneuralnetworksthatuseconvolutioninplaceofgeneralmatrix\\nmultiplicationinatleastoneoftheirlayers.\\nInthis\\xa0chapter,\\xa0wewillﬁrst\\xa0describewhatconvolutionis.Next,\\xa0wewill\\nexplainthemotivationbehindusingconvolutioninaneuralnetwork.Wewillthen\\ndescribeanoperationcalled p o o l i ng,whichalmostallconvolutionalnetworks\\nemploy.Usually,theoperationusedinaconvolutionalneuralnetworkdoesnot\\ncorrespondpreciselytothedeﬁnitionofconvolutionasusedinotherﬁeldssuch\\nasengineeringorpuremathematics.Wewilldescribeseveralvariantsonthe\\nconvolutionfunctionthatarewidelyusedinpracticeforneuralnetworks.We\\nwillalso\\xa0show\\xa0how\\xa0convolutionmaybeappliedtomanykindsofdata,\\xa0with\\ndiﬀerentnumbersofdimensions.Wethendiscussmeansofmakingconvolution\\nmoreeﬃcient.Convolutionalnetworksstandoutasanexampleofneuroscientiﬁc\\nprinciplesinﬂuencingdeeplearning.Wewilldiscusstheseneuroscientiﬁcprinciples,\\nthenconcludewithcommentsabouttheroleconvolutionalnetworkshaveplayed\\ninthehistoryofdeeplearning.Onetopicthischapterdoesnotaddressishowto\\nchoosethearchitectureofyourconvolutionalnetwork.Thegoalofthischapteris\\ntodescribethekindsoftoolsthatconvolutionalnetworksprovide,whilechapter11\\n330', 'CHAPTER9.CONVOLUTIONALNETWORKS\\ndescribesgeneralguidelinesforchoosingwhichtoolstouseinwhichcircumstances.\\nResearchintoconvolutionalnetworkarchitecturesproceedssorapidlythatanew\\nbestarchitectureforagivenbenchmarkisannouncedeveryfewweekstomonths,\\nrenderingitimpracticaltodescribethebestarchitectureinprint.However,the\\nbestarchitectureshaveconsistentlybeencomposedofthebuildingblocksdescribed\\nhere.\\n9.1TheConvolutionOperation\\nInitsmostgeneralform,convolutionisanoperationontwofunctionsofareal-\\nvaluedargument.Tomotivatethedeﬁnitionofconvolution,westartwithexamples\\noftwofunctionswemightuse.\\nSupposewearetrackingthelocationofaspaceshipwithalasersensor.Our\\nlasersensorprovidesasingleoutput x( t),thepositionofthespaceshipattime\\nt.Both xand tarereal-valued,i.e.,wecangetadiﬀerentreadingfromthelaser\\nsensoratanyinstantintime.\\nNowsupposethatourlasersensorissomewhatnoisy.Toobtainalessnoisy\\nestimateofthespaceship’sposition,wewouldliketoaveragetogetherseveral\\nmeasurements.Ofcourse,morerecentmeasurementsaremorerelevant,sowewill\\nwantthistobeaweightedaveragethatgivesmoreweighttorecentmeasurements.\\nWecandothiswithaweightingfunction w( a),where aistheageofameasurement.\\nIfweapplysuchaweightedaverageoperationateverymoment,weobtainanew\\nfunctionprovidingasmoothedestimateofthepositionofthespaceship: s\\ns t() =\\ue05a\\nx a w t a d a ()( −) (9.1)\\nThisoperationiscalled c o n v o l ut i o n.Theconvolutionoperationistypically\\ndenotedwithanasterisk:\\ns t x w t () = ( ∗)() (9.2)\\nInourexample, wneedstobeavalidprobabilitydensityfunction,orthe\\noutputisnotaweightedaverage.Also, wneedstobeforallnegativearguments, 0\\noritwilllookintothefuture,whichispresumablybeyondourcapabilities.These\\nlimitationsareparticulartoourexamplethough.Ingeneral,convolutionisdeﬁned\\nforanyfunctionsforwhichtheaboveintegralisdeﬁned,andmaybeusedforother\\npurposesbesidestakingweightedaverages.\\nInconvolutionalnetworkterminology,theﬁrstargument(inthisexample,the\\nfunction x)totheconvolutionisoftenreferredtoasthe i nputandthesecond\\n3 3 1', 'CHAPTER9.CONVOLUTIONALNETWORKS\\nargument(inthisexample,thefunction w)asthe k e r nel.Theoutputissometimes\\nreferredtoasthe . f e at ur e m ap\\nInourexample,theideaofalasersensorthatcanprovidemeasurements\\nateveryinstantintimeisnotrealistic.Usually,whenweworkwithdataona\\ncomputer,timewillbediscretized,andoursensorwillprovidedataatregular\\nintervals.Inourexample,itmightbemorerealistictoassumethatourlaser\\nprovidesameasurementoncepersecond.Thetimeindex tcanthentakeononly\\nintegervalues.Ifwenowassumethat xand waredeﬁnedonlyoninteger t,we\\ncandeﬁnethediscreteconvolution:\\ns t x w t () = ( ∗)() =∞\\ue058\\na = − ∞x a w t a ()( −) (9.3)\\nInmachinelearningapplications,theinputisusuallyamultidimensional array\\nofdataandthekernelisusuallyamultidimensionalarrayofparametersthatare\\nadaptedbythelearningalgorithm.Wewillrefertothesemultidimensional arrays\\nastensors.Becauseeachelementoftheinputandkernelmustbeexplicitlystored\\nseparately,weusuallyassumethatthesefunctionsarezeroeverywherebutthe\\nﬁnitesetofpointsforwhichwestorethevalues.Thismeansthatinpracticewe\\ncanimplementtheinﬁnitesummationasasummationoveraﬁnitenumberof\\narrayelements.\\nFinally,weoftenuseconvolutionsovermorethanoneaxisatatime.For\\nexample,ifweuseatwo-dimensionalimage Iasourinput,weprobablyalsowant\\ntouseatwo-dimensionalkernel: K\\nS i , j I K i , j () = ( ∗)() =\\ue058\\nm\\ue058\\nnI m , n K i m , j n . ( )( − −)(9.4)\\nConvolutioniscommutative,meaningwecanequivalentlywrite:\\nS i , j K I i , j () = ( ∗)() =\\ue058\\nm\\ue058\\nnI i m , j n K m , n . ( − −)( )(9.5)\\nUsuallythelatterformulaismorestraightforwardtoimplementinamachine\\nlearninglibrary,becausethereislessvariationintherangeofvalidvaluesof m\\nand. n\\nThecommutativepropertyofconvolutionarisesbecausewehave ﬂi pp e dthe\\nkernelrelativetotheinput,inthesensethatas mincreases,theindexintothe\\ninputincreases,buttheindexintothekerneldecreases.Theonlyreasontoﬂip\\nthekernelistoobtainthecommutativeproperty.Whilethecommutativeproperty\\n3 3 2', 'CHAPTER9.CONVOLUTIONALNETWORKS\\nisusefulforwritingproofs,itisnotusuallyanimportantpropertyofaneural\\nnetworkimplementation.Instead,manyneuralnetworklibrariesimplementa\\nrelatedfunctioncalledthe c r o ss-c o r r e l a t i o n,whichisthesameasconvolution\\nbutwithoutﬂippingthekernel:\\nS i , j I K i , j () = ( ∗)() =\\ue058\\nm\\ue058\\nnI i m , j n K m , n . (+ +)( )(9.6)\\nManymachinelearninglibrariesimplementcross-correlationbutcallitconvolution.\\nInthistextwewillfollowthisconventionofcallingbothoperationsconvolution,\\nandspecifywhetherwemeantoﬂipthekernelornotincontextswherekernel\\nﬂippingisrelevant.Inthecontextofmachinelearning,thelearningalgorithmwill\\nlearntheappropriatevaluesofthekernelintheappropriateplace,soanalgorithm\\nbasedonconvolutionwithkernelﬂippingwilllearnakernelthatisﬂippedrelative\\ntothekernellearnedbyanalgorithmwithouttheﬂipping.Itisalsorarefor\\nconvolutiontobeusedaloneinmachinelearning;insteadconvolutionisused\\nsimultaneouslywithotherfunctions,andthecombinationofthesefunctionsdoes\\nnotcommuteregardlessofwhethertheconvolutionoperationﬂipsitskernelor\\nnot.\\nSeeﬁgureforanexampleofconvolution(withoutkernelﬂipping)applied 9.1\\ntoa2-Dtensor.\\nDiscreteconvolutioncanbeviewedasmultiplicationbyamatrix.However,the\\nmatrixhasseveralentriesconstrainedtobeequaltootherentries.Forexample,\\nforunivariatediscreteconvolution,eachrowofthematrixisconstrainedtobe\\nequaltotherowaboveshiftedbyoneelement.Thisisknownasa T o e pl i t z\\nm at r i x.Intwodimensions,a doubly bl o c k c i r c ul an t m at r i xcorrespondsto\\nconvolution.Inadditiontotheseconstraintsthatseveralelementsbeequalto\\neachother,convolutionusuallycorrespondstoaverysparsematrix(amatrix\\nwhoseentriesaremostlyequaltozero).Thisisbecausethekernelisusuallymuch\\nsmallerthantheinputimage.Anyneuralnetworkalgorithmthatworkswith\\nmatrixmultiplication anddoesnotdependonspeciﬁcpropertiesofthematrix\\nstructureshouldworkwithconvolution,withoutrequiringanyfurtherchanges\\ntotheneuralnetwork.Typicalconvolutionalneuralnetworksdomakeuseof\\nfurtherspecializationsinordertodealwithlargeinputseﬃciently,buttheseare\\nnotstrictlynecessaryfromatheoreticalperspective.\\n3 3 3', 'CHAPTER9.CONVOLUTIONALNETWORKS\\na b c d\\ne f g h\\ni j k lw x\\ny z\\na w + b x +\\ne y + f za w + b x +\\ne y + f zb w + c x +\\nf y + g zb w + c x +\\nf y + g zc w + d x +\\ng y + h zc w + d x +\\ng y + h z\\ne w + f x +\\ni y + j ze w + f x +\\ni y + j zf w + g x +\\nj y + k zf w + g x +\\nj y + k zg w + h x +\\nk y + l zg w + h x +\\nk y + l zI nput\\nK e r ne l\\nO ut put\\nFigure9.1:Anexampleof2-Dconvolutionwithoutkernel-ﬂipping.Inthiscasewerestrict\\ntheoutputtoonlypositionswherethekernelliesentirelywithintheimage,called“valid”\\nconvolutioninsomecontexts.Wedrawboxeswitharrowstoindicatehowtheupper-left\\nelementoftheoutputtensorisformedbyapplyingthekerneltothecorresponding\\nupper-leftregionoftheinputtensor.\\n3 3 4', 'CHAPTER9.CONVOLUTIONALNETWORKS\\n9.2Motivation\\nConvolutionleveragesthreeimportantideasthatcanhelpimproveamachine\\nlearningsystem: spar se i nt e r ac t i o n s, par ameter shar i ngand e q ui v ar i an t\\nr e pr e se n t at i o ns.Moreover,\\xa0convolutionprovidesameansforworkingwith\\ninputsofvariablesize.Wenowdescribeeachoftheseideasinturn.\\nTraditionalneuralnetworklayersusematrixmultiplicationbyamatrixof\\nparameterswithaseparateparameterdescribingtheinteractionbetweeneachinput\\nunitandeachoutputunit.Thismeanseveryoutputunitinteractswitheveryinput\\nunit.Convolutionalnetworks,however,typicallyhave spar se i n t e r ac t i o ns(also\\nreferredtoas spar se c o nnec t i v i t yor spar se wei g h t s).Thisisaccomplishedby\\nmakingthekernelsmallerthantheinput.Forexample,whenprocessinganimage,\\ntheinputimagemighthavethousandsormillionsofpixels,butwecandetectsmall,\\nmeaningfulfeaturessuchasedgeswithkernelsthatoccupyonlytensorhundredsof\\npixels.Thismeansthatweneedtostorefewerparameters,whichbothreducesthe\\nmemoryrequirementsofthemodelandimprovesitsstatisticaleﬃciency.Italso\\nmeansthatcomputingtheoutputrequiresfeweroperations.Theseimprovements\\nineﬃciencyareusuallyquitelarge.Ifthereare minputsand noutputs,then\\nmatrixmultiplication requires m n ×parametersandthealgorithmsusedinpractice\\nhave O( m n ×)runtime(perexample).Ifwelimitthenumberofconnections\\neachoutputmayhaveto k,thenthesparselyconnectedapproachrequiresonly\\nk n ×parametersand O( k n ×)runtime.Formanypracticalapplications,itis\\npossibletoobtaingoodperformanceonthemachinelearningtaskwhilekeeping\\nkseveralordersofmagnitudesmallerthan m.\\xa0Forgraphicaldemonstrationsof\\nsparseconnectivity,seeﬁgureandﬁgure.Inadeepconvolutionalnetwork, 9.2 9.3\\nunitsinthedeeperlayersmayindirectlyinteractwithalargerportionoftheinput,\\nasshowninﬁgure.Thisallowsthenetworktoeﬃcientlydescribecomplicated 9.4\\ninteractionsbetweenmanyvariablesbyconstructingsuchinteractionsfromsimple\\nbuildingblocksthateachdescribeonlysparseinteractions.\\nP ar amet e r shar i ngreferstousingthesameparameterformorethanone\\nfunctioninamodel.Inatraditionalneuralnet,eachelementoftheweightmatrix\\nisusedexactlyoncewhencomputingtheoutputofalayer.Itismultipliedby\\noneelementoftheinputandthenneverrevisited.Asasynonymforparameter\\nsharing,onecansaythatanetworkhas t i e d w e i g h t s,becausethevalueofthe\\nweightappliedtooneinputistiedtothevalueofaweightappliedelsewhere.In\\naconvolutionalneuralnet,eachmemberofthekernelisusedateveryposition\\noftheinput(exceptperhapssomeoftheboundarypixels,\\xa0dependingonthe\\ndesigndecisionsregardingtheboundary).Theparametersharingusedbythe\\nconvolutionoperationmeansthatratherthanlearningaseparatesetofparameters\\n3 3 5', 'CHAPTER9.CONVOLUTIONALNETWORKS\\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\\nx 4 x 4s 4 s 4\\nx 5 x 5s 5 s 5\\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\\nx 4 x 4s 4 s 4\\nx 5 x 5s 5 s 5\\nFigure9.2: S p a r s e c o n n e c t i v i t y , v i e w e d f r o m b e l o w :Wehighlightoneinputunit, x 3,\\nandalsohighlighttheoutputunitsin sthatareaﬀectedbythisunit. ( T o p )When sis\\nformedbyconvolutionwithakernelofwidth,onlythreeoutputsareaﬀectedby 3 x.\\n( Bottom )Whenisformedbymatrixmultiplication,connectivityisnolongersparse,so s\\nalloftheoutputsareaﬀectedby x 3.\\n3 3 6', 'CHAPTER9.CONVOLUTIONALNETWORKS\\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\\nx 4 x 4s 4 s 4\\nx 5 x 5s 5 s 5\\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\\nx 4 x 4s 4 s 4\\nx 5 x 5s 5 s 5\\nFigure9.3: S p a r s e c o n n e c t i v i t y , v i e w e d f r o m a b o v e : \\xa0Wehighlightoneoutputunit, s 3,\\nandalsohighlighttheinputunitsin xthataﬀectthisunit.Theseunitsareknown\\nasthereceptiveﬁeldof s 3. ( T o p )When sisformedbyconvolutionwithakernelof\\nwidth,onlythreeinputsaﬀect 3 s 3.When ( Bottom ) sisformedbymatrixmultiplication,\\nconnectivityisnolongersparse,soalloftheinputsaﬀect s 3.\\nx 1 x 1 x 2 x 2 x 3 x 3h 2 h 2 h 1 h 1 h 3 h 3\\nx 4 x 4h 4 h 4\\nx 5 x 5h 5 h 5g 2 g 2 g 1 g 1 g 3 g 3 g 4 g 4 g 5 g 5\\nFigure9.4:Thereceptiveﬁeldoftheunitsinthedeeperlayersofaconvolutionalnetwork\\nislargerthanthereceptiveﬁeldoftheunitsintheshallowlayers.Thiseﬀectincreasesif\\nthenetworkincludesarchitecturalfeatureslikestridedconvolution(ﬁgure)orpooling 9.12\\n(section).Thismeansthateventhough 9.3 d i r e c tconnectionsinaconvolutionalnetare\\nverysparse,unitsinthedeeperlayerscanbe i n d i r e c t l yconnectedtoallormostofthe\\ninputimage.\\n3 3 7', 'CHAPTER9.CONVOLUTIONALNETWORKS\\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\\nx 4 x 4s 4 s 4\\nx 5 x 5s 5 s 5\\nx 1 x 1 x 2 x 2 x 3 x 3 x 4 x 4 x 5 x 5s 2 s 2 s 1 s 1 s 3 s 3 s 4 s 4 s 5 s 5\\nFigure9.5:Parametersharing:Blackarrowsindicatetheconnectionsthatuseaparticular\\nparameterintwodiﬀerentmodels.\\xa0 ( T o p )Theblackarrowsindicateusesofthecentral\\nelementofa3-elementkernelinaconvolutionalmodel.Duetoparametersharing,this\\nsingleparameterisusedatallinputlocations.Thesingleblackarrowindicates ( Bottom )\\ntheuseofthecentralelementoftheweightmatrixinafullyconnectedmodel.Thismodel\\nhasnoparametersharingsotheparameterisusedonlyonce.\\nforeverylocation,welearnonlyoneset.Thisdoesnotaﬀecttheruntimeof\\nforwardpropagation—it isstill O( k n ×)—butitdoesfurtherreducethestorage\\nrequirementsofthemodelto kparameters.Recallthat kisusuallyseveralorders\\nofmagnitudelessthan m.Since mand nareusuallyroughlythesamesize, kis\\npracticallyinsigniﬁcantcomparedto m n ×.Convolutionisthusdramatically more\\neﬃcientthandensematrixmultiplication intermsofthememoryrequirements\\nandstatisticaleﬃciency.Foragraphicaldepictionofhowparametersharingworks,\\nseeﬁgure.9.5\\nAsanexampleofbothoftheseﬁrsttwoprinciplesinaction,ﬁgureshows9.6\\nhowsparseconnectivityandparametersharingcandramatically improvethe\\neﬃciencyofalinearfunctionfordetectingedgesinanimage.\\nInthecaseofconvolution,theparticularformofparametersharingcausesthe\\nlayertohaveapropertycalled e q ui v ar i anc etotranslation.Tosayafunctionis\\nequivariantmeansthatiftheinputchanges,theoutputchangesinthesameway.\\nSpeciﬁcally,afunction f( x)isequivarianttoafunction gif f( g( x))= g( f( x)).\\nInthecaseofconvolution,ifwelet gbeanyfunctionthattranslatestheinput,\\ni.e.,shiftsit,thentheconvolutionfunctionisequivariantto g.Forexample,let I\\nbeafunctiongivingimagebrightnessatintegercoordinates.Let gbeafunction\\n3 3 8', 'CHAPTER9.CONVOLUTIONALNETWORKS\\nmappingoneimagefunctiontoanotherimagefunction,suchthat I\\ue030= g( I)is\\ntheimagefunctionwith I\\ue030( x , y)= I( x −1 , y).Thisshiftseverypixelof Ione\\nunittotheright.Ifweapplythistransformationto I,thenapplyconvolution,\\ntheresultwillbethesameasifweappliedconvolutionto I\\ue030,thenappliedthe\\ntransformation gtotheoutput.Whenprocessingtimeseriesdata,thismeans\\nthatconvolutionproducesasortoftimelinethatshowswhendiﬀerentfeatures\\nappearintheinput.Ifwemoveaneventlaterintimeintheinput,theexact\\nsamerepresentationofitwillappearintheoutput,justlaterintime.Similarly\\nwithimages,convolutioncreatesa2-Dmapofwherecertainfeaturesappearin\\ntheinput.Ifwemovetheobjectintheinput,itsrepresentationwillmovethe\\nsameamountintheoutput.Thisisusefulforwhenweknowthatsomefunction\\nofasmallnumberofneighboringpixelsisusefulwhenappliedtomultipleinput\\nlocations.Forexample,whenprocessingimages,itisusefultodetectedgesin\\ntheﬁrstlayerofaconvolutionalnetwork.Thesameedgesappearmoreorless\\neverywhereintheimage,soitispracticaltoshareparametersacrosstheentire\\nimage.Insomecases,wemaynotwishtoshareparametersacrosstheentire\\nimage.Forexample,ifweareprocessingimagesthatarecroppedtobecentered\\nonanindividual’sface,weprobablywanttoextractdiﬀerentfeaturesatdiﬀerent\\nlocations—thepartofthenetworkprocessingthetopofthefaceneedstolookfor\\neyebrows,whilethepartofthenetworkprocessingthebottomofthefaceneedsto\\nlookforachin.\\nConvolutionisnotnaturallyequivarianttosomeothertransformations,such\\naschangesinthescaleorrotationofanimage.Othermechanismsarenecessary\\nforhandlingthesekindsoftransformations.\\nFinally,somekindsofdatacannotbeprocessedbyneuralnetworksdeﬁnedby\\nmatrixmultiplication withaﬁxed-shapematrix.Convolutionenablesprocessing\\nofsomeofthesekindsofdata.Wediscussthisfurtherinsection.9.7\\n9.3Pooling\\nAtypicallayerofaconvolutionalnetworkconsistsofthreestages(seeﬁgure).9.7\\nIntheﬁrststage,thelayerperformsseveralconvolutionsinparalleltoproducea\\nsetoflinearactivations.Inthesecondstage,eachlinearactivationisrunthrough\\nanonlinearactivationfunction,suchastherectiﬁedlinearactivationfunction.\\nThisstageissometimescalledthe det e c t o rstage.\\xa0Inthethirdstage,weusea\\np o o l i ng f unc t i o ntomodifytheoutputofthelayerfurther.\\nApoolingfunctionreplacestheoutputofthenetatacertainlocationwitha\\nsummarystatisticofthenearbyoutputs.Forexample,the m ax p o o l i ng(Zhou\\n3 3 9', 'CHAPTER9.CONVOLUTIONALNETWORKS\\nFigure9.6: E ﬃ c i e n c y o f e d g e d e t e c t i o n.\\xa0Theimageontherightwasformedbytaking\\neachpixelintheoriginalimageandsubtractingthevalueofitsneighboringpixelonthe\\nleft.\\xa0Thisshowsthestrengthofalloftheverticallyorientededgesintheinputimage,\\nwhichcanbeausefuloperationforobjectdetection.Bothimagesare280pixelstall.\\nTheinputimageis320pixelswidewhiletheoutputimageis319pixelswide.This\\ntransformationcanbedescribedbyaconvolutionkernelcontainingtwoelements,and\\nrequires319 ×280 ×3=267 ,960ﬂoatingpointoperations(twomultiplicationsand\\noneadditionperoutputpixel)tocomputeusingconvolution.Todescribethesame\\ntransformationwithamatrixmultiplicationwouldtake320 ×280 ×319 ×280,orover\\neightbillion,entriesinthematrix,makingconvolutionfourbilliontimesmoreeﬃcientfor\\nrepresentingthistransformation.Thestraightforwardmatrixmultiplicationalgorithm\\nperformsoversixteenbillionﬂoatingpointoperations,makingconvolutionroughly60,000\\ntimesmoreeﬃcientcomputationally.Ofcourse,mostoftheentriesofthematrixwouldbe\\nzero.Ifwestoredonlythenonzeroentriesofthematrix,thenbothmatrixmultiplication\\nandconvolutionwouldrequirethesamenumberofﬂoatingpointoperationstocompute.\\nThematrixwouldstillneedtocontain2 ×319 ×280=178 ,640entries.Convolution\\nisanextremelyeﬃcientwayofdescribingtransformationsthatapplythesamelinear\\ntransformationofasmall,localregionacrosstheentireinput.(Photocredit:Paula\\nGoodfellow)\\n3 4 0', 'CHAPTER9.CONVOLUTIONALNETWORKS\\nConvolutional\\xa0Layer\\nInput\\xa0to\\xa0layerConvolution\\xa0stage:\\nAne\\xa0transform ﬃDetector\\xa0stage:\\nNonlinearity\\ne.g.,\\xa0rectiﬁed\\xa0linearPooling\\xa0stageNext\\xa0layer\\nInput\\xa0to\\xa0layersConvolution\\xa0layer:\\nAne\\xa0transform\\xa0 ﬃDetector\\xa0layer:\\xa0Nonlinearity\\ne.g.,\\xa0rectiﬁed\\xa0linearPooling\\xa0layerNext\\xa0layerComplex\\xa0layer\\xa0terminology Simple\\xa0layer\\xa0terminology\\nFigure9.7:Thecomponentsofatypicalconvolutionalneuralnetworklayer.Therearetwo\\ncommonlyusedsetsofterminologyfordescribingtheselayers. ( L e f t )Inthisterminology,\\ntheconvolutionalnetisviewedasasmallnumberofrelativelycomplexlayers,with\\neachlayerhavingmany“stages.”Inthisterminology,thereisaone-to-onemapping\\nbetweenkerneltensorsandnetworklayers.Inthisbookwegenerallyusethisterminology.\\n( R i g h t )Inthisterminology,theconvolutionalnetisviewedasalargernumberofsimple\\nlayers;everystepofprocessingisregardedasalayerinitsownright.Thismeansthat\\nnotevery“layer”hasparameters.\\n3 4 1', 'CHAPTER9.CONVOLUTIONALNETWORKS\\nandChellappa1988,)operationreportsthemaximumoutputwithinarectangular\\nneighborhood.Otherpopularpoolingfunctionsincludetheaverageofarectangular\\nneighborhood,the L2normofarectangularneighborhood,oraweightedaverage\\nbasedonthedistancefromthecentralpixel.\\nInallcases,poolinghelpstomaketherepresentationbecomeapproximately\\ni n v ar i an ttosmalltranslationsoftheinput.Invariancetotranslationmeansthat\\nifwetranslatetheinputbyasmallamount,thevaluesofmostofthepooled\\noutputsdonotchange.Seeﬁgureforanexampleofhowthisworks. 9.8 Invariance\\ntolocaltranslationcanbeaveryusefulpropertyifwecaremoreaboutwhether\\nsomefeatureispresentthanexactlywhereitis.Forexample,whendetermining\\nwhetheranimagecontainsaface,weneednotknowthelocationoftheeyeswith\\npixel-perfectaccuracy,wejustneedtoknowthatthereisaneyeontheleftside\\nofthefaceandaneyeontherightsideoftheface.Inothercontexts,itismore\\nimportanttopreservethelocationofafeature.Forexample,ifwewanttoﬁnda\\ncornerdeﬁnedbytwoedgesmeetingataspeciﬁcorientation,weneedtopreserve\\nthelocationoftheedgeswellenoughtotestwhethertheymeet.\\nTheuseofpoolingcanbeviewedasaddinganinﬁnitelystrongpriorthat\\nthefunctionthelayerlearnsmustbeinvarianttosmalltranslations.Whenthis\\nassumptioniscorrect,itcangreatlyimprovethestatisticaleﬃciencyofthenetwork.\\nPoolingoverspatialregionsproducesinvariancetotranslation,butifwepool\\novertheoutputsofseparatelyparametrized convolutions,thefeaturescanlearn\\nwhichtransformationstobecomeinvariantto(seeﬁgure).9.9\\nBecausepoolingsummarizestheresponsesoverawholeneighborhood,itis\\npossibletousefewerpoolingunitsthandetectorunits,byreportingsummary\\nstatisticsforpoolingregionsspaced kpixelsapartratherthan1pixelapart.See\\nﬁgureforanexample.Thisimprovesthecomputational eﬃciencyofthe 9.10\\nnetworkbecausethenextlayerhasroughly ktimesfewerinputstoprocess.When\\nthenumberofparametersinthenextlayerisafunctionofitsinputsize(suchas\\nwhenthenextlayerisfullyconnectedandbasedonmatrixmultiplication) this\\nreductionintheinputsizecanalsoresultinimprovedstatisticaleﬃciencyand\\nreducedmemoryrequirementsforstoringtheparameters.\\nFormanytasks,poolingisessentialforhandlinginputsofvaryingsize.\\xa0For\\nexample,ifwewanttoclassifyimagesofvariablesize,theinputtotheclassiﬁcation\\nlayermusthaveaﬁxedsize.Thisisusuallyaccomplishedbyvaryingthesizeofan\\noﬀsetbetweenpoolingregionssothattheclassiﬁcationlayeralwaysreceivesthe\\nsamenumberofsummarystatisticsregardlessoftheinputsize.Forexample,the\\nﬁnalpoolinglayerofthenetworkmaybedeﬁnedtooutputfoursetsofsummary\\nstatistics,oneforeachquadrantofanimage,regardlessoftheimagesize.\\n3 4 2', 'CHAPTER9.CONVOLUTIONALNETWORKS\\n0. 1 1. 0. 21. 1. 1.\\n0. 10. 2\\n. . . . . .. . . . . .\\n0. 3 0. 1 1.1. 0. 3 1.\\n0. 21.\\n. . . . . .. . . . . .D E T E C T O R \\xa0 S T A GEP O O L I N G\\xa0 ST A GE\\nP O O L I N G\\xa0 ST A GE\\nD E T E C T O R \\xa0 S T A GE\\nFigure9.8:Maxpoolingintroducesinvariance. ( T o p )Aviewofthemiddleoftheoutput\\nofaconvolutionallayer.Thebottomrowshowsoutputsofthenonlinearity.Thetop\\nrowshowstheoutputsofmaxpooling,withastrideofonepixelbetweenpoolingregions\\nandapoolingregionwidthofthreepixels.Aviewofthesamenetwork,after ( Bottom )\\ntheinputhasbeenshiftedtotherightbyonepixel.Everyvalueinthebottomrowhas\\nchanged,butonlyhalfofthevaluesinthetoprowhavechanged,becausethemaxpooling\\nunitsareonlysensitivetothemaximumvalueintheneighborhood,notitsexactlocation.\\n3 4 3', 'CHAPTER9.CONVOLUTIONALNETWORKS\\nL ar ge \\xa0 r e s pon s e\\ni n\\xa0 po ol i ng\\xa0uni tL ar ge \\xa0 r e s pon s e\\ni n\\xa0 po ol i ng\\xa0uni t\\nL ar ge\\nr e s ponse\\ni n\\xa0 de t e c t or\\nuni t \\xa0 1L ar ge\\nr e s ponse\\ni n\\xa0 de t e c t or\\nuni t \\xa0 3\\nFigure9.9: E x a m p l e o f l e a r n e d i n v a r i a n c e s :Apoolingunitthatpoolsovermultiplefeatures\\nthatarelearnedwithseparateparameterscanlearntobeinvarianttotransformationsof\\ntheinput.Hereweshowhowasetofthreelearnedﬁltersandamaxpoolingunitcanlearn\\ntobecomeinvarianttorotation.Allthreeﬁltersareintendedtodetectahand-written5.\\nEachﬁlterattemptstomatchaslightlydiﬀerentorientationofthe5.Whena5appearsin\\ntheinput,thecorrespondingﬁlterwillmatchitandcausealargeactivationinadetector\\nunit.Themaxpoolingunitthenhasalargeactivationregardlessofwhichdetectorunit\\nwasactivated.Weshowherehowthenetworkprocessestwodiﬀerentinputs,resulting\\nintwodiﬀerentdetectorunitsbeingactivated.Theeﬀectonthepoolingunitisroughly\\nthesameeitherway.Thisprincipleisleveragedbymaxoutnetworks(Goodfellow e t a l .,\\n2013a)andotherconvolutionalnetworks.Maxpoolingoverspatialpositionsisnaturally\\ninvarianttotranslation;thismulti-channelapproachisonlynecessaryforlearningother\\ntransformations.\\n0. 1 1. 0. 21. 0. 2\\n0. 10. 1\\n0. 0 0. 1\\nFigure9.10: P o o l i n g w i t h d o w n s a m p l i n g.Hereweusemax-poolingwithapoolwidthof\\nthreeandastridebetweenpoolsoftwo.Thisreducestherepresentationsizebyafactor\\noftwo,whichreducesthecomputationalandstatisticalburdenonthenextlayer.Note\\nthattherightmostpoolingregionhasasmallersize,butmustbeincludedifwedonot\\nwanttoignoresomeofthedetectorunits.\\n3 4 4', 'CHAPTER9.CONVOLUTIONALNETWORKS\\nSometheoreticalworkgivesguidanceastowhichkindsofpoolingoneshould\\nuseinvarioussituations( ,).Itisalsopossibletodynamically Boureauetal.2010\\npoolfeaturestogether,forexample,byrunningaclusteringalgorithmonthe\\nlocationsofinterestingfeatures( ,).Thisapproachyieldsa Boureauetal.2011\\ndiﬀerentsetofpoolingregionsforeachimage.Anotherapproachistolearna\\nsinglepoolingstructurethatisthenappliedtoallimages(,). Jiaetal.2012\\nPoolingcancomplicatesomekindsofneuralnetworkarchitecturesthatuse\\ntop-downinformation, suchasBoltzmannmachinesandautoencoders.These\\nissueswillbediscussedfurtherwhenwepresentthesetypesofnetworksinpart.III\\nPoolinginconvolutionalBoltzmannmachinesispresentedinsection.\\xa0The20.6\\ninverse-likeoperationsonpoolingunitsneededinsomediﬀerentiablenetworkswill\\nbecoveredinsection.20.10.6\\nSomeexamplesofcompleteconvolutionalnetworkarchitecturesforclassiﬁcation\\nusingconvolutionandpoolingareshowninﬁgure.9.11\\n9.4Convolutionand\\xa0Pooling\\xa0asan\\xa0InﬁnitelyStrong\\nPrior\\nRecalltheconceptofa pr i o r pr o babili t y di st r i but i o nfromsection.Thisis5.2\\naprobabilitydistributionovertheparametersofamodelthatencodesourbeliefs\\naboutwhatmodelsarereasonable,beforewehaveseenanydata.\\nPriorscanbeconsideredweakorstrongdependingonhowconcentratedthe\\nprobabilitydensityintheprioris.Aweakpriorisapriordistributionwithhigh\\nentropy,suchasaGaussiandistributionwithhighvariance.Suchapriorallows\\nthedatatomovetheparametersmoreorlessfreely.Astrongpriorhasverylow\\nentropy,suchasaGaussiandistributionwithlowvariance.Suchapriorplaysa\\nmoreactiveroleindeterminingwheretheparametersendup.\\nAninﬁnitelystrongpriorplaceszeroprobabilityonsomeparametersandsays\\nthattheseparametervaluesarecompletelyforbidden,regardlessofhowmuch\\nsupportthedatagivestothosevalues.\\nWecanimagineaconvolutionalnetasbeingsimilartoafullyconnectednet,\\nbutwithaninﬁnitelystrongprioroveritsweights.Thisinﬁnitelystrongprior\\nsaysthattheweightsforonehiddenunitmustbeidenticaltotheweightsofits\\nneighbor,butshiftedinspace.Theprioralsosaysthattheweightsmustbezero,\\nexceptforinthesmall,spatiallycontiguousreceptiveﬁeldassignedtothathidden\\nunit.Overall,wecanthinkoftheuseofconvolutionasintroducinganinﬁnitely\\nstrongpriorprobabilitydistributionovertheparametersofalayer.Thisprior\\n3 4 5', 'CHAPTER9.CONVOLUTIONALNETWORKS\\nInput\\xa0image:\\xa0\\n256x256x3Output\\xa0of\\xa0\\nconvolution\\xa0+\\xa0\\nReLU:\\xa0256x256x64Output\\xa0of\\xa0pooling\\xa0\\nwith\\xa0stride\\xa04:\\xa0\\n64x64x64Output\\xa0of\\xa0\\nconvolution\\xa0+\\xa0\\nReLU:\\xa064x64x64Output\\xa0of\\xa0pooling\\xa0\\nwith\\xa0stride\\xa04:\\xa0\\n16x16x64Output\\xa0of\\xa0reshape\\xa0to\\xa0\\nvector:\\n16,384\\xa0unitsOutput\\xa0of\\xa0matrix\\xa0\\nmultiply:\\xa01,000\\xa0unitsOutput\\xa0of\\xa0softmax:\\xa0\\n1,000\\xa0class\\xa0\\nprobabilities\\nInput\\xa0image:\\xa0\\n256x256x3Output\\xa0of\\xa0\\nconvolution\\xa0+\\xa0\\nReLU:\\xa0256x256x64Output\\xa0of\\xa0pooling\\xa0\\nwith\\xa0stride\\xa04:\\xa0\\n64x64x64Output\\xa0of\\xa0\\nconvolution\\xa0+\\xa0\\nReLU:\\xa064x64x64Output\\xa0of\\xa0pooling\\xa0to\\xa0\\n3x3\\xa0grid:\\xa03x3x64Output\\xa0of\\xa0reshape\\xa0to\\xa0\\nvector:\\n576\\xa0unitsOutput\\xa0of\\xa0matrix\\xa0\\nmultiply:\\xa01,000\\xa0unitsOutput\\xa0of\\xa0softmax:\\xa0\\n1,000\\xa0class\\xa0\\nprobabilities\\nInput\\xa0image:\\xa0\\n256x256x3Output\\xa0of\\xa0\\nconvolution\\xa0+\\xa0\\nReLU:\\xa0256x256x64Output\\xa0of\\xa0pooling\\xa0\\nwith\\xa0stride\\xa04:\\xa0\\n64x64x64Output\\xa0of\\xa0\\nconvolution\\xa0+\\xa0\\nReLU:\\xa064x64x64Output\\xa0of\\xa0\\nconvolution:\\n16x16x1,000Output\\xa0of\\xa0average\\xa0\\npooling:\\xa01x1x1,000Output\\xa0of\\xa0softmax:\\xa0\\n1,000\\xa0class\\xa0\\nprobabilities\\nOutput\\xa0of\\xa0pooling\\xa0\\nwith\\xa0stride\\xa04:\\xa0\\n16x16x64\\nFigure9.11:Examplesofarchitecturesforclassiﬁcationwithconvolutionalnetworks.The\\nspeciﬁcstridesanddepthsusedinthisﬁgurearenotadvisableforrealuse;theyare\\ndesignedtobeveryshallowinordertoﬁtontothepage.\\xa0Realconvolutionalnetworks\\nalsoofteninvolvesigniﬁcantamountsofbranching,unlikethechainstructuresused\\nhereforsimplicity. ( L e f t )Aconvolutionalnetworkthatprocessesaﬁxedimagesize.\\nAfteralternatingbetweenconvolutionandpoolingforafewlayers,thetensorforthe\\nconvolutionalfeaturemapisreshapedtoﬂattenoutthespatialdimensions.Therest\\nofthenetworkisanordinaryfeedforwardnetworkclassiﬁer,asdescribedinchapter.6\\n( C e n t e r )Aconvolutionalnetworkthatprocessesavariable-sizedimage,butstillmaintains\\nafullyconnectedsection.Thisnetworkusesapoolingoperationwithvariably-sizedpools\\nbutaﬁxednumberofpools,inordertoprovideaﬁxed-sizevectorof576unitstothe\\nfullyconnectedportionofthenetwork.\\xa0Aconvolutionalnetworkthatdoesnot ( R i g h t )\\nhaveanyfullyconnectedweightlayer.Instead,thelastconvolutionallayeroutputsone\\nfeaturemapperclass.Themodelpresumablylearnsamapofhowlikelyeachclassisto\\noccurateachspatiallocation.Averagingafeaturemapdowntoasinglevalueprovides\\ntheargumenttothesoftmaxclassiﬁeratthetop.\\n3 4 6', 'CHAPTER9.CONVOLUTIONALNETWORKS\\nsaysthatthefunctionthelayershouldlearncontainsonlylocalinteractionsandis\\nequivarianttotranslation.Likewise,theuseofpoolingisaninﬁnitelystrongprior\\nthateachunitshouldbeinvarianttosmalltranslations.\\nOfcourse,implementing aconvolutionalnetasafullyconnectednetwithan\\ninﬁnitelystrongpriorwouldbeextremelycomputationally wasteful.Butthinking\\nofaconvolutionalnetasafullyconnectednetwithaninﬁnitelystrongpriorcan\\ngiveussomeinsightsintohowconvolutionalnetswork.\\nOnekeyinsightisthatconvolutionandpoolingcancauseunderﬁtting. Like\\nanyprior,convolutionandpoolingareonlyusefulwhentheassumptionsmade\\nbythepriorarereasonablyaccurate.Ifataskreliesonpreservingprecisespatial\\ninformation, thenusingpoolingonallfeaturescanincreasethetrainingerror.\\nSomeconvolutionalnetworkarchitectures ( ,)aredesignedto Szegedy etal.2014a\\nusepoolingonsomechannelsbutnotonotherchannels,inordertogetboth\\nhighlyinvariantfeaturesandfeaturesthatwillnotunderﬁtwhenthetranslation\\ninvariancepriorisincorrect.Whenataskinvolvesincorporatinginformationfrom\\nverydistantlocationsintheinput,thenthepriorimposedbyconvolutionmaybe\\ninappropriate.\\nAnotherkeyinsightfromthisviewisthatweshouldonlycompareconvolu-\\ntionalmodelstootherconvolutionalmodelsinbenchmarksofstatisticallearning\\nperformance.Modelsthatdonotuseconvolutionwouldbeabletolearneven\\nifwepermutedallofthepixelsintheimage.Formanyimagedatasets,there\\nareseparatebenchmarksformodelsthatare p e r m ut at i o n i nv ar i antandmust\\ndiscovertheconceptoftopologyvialearning,andmodelsthathavetheknowledge\\nofspatialrelationshipshard-codedintothembytheirdesigner.\\n9.5VariantsoftheBasicConvolutionFunction\\nWhendiscussingconvolutioninthecontextofneuralnetworks,weusuallydo\\nnotreferexactlytothestandarddiscreteconvolutionoperationasitisusually\\nunderstoodinthemathematical literature.Thefunctionsusedinpracticediﬀer\\nslightly.Herewedescribethesediﬀerencesindetail,andhighlightsomeuseful\\npropertiesofthefunctionsusedinneuralnetworks.\\nFirst,whenwerefertoconvolutioninthecontextofneuralnetworks,weusually\\nactuallymeananoperationthatconsistsofmanyapplicationsofconvolutionin\\nparallel.Thisisbecauseconvolutionwithasinglekernelcanonlyextractonekind\\noffeature,albeitatmanyspatiallocations.Usuallywewanteachlayerofour\\nnetworktoextractmanykindsoffeatures,atmanylocations.\\n3 4 7', 'CHAPTER9.CONVOLUTIONALNETWORKS\\nAdditionally,theinputisusuallynotjustagridofrealvalues.Rather,itisa\\ngridofvector-valuedobservations.\\xa0Forexample,acolorimagehasared,green\\nandblueintensityateachpixel.Inamultilayerconvolutionalnetwork,theinput\\ntothesecondlayeristheoutputoftheﬁrstlayer,whichusuallyhastheoutput\\nofmanydiﬀerentconvolutionsateachposition.Whenworkingwithimages,we\\nusuallythinkoftheinputandoutputoftheconvolutionasbeing3-Dtensors,with\\noneindexintothediﬀerentchannelsandtwoindicesintothespatialcoordinates\\nofeachchannel.Softwareimplementationsusuallyworkinbatchmode,sothey\\nwillactuallyuse4-Dtensors,withthefourthaxisindexingdiﬀerentexamplesin\\nthebatch,butwewillomitthebatchaxisinourdescriptionhereforsimplicity.\\nBecauseconvolutionalnetworksusuallyusemulti-channelconvolution,the\\nlinearoperationstheyarebasedonarenotguaranteedtobecommutative,evenif\\nkernel-ﬂippingisused.Thesemulti-channeloperationsareonlycommutativeif\\neachoperationhasthesamenumberofoutputchannelsasinputchannels.\\nAssumewehavea4-Dkerneltensor Kwithelement K i , j , k, lgivingtheconnection\\nstrengthbetweenaunitinchannel ioftheoutputandaunitinchannel jofthe\\ninput,withanoﬀsetof krowsand lcolumnsbetweentheoutputunitandthe\\ninputunit.Assumeourinputconsistsofobserveddata Vwithelement V i , j , kgiving\\nthevalueoftheinputunitwithinchannel iatrow jandcolumn k.Assumeour\\noutputconsistsof Zwiththesameformatas V.If Zisproducedbyconvolving K\\nacrosswithoutﬂipping,then V K\\nZ i , j , k=\\ue058\\nl , m , nV l , j m , k n + − 1 + − 1 K i , l , m , n (9.7)\\nwherethesummationover l, mand nisoverallvaluesforwhichthetensorindexing\\noperationsinsidethesummationisvalid.Inlinearalgebranotation,weindexinto\\narraysusingafortheﬁrstentry.Thisnecessitatesthe 1 −1intheaboveformula.\\nProgramminglanguagessuchasCandPythonindexstartingfrom,rendering0\\ntheaboveexpressionevensimpler.\\nWemaywanttoskipoversomepositionsofthekernelinordertoreducethe\\ncomputational cost(attheexpenseofnotextractingourfeaturesasﬁnely).We\\ncanthinkofthisasdownsamplingtheoutputofthefullconvolutionfunction.If\\nwewanttosampleonlyevery spixelsineachdirectionintheoutput,thenwecan\\ndeﬁneadownsampledconvolutionfunctionsuchthat c\\nZ i , j , k= ( ) c K V , , s i , j , k=\\ue058\\nl , m , n\\ue002\\nVl , j s m , k s n ( − × 1 ) + ( − × 1 ) + K i , l , m , n\\ue003\\n.(9.8)\\nWereferto sasthe st r i deofthisdownsampledconvolution.Itisalsopossible\\n3 4 8', 'CHAPTER9.CONVOLUTIONALNETWORKS\\ntodeﬁneaseparatestrideforeachdirectionofmotion.Seeﬁgureforan9.12\\nillustration.\\nOneessentialfeatureofanyconvolutionalnetworkimplementationistheability\\ntoimplicitlyzero-padtheinput Vinordertomakeitwider.Withoutthisfeature,\\nthewidthoftherepresentationshrinksbyonepixellessthanthekernelwidth\\nateachlayer.\\xa0Zeropaddingtheinputallowsustocontrolthekernelwidthand\\nthesizeoftheoutputindependently.Withoutzeropadding,weareforcedto\\nchoosebetweenshrinkingthespatialextentofthenetworkrapidlyandusingsmall\\nkernels—bothscenariosthatsigniﬁcantlylimittheexpressivepowerofthenetwork.\\nSeeﬁgureforanexample. 9.13\\nThreespecialcasesofthezero-paddingsettingareworthmentioning.Oneis\\ntheextremecaseinwhichnozero-paddingisusedwhatsoever,andtheconvolution\\nkernelisonlyallowedtovisitpositionswheretheentirekerneliscontainedentirely\\nwithintheimage.InMATLABterminology,thisiscalled v al i dconvolution.In\\nthiscase,allpixelsintheoutputareafunctionofthesamenumberofpixelsin\\ntheinput,sothebehaviorofanoutputpixelissomewhatmoreregular.However,\\nthesizeoftheoutputshrinksateachlayer.Iftheinputimagehaswidth mand\\nthekernelhaswidth k,theoutputwillbeofwidth m k −+1.\\xa0Therateofthis\\nshrinkagecanbedramaticifthekernelsusedarelarge.Sincetheshrinkageis\\ngreaterthan0,itlimitsthenumberofconvolutionallayersthatcanbeincluded\\ninthenetwork.Aslayersareadded,thespatialdimensionofthenetworkwill\\neventuallydropto1 ×1,atwhichpointadditionallayerscannotmeaningfully\\nbeconsideredconvolutional.Anotherspecialcaseofthezero-paddingsettingis\\nwhenjustenoughzero-paddingisaddedtokeepthesizeoftheoutputequalto\\nthesizeoftheinput.MATLABcallsthis sameconvolution.Inthiscase,the\\nnetworkcancontainasmanyconvolutionallayersastheavailablehardwarecan\\nsupport,sincetheoperationofconvolutiondoesnotmodifythearchitectural\\npossibilitiesavailabletothenextlayer.However,theinputpixelsneartheborder\\ninﬂuencefeweroutputpixelsthantheinputpixelsnearthecenter.Thiscanmake\\ntheborderpixelssomewhatunderrepresen tedinthemodel.Thismotivatesthe\\notherextremecase,whichMATLABreferstoas f ul lconvolution,inwhichenough\\nzeroesareaddedforeverypixeltobevisited ktimesineachdirection,resulting\\ninanoutputimageofwidth m+ k −1.Inthiscase,theoutputpixelsnearthe\\nborderareafunctionoffewerpixelsthantheoutputpixelsnearthecenter.This\\ncanmakeitdiﬃculttolearnasinglekernelthatperformswellatallpositionsin\\ntheconvolutionalfeaturemap.Usuallytheoptimalamountofzeropadding(in\\ntermsoftestsetclassiﬁcationaccuracy)liessomewherebetween“valid”and“same”\\nconvolution.\\n3 4 9', 'CHAPTER9.CONVOLUTIONALNETWORKS\\nx 1 x 1 x 2 x 2 x 3 x 3s 1 s 1 s 2 s 2\\nx 4 x 4 x 5 x 5s 3 s 3\\nx 1 x 1 x 2 x 2 x 3 x 3z 2 z 2 z 1 z 1 z 3 z 3\\nx 4 x 4z 4 z 4\\nx 5 x 5z 5 z 5s 1 s 1 s 2 s 2 s 3 s 3St r i de d\\nc onv ol ut i on\\nD ow nsampl i n g\\nC onv ol ut i on\\nFigure\\xa09.12:Convolution\\xa0witha\\xa0stride.Inthisexample,we\\xa0use\\xa0astride\\xa0oftwo.\\n( T o p )Convolutionwithastridelengthoftwoimplementedinasingleoperation.\\xa0 ( Bot-\\nt o m )Convolutionwithastridegreaterthanonepixelismathematicallyequivalentto\\nconvolutionwithunitstridefollowedbydownsampling.Obviously,thetwo-stepapproach\\ninvolvingdownsamplingiscomputationallywasteful,becauseitcomputesmanyvalues\\nthatarethendiscarded.\\n3 5 0', 'CHAPTER9.CONVOLUTIONALNETWORKS\\n. . . . . .. . .\\n. . . . . .. . . . . .. . . . . .\\nFigure9.13: T h e e ﬀ e c t o f z e r o p a d d i n g o n n e t w o r k s i z e:Consideraconvolutionalnetwork\\nwithakernelofwidthsixateverylayer.Inthisexample,wedonotuseanypooling,so\\nonlytheconvolutionoperationitselfshrinksthenetworksize. ( T o p )Inthisconvolutional\\nnetwork,wedonotuseanyimplicitzeropadding.Thiscausestherepresentationto\\nshrinkbyﬁvepixelsateachlayer.Startingfromaninputofsixteenpixels,weareonly\\nabletohavethreeconvolutionallayers,andthelastlayerdoesnotevermovethekernel,\\nsoarguablyonlytwoofthelayersaretrulyconvolutional.Therateofshrinkingcan\\nbemitigatedbyusingsmallerkernels,butsmallerkernelsarelessexpressiveandsome\\nshrinkingisinevitableinthiskindofarchitecture. Byaddingﬁveimplicitzeroes ( Bottom )\\ntoeachlayer,wepreventtherepresentationfromshrinkingwithdepth.Thisallowsusto\\nmakeanarbitrarilydeepconvolutionalnetwork.\\n3 5 1', 'CHAPTER9.CONVOLUTIONALNETWORKS\\nInsomecases,wedonotactuallywanttouseconvolution,butratherlocally\\nconnectedlayers(,,).Inthiscase,theadjacencymatrixinthe LeCun19861989\\ngraphofourMLPisthesame,buteveryconnectionhasitsownweight,speciﬁed\\nbya6-Dtensor W.\\xa0Theindicesinto Warerespectively: i,theoutputchannel,\\nj,theoutputrow, k,theoutputcolumn, l,theinputchannel, m,therowoﬀset\\nwithintheinput,and n,thecolumnoﬀsetwithintheinput.Thelinearpartofa\\nlocallyconnectedlayeristhengivenby\\nZ i , j , k=\\ue058\\nl , m , n[ V l , j m , k n + − 1 + − 1 w i , j , k, l , m , n] . (9.9)\\nThisissometimesalsocalled unshar e d c o nv o l ut i o n,becauseitisasimilaroper-\\nationtodiscreteconvolutionwithasmallkernel,butwithoutsharingparameters\\nacrosslocations.Figurecompareslocalconnections,convolution,andfull 9.14\\nconnections.\\nLocallyconnectedlayersareusefulwhenweknowthateachfeatureshouldbe\\nafunctionofasmallpartofspace,butthereisnoreasontothinkthatthesame\\nfeatureshouldoccuracrossallofspace.Forexample,ifwewanttotellifanimage\\nisapictureofaface,weonlyneedtolookforthemouthinthebottomhalfofthe\\nimage.\\nItcanalsobeusefultomakeversionsofconvolutionorlocallyconnectedlayers\\ninwhichtheconnectivityisfurtherrestricted,forexampletoconstraineachoutput\\nchannel itobeafunctionofonlyasubsetoftheinputchannels l.Acommon\\nwaytodothisistomaketheﬁrst moutputchannelsconnecttoonlytheﬁrst\\nninputchannels,thesecond moutputchannelsconnecttoonlythesecond n\\ninputchannels,andsoon.Seeﬁgureforanexample.Modelinginteractions 9.15\\nbetweenfewchannelsallowsthenetworktohavefewerparametersinorderto\\nreducememoryconsumptionandincreasestatisticaleﬃciency,andalsoreduces\\ntheamountofcomputationneededtoperformforwardandback-propagation. It\\naccomplishesthesegoalswithoutreducingthenumberofhiddenunits.\\nT i l e d c o n v o l ut i o n( ,;,)oﬀersacom- GregorandLeCun2010aLeetal.2010\\npromisebetweenaconvolutionallayerandalocallyconnectedlayer.Ratherthan\\nlearningaseparatesetofweightsatspatiallocation,welearnasetofkernels every\\nthatwerotatethroughaswemovethroughspace.Thismeansthatimmediately\\nneighboringlocationswillhavediﬀerentﬁlters,likeinalocallyconnectedlayer,\\nbutthememoryrequirementsforstoringtheparameterswillincreaseonlybya\\nfactorofthesizeofthissetofkernels,ratherthanthesizeoftheentireoutput\\nfeaturemap.Seeﬁgureforacomparisonoflocallyconnectedlayers,tiled 9.16\\nconvolution,andstandardconvolution.\\n3 5 2', 'CHAPTER9.CONVOLUTIONALNETWORKS\\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\\nx 4 x 4s 4 s 4\\nx 5 x 5s 5 s 5\\nx 1 x 1 x 2 x 2s 1 s 1 s 3 s 3\\nx 5 x 5s 5 s 5x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\\nx 4 x 4s 4 s 4\\nx 5 x 5s 5 s 5\\na\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 b a\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 b a\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 b a\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 b a\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0a\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 b c \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 d e \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 f g\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 h \\xa0 i \\xa0 \\xa0\\nx 4 x 4 x 3 x 3s 4 s 4 s 2 s 2\\nFigure9.14:Comparisonoflocalconnections,convolution,andfullconnections.\\n( T o p )Alocallyconnectedlayerwithapatchsizeoftwopixels.Eachedgeislabeledwith\\nauniquelettertoshowthateachedgeisassociatedwithitsownweightparameter.\\n( C e n t e r )Aconvolutionallayerwithakernelwidthoftwopixels.Thismodelhasexactly\\nthesameconnectivityasthelocallyconnectedlayer.Thediﬀerenceliesnotinwhichunits\\ninteractwitheachother,butinhowtheparametersareshared.Thelocallyconnectedlayer\\nhasnoparametersharing.Theconvolutionallayerusesthesametwoweightsrepeatedly\\nacrosstheentireinput,asindicatedbytherepetitionoftheletterslabelingeachedge.\\n( Bottom )Afullyconnectedlayerresemblesalocallyconnectedlayerinthesensethateach\\nedgehasitsownparameter(therearetoomanytolabelexplicitlywithlettersinthis\\ndiagram).However,itdoesnothavetherestrictedconnectivityofthelocallyconnected\\nlayer.\\n3 5 3', 'CHAPTER9.CONVOLUTIONALNETWORKS\\nI nput\\xa0T e nsorO ut put\\xa0T e nsor\\nS p a t i a l \\xa0 c o o r d i n a t e sC h a n n e l \\xa0 c o o r d i n a t e s\\nFigure9.15:\\xa0Aconvolutionalnetworkwiththeﬁrsttwooutputchannelsconnectedto\\nonlytheﬁrsttwoinputchannels,andthesecondtwooutputchannelsconnectedtoonly\\nthesecondtwoinputchannels.\\n3 5 4', 'CHAPTER9.CONVOLUTIONALNETWORKS\\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\\nx 4 x 4s 4 s 4\\nx 5 x 5s 5 s 5x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\\nx 4 x 4s 4 s 4\\nx 5 x 5s 5 s 5\\na\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 b a\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 b a\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 b a\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 b a\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0a\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 b c \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 d e \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 f g\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 h \\xa0 i \\xa0 \\xa0\\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\\nx 4 x 4s 4 s 4\\nx 5 x 5s 5 s 5\\na\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 b c \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 d a\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 b c \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 d a\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nFigure9.16:Acomparisonoflocallyconnectedlayers,tiledconvolution,andstandard\\nconvolution.Allthreehavethesamesetsofconnectionsbetweenunits,whenthesame\\nsizeofkernelisused.Thisdiagramillustratestheuseofakernelthatistwopixelswide.\\nThediﬀerencesbetweenthemethodsliesinhowtheyshareparameters. ( T o p )Alocally\\nconnectedlayerhasnosharingatall.Weindicatethateachconnectionhasitsownweight\\nbylabelingeachconnectionwithauniqueletter.Tiledconvolutionhasasetof ( C e n t e r )\\ntdiﬀerentkernels.Hereweillustratethecaseof t= 2.\\xa0Oneofthesekernelshasedges\\nlabeled“a”and“b,”whiletheotherhasedgeslabeled“c”and“d.”\\xa0Eachtimewemoveone\\npixeltotherightintheoutput,wemoveontousingadiﬀerentkernel.Thismeansthat,\\nlikethelocallyconnectedlayer,neighboringunitsintheoutputhavediﬀerentparameters.\\nUnlikethelocallyconnectedlayer,afterwehavegonethroughall tavailablekernels,\\nwecyclebacktotheﬁrstkernel.Iftwooutputunitsareseparatedbyamultipleof t\\nsteps,thentheyshareparameters.Traditionalconvolutionisequivalenttotiled ( Bottom )\\nconvolutionwith t= 1.Thereisonlyonekernelanditisappliedeverywhere,asindicated\\ninthediagrambyusingthekernelwithweightslabeled“a”and“b”everywhere.\\n3 5 5', 'CHAPTER9.CONVOLUTIONALNETWORKS\\nTodeﬁnetiledconvolutionalgebraically,let kbea6-Dtensor,wheretwoof\\nthedimensionscorrespondtodiﬀerentlocationsintheoutputmap.Ratherthan\\nhavingaseparateindexforeachlocationintheoutputmap,outputlocationscycle\\nthroughasetof tdiﬀerentchoicesofkernelstackineachdirection.If tisequalto\\ntheoutputwidth,thisisthesameasalocallyconnectedlayer.\\nZ i , j , k=\\ue058\\nl , m , nV l , j m , k n + − 1 + − 1 K i , l , m , n , j t , k t % + 1 % + 1 ,(9.10)\\nwhereis\\xa0themodulooperation,with % t% t=0(, t+1)% t=1,etc.It\\xa0is\\nstraightforwardtogeneralizethisequationtouseadiﬀerenttilingrangeforeach\\ndimension.\\nBothlocallyconnectedlayersandtiledconvolutionallayershaveaninteresting\\ninteractionwithmax-pooling:thedetectorunitsoftheselayersaredrivenby\\ndiﬀerentﬁlters.Iftheseﬁlterslearntodetectdiﬀerenttransformedversionsof\\nthesameunderlyingfeatures,thenthemax-pooledunitsbecomeinvarianttothe\\nlearnedtransformation(seeﬁgure).Convolutionallayersarehard-codedtobe 9.9\\ninvariantspeciﬁcallytotranslation.\\nOtheroperationsbesidesconvolutionareusuallynecessarytoimplementa\\nconvolutionalnetwork.Toperformlearning,onemustbeabletocomputethe\\ngradientwithrespecttothekernel,giventhegradientwithrespecttotheoutputs.\\nInsomesimplecases,\\xa0thisoperationcanbeperformedusingtheconvolution\\noperation,butmanycasesofinterest,includingthecaseofstridegreaterthan1,\\ndonothavethisproperty.\\nRecallthatconvolutionisalinearoperationandcanthusbedescribedasa\\nmatrixmultiplication (ifweﬁrstreshapetheinputtensorintoaﬂatvector).The\\nmatrixinvolvedisafunctionoftheconvolutionkernel.Thematrixissparseand\\neachelementofthekerneliscopiedtoseveralelementsofthematrix.Thisview\\nhelpsustoderivesomeoftheotheroperationsneededtoimplementaconvolutional\\nnetwork.\\nMultiplication bythetransposeofthematrixdeﬁnedbyconvolutionisone\\nsuchoperation.Thisistheoperationneededtoback-propagate errorderivatives\\nthroughaconvolutionallayer,soitisneededtotrainconvolutionalnetworks\\nthathavemorethanonehiddenlayer.Thissameoperationisalsoneededifwe\\nwishtoreconstructthevisibleunitsfromthehiddenunits( ,). Simard etal.1992\\nReconstructingthevisibleunitsisanoperationcommonlyusedinthemodels\\ndescribedinpartofthisbook,suchasautoencoders,RBMs,andsparsecoding. III\\nTransposeconvolutionisnecessarytoconstructconvolutionalversionsofthose\\nmodels.Likethekernelgradientoperation,thisinputgradientoperationcanbe\\n3 5 6', 'CHAPTER9.CONVOLUTIONALNETWORKS\\nimplementedusingaconvolutioninsomecases,butinthegeneralcaserequires\\nathirdoperationtobeimplemented.Caremustbetakentocoordinatethis\\ntransposeoperationwiththeforwardpropagation. Thesizeoftheoutputthatthe\\ntransposeoperationshouldreturndependsonthezeropaddingpolicyandstrideof\\ntheforwardpropagationoperation,aswellasthesizeoftheforwardpropagation’s\\noutputmap.Insomecases,multiplesizesofinputtoforwardpropagationcan\\nresultinthesamesizeofoutputmap,sothetransposeoperationmustbeexplicitly\\ntoldwhatthesizeoftheoriginalinputwas.\\nThesethreeoperations—convolution,backpropfromoutputtoweights,and\\nbackpropfromoutputtoinputs—aresuﬃcienttocomputeallofthegradients\\nneededtotrainanydepthoffeedforwardconvolutionalnetwork,aswellastotrain\\nconvolutionalnetworkswithreconstructionfunctionsbasedonthetransposeof\\nconvolution.\\xa0See ()forafullderivationoftheequationsinthe Goodfellow2010\\nfullygeneralmulti-dimensional,multi-example case.Togiveasenseofhowthese\\nequationswork,wepresentthetwodimensional,singleexampleversionhere.\\nSupposewewanttotrainaconvolutionalnetworkthatincorporatesstrided\\nconvolutionofkernelstack Kappliedtomulti-channelimage Vwithstride sas\\ndeﬁnedby c( K V , , s)asinequation.Supposewewanttominimizesomeloss 9.8\\nfunction J( V K ,).Duringforwardpropagation, wewillneedtouse citselfto\\noutput Z,whichisthenpropagatedthroughtherestofthenetworkandusedto\\ncomputethecostfunction J.Duringback-propagation, wewillreceiveatensor G\\nsuchthat G i , j , k=∂\\n∂ Z i , j , kJ , . ( V K)\\nTotrainthenetwork,weneedtocomputethederivativeswithrespecttothe\\nweightsinthekernel.Todoso,wecanuseafunction\\ng , , s ( G V) i , j , k, l=∂\\n∂ K i , j , k, lJ ,( V K) =\\ue058\\nm , nG i , m , n V j , m s k, n s l ( − × 1 ) + ( − × 1 ) + .(9.11)\\nIfthislayerisnotthebottomlayerofthenetwork,wewillneedtocompute\\nthegradientwithrespectto Vinordertoback-propagate theerrorfartherdown.\\nTodoso,wecanuseafunction\\nh , , s ( K G) i , j , k=∂\\n∂ V i , j , kJ ,( V K) (9.12)\\n=\\ue058\\nl , m\\ns . t .\\n( 1 ) + = l − × s m j\\ue058\\nn , p\\ns . t .\\n( 1 ) + = n − × s p k\\ue058\\nqK q , i , m , p G q , l , n .(9.13)\\nAutoencodernetworks,\\xa0describedinchapter,\\xa0arefeedforwardnetworks 14\\ntrainedtocopytheirinputtotheiroutput.AsimpleexampleisthePCAalgorithm,\\n3 5 7', 'CHAPTER9.CONVOLUTIONALNETWORKS\\nthatcopiesitsinput xtoanapproximatereconstruction rusingthefunction\\nW\\ue03eW x.Itiscommonformore\\xa0general autoencoders\\xa0tousemultiplication\\nbythetransposeoftheweightmatrixjustasPCAdoes.\\xa0Tomakesuchmodels\\nconvolutional,wecanusethefunction htoperformthetransposeoftheconvolution\\noperation.Supposewehavehiddenunits Hinthesameformatas Zandwedeﬁne\\nareconstruction\\nR K H = ( h , , s .) (9.14)\\nInordertotraintheautoencoder,wewillreceivethegradientwithrespect\\nto Rasatensor E.Totrainthedecoder,weneedtoobtainthegradientwith\\nrespectto K.Thisisgivenby g( H E , , s).Totraintheencoder,weneedtoobtain\\nthegradientwithrespectto H.Thisisgivenby c( K E , , s).Itisalsopossibleto\\ndiﬀerentiatethrough gusing cand h,buttheseoperationsarenotneededforthe\\nback-propagationalgorithmonanystandardnetworkarchitectures.\\nGenerally,wedonotuseonlyalinearoperationinordertotransformfrom\\ntheinputstotheoutputsinaconvolutionallayer.Wegenerallyalsoaddsome\\nbiastermtoeachoutputbeforeapplyingthenonlinearity.Thisraisesthequestion\\nofhowtoshareparametersamongthebiases.\\xa0Forlocallyconnectedlayersitis\\nnaturaltogiveeachunititsownbias,andfortiledconvolution,itisnaturalto\\nsharethebiaseswiththesametilingpatternasthekernels.Forconvolutional\\nlayers,itistypicaltohaveonebiasperchanneloftheoutputandshareitacross\\nalllocationswithineachconvolutionmap.However,iftheinputisofknown,ﬁxed\\nsize,itisalsopossibletolearnaseparatebiasateachlocationoftheoutputmap.\\nSeparatingthebiasesmayslightlyreducethestatisticaleﬃciencyofthemodel,but\\nalsoallowsthemodeltocorrectfordiﬀerencesintheimagestatisticsatdiﬀerent\\nlocations.Forexample,whenusingimplicitzeropadding,detectorunitsatthe\\nedgeoftheimagereceivelesstotalinputandmayneedlargerbiases.\\n9.6StructuredOutputs\\nConvolutionalnetworkscanbeusedtooutputahigh-dimensional,structured\\nobject,ratherthanjustpredictingaclasslabelforaclassiﬁcationtaskorareal\\nvalueforaregressiontask.Typicallythisobjectisjustatensor,emittedbya\\nstandardconvolutionallayer.Forexample,themodelmightemitatensor S,where\\nS i , j , kistheprobabilitythatpixel ( j , k)oftheinputtothenetworkbelongstoclass\\ni.Thisallowsthemodeltolabeleverypixelinanimageanddrawprecisemasks\\nthatfollowtheoutlinesofindividualobjects.\\nOneissuethatoftencomesupisthattheoutputplanecanbesmallerthanthe\\n3 5 8', 'CHAPTER9.CONVOLUTIONALNETWORKS\\nˆ Y( 1 )ˆ Y( 1 )ˆ Y( 2 )ˆ Y( 2 )ˆ Y( 3 )ˆ Y( 3 )\\nH( 1 )H( 1 )H( 2 )H( 2 )H( 3 )H( 3 )\\nXXU U UV V V W W\\nFigure9.17:Anexampleofarecurrentconvolutionalnetworkforpixellabeling.The\\ninputisanimagetensor,withaxescorrespondingtoimagerows,imagecolumns,and X\\nchannels(red,green,blue).Thegoalistooutputatensoroflabelsˆ Y,withaprobability\\ndistributionoverlabelsforeachpixel.Thistensorhasaxescorrespondingtoimagerows,\\nimagecolumns,andthediﬀerentclasses.Ratherthanoutputtingˆ Yinasingleshot,the\\nrecurrentnetworkiterativelyreﬁnesitsestimateˆ Ybyusingapreviousestimateofˆ Y\\nasinputforcreatinganewestimate.\\xa0Thesameparametersareusedforeachupdated\\nestimate,andtheestimatecanbereﬁnedasmanytimesaswewish.Thetensorof\\nconvolutionkernels Uisusedoneachsteptocomputethehiddenrepresentationgiventhe\\ninputimage.Thekerneltensor Visusedtoproduceanestimateofthelabelsgiventhe\\nhiddenvalues.Onallbuttheﬁrststep,thekernels Wareconvolvedoverˆ Ytoprovide\\ninputtothehiddenlayer.Ontheﬁrsttimestep,thistermisreplacedbyzero.Because\\nthesameparametersareusedoneachstep,thisisanexampleofarecurrentnetwork,as\\ndescribedinchapter.10\\ninputplane,asshowninﬁgure.Inthekindsofarchitectures typicallyusedfor 9.13\\nclassiﬁcationofasingleobjectinanimage,thegreatestreductioninthespatial\\ndimensionsofthenetworkcomesfromusingpoolinglayerswithlargestride.In\\nordertoproduceanoutputmapofsimilarsizeastheinput,onecanavoidpooling\\naltogether(,).Anotherstrategyistosimplyemitalower-resolution Jainetal.2007\\ngridoflabels( ,,).Finally,inprinciple,onecould PinheiroandCollobert20142015\\nuseapoolingoperatorwithunitstride.\\nOnestrategyforpixel-wiselabelingofimagesistoproduceaninitialguess\\noftheimagelabels,thenreﬁnethisinitialguessusingtheinteractionsbetween\\nneighboringpixels.Repeatingthisreﬁnementstepseveraltimescorrespondsto\\nusingthesameconvolutionsateachstage,sharingweightsbetweenthelastlayersof\\nthedeepnet(,).Thismakesthesequenceofcomputationsperformed Jainetal.2007\\nbythesuccessiveconvolutionallayerswithweightssharedacrosslayersaparticular\\nkindofrecurrentnetwork( ,,).Figureshows PinheiroandCollobert20142015 9.17\\nthearchitectureofsucharecurrentconvolutionalnetwork.\\n3 5 9', 'CHAPTER9.CONVOLUTIONALNETWORKS\\nOnceapredictionforeachpixelismade,variousmethodscanbeusedto\\nfurtherprocessthesepredictionsinordertoobtainasegmentationoftheimage\\nintoregions( ,; Briggman etal.2009Turaga 2010Farabet2013 etal.,; etal.,).\\nThegeneralideaistoassumethatlargegroupsofcontiguouspixelstendtobe\\nassociatedwiththesamelabel.Graphicalmodelscandescribetheprobabilistic\\nrelationshipsbetweenneighboringpixels.Alternatively,theconvolutionalnetwork\\ncanbetrainedtomaximizeanapproximation ofthegraphicalmodeltraining\\nobjective(,; ,). Ningetal.2005Thompsonetal.2014\\n9.7DataTypes\\nThedatausedwithaconvolutionalnetworkusuallyconsistsofseveralchannels,\\neachchannelbeingtheobservationofadiﬀerentquantityatsomepointinspace\\nortime.Seetableforexamplesofdatatypeswithdiﬀerentdimensionalities 9.1\\nandnumberofchannels.\\nForanexampleofconvolutionalnetworksappliedtovideo,seeChenetal.\\n().2010\\nSofarwehavediscussedonlythecasewhereeveryexampleinthetrainandtest\\ndatahasthesamespatialdimensions.Oneadvantagetoconvolutionalnetworks\\nisthattheycanalsoprocessinputswithvaryingspatialextents.Thesekindsof\\ninputsimplycannotberepresentedbytraditional,matrixmultiplication-based\\nneuralnetworks.Thisprovidesacompellingreasontouseconvolutionalnetworks\\nevenwhencomputational costandoverﬁttingarenotsigniﬁcantissues.\\nForexample,consideracollectionofimages,whereeachimagehasadiﬀerent\\nwidthandheight.Itisunclearhowtomodelsuchinputswithaweightmatrixof\\nﬁxedsize.Convolutionisstraightforwardtoapply;thekernelissimplyapplieda\\ndiﬀerentnumberoftimesdependingonthesizeoftheinput,andtheoutputofthe\\nconvolutionoperationscalesaccordingly.Convolutionmaybeviewedasmatrix\\nmultiplication; thesameconvolutionkernelinducesadiﬀerentsizeofdoublyblock\\ncirculantmatrixforeachsizeofinput.\\xa0Sometimes theoutputofthenetworkis\\nallowedtohavevariablesizeaswellastheinput,forexampleifwewanttoassign\\naclasslabeltoeachpixeloftheinput.Inthiscase,nofurtherdesignworkis\\nnecessary.Inothercases,thenetworkmustproducesomeﬁxed-sizeoutput,for\\nexampleifwewanttoassignasingleclasslabeltotheentireimage.Inthiscase\\nwemustmakesomeadditionaldesignsteps,likeinsertingapoolinglayerwhose\\npoolingregionsscaleinsizeproportionaltothesizeoftheinput,inorderto\\nmaintainaﬁxednumberofpooledoutputs.Someexamplesofthiskindofstrategy\\nareshowninﬁgure.9.11\\n3 6 0', 'CHAPTER9.CONVOLUTIONALNETWORKS\\nSinglechannel Multi-channel\\n1-DAudio\\xa0waveform:The\\xa0axis\\xa0we\\nconvolveovercorrespondsto\\ntime.Wediscretizetimeand\\nmeasuretheamplitudeofthe\\nwaveformoncepertimestep.Skeletonanimationdata:Anima-\\ntionsof3-Dcomputer-rendered\\ncharactersaregeneratedbyalter-\\ningtheposeofa“skeleton”over\\ntime.Ateachpointintime,the\\nposeofthecharacterisdescribed\\nbyaspeciﬁcationoftheanglesof\\neachofthejointsinthecharac-\\nter’sskeleton.Eachchannelin\\nthedatawefeedtotheconvolu-\\ntionalmodelrepresentstheangle\\naboutoneaxisofonejoint.\\n2-DAudiodatathathasbeenprepro-\\ncessedwithaFouriertransform:\\nWecantransformtheaudiowave-\\nformintoa2Dtensorwithdif-\\nferentrowscorrespondingtodif-\\nferentfrequencies\\xa0anddiﬀerent\\ncolumnscorrespondingtodiﬀer-\\nentpointsintime.Usingconvolu-\\ntioninthetimemakesthemodel\\nequivarianttoshiftsintime.Us-\\ningconvolutionacrossthefre-\\nquencyaxismakesthemodel\\nequivarianttofrequency,sothat\\nthesamemelodyplayedinadif-\\nferentoctaveproducesthesame\\nrepresentationbutatadiﬀerent\\nheightinthenetwork’soutput.Colorimagedata:Onechannel\\ncontainstheredpixels,onethe\\ngreen\\xa0pixels,\\xa0and\\xa0one\\xa0theblue\\npixels.Theconvolutionkernel\\nmovesoverboththehorizontal\\nandverticalaxesofthe\\xa0image,\\nconferringtranslationequivari-\\nanceinbothdirections.\\n3-DVolumetricdata:Acommon\\nsourceofthiskindofdataismed-\\nicalimagingtechnology,suchas\\nCTscans.Colorvideodata:Oneaxiscorre-\\nspondstotime,onetotheheight\\nofthevideoframe,andoneto\\nthewidthofthevideoframe.\\nTable9.1:Examplesofdiﬀerentformatsofdatathatcanbeusedwithconvolutional\\nnetworks.\\n3 6 1', 'CHAPTER9.CONVOLUTIONALNETWORKS\\nNotethattheuseofconvolutionforprocessingvariablesizedinputsonlymakes\\nsenseforinputsthathavevariablesizebecausetheycontainvaryingamounts\\nofobservationofthesamekindofthing—diﬀeren tlengthsofrecordingsover\\ntime,diﬀerentwidthsofobservationsoverspace,etc.Convolutiondoesnotmake\\nsenseiftheinputhasvariablesizebecauseitcanoptionallyincludediﬀerent\\nkindsofobservations.Forexample,ifweareprocessingcollegeapplications,and\\nourfeaturesconsistofbothgradesandstandardizedtestscores,butnotevery\\napplicanttookthestandardizedtest,thenitdoesnotmakesensetoconvolvethe\\nsameweightsoverboththefeaturescorrespondingtothegradesandthefeatures\\ncorrespondingtothetestscores.\\n9.8EﬃcientConvolutionAlgorithms\\nModernconvolutionalnetworkapplicationsofteninvolvenetworkscontainingmore\\nthanonemillionunits.Powerfulimplementations exploitingparallelcomputation\\nresources,asdiscussedinsection,areessential.\\xa0However,inmanycasesit 12.1\\nisalsopossibletospeedupconvolutionbyselectinganappropriateconvolution\\nalgorithm.\\nConvolutionisequivalenttoconvertingboththeinputandthekerneltothe\\nfrequencydomainusingaFouriertransform,performingpoint-wisemultiplication\\nofthetwosignals,\\xa0andconvertingbacktothetimedomainusinganinverse\\nFouriertransform.Forsomeproblemsizes,thiscanbefasterthanthenaive\\nimplementationofdiscreteconvolution.\\nWhena d-dimensionalkernelcanbeexpressedas\\xa0theouterproductof d\\nvectors,onevectorperdimension,thekerneliscalled se par abl e.Whenthe\\nkernelisseparable,naiveconvolutionisineﬃcient.Itisequivalenttocompose d\\none-dimensional convolutionswitheachofthesevectors.Thecomposedapproach\\nissigniﬁcantlyfasterthanperformingone d-dimensionalconvolutionwiththeir\\nouterproduct.Thekernelalsotakesfewerparameterstorepresentasvectors.\\nIfthekernelis welementswideineachdimension,thennaivemultidimensional\\nconvolutionrequires O( wd)runtimeandparameterstoragespace,whileseparable\\nconvolutionrequires O( w d ×)runtimeandparameterstoragespace.Ofcourse,\\nnoteveryconvolutioncanberepresentedinthisway.\\nDevisingfasterwaysofperformingconvolutionorapproximateconvolution\\nwithoutharmingtheaccuracyofthemodelisanactiveareaofresearch.Eventech-\\nniquesthatimprovetheeﬃciencyofonlyforwardpropagationareusefulbecause\\ninthecommercialsetting,itistypicaltodevotemoreresourcestodeploymentof\\nanetworkthantoitstraining.\\n3 6 2', 'CHAPTER9.CONVOLUTIONALNETWORKS\\n9.9RandomorUnsupervisedFeatures\\nTypically,themostexpensivepartofconvolutionalnetworktrainingislearningthe\\nfeatures.Theoutputlayerisusuallyrelativelyinexpensiveduetothesmallnumber\\noffeaturesprovidedasinputtothislayerafterpassingthroughseverallayersof\\npooling.Whenperformingsupervisedtrainingwithgradientdescent,everygradient\\nsteprequiresacompleterunofforwardpropagationandbackwardpropagation\\nthroughtheentirenetwork.Onewaytoreducethecostofconvolutionalnetwork\\ntrainingistousefeaturesthatarenottrainedinasupervisedfashion.\\nTherearethreebasicstrategiesforobtaining\\xa0con volutionkernelswithout\\nsupervisedtraining.Oneistosimplyinitializethemrandomly.Anotheristo\\ndesignthembyhand,forexamplebysettingeachkerneltodetectedgesata\\ncertainorientationorscale.Finally,onecanlearnthekernelswithanunsupervised\\ncriterion.Forexample, ()apply Coatesetal.2011 k-meansclusteringtosmall\\nimagepatches,thenuseeachlearnedcentroidasaconvolutionkernel.\\xa0PartIII\\ndescribesmanymoreunsupervisedlearningapproaches.Learningthefeatures\\nwithanunsupervisedcriterionallowsthemtobedeterminedseparatelyfromthe\\nclassiﬁerlayeratthetopofthearchitecture.Onecanthenextractthefeaturesfor\\ntheentiretrainingsetjustonce,essentiallyconstructinganewtrainingsetforthe\\nlastlayer.Learningthelastlayeristhentypicallyaconvexoptimization problem,\\nassumingthelastlayerissomethinglikelogisticregressionoranSVM.\\nRandomﬁltersoftenworksurprisinglywellinconvolutionalnetworks(Jarrett\\netal. etal. etal. ,;2009Saxe,;2011Pinto,;2011CoxandPinto2011Saxe,).etal.\\n()showedthatlayersconsistingofconvolutionfollowingbypoolingnaturally 2011\\nbecomefrequencyselectiveandtranslationinvariantwhenassignedrandomweights.\\nTheyarguethatthisprovidesaninexpensivewaytochoosethearchitectureof\\naconvolutionalnetwork:ﬁrstevaluatetheperformanceofseveralconvolutional\\nnetworkarchitecturesbytrainingonlythelastlayer,thentakethebestofthese\\narchitecturesandtraintheentirearchitectureusingamoreexpensiveapproach.\\nAnintermediate approachistolearnthefeatures,butusingmethodsthatdo\\nnotrequirefullforwardandback-propagationateverygradientstep.Aswith\\nmultilayerperceptrons,weusegreedylayer-wisepretraining,totraintheﬁrstlayer\\ninisolation,thenextractallfeaturesfromtheﬁrstlayeronlyonce,thentrainthe\\nsecondlayerinisolationgiventhosefeatures,andsoon.Chapterhasdescribed 8\\nhowtoperformsupervisedgreedylayer-wisepretraining,andpartextendsthisIII\\ntogreedylayer-wisepretrainingusinganunsupervisedcriterionateachlayer.The\\ncanonicalexampleofgreedylayer-wisepretrainingofaconvolutionalmodelisthe\\nconvolutionaldeepbeliefnetwork(,).Convolutionalnetworksoﬀer Leeetal.2009\\n3 6 3', 'CHAPTER9.CONVOLUTIONALNETWORKS\\nustheopportunitytotakethepretrainingstrategyonestepfurtherthanispossible\\nwithmultilayerperceptrons.Insteadoftraininganentireconvolutionallayerata\\ntime,wecantrainamodelofasmallpatch,as ()dowith Coatesetal.2011 k-means.\\nWecanthenusetheparametersfromthispatch-basedmodeltodeﬁnethekernels\\nofaconvolutionallayer.Thismeansthatitispossibletouseunsupervisedlearning\\ntotrainaconvolutionalnetworkwithouteverusingconvolutionduringthetraining\\nprocess.Usingthisapproach,wecantrainverylargemodelsandincurahigh\\ncomputational costonlyatinferencetime( ,; , Ranzatoetal.2007bJarrettetal.\\n2009Kavukcuoglu2010Coates 2013 ; etal.,; etal.,).Thisapproachwaspopular\\nfromroughly2007–2013,whenlabeleddatasetsweresmallandcomputational\\npowerwasmorelimited.Today,mostconvolutionalnetworksaretrainedina\\npurelysupervisedfashion,usingfullforwardandback-propagation throughthe\\nentirenetworkoneachtrainingiteration.\\nAswithotherapproachestounsupervisedpretraining,itremainsdiﬃcultto\\nteaseapartthecauseofsomeofthebeneﬁtsseenwiththisapproach.Unsupervised\\npretrainingmayoﬀersomeregularizationrelativetosupervisedtraining,oritmay\\nsimplyallowustotrainmuchlargerarchitectures duetothereducedcomputational\\ncostofthelearningrule.\\n9.10TheNeuroscientiﬁcBasisforConvolutionalNet-\\nworks\\nConvolutional\\xa0networksare\\xa0perhaps\\xa0the greatest\\xa0successstory\\xa0ofbiologically\\ninspiredartiﬁcialintelligence.Thoughconvolutionalnetworkshavebeenguided\\nbymanyotherﬁelds,someofthekeydesignprinciplesofneuralnetworkswere\\ndrawnfromneuroscience.\\nThehistoryofconvolutionalnetworksbeginswithneuroscientiﬁcexperiments\\nlongbeforetherelevantcomputational modelsweredeveloped.Neurophysiologists\\nDavidHubelandTorstenWieselcollaboratedforseveralyearstodeterminemany\\nofthemostbasicfactsabouthowthemammalianvisionsystemworks(Hubeland\\nWiesel195919621968 ,,,).Theiraccomplishmentswereeventuallyrecognizedwith\\naNobelprize.Theirﬁndingsthathavehadthegreatestinﬂuenceoncontemporary\\ndeeplearningmodelswerebasedonrecordingtheactivityofindividualneuronsin\\ncats.Theyobservedhowneuronsinthecat’sbrainrespondedtoimagesprojected\\ninpreciselocationsonascreeninfrontofthecat.Theirgreatdiscoverywas\\nthatneuronsintheearlyvisualsystemrespondedmoststronglytoveryspeciﬁc\\npatternsoflight,suchaspreciselyorientedbars,butrespondedhardlyatallto\\notherpatterns.\\n3 6 4', 'CHAPTER9.CONVOLUTIONALNETWORKS\\nTheirworkhelpedtocharacterizemanyaspectsofbrainfunctionthatare\\nbeyondthescopeofthisbook.Fromthepointofviewofdeeplearning,wecan\\nfocusonasimpliﬁed,cartoonviewofbrainfunction.\\nInthissimpliﬁedview,wefocusonapartofthebraincalledV1,alsoknown\\nasthe pr i m ar y v i sual c o r t e x.\\xa0V1istheﬁrstareaofthebrainthatbeginsto\\nperformsigniﬁcantlyadvancedprocessingofvisualinput.\\xa0Inthiscartoonview,\\nimagesareformedbylightarrivingintheeyeandstimulatingtheretina,the\\nlight-sensitivetissueinthebackoftheeye.Theneuronsintheretinaperform\\nsomesimplepreprocessingoftheimagebutdonotsubstantiallyalterthewayitis\\nrepresented.Theimagethenpassesthroughtheopticnerveandabrainregion\\ncalledthelateralgeniculatenucleus.\\xa0Themainrole,asfarasweareconcerned\\nhere,ofbothoftheseanatomicalregionsisprimarilyjusttocarrythesignalfrom\\ntheeyetoV1,whichislocatedatthebackofthehead.\\nAconvolutionalnetworklayerisdesignedtocapturethreepropertiesofV1:\\n1.V1isarrangedinaspatialmap.Itactuallyhasatwo-dimensionalstructure\\nmirroring\\xa0the structure\\xa0of\\xa0theimage\\xa0in\\xa0the retina.For\\xa0example,\\xa0light\\narrivingatthelowerhalfoftheretinaaﬀectsonlythecorrespondinghalfof\\nV1.Convolutionalnetworkscapturethispropertybyhavingtheirfeatures\\ndeﬁnedintermsoftwodimensionalmaps.\\n2.V1containsmany si m pl e c e l l s.Asimplecell’sactivitycantosomeextent\\nbecharacterizedbyalinear\\xa0function oftheimagein\\xa0asmall,\\xa0spatially\\nlocalizedreceptiveﬁeld.Thedetectorunitsofaconvolutionalnetworkare\\ndesignedtoemulatethesepropertiesofsimplecells.\\n3.V1alsocontainsmany c o m pl e x c e l l s.Thesecellsrespondtofeaturesthat\\naresimilartothosedetectedbysimplecells,butcomplexcellsareinvariant\\ntosmallshiftsinthepositionofthefeature.Thisinspiresthepoolingunits\\nofconvolutionalnetworks.Complexcellsarealsoinvarianttosomechanges\\ninlightingthatcannotbecapturedsimplybypoolingoverspatiallocations.\\nTheseinvarianceshaveinspiredsomeofthecross-channelpoolingstrategies\\ninconvolutionalnetworks,suchasmaxoutunits( ,). Goodfellow etal.2013a\\nThoughweknowthemostaboutV1,itisgenerallybelievedthatthesame\\nbasicprinciplesapplytootherareasofthevisualsystem.Inourcartoonviewof\\nthevisualsystem,thebasicstrategyofdetectionfollowedbypoolingisrepeatedly\\nappliedaswemovedeeperintothebrain.Aswepassthroughmultipleanatomical\\nlayersofthebrain,weeventuallyﬁndcellsthatrespondtosomespeciﬁcconcept\\nandareinvarianttomanytransformationsoftheinput.Thesecellshavebeen\\n3 6 5', 'CHAPTER9.CONVOLUTIONALNETWORKS\\nnicknamed“grandmother cells”—theideaisthatapersoncouldhaveaneuronthat\\nactivateswhenseeinganimageoftheirgrandmother, regardlessofwhethershe\\nappearsintheleftorrightsideoftheimage,whethertheimageisaclose-upof\\nherfaceorzoomedoutshotofherentirebody,whethersheisbrightlylit,orin\\nshadow,etc.\\nThesegrandmother cellshavebeenshowntoactuallyexistinthehumanbrain,\\ninaregioncalledthemedialtemporallobe( ,).Researchers Quiroga etal.2005\\ntestedwhetherindividualneuronswouldrespondtophotosoffamousindividuals.\\nTheyfoundwhathascometobecalledthe“HalleBerryneuron”:anindividual\\nneuronthatisactivatedbytheconceptofHalleBerry.Thisneuronﬁreswhena\\npersonseesaphotoofHalleBerry,adrawingofHalleBerry,oreventextcontaining\\nthewords“HalleBerry.”Ofcourse,thishasnothingtodowithHalleBerryherself;\\notherneuronsrespondedtothepresenceofBillClinton,JenniferAniston,etc.\\nThesemedialtemporallobeneuronsaresomewhatmoregeneralthanmodern\\nconvolutionalnetworks,whichwouldnotautomatically generalizetoidentifying\\napersonorobjectwhenreadingitsname.Theclosestanalogtoaconvolutional\\nnetwork’slastlayeroffeaturesisabrainareacalledtheinferotemporal cortex\\n(IT).Whenviewinganobject,informationﬂowsfromtheretina,throughthe\\nLGN,toV1,thenonwardtoV2,thenV4,thenIT.Thishappenswithintheﬁrst\\n100msofglimpsinganobject.\\xa0Ifapersonisallowedtocontinuelookingatthe\\nobjectformoretime,theninformationwillbegintoﬂowbackwardsasthebrain\\nusestop-downfeedbacktoupdatetheactivationsinthelowerlevelbrainareas.\\nHowever,ifweinterrupttheperson’sgaze,andobserveonlytheﬁringratesthat\\nresultfromtheﬁrst100msofmostlyfeedforwardactivation,thenITprovestobe\\nverysimilartoaconvolutionalnetwork.ConvolutionalnetworkscanpredictIT\\nﬁringrates,andalsoperformverysimilarlyto(timelimited)humansonobject\\nrecognitiontasks(,). DiCarlo2013\\nThatbeingsaid,therearemanydiﬀerencesbetweenconvolutionalnetworks\\nandthemammalianvisionsystem.Someofthesediﬀerencesarewellknown\\ntocomputational neuroscientists,butoutsidethescopeofthisbook.Someof\\nthesediﬀerencesarenotyetknown,becausemanybasicquestionsabouthowthe\\nmammalianvisionsystemworksremainunanswered.Asabrieflist:\\n•Thehumaneyeismostlyverylowresolution,exceptforatinypatchcalledthe\\nf o v e a.Thefoveaonlyobservesanareaaboutthesizeofathumbnailheldat\\narmslength.Thoughwefeelasifwecanseeanentiresceneinhighresolution,\\nthisisanillusioncreatedbythesubconsciouspartofourbrain,asitstitches\\ntogetherseveralglimpsesofsmallareas.Mostconvolutionalnetworksactually\\nreceivelargefullresolutionphotographsasinput.Thehumanbrainmakes\\n3 6 6', 'CHAPTER9.CONVOLUTIONALNETWORKS\\nseveraleyemovementscalled sac c adestoglimpsethemostvisuallysalient\\nortask-relevantpartsofascene.Incorporatingsimilarattentionmechanisms\\nintodeeplearningmodelsisanactiveresearchdirection.Inthecontextof\\ndeeplearning,attentionmechanismshavebeenmostsuccessfulfornatural\\nlanguageprocessing,asdescribedinsection.Severalvisualmodels 12.4.5.1\\nwithfoveationmechanismshavebeendevelopedbutsofarhavenotbecome\\nthedominantapproach(LarochelleandHinton2010Denil2012 ,;etal.,).\\n•Thehumanvisualsystemisintegratedwithmanyothersenses,suchas\\nhearing,andfactorslikeourmoodsandthoughts.Convolutionalnetworks\\nsofararepurelyvisual.\\n•Thehumanvisualsystemdoesmuchmorethanjustrecognizeobjects.Itis\\nabletounderstandentirescenesincludingmanyobjectsandrelationships\\nbetweenobjects,andprocessesrich3-Dgeometricinformationneededfor\\nourbodiestointerfacewiththeworld.Convolutionalnetworkshavebeen\\nappliedtosomeoftheseproblemsbuttheseapplicationsareintheirinfancy.\\n•EvensimplebrainareaslikeV1areheavilyimpactedbyfeedbackfromhigher\\nlevels.Feedbackhasbeenexploredextensivelyinneuralnetworkmodelsbut\\nhasnotyetbeenshowntooﬀeracompellingimprovement.\\n•WhilefeedforwardITﬁringratescapturemuchofthesameinformationas\\nconvolutionalnetworkfeatures,itisnotclearhowsimilartheintermediate\\ncomputations are.Thebrainprobablyusesverydiﬀerentactivationand\\npoolingfunctions.Anindividualneuron’sactivationprobablyisnotwell-\\ncharacterizedbyasinglelinearﬁlterresponse.ArecentmodelofV1involves\\nmultiplequadraticﬁltersforeachneuron(,).Indeedour Rustetal.2005\\ncartoonpictureof“simplecells”\\xa0and“complexcells”\\xa0mightcreateanon-\\nexistentdistinction;simplecellsandcomplexcellsmightbothbethesame\\nkindofcellbutwiththeir“parameters”enablingacontinuumofbehaviors\\nrangingfromwhatwecall“simple”towhatwecall“complex.”\\nItis\\xa0alsoworthmentioningthatneuroscience\\xa0hastold\\xa0usrelativelylittle\\nabouthowtotrainconvolutionalnetworks.Modelstructureswithparameter\\nsharingacrossmultiplespatiallocationsdatebacktoearlyconnectionistmodels\\nofvision( ,),butthesemodelsdidnotusethemodern MarrandPoggio1976\\nback-propagationalgorithmandgradientdescent.Forexample,theNeocognitron\\n(Fukushima1980,)incorporatedmostofthemodelarchitecturedesignelementsof\\nthemodernconvolutionalnetworkbutreliedonalayer-wiseunsupervisedclustering\\nalgorithm.\\n3 6 7', 'CHAPTER9.CONVOLUTIONALNETWORKS\\nLang\\xa0andHinton\\xa01988()introducedthe\\xa0use\\xa0ofback-propagation\\xa0totrain\\nt i m e - del a y neur al net w o r k s(TDNNs).Tousecontemporary terminology,\\nTDNNsareone-dimensional convolutionalnetworksappliedtotimeseries.Back-\\npropagationappliedtothesemodelswasnotinspiredbyanyneuroscientiﬁcobserva-\\ntionandisconsideredbysometobebiologicallyimplausible.Followingthesuccess\\nofback-propagation-based trainingofTDNNs,( ,)developed LeCunetal.1989\\nthemodernconvolutionalnetworkbyapplyingthesametrainingalgorithmto2-D\\nconvolutionappliedtoimages.\\nSofarwehavedescribedhowsimplecellsareroughlylinearandselectivefor\\ncertainfeatures,complexcellsaremorenonlinearandbecomeinvarianttosome\\ntransformationsofthesesimplecellfeatures,andstacksoflayersthatalternate\\nbetweenselectivityandinvariancecanyieldgrandmother cellsforveryspeciﬁc\\nphenomena.Wehavenotyetdescribedpreciselywhattheseindividualcellsdetect.\\nInadeep,nonlinearnetwork,itcanbediﬃculttounderstandthefunctionof\\nindividualcells.Simplecellsintheﬁrstlayerareeasiertoanalyze,becausetheir\\nresponsesaredrivenbyalinearfunction.Inanartiﬁcialneuralnetwork,wecan\\njustdisplayanimageoftheconvolutionkerneltoseewhatthecorresponding\\nchannelofaconvolutionallayerrespondsto.Inabiologicalneuralnetwork,we\\ndonothaveaccesstotheweightsthemselves.Instead,weputanelectrodeinthe\\nneuronitself,displayseveralsamplesofwhitenoiseimagesinfrontoftheanimal’s\\nretina,andrecordhoweachofthesesamplescausestheneurontoactivate.Wecan\\nthenﬁtalinearmodeltotheseresponsesinordertoobtainanapproximation of\\ntheneuron’sweights.Thisapproachisknownas r e v e r se c o r r e l at i o n(Ringach\\nandShapley2004,).\\nReversecorrelationshowsusthatmostV1cellshaveweightsthataredescribed\\nby G ab o r f unc t i o ns.\\xa0TheGaborfunctiondescribestheweightata2-Dpoint\\nintheimage.Wecanthinkofanimageasbeingafunctionof2-Dcoordinates,\\nI( x , y).Likewise,wecanthinkofasimplecellassamplingtheimageatasetof\\nlocations,deﬁnedbyasetof xcoordinates Xandasetof ycoordinates, Y,and\\napplyingweightsthatarealsoafunctionofthelocation, w( x , y).Fromthispoint\\nofview,theresponseofasimplecelltoanimageisgivenby\\ns I() =\\ue058\\nx ∈ X\\ue058\\ny ∈ Yw x , y I x , y . ()() (9.15)\\nSpeciﬁcally,takestheformofaGaborfunction: w x , y()\\nw x , y α , β (; x , β y , f , φ , x 0 , y 0 , τ α) = exp\\ue000− β x x\\ue030 2− β y y\\ue030 2\\ue001\\ncos( f x\\ue030+) φ ,(9.16)\\nwhere\\nx\\ue030= ( x x − 0)cos()+( τ y y − 0)sin() τ (9.17)\\n3 6 8', 'CHAPTER9.CONVOLUTIONALNETWORKS\\nand\\ny\\ue030= ( − x x − 0)sin()+( τ y y − 0)cos() τ . (9.18)\\nHere, α, β x, β y, f, φ, x 0, y 0,and τareparametersthatcontroltheproperties\\noftheGaborfunction.FigureshowssomeexamplesofGaborfunctionswith 9.18\\ndiﬀerentsettingsoftheseparameters.\\nTheparameters x 0, y 0,and τdeﬁneacoordinatesystem.\\xa0Wetranslateand\\nrotate xand ytoform x\\ue030and y\\ue030.Speciﬁcally,thesimplecellwillrespondtoimage\\nfeaturescenteredatthepoint( x 0, y 0),anditwillrespondtochangesinbrightness\\naswemovealongalinerotatedradiansfromthehorizontal. τ\\nViewedasafunctionof x\\ue030and y\\ue030,thefunction wthenrespondstochangesin\\nbrightnessaswemovealongthe x\\ue030axis.\\xa0Ithastwoimportantfactors:oneisa\\nGaussianfunctionandtheotherisacosinefunction.\\nTheGaussianfactor αexp\\ue000\\n− β x x\\ue030 2− β y y\\ue030 2\\ue001\\ncanbeseenasagatingtermthat\\nensuresthesimplecellwillonlyrespondtovaluesnearwhere x\\ue030and y\\ue030areboth\\nzero,inotherwords,nearthecenterofthecell’sreceptiveﬁeld.Thescalingfactor\\nαadjuststhetotalmagnitudeofthesimplecell’sresponse,while β xand β ycontrol\\nhowquicklyitsreceptiveﬁeldfallsoﬀ.\\nThecosinefactor cos( f x\\ue030+ φ) controlshowthesimplecellrespondstochanging\\nbrightnessalongthe x\\ue030axis.Theparameter fcontrolsthefrequencyofthecosine\\nandcontrolsitsphaseoﬀset. φ\\nAltogether,thiscartoonviewofsimplecellsmeansthatasimplecellresponds\\ntoaspeciﬁcspatialfrequencyofbrightnessinaspeciﬁcdirectionataspeciﬁc\\nlocation.Simplecellsaremostexcitedwhenthewaveofbrightnessintheimage\\nhasthesamephaseastheweights.Thisoccurswhentheimageisbrightwherethe\\nweightsarepositiveanddarkwheretheweightsarenegative.Simplecellsaremost\\ninhibitedwhenthewaveofbrightnessisfullyoutofphasewiththeweights—when\\ntheimageisdarkwheretheweightsarepositiveandbrightwheretheweightsare\\nnegative.\\nThecartoonviewofacomplexcellisthatitcomputesthe L2normofthe\\n2-Dvectorcontainingtwosimplecells’responses: c( I)=\\ue070\\ns 0() I2+ s 1() I2.\\xa0An\\nimportantspecialcaseoccurswhen s 1hasallofthesameparametersas s 0except\\nfor φ,and φissetsuchthat s 1isonequartercycleoutofphasewith s 0.Inthis\\ncase, s 0and s 1forma q uadr at u r e pai r.Acomplexcelldeﬁnedinthisway\\nrespondswhentheGaussianreweightedimage I( x , y)exp( − β x x\\ue030 2− β y y\\ue030 2) contains\\nahighamplitudesinusoidalwavewithfrequency findirection τnear ( x 0 , y 0),\\nregardlessofthephaseoﬀsetofthiswave.Inotherwords,thecomplexcellis\\ninvarianttosmalltranslationsoftheimageindirection τ,ortonegatingtheimage\\n3 6 9', 'CHAPTER9.CONVOLUTIONALNETWORKS\\nFigure9.18:Gaborfunctionswithavarietyofparametersettings.Whiteindicates\\nlargepositiveweight,blackindicateslargenegativeweight,andthebackgroundgray\\ncorrespondstozeroweight. ( L e f t )Gaborfunctionswithdiﬀerentvaluesoftheparameters\\nthatcontrolthecoordinatesystem: x 0, y 0,and τ.\\xa0EachGaborfunctioninthisgridis\\nassignedavalueof x 0and y 0proportionaltoitspositioninitsgrid,and τischosenso\\nthateachGaborﬁlterissensitivetothedirectionradiatingoutfromthecenterofthegrid.\\nFortheothertwoplots, x 0, y 0,and τareﬁxedtozero.\\xa0 Gaborfunctionswith ( C e n t e r )\\ndiﬀerentGaussianscaleparameters β xand β y.Gaborfunctionsarearrangedinincreasing\\nwidth(decreasing β x)aswemovelefttorightthroughthegrid,andincreasingheight\\n(decreasing β y)aswemovetoptobottom.Fortheothertwoplots,the βvaluesareﬁxed\\nto1.5 ×theimagewidth.Gaborfunctionswithdiﬀerentsinusoidparameters ( R i g h t ) f\\nand φ.Aswemovetoptobottom, fincreases,andaswemovelefttoright, φincreases.\\nFortheothertwoplots,isﬁxedto0andisﬁxedto5theimagewidth. φ f ×\\n(replacingblackwithwhiteandviceversa).\\nSomeofthemoststrikingcorrespondencesbetweenneuroscienceandmachine\\nlearningcomefromvisuallycomparingthefeatureslearnedbymachinelearning\\nmodelswiththoseemployedbyV1. ()showedthat OlshausenandField1996\\nasimpleunsupervisedlearningalgorithm,\\xa0sparse coding,learnsfeatureswith\\nreceptiveﬁeldssimilartothoseofsimplecells.Sincethen,wehavefoundthat\\nanextremelywidevarietyofstatisticallearningalgorithmslearnfeatureswith\\nGabor-likefunctionswhenappliedtonaturalimages.Thisincludesmostdeep\\nlearningalgorithms,whichlearnthesefeaturesintheirﬁrstlayer.Figure9.19\\nshowssomeexamples.Becausesomanydiﬀerentlearningalgorithmslearnedge\\ndetectors,itisdiﬃculttoconcludethatanyspeciﬁclearningalgorithmisthe\\n“right”modelofthebrainjustbasedonthefeaturesthatitlearns(thoughitcan\\ncertainlybeabadsignifanalgorithmdoeslearnsomesortofedgedetector not\\nwhenappliedtonaturalimages).Thesefeaturesareanimportantpartofthe\\nstatisticalstructureofnaturalimagesandcanberecoveredbymanydiﬀerent\\napproachestostatisticalmodeling.SeeHyvärinen 2009etal.()forareviewofthe\\nﬁeldofnaturalimagestatistics.\\n3 7 0', 'CHAPTER9.CONVOLUTIONALNETWORKS\\nFigure9.19:Manymachinelearningalgorithmslearnfeaturesthatdetectedgesorspeciﬁc\\ncolorsofedgeswhenappliedtonaturalimages.Thesefeaturedetectorsarereminiscentof\\ntheGaborfunctionsknowntobepresentinprimaryvisualcortex. ( L e f t )Weightslearned\\nbyanunsupervisedlearningalgorithm(spikeandslabsparsecoding)appliedtosmall\\nimagepatches. ( R i g h t )Convolutionkernelslearnedbytheﬁrstlayerofafullysupervised\\nconvolutionalmaxoutnetwork.Neighboringpairsofﬁltersdrivethesamemaxoutunit.\\n9.11ConvolutionalNetworksandtheHistoryofDeep\\nLearning\\nConvolutionalnetworkshaveplayedanimportantroleinthehistoryofdeep\\nlearning.Theyareakeyexampleofasuccessfulapplicationofinsightsobtained\\nbystudyingthebraintomachinelearningapplications.Theywerealsosomeof\\ntheﬁrstdeepmodelstoperformwell,longbeforearbitrarydeepmodelswere\\nconsideredviable.Convolutionalnetworkswerealsosomeoftheﬁrstneural\\nnetworkstosolveimportantcommercialapplicationsandremainattheforefront\\nofcommercialapplicationsofdeeplearningtoday.Forexample,inthe1990s,the\\nneuralnetworkresearchgroupatAT&Tdevelopedaconvolutionalnetworkfor\\nreadingchecks(,).Bytheendofthe1990s,thissystemdeployed LeCunetal.1998b\\nbyNECwasreadingover10%ofallthechecksintheUS.Later,severalOCR\\nandhandwritingrecognitionsystemsbasedonconvolutionalnetsweredeployedby\\nMicrosoft( ,).Seechapterformoredetailsonsuchapplications Simardetal.2003 12\\nandmoremodernapplicationsofconvolutionalnetworks.See () LeCunetal.2010\\nforamorein-depthhistoryofconvolutionalnetworksupto2010.\\nConvolutionalnetworkswerealsousedtowinmanycontests.Thecurrent\\nintensityofcommercialinterestindeeplearningbeganwhenKrizhevskyetal.\\n()wontheImageNetobjectrecognitionchallenge,butconvolutionalnetworks 2012\\n3 7 1', 'CHAPTER9.CONVOLUTIONALNETWORKS\\nhadbeenusedtowinothermachinelearningandcomputervisioncontestswith\\nlessimpactforyearsearlier.\\nConvolutionalnetsweresomeoftheﬁrstworkingdeepnetworkstrainedwith\\nback-propagation.Itisnotentirelyclearwhyconvolutionalnetworkssucceeded\\nwhengeneralback-propagationnetworkswereconsideredtohavefailed.Itmay\\nsimplybethatconvolutionalnetworksweremorecomputationally eﬃcientthan\\nfullyconnectednetworks,soitwaseasiertorunmultipleexperimentswiththem\\nandtunetheirimplementation andhyperparameters.Largernetworksalsoseem\\ntobeeasiertotrain.Withmodernhardware,largefullyconnectednetworks\\nappeartoperformreasonablyonmanytasks,evenwhenusingdatasetsthatwere\\navailableandactivationfunctionsthatwerepopularduringthetimeswhenfully\\nconnectednetworkswerebelievednottoworkwell.Itmaybethattheprimary\\nbarrierstothesuccessofneuralnetworkswerepsychological(practitioners did\\nnotexpectneuralnetworkstowork,sotheydidnotmakeaseriouseﬀorttouse\\nneuralnetworks).Whateverthecase,itisfortunatethatconvolutionalnetworks\\nperformedwelldecadesago.Inmanyways,theycarriedthetorchfortherestof\\ndeeplearningandpavedthewaytotheacceptanceofneuralnetworksingeneral.\\nConvolutionalnetworksprovideawaytospecializeneuralnetworkstowork\\nwithdatathathasacleargrid-structuredtopologyandtoscalesuchmodelsto\\nverylargesize.Thisapproachhasbeenthemostsuccessfulonatwo-dimensional,\\nimagetopology.Toprocessone-dimensional, sequentialdata,weturnnextto\\nanotherpowerfulspecializationoftheneuralnetworksframework:recurrentneural\\nnetworks.\\n3 7 2']}, 'DeepLearning-chapter-8.pdf': {'text': ['C h a p t e r 8\\nOptimizationforTrainingDeep\\nModels\\nDeeplearningalgorithmsinvolveoptimization inmanycontexts.Forexample,\\nperforminginferenceinmodelssuchasPCAinvolvessolvinganoptimization\\nproblem.Weoftenuseanalyticaloptimization towriteproofsordesignalgorithms.\\nOfallofthemanyoptimization problemsinvolvedindeeplearning,themost\\ndiﬃcultisneuralnetworktraining.Itisquitecommontoinvestdaystomonthsof\\ntimeonhundredsofmachinesinordertosolveevenasingleinstanceoftheneural\\nnetworktrainingproblem.Becausethisproblemissoimportantandsoexpensive,\\naspecializedsetofoptimization techniqueshavebeendevelopedforsolvingit.\\nThischapterpresentstheseoptimization techniquesforneuralnetworktraining.\\nIfyouareunfamiliarwiththebasicprinciplesofgradient-basedoptimization,\\nwesuggestreviewingchapter.Thatchapterincludesabriefoverviewofnumerical 4\\noptimization ingeneral.\\nThischapterfocusesononeparticularcaseofoptimization: ﬁndingtheparam-\\netersθofaneuralnetworkthatsigniﬁcantlyreduceacostfunction J(θ),which\\ntypicallyincludesaperformancemeasureevaluatedontheentiretrainingsetas\\nwellasadditionalregularizationterms.\\nWebeginwithadescriptionofhowoptimization usedasatrainingalgorithm\\nforamachinelearningtaskdiﬀersfrompureoptimization. Next,wepresentseveral\\noftheconcretechallengesthatmakeoptimization ofneuralnetworksdiﬃcult.We\\nthendeﬁneseveralpracticalalgorithms,includingbothoptimization algorithms\\nthemselvesandstrategiesforinitializingtheparameters.Moreadvancedalgorithms\\nadapttheirlearningratesduringtrainingorleverageinformationcontainedin\\n274', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nthesecondderivativesofthecostfunction.Finally,weconcludewithareviewof\\nseveraloptimization strategiesthatareformedbycombiningsimpleoptimization\\nalgorithmsintohigher-levelprocedures.\\n8.1HowLearningDiﬀersfromPureOptimization\\nOptimization algorithmsusedfortrainingofdeepmodelsdiﬀerfromtraditional\\noptimization algorithmsinseveralways.Machinelearningusuallyactsindirectly.\\nInmostmachinelearningscenarios,wecareaboutsomeperformancemeasure\\nP,thatisdeﬁnedwithrespecttothetestsetandmayalsobeintractable.We\\nthereforeoptimize Ponlyindirectly.Wereduceadiﬀerentcostfunction J(θ)in\\nthehopethatdoingsowillimprove P.Thisisincontrasttopureoptimization,\\nwhereminimizing Jisagoalinandofitself.Optimization algorithmsfortraining\\ndeepmodelsalsotypicallyincludesomespecializationonthespeciﬁcstructureof\\nmachinelearningobjectivefunctions.\\nTypically,thecostfunctioncanbewrittenasanaverageoverthetrainingset,\\nsuchas\\nJ() = θ E ( ) ˆ x ,y ∼ pdataL f , y , ((;)xθ) (8.1)\\nwhere Listheper-examplelossfunction, f(x;θ)isthepredictedoutputwhen\\ntheinputisx,ˆ p da t aistheempiricaldistribution.Inthesupervisedlearningcase,\\nyisthetargetoutput.Throughoutthischapter,wedeveloptheunregularized\\nsupervisedcase,wheretheargumentsto Lare f(x;θ)and y.However,itistrivial\\ntoextendthisdevelopment,forexample,toincludeθorxasarguments,orto\\nexclude yasarguments,inordertodevelopvariousformsofregularizationor\\nunsupervisedlearning.\\nEquationdeﬁnesanobjectivefunctionwithrespecttothetrainingset.We 8.1\\nwouldusuallyprefertominimizethecorrespondingobjectivefunctionwherethe\\nexpectationistakenacrossthedatageneratingdistribution p da t aratherthanjust\\novertheﬁnitetrainingset:\\nJ∗() = θ E ( ) x ,y ∼ pdataL f , y . ((;)xθ) (8.2)\\n8.1.1EmpiricalRiskMinimization\\nThegoalofamachinelearningalgorithmistoreducetheexpectedgeneralization\\nerrorgivenbyequation.Thisquantityisknownasthe 8.2 risk.Weemphasizehere\\nthattheexpectationistakenoverthetrueunderlyingdistribution p da t a.Ifweknew\\nthetruedistribution p da t a(x , y),riskminimization wouldbeanoptimization task\\n2 7 5', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nsolvablebyanoptimization algorithm.However,whenwedonotknow p da t a(x , y)\\nbutonlyhaveatrainingsetofsamples,wehaveamachinelearningproblem.\\nThesimplestwaytoconvertamachinelearningproblembackintoanop-\\ntimizationproblemistominimizetheexpectedlossonthetrainingset.This\\nmeansreplacingthetruedistribution p(x , y) withtheempiricaldistributionˆ p(x , y)\\ndeﬁnedbythetrainingset.Wenowminimizetheempiricalrisk\\nE x ,y ∼ ˆ pdata ( ) x , y[((;))] = L fxθ , y1\\nmm \\ue058\\ni = 1L f((x( ) i;)θ , y( ) i)(8.3)\\nwhereisthenumberoftrainingexamples. m\\nThetrainingprocessbasedonminimizingthisaveragetrainingerrorisknown\\nasempiricalriskminimization.Inthissetting,machinelearningisstillvery\\nsimilartostraightforwardoptimization. Ratherthanoptimizingtheriskdirectly,\\nweoptimizetheempiricalrisk,andhopethattheriskdecreasessigniﬁcantlyas\\nwell.Avarietyoftheoreticalresultsestablishconditionsunderwhichthetruerisk\\ncanbeexpectedtodecreasebyvariousamounts.\\nHowever,empiricalriskminimization ispronetooverﬁtting.Modelswith\\nhighcapacitycansimplymemorizethetrainingset.Inmanycases,empirical\\nriskminimization isnotreallyfeasible.Themosteﬀectivemodernoptimization\\nalgorithmsarebasedongradientdescent,butmanyusefullossfunctions,such\\nas0-1loss,havenousefulderivatives(thederivativeiseitherzeroorundeﬁned\\neverywhere).Thesetwoproblemsmeanthat,inthecontextofdeeplearning,we\\nrarelyuseempiricalriskminimization. Instead,wemustuseaslightlydiﬀerent\\napproach,inwhichthequantitythatweactuallyoptimizeisevenmorediﬀerent\\nfromthequantitythatwetrulywanttooptimize.\\n8.1.2SurrogateLossFunctionsandEarlyStopping\\nSometimes,thelossfunctionweactuallycareabout(sayclassiﬁcationerror)isnot\\nonethatcanbeoptimizedeﬃciently.Forexample,exactlyminimizingexpected0-1\\nlossistypicallyintractable(exponentialintheinputdimension),evenforalinear\\nclassiﬁer(MarcotteandSavard1992,).Insuchsituations,onetypicallyoptimizes\\nasurrogatelossfunctioninstead,whichactsasaproxybuthasadvantages.\\nForexample,thenegativelog-likelihoodofthecorrectclassistypicallyusedasa\\nsurrogateforthe0-1loss.Thenegativelog-likelihoodallowsthemodeltoestimate\\ntheconditionalprobabilityoftheclasses,giventheinput,andifthemodelcan\\ndothatwell,thenitcanpicktheclassesthatyieldtheleastclassiﬁcationerrorin\\nexpectation.\\n2 7 6', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nInsomecases,asurrogatelossfunctionactuallyresultsinbeingabletolearn\\nmore.Forexample,thetestset0-1lossoftencontinuestodecreaseforalong\\ntimeafterthetrainingset0-1losshasreachedzero,whentrainingusingthe\\nlog-likelihoodsurrogate.Thisisbecauseevenwhentheexpected0-1lossiszero,\\nonecanimprovetherobustnessoftheclassiﬁerbyfurtherpushingtheclassesapart\\nfromeachother,obtainingamoreconﬁdentandreliableclassiﬁer,thusextracting\\nmoreinformationfromthetrainingdatathanwouldhavebeenpossiblebysimply\\nminimizingtheaverage0-1lossonthetrainingset.\\nAveryimportantdiﬀerencebetweenoptimization ingeneralandoptimization\\nasweuseitfortrainingalgorithmsisthattrainingalgorithmsdonotusuallyhalt\\natalocalminimum.Instead,amachinelearningalgorithmusuallyminimizes\\nasurrogatelossfunctionbuthaltswhenaconvergencecriterionbasedonearly\\nstopping(section)issatisﬁed.Typicallytheearlystoppingcriterionisbased 7.8\\nonthetrueunderlyinglossfunction,suchas0-1lossmeasuredonavalidationset,\\nandisdesignedtocausethealgorithmtohaltwheneveroverﬁttingbeginstooccur.\\nTrainingoftenhaltswhilethesurrogatelossfunctionstillhaslargederivatives,\\nwhichisverydiﬀerentfromthepureoptimization setting,whereanoptimization\\nalgorithmisconsideredtohaveconvergedwhenthegradientbecomesverysmall.\\n8.1.3BatchandMinibatchAlgorithms\\nOneaspectofmachinelearningalgorithmsthatseparatesthemfromgeneral\\noptimization algorithmsisthattheobjectivefunctionusuallydecomposesasasum\\noverthetrainingexamples.Optimization algorithmsformachinelearningtypically\\ncomputeeachupdatetotheparametersbasedonanexpectedvalueofthecost\\nfunctionestimatedusingonlyasubsetofthetermsofthefullcostfunction.\\nForexample,maximumlikelihoodestimationproblems,whenviewedinlog\\nspace,decomposeintoasumovereachexample:\\nθ M L= argmax\\nθm \\ue058\\ni = 1log p m o de l(x( ) i, y( ) i;)θ . (8.4)\\nMaximizingthissumisequivalenttomaximizingtheexpectationoverthe\\nempiricaldistributiondeﬁnedbythetrainingset:\\nJ() = θ E x ,y ∼ ˆ pdatalog p m o de l(;)x , yθ . (8.5)\\nMostofthepropertiesoftheobjectivefunction Jusedbymostofouropti-\\nmizationalgorithmsarealsoexpectationsoverthetrainingset.Forexample,the\\n2 7 7', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nmostcommonlyusedpropertyisthegradient:\\n∇ θ J() = θ E x ,y ∼ ˆ pdata∇ θlog p m o de l(;)x , yθ . (8.6)\\nComputing\\xa0this expectation\\xa0exactly\\xa0isvery\\xa0expensive\\xa0because\\xa0it\\xa0requires\\nevaluatingthemodeloneveryexampleintheentiredataset.Inpractice,wecan\\ncomputetheseexpectationsbyrandomlysamplingasmallnumberofexamples\\nfromthedataset,thentakingtheaverageoveronlythoseexamples.\\nRecallthatthestandarderrorofthemean(equation)estimatedfrom 5.46 n\\nsamplesisgivenby σ /√n ,where σisthetruestandarddeviationofthevalueof\\nthesamples.Thedenominator of√nshowsthattherearelessthanlinearreturns\\ntousingmoreexamplestoestimatethegradient.Comparetwohypothetical\\nestimatesofthegradient,onebasedon100examplesandanotherbasedon10,000\\nexamples.Thelatterrequires100timesmorecomputationthantheformer,but\\nreducesthestandarderrorofthemeanonlybyafactorof10.Mostoptimization\\nalgorithmsconvergemuchfaster(intermsoftotalcomputation,notintermsof\\nnumberofupdates)iftheyareallowedtorapidlycomputeapproximate estimates\\nofthegradientratherthanslowlycomputingtheexactgradient.\\nAnotherconsiderationmotivatingstatisticalestimationofthegradientfroma\\nsmallnumberofsamplesisredundancyinthetrainingset.Intheworstcase,all\\nmsamplesinthetrainingsetcouldbeidenticalcopiesofeachother.Asampling-\\nbasedestimateofthegradientcouldcomputethecorrectgradientwithasingle\\nsample,using mtimeslesscomputationthanthenaiveapproach.Inpractice,we\\nareunlikelytotrulyencounterthisworst-casesituation,butwemayﬁndlarge\\nnumbersofexamplesthatallmakeverysimilarcontributionstothegradient.\\nOptimization algorithmsthatusetheentiretrainingsetarecalledbatchor\\ndeterministicgradientmethods,becausetheyprocessallofthetrainingexamples\\nsimultaneouslyinalargebatch.Thisterminologycanbesomewhatconfusing\\nbecausetheword“batch”isalsooftenusedtodescribetheminibatchusedby\\nminibatchstochasticgradientdescent.Typicallytheterm“batchgradientdescent”\\nimpliestheuseofthefulltrainingset,whiletheuseoftheterm“batch”todescribe\\nagroupofexamplesdoesnot.\\xa0Forexample,itisverycommontousetheterm\\n“batchsize”todescribethesizeofaminibatch.\\nOptimization algorithmsthatuseonlyasingleexampleatatimearesometimes\\ncalledstochasticorsometimesonlinemethods.Thetermonlineisusually\\nreservedforthecasewheretheexamplesaredrawnfromastreamofcontinually\\ncreatedexamplesratherthanfromaﬁxed-sizetrainingsetoverwhichseveral\\npassesaremade.\\nMostalgorithmsusedfordeeplearningfallsomewhereinbetween,usingmore\\n2 7 8', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nthanonebutlessthanallofthetrainingexamples.Theseweretraditionallycalled\\nminibatchorminibatchstochasticmethodsanditisnowcommontosimply\\ncallthemstochasticmethods.\\nThecanonicalexampleofastochasticmethodisstochasticgradientdescent,\\npresentedindetailinsection.8.3.1\\nMinibatchsizesaregenerallydrivenbythefollowingfactors:\\n•Largerbatchesprovideamoreaccurateestimateofthegradient,butwith\\nlessthanlinearreturns.\\n•Multicorearchitectures areusuallyunderutilized byextremelysmallbatches.\\nThismotivatesusingsomeabsoluteminimumbatchsize,belowwhichthere\\nisnoreductioninthetimetoprocessaminibatch.\\n•Ifallexamplesinthebatcharetobeprocessedinparallel(asistypically\\nthecase),thentheamountofmemoryscaleswiththebatchsize.Formany\\nhardwaresetupsthisisthelimitingfactorinbatchsize.\\n•Somekindsofhardwareachievebetterruntimewithspeciﬁcsizesofarrays.\\nEspeciallywhenusingGPUs,itiscommonforpowerof2batchsizestooﬀer\\nbetterruntime.Typicalpowerof2batchsizesrangefrom32to256,with16\\nsometimesbeingattemptedforlargemodels.\\n•Smallbatchescanoﬀeraregularizingeﬀect( ,), WilsonandMartinez2003\\nperhapsduetothenoisetheyaddtothelearningprocess.Generalization\\nerrorisoftenbestforabatchsizeof1.\\xa0Trainingwithsuchasmallbatch\\nsizemightrequireasmalllearningratetomaintainstabilityduetothehigh\\nvarianceintheestimateofthegradient.Thetotalruntimecanbeveryhigh\\nduetotheneedtomakemoresteps,bothbecauseofthereducedlearning\\nrateandbecauseittakesmorestepstoobservetheentiretrainingset.\\nDiﬀerentkindsofalgorithmsusediﬀerentkindsofinformationfromthemini-\\nbatchindiﬀerentways.Somealgorithmsaremoresensitivetosamplingerrorthan\\nothers,eitherbecausetheyuseinformationthatisdiﬃculttoestimateaccurately\\nwithfewsamples,orbecausetheyuseinformationinwaysthatamplifysampling\\nerrorsmore.Methodsthatcomputeupdatesbasedonlyonthegradientgare\\nusuallyrelativelyrobustandcanhandlesmallerbatchsizeslike100.Second-order\\nmethods,whichusealsotheHessianmatrixHandcomputeupdatessuchas\\nH− 1g,typicallyrequiremuchlargerbatchsizeslike10,000.Theselargebatch\\nsizesarerequiredtominimizeﬂuctuationsintheestimatesofH− 1g.Suppose\\nthatHisestimatedperfectlybuthasapoorconditionnumber.Multiplication by\\n2 7 9', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nHoritsinverseampliﬁespre-existingerrors,inthiscase,estimationerrorsing.\\nVerysmallchangesintheestimateofgcanthuscauselargechangesintheupdate\\nH− 1g,evenifHwereestimatedperfectly.Ofcourse,Hwillbeestimatedonly\\napproximately,sotheupdateH− 1gwillcontainevenmoreerrorthanwewould\\npredictfromapplyingapoorlyconditionedoperationtotheestimateof.g\\nItisalsocrucialthattheminibatchesbeselectedrandomly.Computingan\\nunbiasedestimateoftheexpectedgradientfromasetofsamplesrequiresthatthose\\nsamplesbeindependent.Wealsowishfortwosubsequentgradientestimatestobe\\nindependentfromeachother,sotwosubsequentminibatchesofexamplesshould\\nalsobeindependentfromeachother.Manydatasetsaremostnaturallyarranged\\ninawaywheresuccessiveexamplesarehighlycorrelated.Forexample,wemight\\nhaveadatasetofmedicaldatawithalonglistofbloodsampletestresults.This\\nlistmightbearrangedsothatﬁrstwehaveﬁvebloodsamplestakenatdiﬀerent\\ntimesfromtheﬁrstpatient,thenwehavethreebloodsamplestakenfromthe\\nsecondpatient,thenthebloodsamplesfromthethirdpatient,andsoon.Ifwe\\nweretodrawexamplesinorderfromthislist,theneachofourminibatcheswould\\nbeextremelybiased,becauseitwouldrepresentprimarilyonepatientoutofthe\\nmanypatientsinthedataset.Incasessuchasthesewheretheorderofthedataset\\nholdssomesigniﬁcance,itisnecessarytoshuﬄetheexamplesbeforeselecting\\nminibatches.Forverylargedatasets,forexampledatasetscontainingbillionsof\\nexamplesinadatacenter,itcanbeimpracticaltosampleexamplestrulyuniformly\\natrandomeverytimewewanttoconstructaminibatch.Fortunately,inpractice\\nitisusuallysuﬃcienttoshuﬄetheorderofthedatasetonceandthenstoreitin\\nshuﬄedfashion.Thiswillimposeaﬁxedsetofpossibleminibatchesofconsecutive\\nexamplesthatallmodelstrainedthereafterwilluse,andeachindividualmodel\\nwillbeforcedtoreusethisorderingeverytimeitpassesthroughthetraining\\ndata.However,thisdeviationfromtruerandomselectiondoesnotseemtohavea\\nsigniﬁcantdetrimentaleﬀect.Failingtoevershuﬄetheexamplesinanywaycan\\nseriouslyreducetheeﬀectivenessofthealgorithm.\\nManyoptimization problemsinmachinelearningdecomposeoverexamples\\nwellenoughthatwecancomputeentireseparateupdatesoverdiﬀerentexamples\\ninparallel.Inotherwords,wecancomputetheupdatethatminimizes J(X)for\\noneminibatchofexamplesXatthesametimethatwecomputetheupdatefor\\nseveralotherminibatches.Suchasynchronousparalleldistributedapproachesare\\ndiscussedfurtherinsection.12.1.3\\nAninterestingmotivationforminibatchstochasticgradientdescentisthatit\\nfollowsthegradientofthetruegeneralizationerror(equation)solongasno 8.2\\nexamplesarerepeated.Mostimplementations ofminibatchstochasticgradient\\n2 8 0', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\ndescentshuﬄethedatasetonceandthenpassthroughitmultipletimes.Onthe\\nﬁrstpass,eachminibatchisusedtocomputeanunbiasedestimateofthetrue\\ngeneralization error.Onthesecondpass,theestimatebecomesbiasedbecauseitis\\nformedbyre-samplingvaluesthathavealreadybeenused,ratherthanobtaining\\nnewfairsamplesfromthedatageneratingdistribution.\\nThefactthatstochasticgradientdescentminimizesgeneralization erroris\\neasiesttoseeintheonlinelearningcase,whereexamplesorminibatchesaredrawn\\nfromastreamofdata.Inotherwords,insteadofreceivingaﬁxed-sizetraining\\nset,thelearnerissimilartoalivingbeingwhoseesanewexampleateachinstant,\\nwitheveryexample (x , y)comingfromthedatageneratingdistribution p da t a(x , y).\\nInthisscenario,examplesareneverrepeated;everyexperienceisafairsample\\nfrom p da t a.\\nTheequivalenceiseasiesttoderivewhenbothxand yarediscrete.\\xa0Inthis\\ncase,thegeneralization error(equation)canbewrittenasasum 8.2\\nJ∗() =θ\\ue058\\nx\\ue058\\nyp da t a()((;)) x , y L fxθ , y , (8.7)\\nwiththeexactgradient\\ng= ∇ θ J∗() =θ\\ue058\\nx\\ue058\\nyp da t a()x , y∇ θ L f , y . ((;)xθ)(8.8)\\nWehavealreadyseenthesamefactdemonstratedforthelog-likelihoodinequa-\\ntionandequation;weobservenowthatthisholdsforotherfunctions 8.5 8.6 L\\nbesidesthelikelihood.Asimilarresultcanbederivedwhenxand yarecontinuous,\\nundermildassumptionsregarding p da t aand. L\\nHence,\\xa0wecanobtainanunbiasedestimatoroftheexactgradientof\\xa0the\\ngeneralization errorbysamplingaminibatchofexamples {x( 1 ), . . .x( ) m}withcor-\\nrespondingtargets y( ) ifromthedatageneratingdistribution p da t a,andcomputing\\nthegradientofthelosswithrespecttotheparametersforthatminibatch:\\nˆg=1\\nm∇ θ\\ue058\\niL f((x( ) i;)θ , y( ) i) . (8.9)\\nUpdatinginthedirectionof θ ˆgperformsSGDonthegeneralization error.\\nOfcourse,\\xa0thisinterpretation only\\xa0applies whenexamplesarenotreused.\\nNonetheless,itisusuallybesttomakeseveralpassesthroughthetrainingset,\\nunlessthetrainingsetisextremelylarge.\\xa0When multiplesuchepochsareused,\\nonlytheﬁrstepochfollowstheunbiasedgradientofthegeneralization error,but\\n2 8 1', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nofcourse,theadditionalepochsusuallyprovideenoughbeneﬁtduetodecreased\\ntrainingerrortooﬀsettheharmtheycausebyincreasingthegapbetweentraining\\nerrorandtesterror.\\nWithsomedatasetsgrowingrapidlyinsize,fasterthancomputingpower,it\\nisbecomingmorecommonformachinelearningapplicationstouseeachtraining\\nexampleonlyonceoreventomakeanincompletepassthroughthetraining\\nset.Whenusinganextremelylargetrainingset,overﬁttingisnotanissue,so\\nunderﬁttingandcomputational eﬃciencybecomethepredominant concerns.See\\nalso ()foradiscussionoftheeﬀectofcomputational BottouandBousquet2008\\nbottlenecksongeneralization error,asthenumberoftrainingexamplesgrows.\\n8.2ChallengesinNeuralNetworkOptimization\\nOptimization ingeneralisanextremelydiﬃculttask.Traditionally,machine\\nlearninghasavoidedthediﬃcultyofgeneraloptimization bycarefullydesigning\\ntheobjectivefunctionandconstraintstoensurethattheoptimization problemis\\nconvex.Whentrainingneuralnetworks,wemustconfrontthegeneralnon-convex\\ncase.Evenconvexoptimization isnotwithoutitscomplications. Inthissection,\\nwesummarizeseveralofthemostprominentchallengesinvolvedinoptimization\\nfortrainingdeepmodels.\\n8.2.1Ill-Conditioning\\nSomechallengesariseevenwhenoptimizingconvexfunctions.Ofthese,themost\\nprominentisill-conditioning oftheHessianmatrixH.Thisisaverygeneral\\nprobleminmostnumericaloptimization, convexorotherwise,andisdescribedin\\nmoredetailinsection.4.3.1\\nTheill-conditioning problemisgenerallybelievedtobepresentinneural\\nnetworktrainingproblems.Ill-conditioningcanmanifestbycausingSGDtoget\\n“stuck”inthesensethatevenverysmallstepsincreasethecostfunction.\\nRecallfromequationthatasecond-orderTaylorseriesexpansionofthe 4.9\\ncostfunctionpredictsthatagradientdescentstepofwilladd − \\ue00fg\\n1\\n2\\ue00f2g\\ue03eHgg− \\ue00f\\ue03eg (8.10)\\ntothecost.Ill-conditioningofthegradientbecomesaproblemwhen1\\n2\\ue00f2g\\ue03eHg\\nexceeds \\ue00fg\\ue03eg.\\xa0Todeterminewhetherill-conditioning isdetrimentaltoaneural\\nnetwork\\xa0training task,\\xa0one\\xa0canmonitorthe\\xa0squaredgradientnormg\\ue03egand\\n2 8 2', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\n−50050100150200250\\nTrainingtime(epochs)−20246810121416Gradient norm\\n0 50100150200250\\nTrainingtime(epochs)01 .02 .03 .04 .05 .06 .07 .08 .09 .10 .Classiﬁcationerrorrate\\nFigure8.1:Gradientdescentoftendoesnotarriveatacriticalpointofanykind.Inthis\\nexample,thegradientnormincreasesthroughouttrainingofaconvolutionalnetworkused\\nforobjectdetection. ( L e f t )Ascatterplotshowinghowthenormsofindividualgradient\\nevaluationsaredistributedovertime.Toimprovelegibility,onlyonegradientnorm\\nisplottedperepoch.Therunningaverageofallgradientnormsisplottedasasolid\\ncurve.Thegradientnormclearlyincreasesovertime,ratherthandecreasingaswewould\\nexpectifthetrainingprocessconvergedtoacriticalpoint.Despitetheincreasing ( R i g h t )\\ngradient,thetrainingprocessisreasonablysuccessful.Thevalidationsetclassiﬁcation\\nerrordecreasestoalowlevel.\\ntheg\\ue03eHgterm.Inmanycases,thegradientnormdoesnotshrinksigniﬁcantly\\nthroughoutlearning,buttheg\\ue03eHgtermgrowsbymorethananorderofmagnitude.\\nTheresultisthatlearningbecomesveryslowdespitethepresenceofastrong\\ngradientbecausethelearningratemustbeshrunktocompensateforevenstronger\\ncurvature.Figureshowsanexampleofthegradientincreasingsigniﬁcantly 8.1\\nduringthesuccessfultrainingofaneuralnetwork.\\nThoughill-conditioning ispresentinothersettingsbesidesneuralnetwork\\ntraining,someofthetechniquesusedtocombatitinothercontextsareless\\napplicabletoneuralnetworks.Forexample,Newton’smethodisanexcellenttool\\nforminimizingconvexfunctionswithpoorlyconditionedHessianmatrices,butin\\nthesubsequentsectionswewillarguethatNewton’smethodrequiressigniﬁcant\\nmodiﬁcationbeforeitcanbeappliedtoneuralnetworks.\\n8.2.2LocalMinima\\nOneofthemostprominentfeaturesofaconvexoptimization problemisthatit\\ncanbereducedtotheproblemofﬁndingalocalminimum.Anylocalminimumis\\n2 8 3', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nguaranteedtobeaglobalminimum.Someconvexfunctionshaveaﬂatregionat\\nthebottomratherthanasingleglobalminimumpoint,butanypointwithinsuch\\naﬂatregionisanacceptablesolution.Whenoptimizingaconvexfunction,we\\nknowthatwehavereachedagoodsolutionifweﬁndacriticalpointofanykind.\\nWithnon-convexfunctions,suchasneuralnets,itispossibletohavemany\\nlocalminima.Indeed,nearlyanydeepmodelisessentiallyguaranteedtohave\\nanextremelylargenumberoflocalminima.However,aswewillsee,thisisnot\\nnecessarilyamajorproblem.\\nNeuralnetworksandanymodelswithmultipleequivalentlyparametrized latent\\nvariablesallhavemultiplelocalminimabecauseofthemodelidentiﬁability\\nproblem.Amodelissaidtobeidentiﬁableifasuﬃcientlylargetrainingsetcan\\nruleoutallbutonesettingofthemodel’sparameters.Modelswithlatentvariables\\nareoftennotidentiﬁablebecausewecanobtainequivalentmodelsbyexchanging\\nlatentvariableswitheachother.Forexample,wecouldtakeaneuralnetworkand\\nmodifylayer1byswappingtheincomingweightvectorforunit iwiththeincoming\\nweightvectorforunit j,thendoingthesamefortheoutgoingweightvectors.Ifwe\\nhave mlayerswith nunitseach,thenthereare n!mwaysofarrangingthehidden\\nunits.Thiskindofnon-identiﬁabilit yisknownasweightspacesymmetry.\\nInadditiontoweightspacesymmetry,manykindsofneuralnetworkshave\\nadditionalcausesofnon-identiﬁabilit y.Forexample,inanyrectiﬁedlinearor\\nmaxoutnetwork,wecanscalealloftheincomingweightsandbiasesofaunitby\\nαifwealsoscaleallofitsoutgoingweightsby1\\nα.Thismeansthat—ifthecost\\nfunctiondoesnotincludetermssuchasweightdecaythatdependdirectlyonthe\\nweightsratherthanthemodels’outputs—everylocalminimumofarectiﬁedlinear\\normaxoutnetworkliesonan( m n×)-dimensionalhyperbolaofequivalentlocal\\nminima.\\nThesemodelidentiﬁabilityissuesmeanthattherecanbeanextremelylarge\\norevenuncountablyinﬁniteamountoflocalminimainaneuralnetworkcost\\nfunction.However,alloftheselocalminimaarisingfromnon-identiﬁabilit yare\\nequivalenttoeachotherincostfunctionvalue.Asaresult,theselocalminimaare\\nnotaproblematicformofnon-convexity.\\nLocalminimacanbeproblematiciftheyhavehighcostincomparisontothe\\nglobalminimum.Onecanconstructsmallneuralnetworks,evenwithouthidden\\nunits,thathavelocalminimawithhighercostthantheglobalminimum(Sontag\\nandSussman1989Brady1989GoriandTesi1992 ,; etal.,; ,).Iflocalminima\\nwithhighcostarecommon,thiscouldposeaseriousproblemforgradient-based\\noptimization algorithms.\\nItremainsanopenquestionwhethertherearemanylocalminimaofhighcost\\n2 8 4', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nfornetworksofpracticalinterestandwhetheroptimization algorithmsencounter\\nthem.Formanyyears,mostpractitioners believedthatlocalminimawerea\\ncommonproblemplaguingneuralnetworkoptimization. Today,thatdoesnot\\nappeartobethecase.Theproblemremainsanactiveareaofresearch,butexperts\\nnowsuspectthat,forsuﬃcientlylargeneuralnetworks,mostlocalminimahavea\\nlowcostfunctionvalue,andthatitisnotimportanttoﬁndatrueglobalminimum\\nratherthantoﬁndapointinparameterspacethathaslowbutnotminimalcost\\n(,; ,; ,; Saxeetal.2013Dauphinetal.2014Goodfellow etal.2015Choromanska\\netal.,).2014\\nManypractitioners attributenearlyalldiﬃcultywithneuralnetworkoptimiza-\\ntiontolocalminima.Weencouragepractitioners tocarefullytestforspeciﬁc\\nproblems.Atestthatcanruleoutlocalminimaastheproblemistoplotthe\\nnormofthegradientovertime.Ifthenormofthegradientdoesnotshrinkto\\ninsigniﬁcantsize,theproblemisneitherlocalminimanoranyotherkindofcritical\\npoint.Thiskindofnegativetestcanruleoutlocalminima.Inhighdimensional\\nspaces,itcanbeverydiﬃculttopositivelyestablishthatlocalminimaarethe\\nproblem.Manystructuresotherthanlocalminimaalsohavesmallgradients.\\n8.2.3Plateaus,SaddlePointsandOtherFlatRegions\\nFormanyhigh-dimensionalnon-convexfunctions,localminima(andmaxima)\\nareinfactrarecomparedtoanotherkindofpointwithzerogradient:asaddle\\npoint.Somepointsaroundasaddlepointhavegreatercostthanthesaddlepoint,\\nwhileothershavealowercost.\\xa0Atasaddlepoint,theHessianmatrixhasboth\\npositiveandnegativeeigenvalues.Pointslyingalongeigenvectorsassociatedwith\\npositiveeigenvalueshavegreatercostthanthesaddlepoint,whilepointslying\\nalongnegativeeigenvalueshavelowervalue.Wecanthinkofasaddlepointas\\nbeingalocalminimumalongonecross-sectionofthecostfunctionandalocal\\nmaximumalonganothercross-section.Seeﬁgureforanillustration. 4.5\\nManyclasses\\xa0ofrandomfunctionsexhibitthefollowingbehavior:inlow-\\ndimensionalspaces,localminimaarecommon.Inhigherdimensionalspaces,local\\nminimaarerareandsaddlepointsaremorecommon.Forafunction f: Rn→ Rof\\nthistype,theexpectedratioofthenumberofsaddlepointstolocalminimagrows\\nexponentiallywith n.Tounderstandtheintuitionbehindthisbehavior,observe\\nthattheHessianmatrixatalocalminimumhasonlypositiveeigenvalues.\\xa0The\\nHessianmatrixatasaddlepointhasamixtureofpositiveandnegativeeigenvalues.\\nImaginethatthesignofeacheigenvalueisgeneratedbyﬂippingacoin.Inasingle\\ndimension,itiseasytoobtainalocalminimumbytossingacoinandgettingheads\\nonce.In n-dimensionalspace,itisexponentiallyunlikelythatall ncointosseswill\\n2 8 5', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nbeheads.See ()forareviewoftherelevanttheoreticalwork. Dauphinetal.2014\\nAnamazingpropertyofmanyrandomfunctionsisthattheeigenvaluesofthe\\nHessianbecomemorelikelytobepositiveaswereachregionsoflowercost.\\xa0In\\nourcointossinganalogy,thismeanswearemorelikelytohaveourcoincomeup\\nheads ntimesifweareatacriticalpointwithlowcost.\\xa0Thismeansthatlocal\\nminimaaremuchmorelikelytohavelowcostthanhighcost.Criticalpointswith\\nhighcostarefarmorelikelytobesaddlepoints.Criticalpointswithextremely\\nhighcostaremorelikelytobelocalmaxima.\\nThishappensformanyclassesofrandomfunctions.Doesithappenforneural\\nnetworks? ()showedtheoreticallythatshallowautoencoders BaldiandHornik1989\\n(feedforwardnetworkstrainedtocopytheirinputtotheiroutput,describedin\\nchapter)withnononlinearities haveglobalminimaandsaddlepointsbutno 14\\nlocalminimawithhighercostthantheglobalminimum.Theyobservedwithout\\nproofthattheseresultsextendtodeepernetworkswithoutnonlinearities. The\\noutputofsuchnetworksisalinearfunctionoftheirinput,buttheyareuseful\\ntostudyasamodelofnonlinearneuralnetworksbecausetheirlossfunctionis\\nanon-convexfunctionoftheirparameters.Suchnetworksareessentiallyjust\\nmultiplematricescomposedtogether. ()providedexactsolutions Saxeetal.2013\\ntothecompletelearningdynamicsinsuchnetworksandshowedthatlearningin\\nthesemodelscapturesmanyofthequalitativefeaturesobservedinthetrainingof\\ndeepmodelswithnonlinearactivationfunctions. ()showed Dauphinetal.2014\\nexperimentallythatrealneuralnetworksalsohavelossfunctionsthatcontainvery\\nmanyhigh-costsaddlepoints.Choromanska2014etal.()providedadditional\\ntheoreticalarguments,showingthatanotherclassofhigh-dimensionalrandom\\nfunctionsrelatedtoneuralnetworksdoessoaswell.\\nWhataretheimplicationsoftheproliferationofsaddlepointsfortrainingalgo-\\nrithms?Forﬁrst-orderoptimization algorithmsthatuseonlygradientinformation,\\nthesituationisunclear.Thegradientcanoftenbecomeverysmallnearasaddle\\npoint.Ontheotherhand,gradientdescentempiricallyseemstobeabletoescape\\nsaddlepointsinmanycases. ()providedvisualizationsof Goodfellowetal.2015\\nseverallearningtrajectoriesofstate-of-the-art neuralnetworks,withanexample\\ngiveninﬁgure.Thesevisualizationsshowaﬂatteningofthecostfunctionnear 8.2\\naprominentsaddlepointwheretheweightsareallzero,buttheyalsoshowthe\\ngradientdescenttrajectoryrapidlyescapingthisregion. () Goodfellowetal.2015\\nalsoarguethatcontinuous-timegradientdescentmaybeshownanalyticallytobe\\nrepelledfrom,ratherthanattractedto,anearbysaddlepoint,butthesituation\\nmaybediﬀerentformorerealisticusesofgradientdescent.\\nForNewton’smethod,\\xa0itisclearthatsaddlepointsconstituteaproblem.\\n2 8 6', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nP r o j e c t i o n2o f θ\\nP r o j e c t i o n 1 o f θJ(\\n)θ\\nFigure8.2:Avisualizationofthecostfunctionofaneuralnetwork.Imageadapted\\nwithpermissionfromGoodfellow2015 e t a l .().\\xa0Thesevisualizationsappearsimilarfor\\nfeedforwardneuralnetworks,convolutionalnetworks,andrecurrentnetworksapplied\\ntorealobjectrecognition andnaturallanguageprocessingtasks.Surprisingly,these\\nvisualizationsusuallydonotshowmanyconspicuousobstacles.\\xa0Priortothesuccessof\\nstochasticgradientdescentfortrainingverylargemodelsbeginninginroughly2012,\\nneuralnetcostfunctionsurfacesweregenerallybelievedtohavemuchmorenon-convex\\nstructurethanisrevealedbytheseprojections.\\xa0Theprimaryobstaclerevealedbythis\\nprojectionisasaddlepointofhighcostnearwheretheparametersareinitialized,but,as\\nindicatedbythebluepath,theSGDtrainingtrajectoryescapesthissaddlepointreadily.\\nMostoftrainingtimeisspenttraversingtherelativelyﬂatvalleyofthecostfunction,\\nwhichmaybeduetohighnoiseinthegradient,poorconditioningoftheHessianmatrix\\ninthisregion,orsimplytheneedtocircumnavigatethetall“mountain”visibleinthe\\nﬁgureviaanindirectarcingpath.\\n2 8 7', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nGradientdescentisdesignedtomove“downhill”andisnotexplicitlydesigned\\ntoseekacriticalpoint.Newton’smethod,however,isdesignedtosolvefora\\npointwherethegradientiszero.Withoutappropriatemodiﬁcation,itcanjump\\ntoasaddlepoint.Theproliferation ofsaddlepointsinhighdimensionalspaces\\npresumablyexplainswhysecond-ordermethodshavenotsucceededinreplacing\\ngradientdescentforneuralnetworktraining. ()introduceda Dauphinetal.2014\\nsaddle-freeNewtonmethodforsecond-orderoptimization andshowedthatit\\nimprovessigniﬁcantlyoverthetraditionalversion.Second-order methodsremain\\ndiﬃculttoscaletolargeneuralnetworks,butthissaddle-freeapproachholds\\npromiseifitcouldbescaled.\\nThereareotherkindsofpointswithzerogradientbesidesminimaandsaddle\\npoints.Therearealsomaxima,\\xa0whic haremuchlikesaddlepointsfromthe\\nperspectiveofoptimization—many algorithmsarenotattractedtothem,\\xa0but\\nunmodiﬁedNewton’smethodis.Maximaofmanyclassesofrandomfunctions\\nbecomeexponentiallyrareinhighdimensionalspace,justlikeminimado.\\nTheremayalsobewide,ﬂatregionsofconstantvalue.Intheselocations,the\\ngradientandalsotheHessianareallzero.Suchdegeneratelocationsposemajor\\nproblemsforallnumericaloptimization algorithms.Inaconvexproblem,awide,\\nﬂatregionmustconsistentirelyofglobalminima,butinageneraloptimization\\nproblem,sucharegioncouldcorrespondtoahighvalueoftheobjectivefunction.\\n8.2.4CliﬀsandExplodingGradients\\nNeuralnetworkswithmanylayersoftenhaveextremelysteepregionsresembling\\ncliﬀs,asillustratedinﬁgure.Theseresultfromthemultiplicationofseveral 8.3\\nlargeweightstogether.Onthefaceofanextremelysteepcliﬀstructure,the\\ngradientupdatestepcanmovetheparametersextremelyfar,usuallyjumpingoﬀ\\nofthecliﬀstructurealtogether.\\n2 8 8', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\n\\ue077\\n\\ue062\\ue04a\\ue077\\ue03b \\ue062\\n\\ue028\\ue029\\nFigure8.3:Theobjectivefunctionforhighlynonlineardeepneuralnetworksorfor\\nrecurrentneuralnetworksoftencontainssharpnonlinearitiesinparameterspaceresulting\\nfromthemultiplicationofseveralparameters.Thesenonlinearitiesgiverisetovery\\nhighderivativesinsomeplaces.Whentheparametersgetclosetosuchacliﬀregion,a\\ngradientdescentupdatecancatapulttheparametersveryfar,possiblylosingmostofthe\\noptimizationworkthathadbeendone.\\xa0FigureadaptedwithpermissionfromPascanu\\ne t a l .().2013\\nThecliﬀcanbedangerouswhetherweapproachitfromaboveorfrombelow,\\nbutfortunatelyitsmostseriousconsequencescanbeavoidedusingthegradient\\nclippingheuristicdescribedinsection.Thebasicideaistorecallthat 10.11.1\\nthegradientdoesnotspecifytheoptimalstepsize,butonlytheoptimaldirection\\nwithinaninﬁnitesimalregion.Whenthetraditionalgradientdescentalgorithm\\nproposestomakeaverylargestep,thegradientclippingheuristicintervenesto\\nreducethestepsizetobesmallenoughthatitislesslikelytogooutsidetheregion\\nwherethegradientindicatesthedirectionofapproximately steepestdescent.Cliﬀ\\nstructuresaremostcommoninthecostfunctionsforrecurrentneuralnetworks,\\nbecausesuchmodelsinvolveamultiplication ofmanyfactors,withonefactor\\nforeachtimestep.Longtemporalsequencesthusincuranextremeamountof\\nmultiplication.\\n8.2.5Long-TermDependencies\\nAnotherdiﬃcultythatneuralnetworkoptimization algorithmsmustovercome\\narises\\xa0when\\xa0thecomputational\\xa0gra ph\\xa0becomes\\xa0extremely\\xa0deep.Feedforward\\nnetworkswithmanylayershavesuchdeepcomputational graphs.Sodorecurrent\\nnetworks,describedinchapter,whichconstructverydeepcomputational graphs 10\\n2 8 9', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nbyrepeatedlyapplyingthesameoperationateachtimestepofalongtemporal\\nsequence.Repeatedapplicationofthesameparametersgivesrisetoespecially\\npronounceddiﬃculties.\\nForexample,supposethatacomputational graphcontainsapaththatconsists\\nofrepeatedlymultiplyingbyamatrixW.After tsteps,thisisequivalenttomul-\\ntiplyingbyWt.SupposethatWhasaneigendecompositionW=Vdiag(λ)V− 1.\\nInthissimplecase,itisstraightforwardtoseethat\\nWt=\\ue000\\nVλVdiag()− 1\\ue001t= ()VdiagλtV− 1. (8.11)\\nAnyeigenvalues λ ithatarenotnearanabsolutevalueofwilleitherexplodeifthey 1\\naregreaterthaninmagnitudeorvanishiftheyarelessthaninmagnitude.The 1 1\\nvanishingandexplodinggradientproblemreferstothefactthatgradients\\nthroughsuchagrapharealsoscaledaccordingtodiag(λ)t.Vanishinggradients\\nmakeitdiﬃculttoknowwhichdirectiontheparametersshouldmovetoimprove\\nthecostfunction,whileexplodinggradientscanmakelearningunstable.Thecliﬀ\\nstructuresdescribedearlierthatmotivategradientclippingareanexampleofthe\\nexplodinggradientphenomenon.\\nTherepeatedmultiplication byWateachtimestepdescribedhereisvery\\nsimilartothepowermethodalgorithmusedtoﬁndthelargesteigenvalueof\\namatrixWandthecorrespondingeigenvector.Fromthispointofviewitis\\nnotsurprisingthatx\\ue03eWtwilleventuallydiscardallcomponentsofxthatare\\northogonaltotheprincipaleigenvectorof.W\\nRecurrentnetworksusethesamematrixWateachtimestep,butfeedforward\\nnetworksdonot,soevenverydeepfeedforwardnetworkscanlargelyavoidthe\\nvanishingandexplodinggradientproblem(,). Sussillo2014\\nWedeferafurtherdiscussionofthechallengesoftrainingrecurrentnetworks\\nuntilsection,afterrecurrentnetworkshavebeendescribedinmoredetail. 10.7\\n8.2.6InexactGradients\\nMostoptimization algorithmsaredesignedwiththeassumptionthatwehave\\naccesstotheexactgradientorHessianmatrix.Inpractice,weusuallyonlyhave\\nanoisyorevenbiasedestimateofthesequantities.\\xa0Nearlyeverydeeplearning\\nalgorithmreliesonsampling-basedestimatesatleastinsofarasusingaminibatch\\noftrainingexamplestocomputethegradient.\\nInothercases,theobjectivefunctionwewanttominimizeisactuallyintractable.\\nWhentheobjectivefunctionisintractable,typicallyitsgradientisintractableas\\nwell.Insuchcaseswecanonlyapproximatethegradient.Theseissuesmostlyarise\\n2 9 0', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nwiththemoreadvancedmodelsinpart.Forexample,contrastivedivergence III\\ngivesatechniqueforapproximatingthegradientoftheintractablelog-likelihood\\nofaBoltzmannmachine.\\nVariousneuralnetworkoptimization algorithmsaredesignedtoaccountfor\\nimperfectionsinthegradientestimate.Onecanalsoavoidtheproblembychoosing\\nasurrogatelossfunctionthatiseasiertoapproximate thanthetrueloss.\\n8.2.7PoorCorrespondencebetweenLocalandGlobalStructure\\nManyoftheproblemswehavediscussedsofarcorrespondtopropertiesofthe\\nlossfunctionatasinglepoint—itcanbediﬃculttomakeasinglestepif J(θ)is\\npoorlyconditionedatthecurrentpointθ,orifθliesonacliﬀ,orifθisasaddle\\npointhidingtheopportunitytomakeprogressdownhillfromthegradient.\\nItispossibletoovercomealloftheseproblemsatasinglepointandstill\\nperformpoorlyifthedirectionthatresultsinthemostimprovementlocallydoes\\nnotpointtowarddistantregionsofmuchlowercost.\\nGoodfellow2015etal.()arguethatmuchoftheruntimeoftrainingisdueto\\nthelengthofthetrajectoryneededtoarriveatthesolution.Figureshowsthat8.2\\nthelearningtrajectoryspendsmostofitstimetracingoutawidearcarounda\\nmountain-shapedstructure.\\nMuchofresearchintothediﬃcultiesofoptimization hasfocusedonwhether\\ntrainingarrivesataglobalminimum,alocalminimum,orasaddlepoint,butin\\npracticeneuralnetworksdonotarriveatacriticalpointofanykind.Figure8.1\\nshowsthatneuralnetworksoftendonotarriveataregionofsmallgradient.Indeed,\\nsuchcriticalpointsdonotevennecessarilyexist.Forexample,thelossfunction\\n−log p( y|x;θ)canlackaglobalminimumpointandinsteadasymptotically\\napproachsomevalueasthemodelbecomesmoreconﬁdent.Foraclassiﬁerwith\\ndiscrete yand p( y|x)providedbyasoftmax,thenegativelog-likelihoodcan\\nbecomearbitrarilyclosetozeroifthemodelisabletocorrectlyclassifyevery\\nexampleinthetrainingset,butitisimpossibletoactuallyreachthevalueof\\nzero.Likewise,amodelofrealvalues p( y|x) =N( y; f(θ) , β− 1)canhavenegative\\nlog-likelihoodthatasymptotestonegativeinﬁnity—if f(θ)isabletocorrectly\\npredictthevalueofalltrainingset ytargets,thelearningalgorithmwillincrease\\nβwithoutbound.Seeﬁgureforanexampleofafailureoflocaloptimization to 8.4\\nﬁndagoodcostfunctionvalueevenintheabsenceofanylocalminimaorsaddle\\npoints.\\nFutureresearchwillneedtodevelopfurtherunderstandingofthefactorsthat\\ninﬂuencethelengthofthelearningtrajectoryandbettercharacterizetheoutcome\\n2 9 1', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nθJ ( ) θ\\nFigure8.4:Optimizationbasedonlocaldownhillmovescanfailifthelocalsurfacedoes\\nnotpointtowardtheglobalsolution.Hereweprovideanexampleofhowthiscanoccur,\\neveniftherearenosaddlepointsandnolocalminima.Thisexamplecostfunction\\ncontainsonlyasymptotestowardlowvalues,notminima.Themaincauseofdiﬃcultyin\\nthiscaseisbeinginitializedonthewrongsideofthe“mountain”andnotbeingableto\\ntraverseit.\\xa0Inhigherdimensionalspace,learningalgorithmscanoftencircumnavigate\\nsuchmountainsbutthetrajectoryassociatedwithdoingsomaybelongandresultin\\nexcessivetrainingtime,asillustratedinﬁgure.8.2\\noftheprocess.\\nManyexistingresearchdirectionsareaimedatﬁndinggoodinitialpointsfor\\nproblemsthathavediﬃcultglobalstructure,ratherthandevelopingalgorithms\\nthatusenon-localmoves.\\nGradientdescentandessentiallyalllearningalgorithmsthatareeﬀectivefor\\ntrainingneuralnetworksarebasedonmakingsmall,localmoves.Theprevious\\nsectionshaveprimarilyfocusedonhowthecorrectdirectionoftheselocalmoves\\ncanbediﬃculttocompute.Wemaybeabletocomputesomepropertiesofthe\\nobjectivefunction,suchasitsgradient,onlyapproximately ,withbiasorvariance\\ninourestimateofthecorrectdirection.Inthesecases,localdescentmayormay\\nnotdeﬁneareasonablyshortpathtoavalidsolution,butwearenotactually\\nabletofollowthelocaldescentpath.Theobjectivefunctionmayhaveissues\\nsuchaspoorconditioningordiscontinuousgradients,causingtheregionwhere\\nthegradientprovidesagoodmodeloftheobjectivefunctiontobeverysmall.In\\nthesecases,localdescentwithstepsofsize \\ue00fmaydeﬁneareasonablyshortpath\\ntothesolution,butweareonlyabletocomputethelocaldescentdirectionwith\\nstepsofsize δ \\ue00f\\ue01c.Inthesecases,localdescentmayormaynotdeﬁneapath\\ntothesolution,butthepathcontainsmanysteps,sofollowingthepathincursa\\n2 9 2', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nhighcomputational cost.Sometimeslocalinformationprovidesusnoguide,when\\nthefunctionhasawideﬂatregion,orifwemanagetolandexactlyonacritical\\npoint(usuallythislatterscenarioonlyhappenstomethodsthatsolveexplicitly\\nforcriticalpoints,suchasNewton’smethod).Inthesecases,localdescentdoes\\nnotdeﬁneapathtoasolutionatall.Inothercases,localmovescanbetoogreedy\\nandleadusalongapaththatmovesdownhillbutawayfromanysolution,asin\\nﬁgure,oralonganunnecessarilylongtrajectorytothesolution,asinﬁgure. 8.4 8.2\\nCurrently,wedonotunderstandwhichoftheseproblemsaremostrelevantto\\nmakingneuralnetworkoptimization diﬃcult,andthisisanactiveareaofresearch.\\nRegardlessofwhichoftheseproblemsaremostsigniﬁcant,allofthemmightbe\\navoidedifthereexistsaregionofspaceconnectedreasonablydirectlytoasolution\\nbyapaththatlocaldescentcanfollow,andifweareabletoinitializelearning\\nwithinthatwell-behavedregion.\\xa0Thislastviewsuggestsresearchintochoosing\\ngoodinitialpointsfortraditionaloptimization algorithmstouse.\\n8.2.8TheoreticalLimitsofOptimization\\nSeveraltheoreticalresultsshowthattherearelimitsontheperformanceofany\\noptimization algorithmwemightdesignforneuralnetworks(BlumandRivest,\\n1992Judd1989WolpertandMacReady1997 ;,; ,).Typicallytheseresultshave\\nlittlebearingontheuseofneuralnetworksinpractice.\\nSometheoreticalresultsapplyonlytothecasewheretheunitsofaneural\\nnetworkoutput\\xa0discretevalues.However,\\xa0most\\xa0neuralnetworkunitsoutput\\nsmoothlyincreasingvaluesthatmakeoptimization vialocalsearchfeasible.Some\\ntheoreticalresultsshowthatthereexistproblemclassesthatareintractable,but\\nitcanbediﬃculttotellwhetheraparticularproblemfallsintothatclass.Other\\nresultsshowthatﬁndingasolutionforanetworkofagivensizeisintractable,but\\ninpracticewecanﬁndasolutioneasilybyusingalargernetworkforwhichmany\\nmoreparametersettingscorrespondtoanacceptablesolution.Moreover,inthe\\ncontextofneuralnetworktraining,weusuallydonotcareaboutﬁndingtheexact\\nminimumofafunction,butseekonlytoreduceitsvaluesuﬃcientlytoobtaingood\\ngeneralization error.\\xa0Theoretical analysisofwhetheranoptimization algorithm\\ncanaccomplishthisgoalisextremelydiﬃcult.Developingmorerealisticbounds\\nontheperformanceofoptimization algorithmsthereforeremainsanimportant\\ngoalformachinelearningresearch.\\n2 9 3', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\n8.3BasicAlgorithms\\nWehavepreviouslyintroducedthegradientdescent(section)algorithmthat 4.3\\nfollowsthegradientofanentiretrainingsetdownhill.Thismaybeaccelerated\\nconsiderablybyusingstochasticgradientdescenttofollowthegradientofrandomly\\nselectedminibatchesdownhill,asdiscussedinsectionandsection. 5.9 8.1.3\\n8.3.1StochasticGradientDescent\\nStochasticgradientdescent(SGD)anditsvariantsareprobablythemostused\\noptimization algorithmsformachinelearningingeneralandfordeeplearning\\ninparticular.\\xa0As discussedinsection,itispossibletoobtainanunbiased 8.1.3\\nestimateofthegradientbytakingtheaveragegradientonaminibatchof m\\nexamplesdrawni.i.dfromthedatageneratingdistribution.\\nAlgorithmshowshowtofollowthisestimateofthegradientdownhill. 8.1\\nAlgorithm8.1Stochasticgradientdescent(SGD)updateattrainingiteration k\\nRequire:Learningrate \\ue00f k.\\nRequire:Initialparameterθ\\nwhile do stoppingcriterionnotmet\\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\\ncorrespondingtargetsy( ) i.\\nComputegradientestimate: ˆg←+1\\nm∇ θ\\ue050\\ni L f((x( ) i;)θ ,y( ) i)\\nApplyupdate:θθ← − \\ue00fˆg\\nendwhile\\nAcrucialparameterfortheSGDalgorithmisthelearningrate.Previously,we\\nhavedescribedSGDasusingaﬁxedlearningrate \\ue00f.Inpractice,itisnecessaryto\\ngraduallydecreasethelearningrateovertime,sowenowdenotethelearningrate\\natiterationas k \\ue00f k.\\nThisisbecausetheSGDgradientestimatorintroducesasourceofnoise(the\\nrandomsamplingof mtrainingexamples)thatdoesnotvanishevenwhenwearrive\\nataminimum.Bycomparison,thetruegradientofthetotalcostfunctionbecomes\\nsmallandthen 0whenweapproachandreachaminimumusingbatchgradient\\ndescent,sobatchgradientdescentcanuseaﬁxedlearningrate.Asuﬃcient\\nconditiontoguaranteeconvergenceofSGDisthat\\n∞\\ue058\\nk = 1\\ue00f k= and ∞ , (8.12)\\n2 9 4', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\n∞\\ue058\\nk = 1\\ue00f2\\nk < .∞ (8.13)\\nInpractice,itiscommontodecaythelearningratelinearlyuntiliteration: τ\\n\\ue00f k= (1 )− α \\ue00f 0+ α \\ue00f τ (8.14)\\nwith α=k\\nτ.Afteriteration,itiscommontoleaveconstant. τ \\ue00f\\nThelearningratemaybechosenbytrialanderror,butitisusuallybest\\ntochooseitbymonitoringlearningcurvesthatplottheobjectivefunctionasa\\nfunctionoftime.Thisismoreofanartthanascience,andmostguidanceonthis\\nsubjectshouldberegardedwithsomeskepticism.Whenusingthelinearschedule,\\ntheparameterstochooseare \\ue00f 0, \\ue00f τ,and τ.Usually τmaybesettothenumberof\\niterationsrequiredtomakeafewhundredpassesthroughthetrainingset.Usually\\n\\ue00f τshouldbesettoroughlythevalueof 1% \\ue00f 0.Themainquestionishowtoset \\ue00f 0.\\nIfitistoolarge,thelearningcurvewillshowviolentoscillations,withthecost\\nfunctionoftenincreasingsigniﬁcantly.Gentleoscillationsareﬁne,especiallyif\\ntrainingwithastochasticcostfunctionsuchasthecostfunctionarisingfromthe\\nuseofdropout.Ifthelearningrateistoolow,learningproceedsslowly,andifthe\\ninitiallearningrateistoolow,learningmaybecomestuckwithahighcostvalue.\\nTypically,theoptimalinitiallearningrate,intermsoftotaltrainingtimeandthe\\nﬁnalcostvalue,ishigherthanthelearningratethatyieldsthebestperformance\\naftertheﬁrst100iterationsorso.Therefore,itisusuallybesttomonitortheﬁrst\\nseveraliterationsandusealearningratethatishigherthanthebest-performing\\nlearningrateatthistime,butnotsohighthatitcausessevereinstability.\\nThemostimportantpropertyofSGDandrelatedminibatchoronlinegradient-\\nbasedoptimization isthatcomputationtimeperupdatedoesnotgrowwiththe\\nnumberoftrainingexamples.Thisallowsconvergenceevenwhenthenumber\\noftrainingexamplesbecomesverylarge.Foralargeenoughdataset,SGDmay\\nconvergetowithinsomeﬁxedtoleranceofitsﬁnaltestseterrorbeforeithas\\nprocessedtheentiretrainingset.\\nTostudytheconvergencerateofanoptimization algorithmitiscommonto\\nmeasuretheexcesserror J(θ)−min θ J(θ),whichistheamountthatthecurrent\\ncostfunctionexceedstheminimumpossiblecost.WhenSGDisappliedtoaconvex\\nproblem,theexcesserroris O(1√\\nk)after kiterations,whileinthestronglyconvex\\ncaseitis O(1\\nk).Theseboundscannotbeimprovedunlessextraconditionsare\\nassumed.Batchgradientdescentenjoysbetterconvergenceratesthanstochastic\\ngradientdescentintheory.However,theCramér-Raobound(,;, Cramér1946Rao\\n1945)statesthatgeneralization errorcannotdecreasefasterthan O(1\\nk).Bottou\\n2 9 5', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nandBousquet2008()arguethatitthereforemaynotbeworthwhiletopursue\\nanoptimization algorithmthatconvergesfasterthan O(1\\nk)formachinelearning\\ntasks—fasterconvergencepresumablycorrespondstooverﬁtting.Moreover,the\\nasymptoticanalysisobscuresmanyadvantagesthatstochasticgradientdescent\\nhasafterasmallnumberofsteps.Withlargedatasets,theabilityofSGDtomake\\nrapidinitialprogresswhileevaluatingthegradientforonlyveryfewexamples\\noutweighsitsslowasymptoticconvergence.Mostofthealgorithmsdescribedin\\ntheremainderofthischapterachievebeneﬁtsthatmatterinpracticebutarelost\\nintheconstantfactorsobscuredbythe O(1\\nk)asymptoticanalysis.Onecanalso\\ntradeoﬀthebeneﬁtsofbothbatchandstochasticgradientdescentbygradually\\nincreasingtheminibatchsizeduringthecourseoflearning.\\nFormoreinformationonSGD,see(). Bottou1998\\n8.3.2Momentum\\nWhilestochasticgradientdescentremainsaverypopularoptimization strategy,\\nlearningwithitcansometimesbeslow.Themethodofmomentum(Polyak1964,)\\nisdesignedtoacceleratelearning,especiallyinthefaceofhighcurvature,smallbut\\nconsistentgradients,ornoisygradients.Themomentumalgorithmaccumulates\\nanexponentiallydecayingmovingaverageofpastgradientsandcontinuestomove\\nintheirdirection.Theeﬀectofmomentumisillustratedinﬁgure.8.5\\nFormally,themomentumalgorithmintroducesavariablevthatplaystherole\\nofvelocity—itisthedirectionandspeedatwhichtheparametersmovethrough\\nparameterspace.Thevelocityissettoanexponentiallydecayingaverageofthe\\nnegativegradient.Thenamemomentumderivesfromaphysicalanalogy,in\\nwhichthenegativegradientisaforcemovingaparticlethroughparameterspace,\\naccordingtoNewton’slawsofmotion.Momentuminphysicsismasstimesvelocity.\\nInthemomentumlearningalgorithm,weassumeunitmass,sothevelocityvectorv\\nmayalsoberegardedasthemomentumoftheparticle.Ahyperparameter α∈[0 ,1)\\ndetermineshowquicklythecontributionsofpreviousgradientsexponentiallydecay.\\nTheupdateruleisgivenby:\\nvv← α−∇ \\ue00f θ\\ue020\\n1\\nmm \\ue058\\ni = 1L((fx( ) i;)θ ,y( ) i)\\ue021\\n, (8.15)\\nθθv ← + . (8.16)\\nThevelocityvaccumulatesthegradientelements∇ θ\\ue0001\\nm\\ue050m\\ni = 1 L((fx( ) i;)θ ,y( ) i)\\ue001\\n.\\nThelarger αisrelativeto \\ue00f,themorepreviousgradientsaﬀectthecurrentdirection.\\nTheSGDalgorithmwithmomentumisgiveninalgorithm .8.2\\n2 9 6', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\n− − − 3 0 2 0 1 0 0 1 0 2 0− 3 0− 2 0− 1 001 02 0\\nFigure8.5:Momentumaimsprimarilytosolvetwoproblems:poorconditioningofthe\\nHessianmatrixandvarianceinthestochasticgradient.Here,weillustratehowmomentum\\novercomestheﬁrstofthesetwoproblems.Thecontourlinesdepictaquadraticloss\\nfunctionwithapoorlyconditionedHessianmatrix.Theredpathcuttingacrossthe\\ncontoursindicatesthepathfollowedbythemomentumlearningruleasitminimizesthis\\nfunction.Ateachstepalongtheway,wedrawanarrowindicatingthestepthatgradient\\ndescentwouldtakeatthatpoint.Wecanseethatapoorlyconditionedquadraticobjective\\nlookslikealong,narrowvalleyorcanyonwithsteepsides.Momentumcorrectlytraverses\\nthecanyonlengthwise,whilegradientstepswastetimemovingbackandforthacrossthe\\nnarrowaxisofthecanyon.Comparealsoﬁgure,whichshowsthebehaviorofgradient 4.6\\ndescentwithoutmomentum.\\n2 9 7', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nPreviously,thesizeofthestepwassimplythenormofthegradientmultiplied\\nbythelearningrate.Now,thesizeofthestepdependsonhowlargeandhow\\nalignedasequenceofgradientsare.Thestepsizeislargestwhenmanysuccessive\\ngradientspointinexactlythesamedirection.Ifthemomentumalgorithmalways\\nobservesgradientg,thenitwillaccelerateinthedirectionof−g,untilreachinga\\nterminalvelocitywherethesizeofeachstepis\\n\\ue00f||||g\\n1− α. (8.17)\\nItisthushelpfultothinkofthemomentumhyperparameterintermsof1\\n1 − α.For\\nexample, α= .9correspondstomultiplyingthemaximumspeedbyrelativeto 10\\nthegradientdescentalgorithm.\\nCommonvaluesof αusedinpracticeinclude .5, .9,and .99.Likethelearning\\nrate, αmayalsobeadaptedovertime.Typicallyitbeginswithasmallvalueand\\nislaterraised.Itislessimportanttoadapt αovertimethantoshrink \\ue00fovertime.\\nAlgorithm8.2Stochasticgradientdescent(SGD)withmomentum\\nRequire:Learningrate,momentumparameter. \\ue00f α\\nRequire:Initialparameter,initialvelocity. θ v\\nwhile do stoppingcriterionnotmet\\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\\ncorrespondingtargetsy( ) i.\\nComputegradientestimate:g←1\\nm∇ θ\\ue050\\ni L f((x( ) i;)θ ,y( ) i)\\nComputevelocityupdate:vvg ← α− \\ue00f\\nApplyupdate:θθv ← +\\nendwhile\\nWecanviewthemomentumalgorithmassimulatingaparticlesubjectto\\ncontinuous-timeNewtoniandynamics.Thephysicalanalogycanhelptobuild\\nintuitionforhowthemomentumandgradientdescentalgorithmsbehave.\\nThepositionoftheparticleatanypointintimeisgivenbyθ( t).Theparticle\\nexperiencesnetforce.Thisforcecausestheparticletoaccelerate: f() t\\nf() = t∂2\\n∂ t2θ() t . (8.18)\\nRatherthanviewingthisasasecond-orderdiﬀerentialequationoftheposition,\\nwecanintroducethevariablev( t)representingthevelocityoftheparticleattime\\ntandrewritetheNewtoniandynamicsasaﬁrst-orderdiﬀerentialequation:\\nv() = t∂\\n∂ tθ() t , (8.19)\\n2 9 8', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nf() = t∂\\n∂ tv() t . (8.20)\\nThemomentumalgorithmthenconsistsofsolvingthediﬀerentialequationsvia\\nnumericalsimulation.Asimplenumericalmethodforsolvingdiﬀerentialequations\\nisEuler’smethod,whichsimplyconsistsofsimulatingthedynamicsdeﬁnedby\\ntheequationbytakingsmall,ﬁnitestepsinthedirectionofeachgradient.\\nThisexplainsthebasicformofthemomentumupdate,butwhatspeciﬁcallyare\\ntheforces?Oneforceisproportionaltothenegativegradientofthecostfunction:\\n−∇ θ J(θ).Thisforcepushestheparticledownhillalongthecostfunctionsurface.\\nThegradientdescentalgorithmwouldsimplytakeasinglestepbasedoneach\\ngradient,buttheNewtonianscenariousedbythemomentumalgorithminstead\\nusesthisforcetoalterthevelocityoftheparticle.Wecanthinkoftheparticle\\nasbeinglikeahockeypuckslidingdownanicysurface.Wheneveritdescendsa\\nsteeppartofthesurface,itgathersspeedandcontinuesslidinginthatdirection\\nuntilitbeginstogouphillagain.\\nOneotherforceisnecessary.Iftheonlyforceisthegradientofthecostfunction,\\nthentheparticlemightnevercometorest.Imagineahockeypuckslidingdown\\nonesideofavalleyandstraightuptheotherside,oscillatingbackandforthforever,\\nassumingtheiceisperfectlyfrictionless.Toresolvethisproblem,weaddone\\notherforce,proportionalto−v( t).Inphysicsterminology,thisforcecorresponds\\ntoviscousdrag,asiftheparticlemustpushthrougharesistantmediumsuchas\\nsyrup.Thiscausestheparticletograduallyloseenergyovertimeandeventually\\nconvergetoalocalminimum.\\nWhydoweuse−v( t)andviscousdraginparticular?\\xa0Partofthereasonto\\nuse−v( t)ismathematical convenience—anintegerpowerofthevelocityiseasy\\ntoworkwith.However,otherphysicalsystemshaveotherkindsofdragbased\\nonotherintegerpowersofthevelocity.Forexample,aparticletravelingthrough\\ntheairexperiencesturbulentdrag,withforceproportionaltothesquareofthe\\nvelocity,whileaparticlemovingalongthegroundexperiencesdryfriction,witha\\nforceofconstantmagnitude.Wecanrejecteachoftheseoptions.Turbulentdrag,\\nproportionaltothesquareofthevelocity,becomesveryweakwhenthevelocityis\\nsmall.Itisnotpowerfulenoughtoforcetheparticletocometorest.Aparticle\\nwithanon-zeroinitialvelocitythatexperiencesonlytheforceofturbulentdrag\\nwillmoveawayfromitsinitialpositionforever,withthedistancefromthestarting\\npointgrowinglike O(log t).Wemustthereforeusealowerpowerofthevelocity.\\nIfweuseapowerofzero,representingdryfriction,thentheforceistoostrong.\\nWhentheforceduetothegradientofthecostfunctionissmallbutnon-zero,the\\nconstantforceduetofrictioncancausetheparticletocometorestbeforereaching\\nalocalminimum.Viscousdragavoidsbothoftheseproblems—itisweakenough\\n2 9 9', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nthatthegradientcancontinuetocausemotionuntilaminimumisreached,but\\nstrongenoughtopreventmotionifthegradientdoesnotjustifymoving.\\n8.3.3NesterovMomentum\\nSutskever2013etal.()introducedavariantofthemomentumalgorithmthatwas\\ninspiredbyNesterov’sacceleratedgradientmethod(,,).The Nesterov19832004\\nupdaterulesinthiscasearegivenby:\\nvv← α−∇ \\ue00f θ\\ue022\\n1\\nmm \\ue058\\ni = 1L\\ue010\\nfx(( ) i;+ )θ αv ,y( ) i\\ue011\\ue023\\n,(8.21)\\nθθv ← + , (8.22)\\nwheretheparameters αand \\ue00fplayasimilarroleasinthestandardmomentum\\nmethod.ThediﬀerencebetweenNesterovmomentumandstandardmomentumis\\nwherethegradientisevaluated.WithNesterovmomentumthegradientisevaluated\\nafterthecurrentvelocityisapplied.ThusonecaninterpretNesterovmomentum\\nasattemptingtoaddacorrectionfactortothestandardmethodofmomentum.\\nThecompleteNesterovmomentumalgorithmispresentedinalgorithm .8.3\\nIntheconvexbatchgradientcase,Nesterovmomentumbringstherateof\\nconvergenceoftheexcesserrorfrom O(1 /k)(after ksteps)to O(1 /k2)asshown\\nbyNesterov1983().Unfortunately,\\xa0inthestochasticgradientcase,Nesterov\\nmomentumdoesnotimprovetherateofconvergence.\\nAlgorithm8.3Stochasticgradientdescent(SGD)withNesterovmomentum\\nRequire:Learningrate,momentumparameter. \\ue00f α\\nRequire:Initialparameter,initialvelocity. θ v\\nwhile do stoppingcriterionnotmet\\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\\ncorrespondinglabelsy( ) i.\\nApplyinterimupdate: ˜θθv ← + α\\nComputegradient(atinterimpoint):g←1\\nm∇ ˜ θ\\ue050\\ni L f((x( ) i;˜θy) ,( ) i)\\nComputevelocityupdate:vvg ← α− \\ue00f\\nApplyupdate:θθv ← +\\nendwhile\\n3 0 0', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\n8.4ParameterInitializationStrategies\\nSomeoptimization algorithmsarenotiterativebynatureandsimplysolvefora\\nsolutionpoint.Otheroptimization algorithmsareiterativebynaturebut,when\\nappliedtotherightclassofoptimization problems,convergetoacceptablesolutions\\ninanacceptableamountoftimeregardlessofinitialization. Deeplearningtraining\\nalgorithmsusuallydonothaveeitheroftheseluxuries.Trainingalgorithmsfordeep\\nlearningmodelsareusuallyiterativeinnatureandthusrequiretheusertospecify\\nsomeinitialpointfromwhichtobegintheiterations.Moreover,trainingdeep\\nmodelsisasuﬃcientlydiﬃculttaskthatmostalgorithmsarestronglyaﬀectedby\\nthechoiceofinitialization. Theinitialpointcandeterminewhetherthealgorithm\\nconvergesatall,withsomeinitialpointsbeingsounstablethatthealgorithm\\nencountersnumericaldiﬃcultiesandfailsaltogether.Whenlearningdoesconverge,\\ntheinitialpointcandeterminehowquicklylearningconvergesandwhetherit\\nconvergestoapointwithhigh\\xa0orlowcost.Also,\\xa0pointsofcomparablecost\\ncanhavewildlyvaryinggeneralization error,andtheinitialpointcanaﬀectthe\\ngeneralization aswell.\\nModerninitialization strategiesaresimpleandheuristic.Designingimproved\\ninitialization strategiesisadiﬃculttaskbecauseneuralnetworkoptimization is\\nnotyetwellunderstood.Mostinitialization strategiesarebasedonachievingsome\\nnicepropertieswhenthenetworkisinitialized.However,wedonothaveagood\\nunderstandingofwhichofthesepropertiesarepreservedunderwhichcircumstances\\nafterlearningbeginstoproceed.Afurtherdiﬃcultyisthatsomeinitialpoints\\nmaybebeneﬁcialfromtheviewpointofoptimization butdetrimentalfromthe\\nviewpointofgeneralization. Ourunderstandingofhowtheinitialpointaﬀects\\ngeneralization isespeciallyprimitive,oﬀeringlittletonoguidanceforhowtoselect\\ntheinitialpoint.\\nPerhapstheonlypropertyknownwithcompletecertaintyisthattheinitial\\nparametersneedto“breaksymmetry”\\xa0betweendiﬀerentunits.Iftwohidden\\nunitswiththesameactivationfunctionareconnectedtothesameinputs,then\\ntheseunitsmusthavediﬀerentinitialparameters.\\xa0Iftheyhavethesameinitial\\nparameters,thenadeterministiclearningalgorithmappliedtoadeterministiccost\\nandmodelwillconstantlyupdatebothoftheseunitsinthesameway.Evenifthe\\nmodelortrainingalgorithmiscapableofusingstochasticitytocomputediﬀerent\\nupdatesfordiﬀerentunits(forexample,ifonetrainswithdropout),itisusually\\nbesttoinitializeeachunittocomputeadiﬀerentfunctionfromalloftheother\\nunits.Thismayhelptomakesurethatnoinputpatternsarelostinthenull\\nspaceofforwardpropagationandnogradientpatternsarelostinthenullspace\\nofback-propagation. Thegoalofhavingeachunitcomputeadiﬀerentfunction\\n3 0 1', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nmotivatesrandominitialization oftheparameters.Wecouldexplicitlysearch\\nforalargesetofbasisfunctionsthatareallmutuallydiﬀerentfromeachother,\\nbutthisoftenincursanoticeablecomputational cost.Forexample,ifwehaveat\\nmostasmanyoutputsasinputs,wecoulduseGram-Schmidtorthogonalization\\nonaninitialweightmatrix,andbeguaranteedthateachunitcomputesavery\\ndiﬀerentfunctionfromeachotherunit.Randominitialization fromahigh-entropy\\ndistributionoverahigh-dimensionalspaceiscomputationally cheaperandunlikely\\ntoassignanyunitstocomputethesamefunctionaseachother.\\nTypically,wesetthebiasesforeachunittoheuristicallychosenconstants,and\\ninitializeonlytheweightsrandomly.Extraparameters,forexample,parameters\\nencodingtheconditionalvarianceofaprediction,areusuallysettoheuristically\\nchosenconstantsmuchlikethebiasesare.\\nWealmostalwaysinitializealltheweightsin\\xa0themodel\\xa0tovalues\\xa0drawn\\nrandomly\\xa0froma\\xa0Gaussian\\xa0oruniform\\xa0distribution.Thechoiceof\\xa0Gaussian\\noruniformdistributiondoesnotseemtomatterverymuch,buthasnotbeen\\nexhaustivelystudied.Thescaleoftheinitialdistribution,however,doeshavea\\nlargeeﬀectonboththeoutcomeoftheoptimization procedureandontheability\\nofthenetworktogeneralize.\\nLargerinitialweightswillyieldastrongersymmetrybreakingeﬀect,helping\\ntoavoidredundantunits.Theyalsohelptoavoidlosingsignalduringforwardor\\nback-propagationthroughthelinearcomponentofeachlayer—largervaluesinthe\\nmatrixresultinlargeroutputsofmatrixmultiplication. Initialweightsthatare\\ntoolargemay,however,resultinexplodingvaluesduringforwardpropagationor\\nback-propagation.Inrecurrentnetworks,largeweightscanalsoresultinchaos\\n(suchextremesensitivitytosmallperturbationsoftheinputthatthebehavior\\nofthedeterministicforwardpropagationprocedureappearsrandom).\\xa0Tosome\\nextent,theexplodinggradientproblemcanbemitigatedbygradientclipping\\n(thresholdingthevaluesofthegradientsbeforeperformingagradientdescentstep).\\nLargeweightsmayalsoresultinextremevaluesthatcausetheactivationfunction\\ntosaturate,causingcompletelossofgradientthroughsaturatedunits.These\\ncompetingfactorsdeterminetheidealinitialscaleoftheweights.\\nTheperspectivesofregularizationandoptimization cangiveverydiﬀerent\\ninsightsintohowweshouldinitializeanetwork.Theoptimization perspective\\nsuggeststhattheweightsshouldbelargeenoughtopropagateinformationsuccess-\\nfully,butsomeregularizationconcernsencouragemakingthemsmaller.Theuse\\nofanoptimization algorithmsuchasstochasticgradientdescentthatmakessmall\\nincrementalchangestotheweightsandtendstohaltinareasthatarenearerto\\ntheinitialparameters(whetherduetogettingstuckinaregionoflowgradient,or\\n3 0 2', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nduetotriggeringsomeearlystoppingcriterionbasedonoverﬁtting)expressesa\\npriorthattheﬁnalparametersshouldbeclosetotheinitialparameters.Recall\\nfromsectionthatgradientdescentwithearlystoppingisequivalenttoweight 7.8\\ndecayforsomemodels.Inthegeneralcase,gradientdescentwithearlystoppingis\\nnotthesameasweightdecay,butdoesprovidealooseanalogyforthinkingabout\\ntheeﬀectofinitialization. Wecanthinkofinitializingtheparametersθtoθ 0as\\nbeingsimilartoimposingaGaussianprior p(θ)withmeanθ 0.Fromthispoint\\nofview,itmakessensetochooseθ 0tobenear0.Thispriorsaysthatitismore\\nlikelythatunitsdonotinteractwitheachotherthanthattheydointeract.Units\\ninteractonlyifthelikelihoodtermoftheobjectivefunctionexpressesastrong\\npreferenceforthemtointeract.Ontheotherhand,ifweinitializeθ 0tolarge\\nvalues,thenourpriorspeciﬁeswhichunitsshouldinteractwitheachother,and\\nhowtheyshouldinteract.\\nSomeheuristicsareavailableforchoosingtheinitialscaleoftheweights.One\\nheuristicistoinitializetheweightsofafullyconnectedlayerwith minputsand\\nnoutputsbysamplingeachweightfrom U(−1√m,1√m),whileGlorotandBengio\\n()suggestusingthe 2010 normalizedinitialization\\nW i , j∼ U\\ue020\\n−\\ue072\\n6\\nm n+,\\ue072\\n6\\nm n+\\ue021\\n. (8.23)\\nThislatterheuristicisdesignedtocompromisebetweenthegoalofinitializing\\nalllayerstohavethesameactivationvarianceandthegoalofinitializingall\\nlayerstohavethesamegradientvariance.Theformulaisderivedusingthe\\nassumptionthatthenetworkconsistsonlyofachainofmatrixmultiplications,\\nwithnononlinearities. Realneuralnetworksobviouslyviolatethisassumption,\\nbutmanystrategiesdesignedforthelinearmodelperformreasonablywellonits\\nnonlinearcounterparts.\\nSaxe2013etal.()recommendinitializingtorandomorthogonalmatrices,with\\nacarefullychosenscalingorgainfactor gthataccountsforthenonlinearityapplied\\nateachlayer.Theyderivespeciﬁcvaluesofthescalingfactorfordiﬀerenttypesof\\nnonlinearactivationfunctions.Thisinitialization schemeisalsomotivatedbya\\nmodelofadeepnetworkasasequenceofmatrixmultiplieswithoutnonlinearities.\\nUndersuchamodel,thisinitialization schemeguaranteesthatthetotalnumberof\\ntrainingiterationsrequiredtoreachconvergenceisindependentofdepth.\\nIncreasingthescalingfactor gpushesthenetworktowardtheregimewhere\\nactivationsincreaseinnormastheypropagateforwardthroughthenetworkand\\ngradientsincreaseinnormastheypropagatebackward.\\xa0 ()showed Sussillo2014\\nthatsettingthegainfactorcorrectlyissuﬃcienttotrainnetworksasdeepas\\n3 0 3', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\n1,000layers,withoutneedingtouseorthogonalinitializations.\\xa0A keyinsightof\\nthisapproachisthatinfeedforwardnetworks,activationsandgradientscangrow\\norshrinkoneachstepofforwardorback-propagation, followingarandomwalk\\nbehavior.Thisisbecausefeedforwardnetworksuseadiﬀerentweightmatrixat\\neachlayer.Ifthisrandomwalkistunedtopreservenorms,thenfeedforward\\nnetworkscanmostlyavoidthevanishingandexplodinggradientsproblemthat\\nariseswhenthesameweightmatrixisusedateachstep,describedinsection.8.2.5\\nUnfortunately,theseoptimalcriteriaforinitialweightsoftendonotleadto\\noptimalperformance.Thismaybeforthreediﬀerentreasons.First,wemay\\nbeusingthewrongcriteria—itmaynotactuallybebeneﬁcialtopreservethe\\nnormofasignalthroughouttheentirenetwork.Second,thepropertiesimposed\\natinitialization maynotpersistafterlearninghasbeguntoproceed.Third,the\\ncriteriamightsucceedatimprovingthespeedofoptimization butinadvertently\\nincreasegeneralization error.Inpractice,weusuallyneedtotreatthescaleofthe\\nweightsasahyperparameter whoseoptimalvalueliessomewhereroughlynearbut\\nnotexactlyequaltothetheoreticalpredictions.\\nOnedrawbacktoscalingrulesthatsetalloftheinitialweightstohavethe\\nsamestandarddeviation,suchas1√m,isthateveryindividualweightbecomes\\nextremelysmallwhenthelayersbecomelarge. ()introducedan Martens2010\\nalternativeinitialization schemecalledsparseinitializationinwhicheachunitis\\ninitializedtohaveexactly knon-zeroweights.Theideaistokeepthetotalamount\\nofinputtotheunitindependentfromthenumberofinputs mwithoutmakingthe\\nmagnitudeofindividualweightelementsshrinkwith m.Sparseinitialization helps\\ntoachievemorediversityamongtheunitsatinitialization time.However,italso\\nimposesaverystrongpriorontheweightsthatarechosentohavelargeGaussian\\nvalues.Becauseittakesalongtimeforgradientdescenttoshrink“incorrect”large\\nvalues,thisinitialization schemecancauseproblemsforunitssuchasmaxoutunits\\nthathaveseveralﬁltersthatmustbecarefullycoordinatedwitheachother.\\nWhencomputational resourcesallowit,itisusuallyagoodideatotreatthe\\ninitialscaleoftheweightsforeachlayerasahyperparameter, andtochoosethese\\nscalesusingahyperparametersearchalgorithmdescribedinsection,such11.4.2\\nasrandomsearch.Thechoiceofwhethertousedenseorsparseinitialization\\ncanalsobemadeahyperparameter.Alternately,onecanmanuallysearchfor\\nthebestinitialscales.Agoodruleofthumbforchoosingtheinitialscalesisto\\nlookattherangeorstandarddeviationofactivationsorgradientsonasingle\\nminibatchofdata.Iftheweightsaretoosmall,therangeofactivationsacrossthe\\nminibatchwillshrinkastheactivationspropagateforwardthroughthenetwork.\\nByrepeatedlyidentifyingtheﬁrstlayerwithunacceptably smallactivationsand\\n3 0 4', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nincreasingitsweights,itispossibletoeventuallyobtainanetworkwithreasonable\\ninitialactivationsthroughout.Iflearningisstilltooslowatthispoint,itcanbe\\nusefultolookattherangeorstandarddeviationofthegradientsaswellasthe\\nactivations.\\xa0Thisprocedurecaninprinciplebeautomatedandisgenerallyless\\ncomputationally costlythanhyperparameter optimization basedonvalidationset\\nerrorbecauseitisbasedonfeedbackfromthebehavioroftheinitialmodelona\\nsinglebatchofdata,ratherthanonfeedbackfromatrainedmodelonthevalidation\\nset.Whilelongusedheuristically,thisprotocolhasrecentlybeenspeciﬁedmore\\nformallyandstudiedby (). MishkinandMatas2015\\nSo\\xa0far\\xa0we\\xa0have\\xa0focused\\xa0on\\xa0the\\xa0initialization\\xa0ofthe\\xa0weights.Fortunately,\\ninitialization ofotherparametersistypicallyeasier.\\nTheapproachforsettingthebiasesmustbecoordinatedwiththeapproach\\nforsettingstheweights.Settingthebiasestozeroiscompatiblewithmostweight\\ninitialization schemes.Thereareafewsituationswherewemaysetsomebiasesto\\nnon-zerovalues:\\n•Ifabiasisforanoutputunit,thenitisoftenbeneﬁcialtoinitializethebiasto\\nobtaintherightmarginalstatisticsoftheoutput.Todothis,weassumethat\\ntheinitialweightsaresmallenoughthattheoutputoftheunitisdetermined\\nonlybythebias.Thisjustiﬁessettingthebiastotheinverseoftheactivation\\nfunctionappliedtothemarginalstatisticsoftheoutputinthetrainingset.\\nForexample,iftheoutputisadistributionoverclassesandthisdistribution\\nisahighlyskeweddistributionwiththemarginalprobabilityofclass igiven\\nbyelement c iofsomevectorc,thenwecansetthebiasvectorbbysolving\\ntheequationsoftmax (b) =c.Thisappliesnotonlytoclassiﬁersbutalsoto\\nmodelswewillencounterinPart,suchasautoencodersandBoltzmann III\\nmachines.Thesemodelshavelayerswhoseoutputshouldresembletheinput\\ndatax,anditcanbeveryhelpfultoinitializethebiasesofsuchlayersto\\nmatchthemarginaldistributionover.x\\n•Sometimeswemay\\xa0wanttochoosethebiastoavoidcausingtoo\\xa0much\\nsaturationatinitialization. Forexample,wemaysetthebiasofaReLU\\nhiddenunitto0.1ratherthan0toavoidsaturatingtheReLUatinitialization.\\nThisapproachisnotcompatiblewithweightinitialization schemesthatdo\\nnotexpectstronginputfromthebiasesthough.Forexample,\\xa0itisnot\\nrecommendedforusewithrandomwalkinitialization (,). Sussillo2014\\n•Sometimesaunitcontrolswhetherotherunitsareabletoparticipateina\\nfunction.Insuchsituations,wehaveaunitwithoutput uandanotherunit\\nh∈[0 ,1],andtheyaremultipliedtogethertoproduceanoutput u h.\\xa0We\\n3 0 5', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\ncanview hasagatethatdetermineswhether u h u≈or u h≈0.\\xa0Inthese\\nsituations,wewanttosetthebiasfor hsothat h≈1mostofthetimeat\\ninitialization. Otherwise udoesnothaveachancetolearn.Forexample,\\nJozefowicz2015etal.()advocatesettingthebiastofortheforgetgateof 1\\ntheLSTMmodel,describedinsection.10.10\\nAnothercommontypeofparameterisavarianceorprecisionparameter.For\\nexample,wecanperformlinearregressionwithaconditionalvarianceestimate\\nusingthemodel\\np y y (| Nx) = (|wTx+1) b , /β (8.24)\\nwhere βisaprecisionparameter.Wecanusuallyinitializevarianceorprecision\\nparametersto1safely.Anotherapproachistoassumetheinitialweightsareclose\\nenoughtozerothatthebiasesmaybesetwhileignoringtheeﬀectoftheweights,\\nthensetthebiasestoproducethecorrectmarginalmeanoftheoutput,andset\\nthevarianceparameterstothemarginalvarianceoftheoutputinthetrainingset.\\nBesidesthesesimpleconstantorrandommethodsofinitializingmodelparame-\\nters,itispossibletoinitializemodelparametersusingmachinelearning.Acommon\\nstrategydiscussedinpartofthisbookistoinitializeasupervisedmodelwith III\\ntheparameterslearnedbyanunsupervisedmodeltrainedonthesameinputs.\\nOnecanalsoperformsupervisedtrainingonarelatedtask.Evenperforming\\nsupervisedtrainingonanunrelatedtaskcansometimesyieldaninitialization that\\noﬀersfasterconvergencethanarandominitialization. Someoftheseinitialization\\nstrategiesmayyieldfasterconvergenceandbettergeneralization becausethey\\nencodeinformationaboutthedistributionintheinitialparametersofthemodel.\\nOthersapparentlyperformwellprimarilybecausetheysettheparameterstohave\\ntherightscaleorsetdiﬀerentunitstocomputediﬀerentfunctionsfromeachother.\\n8.5AlgorithmswithAdaptiveLearningRates\\nNeuralnetworkresearchershavelongrealizedthatthelearningratewasreliablyone\\nofthehyperparameters thatisthemostdiﬃculttosetbecauseithasasigniﬁcant\\nimpactonmodelperformance.Aswehavediscussedinsectionsand,the 4.38.2\\ncostisoftenhighlysensitivetosomedirectionsinparameterspaceandinsensitive\\ntoothers.Themomentumalgorithmcanmitigatetheseissuessomewhat,but\\ndoessoattheexpenseofintroducinganotherhyperparameter. Inthefaceofthis,\\nitisnaturaltoaskifthereisanotherway.Ifwebelievethatthedirectionsof\\nsensitivityaresomewhataxis-aligned,itcanmakesensetouseaseparatelearning\\n3 0 6', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nrateforeachparameter,andautomatically adapttheselearningratesthroughout\\nthecourseoflearning.\\nThe algorithm(,)isanearlyheuristicapproach delta-bar-delta Jacobs1988\\ntoadaptingindividuallearningratesformodelparametersduringtraining.The\\napproachisbasedonasimpleidea:ifthepartialderivativeoftheloss,withrespect\\ntoagivenmodelparameter,remainsthesamesign,thenthelearningrateshould\\nincrease.Ifthepartialderivativewithrespecttothatparameterchangessign,\\nthenthelearningrateshoulddecrease.\\xa0Ofcourse,thiskindofrulecanonlybe\\nappliedtofullbatchoptimization.\\nMorerecently,anumberofincremental(ormini-batch-bas ed)methodshave\\nbeenintroducedthatadaptthelearningratesofmodelparameters.Thissection\\nwillbrieﬂyreviewafewofthesealgorithms.\\n8.5.1AdaGrad\\nTheAdaGradalgorithm,showninalgorithm ,individuallyadaptsthelearning 8.4\\nratesofallmodelparametersbyscalingtheminverselyproportionaltothesquare\\nrootofthesumofalloftheirhistoricalsquaredvalues(,).The Duchietal.2011\\nparameterswiththelargestpartialderivativeofthelosshaveacorrespondingly\\nrapiddecreaseintheirlearningrate,whileparameterswithsmallpartialderivatives\\nhavearelativelysmalldecreaseintheirlearningrate.Theneteﬀectisgreater\\nprogressinthemoregentlyslopeddirectionsofparameterspace.\\nInthecontextofconvexoptimization, theAdaGradalgorithmenjoyssome\\ndesirabletheoreticalproperties.However,empiricallyithasbeenfoundthat—for\\ntrainingdeepneuralnetworkmodels—theaccumulation ofsquaredgradientsfrom\\nthebeginningoftrainingcanresultinaprematureandexcessivedecreaseinthe\\neﬀectivelearningrate.AdaGradperformswellforsomebutnotalldeeplearning\\nmodels.\\n8.5.2RMSProp\\nTheRMSPropalgorithm(,)modiﬁesAdaGradtoperformbetterin Hinton2012\\nthenon-convexsettingbychangingthegradientaccumulation intoanexponentially\\nweightedmovingaverage.AdaGradisdesignedtoconvergerapidlywhenapplied\\ntoaconvexfunction.\\xa0When appliedtoanon-convexfunctiontotrainaneural\\nnetwork,thelearningtrajectorymaypassthroughmanydiﬀerentstructuresand\\neventuallyarriveataregionthatisalocallyconvexbowl.AdaGradshrinksthe\\nlearningrateaccordingtotheentirehistoryofthesquaredgradientandmay\\n3 0 7', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nAlgorithm8.4TheAdaGradalgorithm\\nRequire:Globallearningrate \\ue00f\\nRequire:Initialparameterθ\\nRequire:Smallconstant,perhaps δ 10− 7,fornumericalstability\\nInitializegradientaccumulationvariabler= 0\\nwhile do stoppingcriterionnotmet\\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\\ncorrespondingtargetsy( ) i.\\nComputegradient:g←1\\nm∇ θ\\ue050\\ni L f((x( ) i;)θ ,y( ) i)\\nAccumulatesquaredgradient:rrgg ←+\\ue00c\\nComputeupdate: ∆θ←−\\ue00f\\nδ +√r\\ue00cg.(Divisionandsquarerootapplied\\nelement-wise)\\nApplyupdate:θθθ ← +∆\\nendwhile\\nhavemadethelearningratetoosmallbeforearrivingatsuchaconvexstructure.\\nRMSPropusesanexponentiallydecayingaveragetodiscardhistoryfromthe\\nextremepastsothatitcanconvergerapidlyafterﬁndingaconvexbowl,asifit\\nwereaninstanceoftheAdaGradalgorithminitializedwithinthatbowl.\\nRMSPropisshowninitsstandardforminalgorithm andcombinedwith 8.5\\nNesterovmomentuminalgorithm .ComparedtoAdaGrad,theuseofthe 8.6\\nmovingaverageintroducesanewhyperparameter, ρ,thatcontrolsthelengthscale\\nofthemovingaverage.\\nEmpirically,RMSProphasbeenshowntobeaneﬀectiveandpracticalop-\\ntimizationalgorithmfordeepneuralnetworks.Itiscurrentlyoneofthego-to\\noptimization methodsbeingemployedroutinelybydeeplearningpractitioners.\\n8.5.3Adam\\nAdam( ,)isyetanotheradaptivelearningrateoptimization KingmaandBa2014\\nalgorithmandispresentedinalgorithm .Thename“Adam”\\xa0derivesfrom 8.7\\nthephrase“adaptivemoments.”Inthecontextoftheearlieralgorithms,itis\\nperhapsbestseenasavariantonthecombinationofRMSPropandmomentum\\nwithafewimportantdistinctions.First,inAdam,momentumisincorporated\\ndirectlyasanestimateoftheﬁrstordermoment(withexponentialweighting)of\\nthegradient.ThemoststraightforwardwaytoaddmomentumtoRMSPropisto\\napplymomentumtotherescaledgradients.Theuseofmomentumincombination\\nwithrescalingdoesnothaveacleartheoreticalmotivation.Second,Adamincludes\\n3 0 8', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nAlgorithm8.5TheRMSPropalgorithm\\nRequire:Globallearningrate,decayrate. \\ue00f ρ\\nRequire:Initialparameterθ\\nRequire:Smallconstant δ,\\xa0usually 10− 6,\\xa0usedtostabilizedivision\\xa0bysmall\\nnumbers.\\nInitializeaccumulation variablesr= 0\\nwhile do stoppingcriterionnotmet\\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\\ncorrespondingtargetsy( ) i.\\nComputegradient:g←1\\nm∇ θ\\ue050\\ni L f((x( ) i;)θ ,y( ) i)\\nAccumulatesquaredgradient:rrgg ← ρ+(1 )− ρ\\ue00c\\nComputeparameterupdate: ∆θ=−\\ue00f√\\nδ + r\\ue00cg.(1√\\nδ + rappliedelement-wise)\\nApplyupdate:θθθ ← +∆\\nendwhile\\nbiascorrectionstotheestimatesofboththeﬁrst-ordermoments(themomentum\\nterm)andthe(uncentered)second-ordermomentstoaccountfortheirinitialization\\nattheorigin(seealgorithm ).RMSPropalsoincorporatesanestimateofthe 8.7\\n(uncentered)second-ordermoment,howeveritlacksthecorrectionfactor.Thus,\\nunlikeinAdam,theRMSPropsecond-ordermomentestimatemayhavehighbias\\nearlyintraining.Adamisgenerallyregardedasbeingfairlyrobusttothechoice\\nofhyperparameters ,thoughthelearningratesometimesneedstobechangedfrom\\nthesuggesteddefault.\\n8.5.4ChoosingtheRightOptimizationAlgorithm\\nInthissection,wediscussedaseriesofrelatedalgorithmsthateachseektoaddress\\nthechallengeofoptimizingdeepmodelsbyadaptingthelearningrateforeach\\nmodelparameter.Atthispoint,anaturalquestionis:whichalgorithmshouldone\\nchoose?\\nUnfortunately,thereiscurrentlynoconsensusonthispoint. () Schauletal.2014\\npresentedavaluablecomparisonofalargenumberofoptimization algorithms\\nacrossawiderangeoflearningtasks.Whiletheresultssuggestthatthefamilyof\\nalgorithmswithadaptivelearningrates(representedbyRMSPropandAdaDelta)\\nperformedfairlyrobustly,nosinglebestalgorithmhasemerged.\\nCurrently,themostpopularoptimization algorithmsactivelyinuseinclude\\nSGD,SGDwithmomentum,RMSProp,RMSPropwithmomentum,AdaDelta\\nandAdam.Thechoiceofwhichalgorithmtouse,atthispoint,seemstodepend\\n3 0 9', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nAlgorithm8.6RMSPropalgorithmwithNesterovmomentum\\nRequire:Globallearningrate,decayrate,momentumcoeﬃcient. \\ue00f ρ α\\nRequire:Initialparameter,initialvelocity. θ v\\nInitializeaccumulation variabler= 0\\nwhile do stoppingcriterionnotmet\\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\\ncorrespondingtargetsy( ) i.\\nComputeinterimupdate: ˜θθv ← + α\\nComputegradient:g←1\\nm∇ ˜ θ\\ue050\\ni L f((x( ) i;˜θy) ,( ) i)\\nAccumulategradient:rrgg ← ρ+(1 )− ρ\\ue00c\\nComputevelocityupdate:vv← α−\\ue00f√r\\ue00cg.(1√rappliedelement-wise)\\nApplyupdate:θθv ← +\\nendwhile\\nlargelyontheuser’sfamiliaritywiththealgorithm(foreaseofhyperparameter\\ntuning).\\n8.6ApproximateSecond-OrderMethods\\nInthissectionwediscusstheapplicationofsecond-ordermethodstothetraining\\nofdeepnetworks.See ()foranearliertreatmentofthissubject. LeCunetal.1998a\\nForsimplicityofexposition,theonlyobjectivefunctionweexamineistheempirical\\nrisk:\\nJ() = θ E x ,y ∼ ˆ pdata ( ) x , y[((;))] = L fxθ , y1\\nmm \\ue058\\ni = 1L f((x( ) i;)θ , y( ) i) .(8.25)\\nHoweverthemethodswediscusshereextendreadilytomoregeneralobjective\\nfunctionsthat,forinstance,includeparameterregularizationtermssuchasthose\\ndiscussedinchapter.7\\n8.6.1Newton’sMethod\\nInsection,weintroducedsecond-ordergradientmethods.Incontrasttoﬁrst- 4.3\\nordermethods,second-ordermethodsmakeuseofsecondderivativestoimprove\\noptimization. Themostwidelyusedsecond-ordermethodisNewton’smethod.We\\nnowdescribeNewton’smethodinmoredetail,withemphasisonitsapplicationto\\nneuralnetworktraining.\\n3 1 0', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nAlgorithm8.7TheAdamalgorithm\\nRequire:Stepsize(Suggesteddefault: ) \\ue00f 0001 .\\nRequire:Exponentialdecayratesformomentestimates, ρ 1and ρ 2in[0 ,1).\\n(Suggesteddefaults:andrespectively) 09 . 0999 .\\nRequire:Smallconstant δusedfornumericalstabilization.(Suggesteddefault:\\n10− 8)\\nRequire:Initialparametersθ\\nInitialize1stand2ndmomentvariables ,s= 0r= 0\\nInitializetimestep t= 0\\nwhile do stoppingcriterionnotmet\\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\\ncorrespondingtargetsy( ) i.\\nComputegradient:g←1\\nm∇ θ\\ue050\\ni L f((x( ) i;)θ ,y( ) i)\\nt t←+1\\nUpdatebiasedﬁrstmomentestimate:s← ρ 1s+(1− ρ 1)g\\nUpdatebiasedsecondmomentestimate:r← ρ 2r+(1− ρ 2)gg\\ue00c\\nCorrectbiasinﬁrstmoment:ˆs←s\\n1 − ρt\\n1\\nCorrectbiasinsecondmoment:ˆr←r\\n1 − ρt\\n2\\nComputeupdate: ∆= θ− \\ue00fˆs√\\nˆ r + δ(operationsappliedelement-wise)\\nApplyupdate:θθθ ← +∆\\nendwhile\\nNewton’smethodisanoptimization schemebasedonusingasecond-orderTay-\\nlorseriesexpansiontoapproximate J(θ)nearsomepointθ 0,ignoringderivatives\\nofhigherorder:\\nJ J () θ≈(θ 0)+(θθ− 0)\\ue03e∇ θ J(θ 0)+1\\n2(θθ− 0)\\ue03eHθθ (− 0) ,(8.26)\\nwhereHistheHessianof Jwithrespecttoθevaluatedatθ 0.Ifwethensolvefor\\nthecriticalpointofthisfunction,weobtaintheNewtonparameterupdaterule:\\nθ∗= θ 0−H− 1∇ θ J(θ 0) (8.27)\\nThusforalocallyquadraticfunction(withpositivedeﬁniteH),byrescaling\\nthegradientbyH− 1,Newton’smethodjumpsdirectlytotheminimum.\\xa0If the\\nobjectivefunctionisconvexbutnotquadratic(therearehigher-orderterms),this\\nupdatecanbeiterated,yieldingthetrainingalgorithmassociatedwithNewton’s\\nmethod,giveninalgorithm .8.8\\n3 1 1', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nAlgorithm8.8Newton’smethodwithobjective J(θ) =\\n1\\nm\\ue050m\\ni = 1 L f((x( ) i;)θ , y( ) i).\\nRequire:Initialparameterθ 0\\nRequire:Trainingsetofexamples m\\nwhile do stoppingcriterionnotmet\\nComputegradient:g←1\\nm∇ θ\\ue050\\ni L f((x( ) i;)θ ,y( ) i)\\nComputeHessian:H←1\\nm∇2\\nθ\\ue050\\ni L f((x( ) i;)θ ,y( ) i)\\nComputeHessianinverse:H− 1\\nComputeupdate: ∆= θ−H− 1g\\nApplyupdate:θθθ = +∆\\nendwhile\\nForsurfacesthatarenotquadratic,aslongastheHessianremainspositive\\ndeﬁnite,Newton’smethodcanbeappliediteratively.Thisimpliesatwo-step\\niterativeprocedure.First,updateorcomputetheinverseHessian(i.e.byupdat-\\ningthequadraticapproximation).\\xa0Second, updatetheparametersaccordingto\\nequation.8.27\\nInsection,wediscussedhowNewton’smethodisappropriateonlywhen 8.2.3\\ntheHessianispositivedeﬁnite.Indeeplearning,thesurfaceoftheobjective\\nfunctionistypicallynon-convexwithmanyfeatures,suchassaddlepoints,that\\nareproblematicforNewton’smethod.\\xa0IftheeigenvaluesoftheHessianarenot\\nallpositive,forexample,nearasaddlepoint,thenNewton’smethodcanactually\\ncauseupdatestomoveinthewrongdirection.Thissituationcanbeavoided\\nbyregularizingtheHessian.Commonregularizationstrategiesincludeaddinga\\nconstant,,alongthediagonaloftheHessian.Theregularizedupdatebecomes α\\nθ∗= θ 0−[(( H fθ 0))+ ] αI− 1∇ θ f(θ 0) . (8.28)\\nThisregularizationstrategyisusedinapproximations toNewton’smethod,such\\nastheLevenberg–Marquardt algorithm(Levenberg1944Marquardt1963 ,;,),and\\nworksfairlywellaslongasthenegativeeigenvaluesoftheHessianarestillrelatively\\nclosetozero.Incaseswheretherearemoreextremedirectionsofcurvature,the\\nvalueof αwouldhavetobesuﬃcientlylargetooﬀsetthenegativeeigenvalues.\\nHowever,as αincreasesinsize,theHessianbecomesdominatedbythe αIdiagonal\\nandthedirectionchosenbyNewton’smethodconvergestothestandardgradient\\ndividedby α.\\xa0Whenstrongnegativecurvatureispresent, αmayneedtobeso\\nlargethatNewton’smethodwouldmakesmallerstepsthangradientdescentwith\\naproperlychosenlearningrate.\\nBeyondthechallengescreatedbycertainfeaturesoftheobjectivefunction,\\n3 1 2', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nsuchassaddlepoints,theapplicationofNewton’smethodfortraininglargeneural\\nnetworksislimitedbythesigniﬁcantcomputational burdenitimposes.The\\nnumberofelementsintheHessianissquaredinthenumberofparameters,sowith\\nkparameters(andforevenverysmallneuralnetworksthenumberofparameters\\nkcanbeinthemillions),Newton’smethodwouldrequiretheinversionofa k k×\\nmatrix—with computational complexityof O( k3).Also,sincetheparameterswill\\nchangewitheveryupdate,theinverseHessianhastobecomputed ateverytraining\\niteration.Asaconsequence,onlynetworkswithaverysmallnumberofparameters\\ncanbepracticallytrainedviaNewton’smethod.Intheremainderofthissection,\\nwewilldiscussalternativesthatattempttogainsomeoftheadvantagesofNewton’s\\nmethodwhileside-steppingthecomputational hurdles.\\n8.6.2ConjugateGradients\\nConjugategradientsisamethodtoeﬃcientlyavoidthecalculationoftheinverse\\nHessianbyiterativelydescendingconjugatedirections.Theinspirationforthis\\napproachfollowsfromacarefulstudyoftheweaknessofthemethodofsteepest\\ndescent(seesectionfordetails),wherelinesearchesareappliediterativelyin 4.3\\nthedirectionassociatedwiththegradient.Figureillustrateshowthemethodof 8.6\\nsteepestdescent,whenappliedinaquadraticbowl,progressesinaratherineﬀective\\nback-and-forth,zig-zagpattern.Thishappensbecauseeachlinesearchdirection,\\nwhengivenbythegradient,isguaranteedtobeorthogonaltothepreviousline\\nsearchdirection.\\nLettheprevioussearchdirectionbed t − 1.Attheminimum,wheretheline\\nsearchterminates,thedirectionalderivativeiszeroindirectiond t − 1:∇ θ J(θ)·\\nd t − 1=0.Sincethegradientatthispointdeﬁnesthecurrentsearchdirection,\\nd t=∇ θ J(θ) willhavenocontributioninthedirectiond t − 1.Thusd tisorthogonal\\ntod t − 1.Thisrelationshipbetweend t − 1andd tisillustratedinﬁgurefor8.6\\nmultipleiterationsofsteepestdescent.Asdemonstratedintheﬁgure,thechoiceof\\northogonaldirectionsofdescentdonotpreservetheminimumalongtheprevious\\nsearchdirections.Thisgivesrisetothezig-zagpatternofprogress,whereby\\ndescendingtotheminimuminthecurrentgradientdirection,wemustre-minimize\\ntheobjectiveinthepreviousgradientdirection.Thus,byfollowingthegradientat\\ntheendofeachlinesearchweare,inasense,undoingprogresswehavealready\\nmadeinthedirectionofthepreviouslinesearch.Themethodofconjugategradients\\nseekstoaddressthisproblem.\\nInthemethodofconjugategradients,weseektoﬁndasearchdirectionthat\\nisconjugatetothepreviouslinesearchdirection,i.e.itwillnotundoprogress\\nmadeinthatdirection.Attrainingiteration t,thenextsearchdirectiond ttakes\\n3 1 3', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\n\\x00\\ue033 \\ue030 \\x00\\ue032 \\ue030 \\x00\\ue031 \\ue030 \\ue030 \\ue031 \\ue030 \\ue032 \\ue030\\x00\\ue033 \\ue030\\x00\\ue032 \\ue030\\x00\\ue031 \\ue030\\ue030\\ue031 \\ue030\\ue032 \\ue030\\nFigure8.6:Themethodofsteepestdescentappliedtoaquadraticcostsurface.The\\nmethodofsteepestdescentinvolvesjumpingtothepointoflowestcostalongtheline\\ndeﬁnedbythegradientattheinitialpointoneachstep.Thisresolvessomeoftheproblems\\nseenwithusingaﬁxedlearningrateinﬁgure,butevenwiththeoptimalstepsize 4.6\\nthealgorithmstillmakesback-and-forthprogresstowardtheoptimum.Bydeﬁnition,at\\ntheminimumoftheobjectivealongagivendirection,thegradientattheﬁnalpointis\\northogonaltothatdirection.\\ntheform:\\nd t= ∇ θ J β ()+θ td t − 1 (8.29)\\nwhere β tisacoeﬃcientwhosemagnitudecontrolshowmuchofthedirection,d t − 1,\\nweshouldaddbacktothecurrentsearchdirection.\\nTwodirections,d tandd t − 1,aredeﬁnedasconjugateifd\\ue03e\\ntHd t − 1= 0,where\\nHistheHessianmatrix.\\nThestraightforwardwaytoimposeconjugacywouldinvolvecalculationofthe\\neigenvectorsofHtochoose β t,whichwouldnotsatisfyourgoalofdeveloping\\namethodthatismorecomputationally viablethanNewton’smethodforlarge\\nproblems.\\xa0Canwecalculatetheconjugatedirectionswithoutresortingtothese\\ncalculations?Fortunatelytheanswertothatisyes.\\nTwopopularmethodsforcomputingthe β tare:\\n1.\\xa0Fletcher-Reeves:\\nβ t=∇ θ J(θ t)\\ue03e∇ θ J(θ t)\\n∇ θ J(θ t − 1)\\ue03e∇ θ J(θ t − 1)(8.30)\\n3 1 4', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\n2.\\xa0Polak-Ribière:\\nβ t=(∇ θ J(θ t)−∇ θ J(θ t − 1))\\ue03e∇ θ J(θ t)\\n∇ θ J(θ t − 1)\\ue03e∇ θ J(θ t − 1)(8.31)\\nForaquadraticsurface,theconjugatedirectionsensurethatthegradientalong\\nthepreviousdirectiondoesnotincreaseinmagnitude.Wethereforestayatthe\\nminimumalongthepreviousdirections.Asaconsequence,ina k-dimensional\\nparameterspace,theconjugategradientmethodrequiresatmost klinesearchesto\\nachievetheminimum.Theconjugategradientalgorithmisgiveninalgorithm .8.9\\nAlgorithm8.9Theconjugategradientmethod\\nRequire:Initialparametersθ 0\\nRequire:Trainingsetofexamples m\\nInitializeρ 0= 0\\nInitialize g 0= 0\\nInitialize t= 1\\nwhile do stoppingcriterionnotmet\\nInitializethegradientg t= 0\\nComputegradient:g t←1\\nm∇ θ\\ue050\\ni L f((x( ) i;)θ ,y( ) i)\\nCompute β t=( g t − g t −1 )\\ue03eg t\\ng\\ue03e\\nt −1g t −1(Polak-Ribière)\\n(Nonlinearconjugategradient:optionallyreset β ttozero,forexampleif tis\\namultipleofsomeconstant,suchas) k k= 5\\nComputesearchdirection:ρ t= −g t+ β tρ t − 1\\nPerformlinesearchtoﬁnd: \\ue00f∗= argmin \\ue00f1\\nm\\ue050m\\ni = 1 L f((x( ) i;θ t+ \\ue00fρ t) ,y( ) i)\\n(Onatrulyquadraticcostfunction,analyticallysolvefor \\ue00f∗ratherthan\\nexplicitlysearchingforit)\\nApplyupdate:θ t + 1= θ t+ \\ue00f∗ρ t\\nt t←+1\\nendwhile\\nNonlinearConjugateGradients:Sofarwehavediscussedthemethodof\\nconjugategradientsasitisappliedtoquadraticobjectivefunctions.\\xa0Ofcourse,\\nourprimaryinterestinthischapteristoexploreoptimization methodsfortraining\\nneuralnetworksandotherrelateddeeplearningmodelswherethecorresponding\\nobjectivefunctionisfarfromquadratic.Perhapssurprisingly,themethodof\\nconjugategradientsisstillapplicableinthissetting,thoughwithsomemodiﬁcation.\\nWithoutanyassurancethattheobjectiveisquadratic,theconjugatedirections\\n3 1 5', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\narenolongerassuredtoremainattheminimumoftheobjectiveforprevious\\ndirections.Asaresult,thenonlinearconjugategradientsalgorithmincludes\\noccasionalresetswherethemethodofconjugategradientsisrestartedwithline\\nsearchalongtheunalteredgradient.\\nPractitionersreportreasonableresultsinapplicationsofthenonlinearconjugate\\ngradientsalgorithmtotrainingneuralnetworks,thoughitisoftenbeneﬁcialto\\ninitializetheoptimizationwithafewiterationsofstochasticgradientdescentbefore\\ncommencingnonlinearconjugategradients.Also,whilethe(nonlinear)conjugate\\ngradientsalgorithmhastraditionallybeencastasabatchmethod,minibatch\\nversionshavebeenusedsuccessfullyforthetrainingofneuralnetworks(,Leetal.\\n2011).\\xa0Adaptationsofconjugategradientsspeciﬁcallyforneuralnetworkshave\\nbeenproposedearlier,suchasthescaledconjugategradientsalgorithm(,Moller\\n1993).\\n8.6.3BFGS\\nTheBroyden–Fletcher–Goldfarb–Shanno(BFGS)algorithmattemptsto\\nbringsomeoftheadvantagesofNewton’smethodwithoutthecomputational\\nburden.In\\xa0thatrespect,\\xa0BFGSissimilartotheconjugategradientmethod.\\nHowever,BFGStakesamoredirectapproachtotheapproximation ofNewton’s\\nupdate.RecallthatNewton’supdateisgivenby\\nθ∗= θ 0−H− 1∇ θ J(θ 0) , (8.32)\\nwhereHistheHessianof Jwithrespecttoθevaluatedatθ 0.Theprimary\\ncomputational diﬃcultyinapplyingNewton’supdateisthecalculationofthe\\ninverseHessianH− 1.Theapproachadoptedbyquasi-Newtonmethods(ofwhich\\ntheBFGSalgorithmisthemostprominent)istoapproximate theinversewith\\namatrixM tthatisiterativelyreﬁnedbylowrankupdatestobecomeabetter\\napproximationofH− 1.\\nThespeciﬁcationandderivationoftheBFGSapproximationisgiveninmany\\ntextbooksonoptimization, includingLuenberger1984().\\nOncetheinverseHessianapproximationM tisupdated,thedirectionofdescent\\nρ tisdeterminedbyρ t=M tg t.Alinesearchisperformedinthisdirectionto\\ndeterminethesizeofthestep, \\ue00f∗,takeninthisdirection.Theﬁnalupdatetothe\\nparametersisgivenby:\\nθ t + 1= θ t+ \\ue00f∗ρ t . (8.33)\\nLikethemethodofconjugategradients,theBFGSalgorithmiteratesaseriesof\\nlinesearcheswiththedirectionincorporatingsecond-orderinformation. However\\n3 1 6', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nunlikeconjugategradients,thesuccessoftheapproachisnotheavilydependent\\nonthelinesearchﬁndingapointveryclosetothetrueminimumalongtheline.\\nThus,relativetoconjugategradients,BFGShastheadvantagethatitcanspend\\nlesstimereﬁningeachlinesearch.Ontheotherhand,theBFGSalgorithmmust\\nstoretheinverseHessianmatrix,M,thatrequires O( n2)memory,makingBFGS\\nimpracticalformostmoderndeeplearningmodelsthattypicallyhavemillionsof\\nparameters.\\nLimited\\xa0Memory\\xa0BFGS\\xa0(or\\xa0L-BFGS)The\\xa0memory costs\\xa0ofthe\\xa0BFGS\\nalgorithmcanbesigniﬁcantlydecreasedbyavoidingstoringthecompleteinverse\\nHessianapproximationM.TheL-BFGSalgorithmcomputestheapproximationM\\nusingthesamemethodastheBFGSalgorithm,butbeginningwiththeassumption\\nthatM( 1 ) t −istheidentitymatrix,ratherthanstoringtheapproximation fromone\\nsteptothenext.Ifusedwithexactlinesearches,thedirectionsdeﬁnedbyL-BFGS\\naremutuallyconjugate.However,unlikethemethodofconjugategradients,this\\nprocedureremainswellbehavedwhentheminimumofthelinesearchisreached\\nonlyapproximately .TheL-BFGSstrategywithnostoragedescribedherecanbe\\ngeneralizedtoincludemoreinformationabouttheHessianbystoringsomeofthe\\nvectorsusedtoupdateateachtimestep,whichcostsonlyperstep. M O n()\\n8.7OptimizationStrategiesandMeta-Algorithms\\nManyoptimization techniquesarenotexactlyalgorithms,\\xa0butrathergeneral\\ntemplatesthatcanbespecializedtoyieldalgorithms,orsubroutinesthatcanbe\\nincorporatedintomanydiﬀerentalgorithms.\\n8.7.1BatchNormalization\\nBatchnormalization ( ,)isoneofthemostexcitingrecent IoﬀeandSzegedy2015\\ninnovationsinoptimizingdeepneuralnetworksanditisactuallynotanoptimization\\nalgorithmatall.Instead,itisamethodofadaptivereparametrization, motivated\\nbythediﬃcultyoftrainingverydeepmodels.\\nVerydeepmodelsinvolvethecompositionofseveralfunctionsorlayers.The\\ngradienttellshowtoupdateeachparameter,undertheassumptionthattheother\\nlayersdonotchange.Inpractice,weupdateallofthelayerssimultaneously.\\nWhenwemaketheupdate,unexpectedresultscanhappenbecausemanyfunctions\\ncomposedtogetherarechangedsimultaneously,usingupdatesthatwerecomputed\\nundertheassumptionthattheotherfunctionsremainconstant.Asasimple\\n3 1 7', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nexample,supposewehaveadeepneuralnetworkthathasonlyoneunitperlayer\\nanddoesnotuseanactivationfunctionateachhiddenlayer:ˆ y= x w 1 w 2 w 3 . . . w l.\\nHere, w iprovidestheweightusedbylayer i.Theoutputoflayer iis h i= h i − 1 w i.\\nTheoutput ˆ yisalinearfunctionoftheinput x,butanonlinearfunctionofthe\\nweights w i.Supposeourcostfunctionhasputagradientofon1 ˆ y,sowewishto\\ndecreaseˆ yslightly.Theback-propagationalgorithmcanthencomputeagradient\\ng=∇ wˆ y.Considerwhathappenswhenwemakeanupdatewwg ← − \\ue00f.The\\nﬁrst-orderTaylorseriesapproximation ofˆ ypredictsthatthevalueofˆ ywilldecrease\\nby \\ue00fg\\ue03eg.Ifwewantedtodecreaseˆ yby .1,thisﬁrst-orderinformationavailablein\\nthegradientsuggestswecouldsetthelearningrate \\ue00fto. 1\\ng\\ue03eg.However,theactual\\nupdatewillincludesecond-orderandthird-ordereﬀects,onuptoeﬀectsoforder l.\\nThenewvalueofˆ yisgivenby\\nx w( 1− \\ue00f g 1)( w 2− \\ue00f g 2)( . . . w l− \\ue00f g l) . (8.34)\\nAnexampleofonesecond-ordertermarisingfromthisupdateis \\ue00f2g 1 g 2\\ue051l\\ni = 3 w i.\\nThistermmightbenegligibleif\\ue051l\\ni = 3 w iissmall,ormightbeexponentiallylarge\\niftheweightsonlayersthrough3 laregreaterthan.Thismakesitveryhard 1\\ntochooseanappropriatelearningrate,becausetheeﬀectsofanupdatetothe\\nparametersforonelayerdependssostronglyonalloftheotherlayers.Second-order\\noptimizationalgorithmsaddressthisissuebycomputinganupdatethattakesthese\\nsecond-orderinteractionsintoaccount,butwecanseethatinverydeepnetworks,\\nevenhigher-orderinteractionscanbesigniﬁcant.Evensecond-orderoptimization\\nalgorithmsareexpensiveandusuallyrequirenumerousapproximations thatprevent\\nthemfromtrulyaccountingforallsigniﬁcantsecond-orderinteractions. Building\\nan n-thorderoptimization algorithmfor n >2thusseemshopeless.Whatcanwe\\ndoinstead?\\nBatchnormalization providesanelegantwayofreparametrizing almostanydeep\\nnetwork.Thereparametrization signiﬁcantlyreducestheproblemofcoordinating\\nupdatesacrossmanylayers.Batchnormalization canbeappliedtoanyinput\\norhiddenlayerinanetwork.LetHbeaminibatchofactivationsofthelayer\\ntonormalize,arrangedasadesignmatrix,withtheactivationsforeachexample\\nappearinginarowofthematrix.Tonormalize,wereplaceitwith H\\nH\\ue030=Hµ−\\nσ, (8.35)\\nwhereµisavectorcontainingthemeanofeachunitandσisavectorcontaining\\nthestandarddeviationofeachunit.Thearithmetichereisbasedonbroadcasting\\nthevectorµandthevectorσtobeappliedtoeveryrowofthematrixH.Within\\neachrow,thearithmeticiselement-wise,so H i , jisnormalizedbysubtracting µ j\\n3 1 8', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nanddividingby σ j.TherestofthenetworkthenoperatesonH\\ue030inexactlythe\\nsamewaythattheoriginalnetworkoperatedon.H\\nAttrainingtime,\\nµ=1\\nm\\ue058\\niH i , : (8.36)\\nand\\nσ=\\ue073\\nδ+1\\nm\\ue058\\ni( )Hµ−2\\ni , (8.37)\\nwhere δisasmallpositivevaluesuchas10− 8imposedtoavoidencountering\\ntheundeﬁnedgradientof√zat z=0.Crucially,\\xa0weback-propagatethrough\\ntheseoperationsforcomputingthemeanandthestandarddeviation,andfor\\napplyingthemtonormalizeH.Thismeansthatthegradientwillneverpropose\\nanoperation\\xa0that actssimplytoincreasethestandard\\xa0deviationormeanof\\nh i;thenormalization operationsremovetheeﬀectofsuchanactionandzero\\noutitscomponentinthegradient.Thiswasamajorinnovationofthebatch\\nnormalization approach.\\xa0Previous approacheshadinvolvedaddingpenaltiesto\\nthecostfunctiontoencourageunitstohavenormalizedactivationstatisticsor\\ninvolvedinterveningtorenormalizeunitstatisticsaftereachgradientdescentstep.\\nTheformerapproachusuallyresultedinimperfectnormalization andthelatter\\nusuallyresultedinsigniﬁcantwastedtimeasthelearningalgorithmrepeatedly\\nproposedchangingthemeanandvarianceandthenormalization steprepeatedly\\nundidthischange.Batchnormalization reparametrizes themodeltomakesome\\nunitsalwaysbestandardizedbydeﬁnition,deftlysidesteppingbothproblems.\\nAttesttime,µandσmaybereplacedbyrunningaveragesthatwerecollected\\nduringtrainingtime.Thisallowsthemodeltobeevaluatedonasingleexample,\\nwithoutneedingtousedeﬁnitionsofµandσthatdependonanentireminibatch.\\nRevisitingtheˆ y= x w 1 w 2 . . . w lexample,weseethatwecanmostlyresolvethe\\ndiﬃcultiesinlearningthismodelbynormalizing h l − 1.Supposethat xisdrawn\\nfromaunitGaussian.Then h l − 1willalsocomefromaGaussian,becausethe\\ntransformationfrom xto h lislinear.However, h l − 1willnolongerhavezeromean\\nandunitvariance.Afterapplyingbatchnormalization, weobtainthenormalized\\nˆh l − 1thatrestoresthezeromeanandunitvarianceproperties.Foralmostany\\nupdatetothelowerlayers,ˆh l − 1willremainaunitGaussian.Theoutput ˆ ymay\\nthenbelearnedasasimplelinearfunction ˆ y= w lˆ h l − 1.Learninginthismodelis\\nnowverysimplebecausetheparametersatthelowerlayerssimplydonothavean\\neﬀectinmostcases;theiroutputisalwaysrenormalizedtoaunitGaussian.\\xa0In\\nsomecornercases,thelowerlayerscanhaveaneﬀect.Changingoneofthelower\\nlayerweightstocanmaketheoutputbecomedegenerate,andchangingthesign 0\\n3 1 9', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nofoneofthelowerweightscanﬂiptherelationshipbetweenˆ h l − 1and y.\\xa0These\\nsituationsareveryrare.Withoutnormalization, nearlyeveryupdatewouldhave\\nanextremeeﬀectonthestatisticsof h l − 1.Batchnormalization hasthusmade\\nthismodelsigniﬁcantlyeasiertolearn.\\xa0Inthisexample,theeaseoflearningof\\ncoursecameatthecostofmakingthelowerlayersuseless.Inourlinearexample,\\nthelowerlayersnolongerhaveanyharmfuleﬀect,buttheyalsonolongerhave\\nanybeneﬁcialeﬀect.Thisisbecausewehavenormalizedouttheﬁrstandsecond\\norderstatistics,whichisallthatalinearnetworkcaninﬂuence.Inadeepneural\\nnetworkwithnonlinearactivationfunctions,thelowerlayerscanperformnonlinear\\ntransformationsofthedata,sotheyremainuseful.Batchnormalization actsto\\nstandardizeonlythemeanandvarianceofeachunitinordertostabilizelearning,\\nbutallowstherelationshipsbetweenunitsandthenonlinearstatisticsofasingle\\nunittochange.\\nBecausetheﬁnallayerofthenetworkisabletolearnalineartransformation,\\nwemayactuallywishtoremovealllinearrelationshipsbetweenunitswithina\\nlayer.Indeed,thisistheapproachtakenby (),whoprovided Desjardinsetal.2015\\ntheinspirationforbatchnormalization. Unfortunately,\\xa0eliminating alllinear\\ninteractionsismuchmoreexpensivethanstandardizingthemeanandstandard\\ndeviationofeachindividualunit,andsofarbatchnormalization remainsthemost\\npracticalapproach.\\nNormalizingthemeanandstandarddeviationofaunitcanreducetheexpressive\\npowerofthe\\xa0neuralnetworkcontainingthatunit.Inordertomaintainthe\\nexpressivepowerofthenetwork,itiscommontoreplacethebatchofhiddenunit\\nactivationsHwithγH\\ue030+βratherthansimplythenormalizedH\\ue030.Thevariables\\nγandβarelearnedparametersthatallowthenewvariabletohaveanymean\\nandstandarddeviation.Atﬁrstglance,thismayseemuseless—whydidweset\\nthemeanto 0,andthenintroduceaparameterthatallowsittobesetbackto\\nanyarbitraryvalueβ?Theansweristhatthenewparametrization canrepresent\\nthesamefamilyoffunctionsoftheinputastheoldparametrization, butthenew\\nparametrization hasdiﬀerentlearningdynamics.Intheoldparametrization, the\\nmeanofHwasdeterminedbyacomplicatedinteractionbetweentheparameters\\ninthelayersbelowH.Inthenewparametrization, themeanofγH\\ue030+βis\\ndeterminedsolelybyβ.Thenewparametrization ismucheasiertolearnwith\\ngradientdescent.\\nMostneuralnetworklayerstaketheformof φ(XW+b)where φissome\\nﬁxednonlinearactivationfunctionsuchastherectiﬁedlineartransformation.It\\nisnaturaltowonderwhetherweshouldapplybatchnormalization totheinput\\nX,ortothetransformedvalueXW+b. ()recommend IoﬀeandSzegedy2015\\n3 2 0', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nthelatter.Morespeciﬁcally,XW+bshouldbereplacedbyanormalizedversion\\nofXW.Thebiastermshouldbeomittedbecauseitbecomesredundantwith\\nthe βparameterappliedbythebatchnormalization reparametrization. Theinput\\ntoalayerisusuallytheoutputofanonlinearactivationfunctionsuchasthe\\nrectiﬁedlinearfunctioninapreviouslayer.\\xa0Thestatisticsoftheinputarethus\\nmorenon-Gaussianandlessamenabletostandardizationbylinearoperations.\\nInconvolutionalnetworks,describedinchapter,itisimportanttoapplythe 9\\nsamenormalizing µand σateveryspatiallocationwithinafeaturemap,sothat\\nthestatisticsofthefeaturemapremainthesameregardlessofspatiallocation.\\n8.7.2CoordinateDescent\\nInsomecases,itmaybepossibletosolveanoptimization problemquicklyby\\nbreakingitintoseparatepieces.Ifweminimize f(x)withrespecttoasingle\\nvariable x i,\\xa0then\\xa0minimize\\xa0it with\\xa0respect\\xa0to\\xa0another variable x jand\\xa0soon,\\nrepeatedlycyclingthroughallvariables,weareguaranteedtoarriveata(local)\\nminimum.Thispracticeisknownascoordinatedescent,becauseweoptimize\\nonecoordinateatatime.\\xa0Moregenerally,blockcoordinatedescentrefersto\\nminimizingwithrespecttoasubsetofthevariablessimultaneously.Theterm\\n“coordinatedescent”isoftenusedtorefertoblockcoordinatedescentaswellas\\nthestrictlyindividualcoordinatedescent.\\nCoordinatedescentmakesthemostsensewhenthediﬀerentvariablesinthe\\noptimization problemcanbeclearlyseparatedintogroupsthatplayrelatively\\nisolatedroles,orwhenoptimization withrespecttoonegroupofvariablesis\\nsigniﬁcantlymoreeﬃcientthanoptimization withrespecttoallofthevariables.\\nForexample,considerthecostfunction\\nJ ,(HW) =\\ue058\\ni , j| H i , j|+\\ue058\\ni , j\\ue010\\nXW−\\ue03eH\\ue0112\\ni , j.(8.38)\\nThisfunctiondescribesalearningproblemcalledsparsecoding,wherethegoalis\\ntoﬁndaweightmatrixWthatcanlinearlydecodeamatrixofactivationvalues\\nHtoreconstructthetrainingsetX.Mostapplicationsofsparsecodingalso\\ninvolveweightdecayoraconstraintonthenormsofthecolumnsofW,inorder\\ntopreventthepathologicalsolutionwithextremelysmallandlarge.HW\\nThefunction Jisnotconvex.However,\\xa0wecandividetheinputstothe\\ntrainingalgorithmintotwosets:thedictionaryparametersWandthecode\\nrepresentationsH.Minimizingtheobjectivefunctionwithrespecttoeitheroneof\\nthesesetsofvariablesisaconvexproblem.Blockcoordinatedescentthusgives\\n3 2 1', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nusanoptimization strategythatallowsustouseeﬃcientconvexoptimization\\nalgorithms,byalternatingbetweenoptimizingWwithHﬁxed,thenoptimizing\\nHWwithﬁxed.\\nCoordinatedescentisnotaverygoodstrategywhenthevalueofonevariable\\nstronglyinﬂuencestheoptimalvalueofanothervariable,asinthefunction f(x) =\\n( x 1− x 2)2+ α\\ue000\\nx2\\n1+ x2\\n2\\ue001\\nwhere αisapositiveconstant.Theﬁrsttermencourages\\nthetwovariablestohavesimilarvalue,whilethesecondtermencouragesthem\\ntobenearzero.Thesolutionistosetbothtozero.Newton’smethodcansolve\\ntheprobleminasinglestepbecauseitisapositivedeﬁnitequadraticproblem.\\nHowever,forsmall α,coordinatedescentwillmakeveryslowprogressbecausethe\\nﬁrsttermdoesnotallowasinglevariabletobechangedtoavaluethatdiﬀers\\nsigniﬁcantlyfromthecurrentvalueoftheothervariable.\\n8.7.3PolyakAveraging\\nPolyakaveraging(PolyakandJuditsky1992,)consistsofaveragingtogetherseveral\\npoints\\xa0inthe\\xa0trajectory\\xa0through parameter\\xa0spacevisited\\xa0by\\xa0anoptimization\\nalgorithm.\\xa0If titerationsofgradientdescentvisitpointsθ( 1 ), . . . ,θ( ) t,thenthe\\noutputofthePolyakaveragingalgorithmisˆθ( ) t=1\\nt\\ue050\\niθ( ) i.\\xa0Onsomeproblem\\nclasses,suchasgradientdescentappliedtoconvexproblems,thisapproachhas\\nstrongconvergenceguarantees.Whenappliedtoneuralnetworks,itsjustiﬁcation\\nismoreheuristic,butitperformswellinpractice.Thebasicideaisthatthe\\noptimization algorithmmayleapbackandforthacrossavalleyseveraltimes\\nwithoutevervisitingapointnearthebottomofthevalley.Theaverageofallof\\nthelocationsoneithersideshouldbeclosetothebottomofthevalleythough.\\nInnon-convexproblems,thepathtakenbytheoptimization trajectorycanbe\\nverycomplicatedandvisitmanydiﬀerentregions.Includingpointsinparameter\\nspacefromthedistantpastthatmaybeseparatedfromthecurrentpointbylarge\\nbarriersinthecostfunctiondoesnotseemlikeausefulbehavior.Asaresult,\\nwhenapplyingPolyakaveragingtonon-convexproblems,itistypicaltousean\\nexponentiallydecayingrunningaverage:\\nˆθ( ) t= αˆθ( 1 ) t −+(1 )− αθ( ) t. (8.39)\\nTherunningaverageapproachisusedinnumerousapplications.SeeSzegedy\\netal.()forarecentexample. 2015\\n3 2 2', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\n8.7.4SupervisedPretraining\\nSometimes,directlytrainingamodeltosolveaspeciﬁctaskcanbetooambitious\\nifthemodeliscomplexandhardtooptimizeorifthetaskisverydiﬃcult.Itis\\nsometimesmoreeﬀectivetotrainasimplermodeltosolvethetask,thenmake\\nthemodelmorecomplex.Itcanalsobemoreeﬀectivetotrainthemodeltosolve\\nasimplertask,thenmoveontoconfronttheﬁnaltask.Thesestrategiesthat\\ninvolvetrainingsimplemodelsonsimpletasksbeforeconfrontingthechallengeof\\ntrainingthedesiredmodeltoperformthedesiredtaskarecollectivelyknownas\\npretraining.\\nGreedyalgorithmsbreakaproblemintomanycomponents,thensolvefor\\ntheoptimalversionofeachcomponentinisolation.Unfortunately,combiningthe\\nindividuallyoptimalcomponentsisnotguaranteedtoyieldanoptimalcomplete\\nsolution.However,greedyalgorithmscanbecomputationally muchcheaperthan\\nalgorithmsthatsolveforthebestjointsolution,andthequalityofagreedysolution\\nisoftenacceptableifnotoptimal.Greedyalgorithmsmayalsobefollowedbya\\nﬁne-tuningstageinwhichajointoptimization algorithmsearchesforanoptimal\\nsolutiontothefullproblem.Initializingthejointoptimization algorithmwitha\\ngreedysolutioncangreatlyspeeditupandimprovethequalityofthesolutionit\\nﬁnds.\\nPretraining,andespeciallygreedypretraining,algorithmsareubiquitousin\\ndeeplearning.Inthissection,wedescribespeciﬁcallythosepretrainingalgorithms\\nthatbreaksupervisedlearningproblemsintoothersimplersupervisedlearning\\nproblems.Thisapproachisknownas . greedysupervisedpretraining\\nIntheoriginal( ,)versionofgreedysupervisedpretraining, Bengioetal.2007\\neachstageconsistsofasupervisedlearningtrainingtaskinvolvingonlyasubsetof\\nthelayersintheﬁnalneuralnetwork.Anexampleofgreedysupervisedpretraining\\nisillustratedinﬁgure,inwhicheachaddedhiddenlayerispretrainedaspart 8.7\\nofashallowsupervisedMLP,takingasinputtheoutputofthepreviouslytrained\\nhiddenlayer.Insteadofpretrainingonelayeratatime,SimonyanandZisserman\\n()pretrainadeepconvolutionalnetwork(elevenweightlayers)andthenuse 2015\\ntheﬁrstfourandlastthreelayersfromthisnetworktoinitializeevendeeper\\nnetworks(withuptonineteenlayersofweights).Themiddlelayersofthenew,\\nverydeepnetworkareinitializedrandomly.Thenewnetworkisthenjointlytrained.\\nAnotheroption,exploredbyYu2010etal.()istousetheofthepreviously outputs\\ntrainedMLPs,aswellastherawinput,asinputsforeachaddedstage.\\nWhy\\xa0would\\xa0greedy\\xa0sup ervised\\xa0pretraining help?The\\xa0hypothesis \\xa0initially\\ndiscussedby ()isthatithelpstoprovidebetterguidancetothe Bengioetal.2007\\n3 2 3', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\ny y\\nh( 1 )h( 1 )\\nx x\\n( a )U( 1 )U( 1 )\\nW( 1 )W( 1 ) y yh( 1 )h( 1 )\\nx x\\n( b )U( 1 )U( 1 )W( 1 )W( 1 )\\ny yh( 1 )h( 1 )\\nx x\\n( c )U( 1 )U( 1 )W( 1 )W( 1 )h( 2 )h( 2 )\\ny y U( 2 )U( 2 ) W( 2 )W( 2 )\\ny yh( 1 )h( 1 )\\nx x\\n( d )U( 1 )U( 1 )W( 1 )W( 1 )h( 2 )h( 2 )y\\nU( 2 )U( 2 )\\nW( 2 )W( 2 )\\nFigure8.7:Illustrationofoneformofgreedysupervisedpretraining( ,). Bengio e t a l .2007\\n( a )Westartbytrainingasuﬃcientlyshallowarchitecture.Anotherdrawingofthe ( b )\\nsamearchitecture.Wekeeponlytheinput-to-hiddenlayeroftheoriginalnetworkand ( c )\\ndiscardthehidden-to-outputlayer.Wesendtheoutputoftheﬁrsthiddenlayerasinput\\ntoanothersupervisedsinglehiddenlayerMLPthatistrainedwiththesameobjective\\nastheﬁrstnetworkwas,thusaddingasecondhiddenlayer.Thiscanberepeatedforas\\nmanylayersasdesired.Anotherdrawingoftheresult,viewedasafeedforwardnetwork. ( d )\\nTofurtherimprovetheoptimization,wecanjointlyﬁne-tuneallthelayers,eitheronlyat\\ntheendorateachstageofthisprocess.\\n3 2 4', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nintermediatelevelsofadeephierarchy.Ingeneral,pretrainingmayhelpbothin\\ntermsofoptimization andintermsofgeneralization.\\nAnapproachrelatedtosupervisedpretrainingextendstheideatothecontext\\noftransferlearning:Yosinski2014etal.()pretrainadeepconvolutionalnetwith8\\nlayersofweightsonasetoftasks(asubsetofthe1000ImageNetobjectcategories)\\nandtheninitializeasame-sizenetworkwiththeﬁrst klayersoftheﬁrstnet.All\\nthelayersofthesecondnetwork(withtheupperlayersinitializedrandomly)are\\nthenjointlytrainedtoperformadiﬀerentsetoftasks(anothersubsetofthe1000\\nImageNetobjectcategories),withfewertrainingexamplesthanfortheﬁrstsetof\\ntasks.Otherapproachestotransferlearningwithneuralnetworksarediscussedin\\nsection.15.2\\nAnotherrelatedlineofworkistheFitNets( ,)approach. Romeroetal.2015\\nThisapproachbeginsbytraininganetworkthathaslowenoughdepthandgreat\\nenoughwidth(numberofunitsperlayer)tobeeasytotrain.Thisnetworkthen\\nbecomesateacherforasecondnetwork,designatedthestudent.Thestudent\\nnetworkismuchdeeperandthinner(eleventonineteenlayers)andwouldbe\\ndiﬃculttotrainwithSGDundernormalcircumstances.Thetrainingofthe\\nstudentnetworkismadeeasierbytrainingthestudentnetworknotonlytopredict\\ntheoutputfortheoriginaltask,butalsotopredictthevalueofthemiddlelayer\\noftheteachernetwork.Thisextrataskprovidesasetofhintsabouthowthe\\nhiddenlayersshouldbeusedandcansimplifytheoptimizationproblem.Additional\\nparametersareintroducedtoregressthemiddlelayerofthe5-layerteachernetwork\\nfromthemiddlelayerofthedeeperstudentnetwork.However,insteadofpredicting\\ntheﬁnalclassiﬁcationtarget,theobjectiveistopredictthemiddlehiddenlayer\\noftheteachernetwork.\\xa0Thelowerlayersofthestudentnetworksthushavetwo\\nobjectives:tohelptheoutputsofthestudentnetworkaccomplishtheirtask,as\\nwellastopredicttheintermediatelayeroftheteachernetwork.Althoughathin\\nanddeepnetworkappearstobemorediﬃculttotrainthanawideandshallow\\nnetwork,thethinanddeepnetworkmaygeneralizebetterandcertainlyhaslower\\ncomputational costifitisthinenoughtohavefarfewerparameters.Without\\nthehintsonthehiddenlayer,thestudentnetworkperformsverypoorlyinthe\\nexperiments,bothonthetrainingandtestset.Hintsonmiddlelayersmaythus\\nbeoneofthetoolstohelptrainneuralnetworksthatotherwiseseemdiﬃcultto\\ntrain,butotheroptimization techniquesorchangesinthearchitecturemayalso\\nsolvetheproblem.\\n3 2 5', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\n8.7.5DesigningModelstoAidOptimization\\nToimproveoptimization, thebeststrategyisnotalwaystoimprovetheoptimization\\nalgorithm.Instead,manyimprovementsintheoptimization ofdeepmodelshave\\ncomefromdesigningthemodelstobeeasiertooptimize.\\nInprinciple,wecoulduseactivationfunctionsthatincreaseanddecreasein\\njaggednon-monotonic patterns.However,thiswouldmakeoptimization extremely\\ndiﬃcult.Inpractice, itismoreimportanttochooseamodelfamilythatiseasyto\\noptimizethantouseapowerfuloptimizationalgorithm.Mostoftheadvancesin\\nneuralnetworklearningoverthepast30yearshavebeenobtainedbychanging\\nthemodelfamilyratherthanchangingtheoptimization procedure.Stochastic\\ngradientdescentwithmomentum,whichwasusedtotrainneuralnetworksinthe\\n1980s,remainsinuseinmodernstateoftheartneuralnetworkapplications.\\nSpeciﬁcally,modernneuralnetworksreﬂectadesignchoicetouselineartrans-\\nformationsbetweenlayersandactivationfunctionsthatarediﬀerentiable almost\\neverywhereandhavesigniﬁcantslopeinlargeportionsoftheirdomain.\\xa0Inpar-\\nticular,modelinnovationsliketheLSTM,rectiﬁedlinearunitsandmaxoutunits\\nhaveallmovedtowardusingmorelinearfunctionsthanpreviousmodelslikedeep\\nnetworksbasedonsigmoidalunits.Thesemodelshavenicepropertiesthatmake\\noptimization easier.Thegradientﬂowsthroughmanylayersprovidedthatthe\\nJacobianofthelineartransformationhasreasonablesingularvalues.\\xa0Moreover,\\nlinearfunctionsconsistentlyincreaseinasingledirection,soevenifthemodel’s\\noutputisveryfarfromcorrect,itisclearsimplyfromcomputingthegradient\\nwhichdirectionitsoutputshouldmovetoreducethelossfunction.Inotherwords,\\nmodernneuralnetshavebeendesignedsothattheirlocalgradientinformation\\ncorrespondsreasonablywelltomovingtowardadistantsolution.\\nOthermodeldesignstrategiescanhelptomakeoptimization easier.For\\nexample,linearpathsorskipconnectionsbetweenlayersreducethelengthof\\ntheshortestpathfromthelower\\xa0layer’sparameters\\xa0totheoutput,\\xa0and thus\\nmitigatethevanishinggradientproblem(Srivastava2015etal.,).Arelatedidea\\ntoskipconnectionsisaddingextracopiesoftheoutputthatareattachedtothe\\nintermediatehiddenlayersofthenetwork,asinGoogLeNet( ,) Szegedy etal.2014a\\nanddeeply-supervisednets(,).These“auxiliaryheads”aretrained Leeetal.2014\\ntoperformthesametaskastheprimaryoutputatthetopofthenetworkinorder\\ntoensurethatthelowerlayersreceivealargegradient.Whentrainingiscomplete\\ntheauxiliaryheadsmaybediscarded.\\xa0Thisisanalternativetothepretraining\\nstrategies,whichwereintroducedintheprevioussection.Inthisway,onecan\\ntrainjointlyallthelayersinasinglephasebutchangethearchitecture, sothat\\nintermediatelayers(especiallythelowerones)cangetsomehintsaboutwhatthey\\n3 2 6', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nshoulddo,viaashorterpath.Thesehintsprovideanerrorsignaltolowerlayers.\\n8.7.6ContinuationMethodsandCurriculumLearning\\nAsarguedinsection,manyofthechallengesinoptimization arisefromthe 8.2.7\\nglobalstructureofthecostfunctionandcannotberesolvedmerelybymakingbetter\\nestimatesoflocalupdatedirections.Thepredominant strategyforovercomingthis\\nproblemistoattempttoinitializetheparametersinaregionthatisconnected\\ntothesolutionbyashortpaththroughparameterspacethatlocaldescentcan\\ndiscover.\\nContinuationmethodsareafamilyofstrategiesthatcanmakeoptimization\\neasierbychoosinginitialpointstoensurethatlocaloptimization spendsmostof\\nitstimeinwell-behavedregionsofspace.Theideabehindcontinuationmethodsis\\ntoconstructaseriesofobjectivefunctionsoverthesameparameters.Inorderto\\nminimizeacostfunction J(θ),wewillconstructnewcostfunctions { J( 0 ), . . . , J( ) n}.\\nThesecostfunctionsaredesignedtobeincreasinglydiﬃcult,with J( 0 )beingfairly\\neasytominimize,and J( ) n,themostdiﬃcult,being J(θ),thetruecostfunction\\nmotivatingtheentireprocess.Whenwesaythat J( ) iiseasierthan J( + 1 ) i,we\\nmeanthatitiswellbehavedovermoreofθspace.Arandominitialization ismore\\nlikelytolandintheregionwherelocaldescentcanminimizethecostfunction\\nsuccessfullybecausethisregionislarger.Theseriesofcostfunctionsaredesigned\\nsothatasolutiontooneisagoodinitialpointofthenext.Wethusbeginby\\nsolvinganeasyproblemthenreﬁnethesolutiontosolveincrementally harder\\nproblemsuntilwearriveatasolutiontothetrueunderlyingproblem.\\nTraditionalcontinuationmethods(predatingtheuseofcontinuationmethods\\nforneuralnetworktraining)areusuallybasedonsmoothingtheobjectivefunction.\\nSeeWu1997()foranexampleofsuchamethodandareviewofsomerelated\\nmethods.Continuationmethodsarealsocloselyrelatedtosimulatedannealing,\\nwhichaddsnoisetotheparameters(Kirkpatrick\\xa01983etal.,).Continuation\\nmethodshavebeenextremelysuccessfulinrecentyears.SeeMobahiandFisher\\n()foranoverviewofrecentliterature,especiallyforAIapplications. 2015\\nContinuationmethodstraditionallyweremostlydesignedwiththegoalof\\novercomingthechallengeoflocalminima.Speciﬁcally,theyweredesignedto\\nreachaglobalminimumdespitethepresenceofmanylocalminima.Todoso,\\nthesecontinuationmethodswouldconstructeasiercostfunctionsby“blurring”the\\noriginalcostfunction.Thisblurringoperationcanbedonebyapproximating\\nJ( ) i() = θ Eθ\\ue030∼ N ( θ\\ue030; θ , σ()2 i) J(θ\\ue030) (8.40)\\nviasampling.Theintuitionforthisapproachisthatsomenon-convexfunctions\\n3 2 7', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nbecomeapproximately convexwhenblurred.Inmanycases,thisblurringpreserves\\nenoughinformationaboutthelocationofaglobalminimumthatwecanﬁndthe\\nglobalminimumbysolvingprogressivelylessblurredversionsoftheproblem.This\\napproachcanbreakdowninthreediﬀerentways.First,itmightsuccessfullydeﬁne\\naseriesofcostfunctionswheretheﬁrstisconvexandtheoptimumtracksfrom\\nonefunctiontothenextarrivingattheglobalminimum,butitmightrequireso\\nmanyincrementalcostfunctionsthatthecostoftheentireprocedureremainshigh.\\nNP-hardoptimization problemsremainNP-hard,evenwhencontinuationmethods\\nareapplicable.Theothertwowaysthatcontinuationmethodsfailbothcorrespond\\ntothemethodnotbeingapplicable.First,thefunctionmightnotbecomeconvex,\\nnomatterhowmuchitisblurred.Considerforexamplethefunction J(θ) =−θ\\ue03eθ.\\nSecond,thefunctionmaybecomeconvexasaresultofblurring,buttheminimum\\nofthisblurredfunctionmaytracktoalocalratherthanaglobalminimumofthe\\noriginalcostfunction.\\nThoughcontinuationmethodsweremostlyoriginallydesignedtodealwiththe\\nproblemoflocalminima,localminimaarenolongerbelievedtobetheprimary\\nproblemforneuralnetworkoptimization. Fortunately,continuationmethodscan\\nstillhelp.Theeasierobjectivefunctionsintroducedbythecontinuationmethodcan\\neliminateﬂatregions,decreasevarianceingradientestimates,improveconditioning\\noftheHessianmatrix,ordoanythingelsethatwilleithermakelocalupdates\\neasiertocomputeorimprovethecorrespondencebetweenlocalupdatedirections\\nandprogresstowardaglobalsolution.\\nBengio2009etal.()observedthatanapproachcalledcurriculumlearning\\norshapingcanbeinterpretedasacontinuationmethod.Curriculumlearningis\\nbasedontheideaofplanningalearningprocesstobeginbylearningsimpleconcepts\\nandprogresstolearningmorecomplexconceptsthatdependonthesesimpler\\nconcepts.Thisbasicstrategywaspreviouslyknowntoaccelerateprogressinanimal\\ntraining(,;,; Skinner1958Peterson2004KruegerandDayan2009,)andmachine\\nlearning(,;,;,). () Solomonoﬀ1989Elman1993Sanger1994Bengioetal.2009\\njustiﬁedthisstrategyasacontinuationmethod,whereearlier J( ) iaremadeeasierby\\nincreasingtheinﬂuenceofsimplerexamples(eitherbyassigningtheircontributions\\ntothecostfunctionlargercoeﬃcients,orbysamplingthemmorefrequently),and\\nexperimentallydemonstratedthatbetterresultscouldbeobtainedbyfollowinga\\ncurriculumonalarge-scaleneurallanguagemodelingtask.Curriculumlearning\\nhasbeensuccessfulonawiderangeofnaturallanguage(Spitkovsky2010etal.,;\\nCollobert2011aMikolov2011bTuandHonavar2011 etal.,; etal.,; ,)andcomputer\\nvision( ,; ,; ,) Kumaretal.2010LeeandGrauman2011SupancicandRamanan2013\\ntasks.Curriculumlearningwasalsoveriﬁedasbeingconsistentwiththewayin\\nwhichhumans teach(,):teachersstartbyshowingeasierand Khanetal.2011\\n3 2 8', 'CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nmoreprototypicalexamplesandthenhelpthelearnerreﬁnethedecisionsurface\\nwiththelessobviouscases.Curriculum-based strategiesaremoreeﬀectivefor\\nteachinghumansthanstrategiesbasedonuniformsamplingofexamples,andcan\\nalsoincreasetheeﬀectivenessofotherteachingstrategies( , BasuandChristensen\\n2013).\\nAnotherimportantcontributiontoresearchoncurriculumlearningaroseinthe\\ncontextoftrainingrecurrentneuralnetworkstocapturelong-termdependencies:\\nZarembaandSutskever2014()foundthatmuchbetterresultswereobtainedwitha\\nstochasticcurriculum,inwhicharandommixofeasyanddiﬃcultexamplesisalways\\npresentedtothelearner,butwheretheaverageproportionofthemorediﬃcult\\nexamples(here,thosewithlonger-termdependencies)isgraduallyincreased.With\\nadeterministiccurriculum,noimprovementoverthebaseline(ordinarytraining\\nfromthefulltrainingset)wasobserved.\\nWehavenowdescribedthebasicfamilyofneuralnetworkmodelsandhowto\\nregularizeandoptimizethem.Inthechaptersahead,weturntospecializationsof\\ntheneuralnetworkfamily,thatallowneuralnetworkstoscaletoverylargesizesand\\nprocessinputdatathathasspecialstructure.Theoptimization methodsdiscussed\\ninthischapterareoftendirectlyapplicabletothesespecializedarchitectures with\\nlittleornomodiﬁcation.\\n3 2 9']}, 'DeepLearning-chapter-7.pdf': {'text': ['C h a p t e r 7\\nRegularization f or D e e p L e ar n i n g\\nAcentralprobleminmachinelearningishowtomakeanalgorithmthatwill\\nperformwellnotjustonthetrainingdata,butalsoonnewinputs.Manystrategies\\nusedinmachinelearningareexplicitlydesignedtoreducethetesterror,possibly\\nattheexpenseofincreasedtrainingerror.Thesestrategiesareknowncollectively\\nasregularization.\\xa0As wewillseethereareagreatmanyformsofregularization\\navailabletothedeeplearningpractitioner. Infact,\\xa0developingmoreeﬀective\\nregularizationstrategieshasbeenoneofthemajorresearcheﬀortsintheﬁeld.\\nChapterintroducedthebasicconceptsofgeneralization, underﬁtting,overﬁt- 5\\nting,bias,varianceandregularization. Ifyouarenotalreadyfamiliarwiththese\\nnotions,pleaserefertothatchapterbeforecontinuingwiththisone.\\nInthischapter,wedescriberegularizationinmoredetail,focusingonregular-\\nizationstrategiesfordeepmodelsormodelsthatmaybeusedasbuildingblocks\\ntoformdeepmodels.\\nSomesectionsofthischapterdealwithstandardconceptsinmachinelearning.\\nIfyouarealreadyfamiliarwiththeseconcepts,\\xa0feelfreetoskiptherelevant\\nsections.However,mostofthischapterisconcernedwiththeextensionofthese\\nbasicconceptstotheparticularcaseofneuralnetworks.\\nInsection,wedeﬁnedregularizationas“anymodiﬁcationwemaketo 5.2.2\\nalearningalgorithmthatisintendedtoreduceitsgeneralization errorbutnot\\nitstrainingerror.”Therearemanyregularizationstrategies.Someputextra\\nconstraints\\xa0ona\\xa0machine\\xa0learning\\xa0model, such\\xa0asadding\\xa0restrictionson\\xa0the\\nparametervalues.Someaddextratermsintheobjectivefunctionthatcanbe\\nthoughtofascorrespondingtoasoftconstraintontheparametervalues.Ifchosen\\ncarefully,theseextraconstraintsandpenaltiescanleadtoimprovedperformance\\n228', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nonthetestset.Sometimestheseconstraintsandpenaltiesaredesignedtoencode\\nspeciﬁckindsofpriorknowledge.Othertimes,theseconstraintsandpenalties\\naredesignedtoexpressagenericpreferenceforasimplermodelclassinorderto\\npromotegeneralization. Sometimespenaltiesandconstraintsarenecessarytomake\\nanunderdetermined problemdetermined.Otherformsofregularization,knownas\\nensemblemethods,combinemultiplehypothesesthatexplainthetrainingdata.\\nInthecontextofdeeplearning,mostregularizationstrategiesarebasedon\\nregularizingestimators.Regularizationofanestimatorworksbytradingincreased\\nbiasforreducedvariance.Aneﬀectiveregularizerisonethatmakesaproﬁtable\\ntrade,reducingvariancesigniﬁcantlywhilenotoverlyincreasingthebias.Whenwe\\ndiscussedgeneralization andoverﬁttinginchapter,wefocusedonthreesituations, 5\\nwherethemodelfamilybeingtrainedeither(1)excludedthetruedatagenerating\\nprocess—correspondingtounderﬁttingandinducingbias,or(2)matchedthetrue\\ndatageneratingprocess,or(3)includedthegeneratingprocessbutalsomany\\notherpossiblegeneratingprocesses—theoverﬁttingregimewherevariancerather\\nthanbiasdominatestheestimationerror.Thegoalofregularizationistotakea\\nmodelfromthethirdregimeintothesecondregime.\\nInpractice,anoverlycomplexmodelfamilydoesnotnecessarilyincludethe\\ntargetfunctionorthetruedatageneratingprocess,orevenacloseapproximation\\nofeither.Wealmostneverhaveaccesstothetruedatageneratingprocessso\\nwecanneverknowforsureifthemodelfamilybeingestimatedincludesthe\\ngeneratingprocessornot.However,mostapplicationsofdeeplearningalgorithms\\naretodomainswherethetruedatageneratingprocessisalmostcertainlyoutside\\nthemodelfamily.Deeplearningalgorithmsaretypicallyappliedtoextremely\\ncomplicateddomainssuchasimages,audiosequencesandtext,forwhichthetrue\\ngenerationprocessessentiallyinvolvessimulatingtheentireuniverse.Tosome\\nextent,wearealwaystryingtoﬁtasquarepeg(thedatageneratingprocess)into\\naroundhole(ourmodelfamily).\\nWhatthismeansisthatcontrollingthecomplexityofthemodelisnota\\nsimplematterofﬁndingthemodeloftherightsize,withtherightnumberof\\nparameters.Instead,wemightﬁnd—andindeedinpracticaldeeplearningscenarios,\\nwealmostalwaysdoﬁnd—thatthebestﬁttingmodel(inthesenseofminimizing\\ngeneralization error)isalargemodelthathasbeenregularizedappropriately .\\nWenowreviewseveralstrategiesforhowtocreatesuchalarge,deep,regularized\\nmodel.\\n2 2 9', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\n7.1ParameterNormPenalties\\nRegularizationhasbeenusedfordecadespriortotheadventofdeeplearning.Linear\\nmodelssuchaslinearregressionandlogisticregressionallowsimple,straightforward,\\nandeﬀectiveregularizationstrategies.\\nManyregularizationapproachesarebasedonlimitingthecapacityofmodels,\\nsuchasneuralnetworks,linearregression,orlogisticregression,byaddingapa-\\nrameternormpenalty Ω(θ)totheobjectivefunction J.Wedenotetheregularized\\nobjectivefunctionby˜ J:\\n˜ J , J , α (;θXy) = (;θXy)+Ω()θ (7.1)\\nwhere α∈[0 ,∞)isahyperparameter thatweightstherelativecontributionofthe\\nnormpenaltyterm,,relativetothestandardobjectivefunction Ω J.Setting αto0\\nresultsinnoregularization. Largervaluesof αcorrespondtomoreregularization.\\nWhenourtrainingalgorithmminimizestheregularizedobjectivefunction ˜ Jit\\nwilldecreaseboththeoriginalobjective Jonthetrainingdataandsomemeasure\\nofthesizeoftheparametersθ(orsomesubsetoftheparameters).Diﬀerent\\nchoicesfortheparameternormcanresultindiﬀerentsolutionsbeingpreferred. Ω\\nInthissection,wediscusstheeﬀectsofthevariousnormswhenusedaspenalties\\nonthemodelparameters.\\nBeforedelvingintotheregularizationbehaviorofdiﬀerentnorms,wenotethat\\nforneuralnetworks,wetypicallychoosetouseaparameternormpenaltythatΩ\\npenalizes oftheaﬃnetransformationateachlayerandleaves onlytheweights\\nthebiasesunregularized. Thebiasestypicallyrequirelessdatatoﬁtaccurately\\nthantheweights.\\xa0Eachweightspeciﬁeshowtwovariablesinteract.\\xa0Fittingthe\\nweightwellrequiresobservingbothvariablesinavarietyofconditions.Each\\nbiascontrolsonlyasinglevariable.Thismeansthatwedonotinducetoomuch\\nvariancebyleavingthebiasesunregularized. Also,regularizingthebiasparameters\\ncanintroduceasigniﬁcantamountofunderﬁtting. Wethereforeusethevectorw\\ntoindicatealloftheweightsthatshouldbeaﬀectedbyanormpenalty,whilethe\\nvectorθdenotesalloftheparameters,includingbothwandtheunregularized\\nparameters.\\nInthecontextofneuralnetworks,itissometimesdesirabletouseaseparate\\npenaltywithadiﬀerent αcoeﬃcientforeachlayerofthenetwork.Becauseitcan\\nbeexpensivetosearchforthecorrectvalueofmultiplehyperparameters,itisstill\\nreasonabletousethesameweightdecayatalllayersjusttoreducethesizeof\\nsearchspace.\\n2 3 0', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\n7 . 1 . 1 L2P a ra m et e r Regu l a ri z a t i o n\\nWehavealreadyseen,insection,oneofthesimplestandmostcommonkinds 5.2.2\\nofparameternormpenalty:the L2parameternormpenaltycommonlyknownas\\nweightdecay.Thisregularizationstrategydrivestheweightsclosertotheorigin1\\nbyaddingaregularizationtermΩ(θ) =1\\n2\\ue06b\\ue06bw2\\n2totheobjectivefunction.Inother\\nacademiccommunities, L2regularizationisalsoknownasridgeregressionor\\nTikhonovregularization.\\nWecangainsomeinsightintothebehaviorofweightdecayregularization\\nbystudyingthegradientoftheregularizedobjectivefunction.Tosimplifythe\\npresentation,weassumenobiasparameter,soθisjustw.Suchamodelhasthe\\nfollowingtotalobjectivefunction:\\n˜ J , (;wXy) =α\\n2w\\ue03ewwXy +( J; ,) , (7.2)\\nwiththecorrespondingparametergradient\\n∇ w˜ J , α (;wXy) = w+∇ w J , . (;wXy) (7.3)\\nTotakeasinglegradientsteptoupdatetheweights,weperformthisupdate:\\nwww ← − \\ue00f α( +∇ w J , . (;wXy)) (7.4)\\nWrittenanotherway,theupdateis:\\nww ← −(1 \\ue00f α)−∇ \\ue00f w J , . (;wXy) (7.5)\\nWecanseethattheadditionoftheweightdecaytermhasmodiﬁedthelearning\\nruletomultiplicativelyshrinktheweightvectorbyaconstantfactoroneachstep,\\njustbeforeperformingtheusualgradientupdate.Thisdescribeswhathappensin\\nasinglestep.Butwhathappensovertheentirecourseoftraining?\\nWewillfurthersimplifytheanalysisbymakingaquadraticapproximation\\ntotheobjectivefunctionintheneighborhoodofthevalueoftheweightsthat\\nobtainsminimalunregularized trainingcost,w∗=argminw J(w).Iftheobjective\\nfunctionistrulyquadratic,asinthecaseofﬁttingalinearregressionmodelwith\\n1M o re g e n e ra l l y , we c o u l d re g u l a riz e t h e p a ra m e t e rs t o b e n e a r a n y s p e c i ﬁ c p o i n t i n s p a c e\\na n d , s u rp ris i n g l y , s t i l l g e t a re g u l a riz a t i o n e ﬀ e c t , b u t b e t t e r re s u l t s will b e o b t a i n e d f o r a v a l u e\\nc l o s e r t o t h e t ru e o n e , with z e ro b e i n g a d e f a u l t v a l u e t h a t m a k e s s e n s e wh e n we d o n o t k n o w i f\\nt h e c o rre c t v a l u e s h o u l d b e p o s i t i v e o r n e g a t i v e . S i n c e i t i s f a r m o re c o m m o n t o re g u l a riz e t h e\\nm o d e l p a ra m e t e rs t o w a rd s z e ro , w e will f o c u s o n t h i s s p e c i a l c a s e i n o u r e x p o s i t i o n .\\n2 3 1', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nmeansquarederror,thentheapproximationisperfect.Theapproximation ˆ Jis\\ngivenby\\nˆ J J () = θ (w∗)+1\\n2(ww−∗)\\ue03eHww (−∗) , (7.6)\\nwhereHistheHessianmatrixof Jwithrespecttowevaluatedatw∗.Thereis\\nnoﬁrst-orderterminthisquadraticapproximation, becausew∗isdeﬁnedtobea\\nminimum,wherethegradientvanishes.Likewise,becausew∗isthelocationofa\\nminimumof,wecanconcludethatispositivesemideﬁnite. J H\\nTheminimumofˆ Joccurswhereitsgradient\\n∇ wˆ J() = (wHww−∗) (7.7)\\nisequalto. 0\\nTostudytheeﬀectofweightdecay,wemodifyequationbyaddingthe 7.7\\nweightdecaygradient.Wecannowsolvefortheminimumoftheregularized\\nversionofˆ J.Weusethevariable ˜wtorepresentthelocationoftheminimum.\\nα˜wH+ (˜ww−∗) = 0 (7.8)\\n(+ )H αI˜wHw = ∗(7.9)\\n˜wHI = (+ α)− 1Hw∗. (7.10)\\nAs αapproaches0,theregularizedsolution ˜wapproachesw∗.Butwhat\\nhappensas αgrows?BecauseHisrealandsymmetric,wecandecomposeit\\nintoadiagonalmatrix Λandanorthonormal basisofeigenvectors,Q,suchthat\\nHQQ = Λ\\ue03e.Applyingthedecompositiontoequation,weobtain:7.10\\n˜wQQ = ( Λ\\ue03e+ ) αI− 1QQ Λ\\ue03ew∗(7.11)\\n=\\ue068\\nQIQ (+ Λ α)\\ue03e\\ue069− 1\\nQQ Λ\\ue03ew∗(7.12)\\n= (+ )Q Λ αI− 1ΛQ\\ue03ew∗. (7.13)\\nWeseethattheeﬀectofweightdecayistorescalew∗alongtheaxesdeﬁnedby\\ntheeigenvectorsofH.Speciﬁcally,thecomponentofw∗thatisalignedwiththe\\ni-theigenvectorofHisrescaledbyafactorofλ i\\nλ i + α.(Youmaywishtoreview\\nhowthiskindofscalingworks,ﬁrstexplainedinﬁgure).2.3\\nAlongthedirectionswheretheeigenvaluesofHarerelativelylarge,forexample,\\nwhere λ i\\ue01d α,theeﬀectofregularizationisrelativelysmall.However,components\\nwith λ i\\ue01c αwillbeshrunktohavenearlyzeromagnitude.Thiseﬀectisillustrated\\ninﬁgure.7.1\\n2 3 2', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nw 1w 2w∗\\n˜ w\\nFigure7.1:Anillustrationoftheeﬀectof L2(orweightdecay)regularizationonthevalue\\noftheoptimalw.Thesolidellipsesrepresentcontoursofequalvalueoftheunregularized\\nobjective.Thedottedcirclesrepresentcontoursofequalvalueofthe L2regularizer.At\\nthepoint˜w,thesecompetingobjectivesreachanequilibrium.Intheﬁrstdimension,the\\neigenvalueoftheHessianof Jissmall.\\xa0Theobjectivefunctiondoesnotincreasemuch\\nwhenmovinghorizontallyawayfromw∗.Becausetheobjectivefunctiondoesnotexpress\\nastrongpreferencealongthisdirection,theregularizerhasastrongeﬀectonthisaxis.\\nTheregularizerpulls w1closetozero.Intheseconddimension,theobjectivefunction\\nisverysensitivetomovementsawayfromw∗.Thecorrespondingeigenvalueislarge,\\nindicatinghighcurvature.Asaresult,weightdecayaﬀectsthepositionof w2relatively\\nlittle.\\nOnlydirectionsalongwhichtheparameterscontributesigniﬁcantlytoreducing\\ntheobjectivefunctionarepreservedrelativelyintact.Indirectionsthatdonot\\ncontributetoreducingtheobjectivefunction,asmalleigenvalueoftheHessian\\ntellsusthatmovementinthisdirectionwillnotsigniﬁcantlyincreasethegradient.\\nComponentsoftheweightvectorcorrespondingtosuchunimportant directions\\naredecayedawaythroughtheuseoftheregularizationthroughouttraining.\\nSofarwehavediscussedweightdecayintermsofitseﬀectontheoptimization\\nofanabstract,general,quadraticcostfunction.Howdotheseeﬀectsrelateto\\nmachinelearninginparticular?Wecanﬁndoutbystudyinglinearregression,a\\nmodelforwhichthetruecostfunctionisquadraticandthereforeamenabletothe\\nsamekindofanalysiswehaveusedsofar.Applyingtheanalysisagain,wewill\\nbeabletoobtainaspecialcaseofthesameresults,butwiththesolutionnow\\nphrasedintermsofthetrainingdata.Forlinearregression,thecostfunctionis\\n2 3 3', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nthesumofsquarederrors:\\n( )Xwy−\\ue03e( )Xwy− . (7.14)\\nWhenweadd L2regularization, theobjectivefunctionchangesto\\n( )Xwy−\\ue03e( )+Xwy−1\\n2αw\\ue03ew . (7.15)\\nThischangesthenormalequationsforthesolutionfrom\\nwX= (\\ue03eX)− 1X\\ue03ey (7.16)\\nto\\nwX= (\\ue03eXI+ α)− 1X\\ue03ey . (7.17)\\nThematrixX\\ue03eXinequationisproportionaltothecovariancematrix 7.161\\nmX\\ue03eX.\\nUsing L2regularizationreplacesthismatrixwith\\ue000\\nX\\ue03eXI+ α\\ue001− 1inequation.7.17\\nThenewmatrixisthesameastheoriginalone,butwiththeadditionof αtothe\\ndiagonal.Thediagonalentriesofthismatrixcorrespondtothevarianceofeach\\ninputfeature.Wecanseethat L2regularizationcausesthelearningalgorithm\\nto“perceive”theinputXashavinghighervariance,whichmakesitshrinkthe\\nweightsonfeatureswhosecovariancewiththeoutputtargetislowcomparedto\\nthisaddedvariance.\\n7 . 1 . 2 L1Regu l a ri z a t i o n\\nWhile L2weightdecayisthemostcommonformofweightdecay,thereareother\\nwaystopenalizethesizeofthemodelparameters.\\xa0Anotheroptionistouse L1\\nregularization.\\nFormally, L1regularizationonthemodelparameter isdeﬁnedas:w\\nΩ() = θ ||||w 1=\\ue058\\ni| w i| , (7.18)\\nthatis,asthesumofabsolutevaluesoftheindividualparameters.2Wewill\\nnowdiscusstheeﬀectof L1regularizationonthesimplelinearregressionmodel,\\nwithnobiasparameter,thatwestudiedinouranalysisof L2regularization. In\\nparticular,weareinterestedindelineatingthediﬀerencesbetween L1and L2forms\\n2As with L2re g u l a riz a t i o n , w e c o u l d re g u l a riz e t h e p a ra m e t e rs t o w a rd s a v a l u e t h a t i s n o t\\nz e ro , b u t i n s t e a d t o wa rd s s o m e p a ra m e t e r v a l u e w( ) o. In t h a t c a s e t h e L1re g u l a riz a t i o n wo u l d\\ni n t ro d u c e t h e t e rmΩ() = θ ||− w w( ) o|| 1=\\ue050\\ni| w i− w( ) o\\ni| .\\n2 3 4', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nofregularization. Aswith L2weightdecay, L1weightdecaycontrolsthestrength\\noftheregularizationbyscalingthepenaltyusingapositivehyperparameter Ω α.\\nThus,theregularizedobjectivefunction ˜ J , (;wXy)isgivenby\\n˜ J , α (;wXy) = ||||w 1+(; ) JwXy , , (7.19)\\nwiththecorrespondinggradient(actually,sub-gradient):\\n∇ w˜ J , α (;wXy) = sign( )+w ∇ w J ,(Xyw;) (7.20)\\nwhere issimplythesignofappliedelement-wise. sign( )w w\\nByinspectingequation,wecanseeimmediately thattheeﬀectof 7.20 L1\\nregularizationisquitediﬀerentfromthatof L2regularization. Speciﬁcally,wecan\\nseethattheregularizationcontributiontothegradientnolongerscaleslinearly\\nwitheach w i;insteaditisaconstantfactorwithasignequaltosign( w i).One\\nconsequenceofthisformofthegradientisthatwewillnotnecessarilyseeclean\\nalgebraicsolutionstoquadraticapproximationsof J(Xy ,;w)aswedidfor L2\\nregularization.\\nOursimplelinearmodelhasaquadraticcostfunctionthatwecanrepresent\\nviaitsTaylorseries.Alternately,wecouldimaginethatthisisatruncatedTaylor\\nseriesapproximatingthecostfunctionofamoresophisticatedmodel.Thegradient\\ninthissettingisgivenby\\n∇ wˆ J() = (wHww−∗) , (7.21)\\nwhere,again,istheHessianmatrixofwithrespecttoevaluatedat H J ww∗.\\nBecausethe L1penaltydoesnotadmitcleanalgebraicexpressionsinthecase\\nofafullygeneralHessian,wewillalsomakethefurthersimplifyingassumption\\nthattheHessianisdiagonal,H=diag([ H 1 1 , , . . . , H n , n]),whereeach H i , i >0.\\nThisassumptionholdsifthedataforthelinearregressionproblemhasbeen\\npreprocessedtoremoveallcorrelationbetweentheinputfeatures,whichmaybe\\naccomplishedusingPCA.\\nOurquadraticapproximationofthe L1regularizedobjectivefunctiondecom-\\nposesintoasumovertheparameters:\\nˆ J , J (;wXy) = (w∗; )+Xy ,\\ue058\\ni\\ue0141\\n2H i , i(w i−w∗\\ni)2+ α w| i|\\ue015\\n.(7.22)\\nTheproblemofminimizingthisapproximatecostfunctionhasananalyticalsolution\\n(foreachdimension),withthefollowingform: i\\nw i= sign( w∗\\ni)max\\ue01a\\n| w∗\\ni|−α\\nH i , i,0\\ue01b\\n. (7.23)\\n2 3 5', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nConsiderthesituationwhere w∗\\ni > i 0forall.Therearetwopossibleoutcomes:\\n1.Thecasewhere w∗\\ni≤α\\nH i , i.Heretheoptimalvalueof w iundertheregularized\\nobjectiveissimply w i= 0.Thisoccursbecausethecontributionof J(w;Xy ,)\\ntotheregularizedobjective˜ J(w;Xy ,)isoverwhelmed—indirection i—by\\nthe L1regularizationwhichpushesthevalueof w itozero.\\n2.Thecasewhere w∗\\ni >α\\nH i , i.Inthiscase,theregularizationdoesnotmovethe\\noptimalvalueof w itozerobutinsteaditjustshiftsitinthatdirectionbya\\ndistanceequaltoα\\nH i , i.\\nAsimilarprocesshappenswhen w∗\\ni <0,butwiththe L1penaltymaking w iless\\nnegativebyα\\nH i , i,or0.\\nIncomparisonto L2regularization, L1regularizationresultsinasolutionthat\\nismoresparse.Sparsityinthiscontextreferstothefactthatsomeparameters\\nhaveanoptimalvalueofzero.Thesparsityof L1regularizationisaqualitatively\\ndiﬀerentbehaviorthanariseswith L2regularization. Equationgavethe7.13\\nsolution ˜ wfor L2regularization. Ifwerevisitthatequationusingtheassumption\\nofadiagonalandpositivedeﬁniteHessianHthatweintroducedforouranalysisof\\nL1regularization,weﬁndthat˜ w i=H i , i\\nH i , i + αw∗\\ni.If w∗\\niwasnonzero,then ˜ w iremains\\nnonzero.Thisdemonstratesthat L2regularizationdoesnotcausetheparameters\\ntobecomesparse,while L1regularizationmaydosoforlargeenough. α\\nThesparsitypropertyinducedby L1regularizationhasbeenusedextensively\\nasafeatureselectionmechanism.Featureselectionsimpliﬁesamachinelearning\\nproblembychoosingwhichsubsetoftheavailablefeaturesshouldbeused.In\\nparticular,thewellknownLASSO(,)(leastabsoluteshrinkageand Tibshirani1995\\nselectionoperator)modelintegratesan L1penaltywithalinearmodelandaleast\\nsquarescostfunction.The L1penaltycausesasubsetoftheweightstobecome\\nzero,suggestingthatthecorrespondingfeaturesmaysafelybediscarded.\\nInsection,wesawthatmanyregularizationstrategiescanbeinterpreted 5.6.1\\nasMAPBayesianinference,andthatinparticular, L2regularizationisequivalent\\ntoMAPBayesianinferencewithaGaussianpriorontheweights.\\xa0For L1regu-\\nlarization,thepenalty αΩ(w)= α\\ue050\\ni| w i|usedtoregularizeacostfunctionis\\nequivalenttothelog-priortermthatismaximizedbyMAPBayesianinference\\nwhenthepriorisanisotropicLaplacedistribution(equation)over3.26w∈ Rn:\\nlog() = pw\\ue058\\nilogLaplace( w i;0 ,1\\nα) = −|||| αw 1+log log2 n α n− .(7.24)\\n2 3 6', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nFromthepointofviewoflearningviamaximization withrespecttow,wecan\\nignorethe termsbecausetheydonotdependon. log log2 α− w\\n7.2NormPenaltiesasConstrainedOptimization\\nConsiderthecostfunctionregularizedbyaparameternormpenalty:\\n˜ J , J , α . (;θXy) = (;θXy)+Ω()θ (7.25)\\nRecallfromsectionthatwecanminimizeafunctionsubjecttoconstraints 4.4\\nbyconstructingageneralizedLagrangefunction,consistingoftheoriginalobjective\\nfunctionplusasetofpenalties.Eachpenaltyisaproductbetweenacoeﬃcient,\\ncalledaKarush–Kuhn–Tucker(KKT)multiplier,andafunctionrepresenting\\nwhethertheconstraintissatisﬁed.IfwewantedtoconstrainΩ(θ)tobelessthan\\nsomeconstant,wecouldconstructageneralizedLagrangefunction k\\nL − (; ) = (; )+(Ω() θ , αXy , JθXy , αθ k .) (7.26)\\nThesolutiontotheconstrainedproblemisgivenby\\nθ∗= argmin\\nθmax\\nα , α≥ 0L()θ , α . (7.27)\\nAsdescribedinsection,solvingthisproblemrequiresmodifyingboth 4.4 θ\\nand α.Sectionprovidesaworkedexampleoflinearregressionwithan 4.5 L2\\nconstraint.Manydiﬀerentproceduresarepossible—somemayusegradientdescent,\\nwhileothersmayuseanalyticalsolutionsforwherethegradientiszero—butinall\\nprocedures αmustincreasewheneverΩ(θ) > kanddecreasewheneverΩ(θ) < k.\\nAllpositive αencourage Ω(θ)toshrink.Theoptimalvalue α∗willencourage Ω(θ)\\ntoshrink,butnotsostronglytomakebecomelessthan. Ω()θ k\\nTogainsomeinsightintotheeﬀectoftheconstraint,wecanﬁx α∗andview\\ntheproblemasjustafunctionof:θ\\nθ∗= argmin\\nθL(θ , α∗) = argmin\\nθJ , α (;θXy)+∗Ω()θ .(7.28)\\nThisisexactlythesameastheregularizedtrainingproblemofminimizing ˜ J.\\nWecanthusthinkofaparameternormpenaltyasimposingaconstraintonthe\\nweights.Ifisthe Ω L2norm,thentheweightsareconstrainedtolieinan L2\\nball.\\xa0Ifisthe Ω L1norm,thentheweightsareconstrainedtolieinaregionof\\n2 3 7', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nlimited L1norm.Usuallywedonotknowthesizeoftheconstraintregionthatwe\\nimposebyusingweightdecaywithcoeﬃcient α∗becausethevalueof α∗doesnot\\ndirectlytellusthevalueof k.Inprinciple,onecansolvefor k,buttherelationship\\nbetween kand α∗dependsontheformof J.Whilewedonotknowtheexactsize\\noftheconstraintregion,wecancontrolitroughlybyincreasingordecreasing α\\ninordertogroworshrinktheconstraintregion.Larger αwillresultinasmaller\\nconstraintregion.Smallerwillresultinalargerconstraintregion. α\\nSometimeswemaywishtouseexplicitconstraintsratherthanpenalties.As\\ndescribedinsection,wecanmodifyalgorithmssuchasstochasticgradient 4.4\\ndescenttotakeastepdownhillon J(θ)andthenprojectθbacktothenearest\\npointthatsatisﬁesΩ(θ) < k.Thiscanbeusefulifwehaveanideaofwhatvalue\\nof kisappropriateanddonotwanttospendtimesearchingforthevalueof αthat\\ncorrespondstothis. k\\nAnotherreasontouseexplicitconstraintsandreprojectionratherthanenforcing\\nconstraintswithpenaltiesisthatpenaltiescancausenon-convexoptimization\\nprocedurestogetstuckinlocalminimacorrespondingtosmallθ.Whentraining\\nneuralnetworks,thisusuallymanifestsasneuralnetworksthattrainwithseveral\\n“deadunits.”Theseareunitsthatdonotcontributemuchtothebehaviorofthe\\nfunctionlearnedbythenetworkbecausetheweightsgoingintooroutofthemare\\nallverysmall.\\xa0Whentrainingwithapenaltyonthenormoftheweights,these\\nconﬁgurations canbelocallyoptimal,evenifitispossibletosigniﬁcantlyreduce\\nJbymakingtheweightslarger.Explicitconstraintsimplementedbyre-projection\\ncanworkmuchbetterinthesecasesbecausetheydonotencouragetheweights\\ntoapproachtheorigin.Explicitconstraintsimplemented byre-projectiononly\\nhaveaneﬀectwhentheweightsbecomelargeandattempttoleavetheconstraint\\nregion.\\nFinally,explicitconstraintswithreprojectioncanbeusefulbecausetheyimpose\\nsomestabilityontheoptimization procedure.Whenusinghighlearningrates,it\\nispossibletoencounterapositivefeedbackloopinwhichlargeweightsinduce\\nlargegradientswhichtheninducealargeupdatetotheweights.Iftheseupdates\\nconsistentlyincreasethesizeoftheweights,thenθrapidlymovesawayfrom\\ntheoriginuntilnumericaloverﬂowoccurs.Explicitconstraintswithreprojection\\npreventthisfeedbackloopfromcontinuingtoincreasethemagnitudeoftheweights\\nwithoutbound. ()recommendusingconstraintscombinedwith Hintonetal.2012c\\nahighlearningratetoallowrapidexplorationofparameterspacewhilemaintaining\\nsomestability.\\nInparticular,Hinton2012cetal.()recommendastrategyintroducedbySrebro\\nandShraibman2005():constrainingthenormofeachcolumnoftheweightmatrix\\n2 3 8', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nofaneuralnetlayer,ratherthanconstrainingtheFrobeniusnormoftheentire\\nweightmatrix.Constrainingthenormofeachcolumnseparatelypreventsanyone\\nhiddenunitfromhavingverylargeweights.Ifweconvertedthisconstraintintoa\\npenaltyinaLagrangefunction,itwouldbesimilarto L2weightdecaybutwitha\\nseparateKKTmultiplierfortheweightsofeachhiddenunit.EachoftheseKKT\\nmultiplierswouldbedynamicallyupdatedseparatelytomakeeachhiddenunit\\nobeytheconstraint.Inpractice,columnnormlimitationisalwaysimplementedas\\nanexplicitconstraintwithreprojection.\\n7.3RegularizationandUnder-ConstrainedProblems\\nInsomecases,regularizationisnecessaryformachinelearningproblemstobeprop-\\nerlydeﬁned.Manylinearmodelsinmachinelearning,includinglinearregression\\nandPCA,dependoninvertingthematrixX\\ue03eX.Thisisnotpossiblewhenever\\nX\\ue03eXissingular.Thismatrixcanbesingularwheneverthedatageneratingdistri-\\nbutiontrulyhasnovarianceinsomedirection,orwhennovarianceisobservedin\\nsomedirectionbecausetherearefewerexamples(rowsofX)thaninputfeatures\\n(columnsofX).Inthiscase,manyformsofregularizationcorrespondtoinverting\\nX\\ue03eXI+ αinstead.Thisregularizedmatrixisguaranteedtobeinvertible.\\nTheselinearproblemshaveclosedformsolutionswhentherelevantmatrix\\nisinvertible.Itisalsopossibleforaproblemwithnoclosedformsolutiontobe\\nunderdetermined. Anexampleislogisticregressionappliedtoaproblemwhere\\ntheclassesarelinearlyseparable.Ifaweightvectorwisabletoachieveperfect\\nclassiﬁcation,then2wwillalsoachieveperfectclassiﬁcationandhigherlikelihood.\\nAniterativeoptimization procedurelikestochasticgradientdescentwillcontinually\\nincreasethemagnitudeofwand,intheory,willneverhalt.Inpractice,anumerical\\nimplementationofgradientdescentwilleventuallyreachsuﬃcientlylargeweights\\ntocausenumericaloverﬂow,atwhichpointitsbehaviorwilldependonhowthe\\nprogrammerhasdecidedtohandlevaluesthatarenotrealnumbers.\\nMostformsofregularizationareabletoguaranteetheconvergenceofiterative\\nmethodsappliedtounderdetermined problems.\\xa0Forexample,weightdecaywill\\ncausegradientdescenttoquitincreasingthemagnitudeoftheweightswhenthe\\nslopeofthelikelihoodisequaltotheweightdecaycoeﬃcient.\\nTheideaofusingregularizationtosolveunderdetermined problemsextends\\nbeyondmachinelearning.Thesameideaisusefulforseveralbasiclinearalgebra\\nproblems.\\nAswesawinsection,wecansolveunderdetermined linearequationsusing 2.9\\n2 3 9', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\ntheMoore-Penrosepseudoinverse.Recallthatonedeﬁnitionofthepseudoinverse\\nX+ofamatrixisX\\nX+=lim\\nα\\ue026 0(X\\ue03eXI+ α)− 1X\\ue03e. (7.29)\\nWecannowrecognizeequationasperforminglinearregressionwithweight 7.29\\ndecay.Speciﬁcally,equationisthelimitofequationastheregularization 7.29 7.17\\ncoeﬃcientshrinkstozero.Wecanthusinterpretthepseudoinverseasstabilizing\\nunderdetermined problemsusingregularization.\\n7.4DatasetAugmentation\\nThebestwaytomakeamachinelearningmodelgeneralizebetteristotrainiton\\nmoredata.Ofcourse,inpractice,theamountofdatawehaveislimited.Oneway\\ntogetaroundthisproblemistocreatefakedataandaddittothetrainingset.\\nForsomemachinelearningtasks,itisreasonablystraightforwardtocreatenew\\nfakedata.\\nThisapproachiseasiestforclassiﬁcation.Aclassiﬁerneedstotakeacompli-\\ncated,highdimensionalinputxandsummarizeitwithasinglecategoryidentity y.\\nThismeansthatthemaintaskfacingaclassiﬁeristobeinvarianttoawidevariety\\noftransformations.Wecangeneratenew(x , y)pairseasilyjustbytransforming\\ntheinputsinourtrainingset. x\\nThisapproachisnotasreadilyapplicabletomanyothertasks.Forexample,it\\nisdiﬃculttogeneratenewfakedataforadensityestimationtaskunlesswehave\\nalreadysolvedthedensityestimationproblem.\\nDatasetaugmentationhasbeenaparticularlyeﬀectivetechniqueforaspeciﬁc\\nclassiﬁcationproblem:objectrecognition.Imagesarehighdimensionalandinclude\\nanenormousvarietyoffactorsofvariation,manyofwhichcanbeeasilysimulated.\\nOperationsliketranslatingthetrainingimagesafewpixelsineachdirectioncan\\noftengreatlyimprovegeneralization, evenifthemodelhasalreadybeendesignedto\\nbepartiallytranslationinvariantbyusingtheconvolutionandpoolingtechniques\\ndescribedinchapter.Manyotheroperationssuchasrotatingtheimageorscaling 9\\ntheimagehavealsoprovenquiteeﬀective.\\nOnemustbecarefulnottoapplytransformationsthatwouldchangethecorrect\\nclass.Forexample,opticalcharacterrecognitiontasksrequirerecognizingthe\\ndiﬀerencebetween‘b’and‘d’andthediﬀerencebetween‘6’and‘9’,sohorizontal\\nﬂipsand180◦rotationsarenotappropriatewaysofaugmentingdatasetsforthese\\ntasks.\\n2 4 0', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nTherearealsotransformationsthatwewouldlikeourclassiﬁerstobeinvariant\\nto,butwhicharenoteasytoperform.Forexample,out-of-planerotationcannot\\nbeimplementedasasimplegeometricoperationontheinputpixels.\\nDatasetaugmentationiseﬀectiveforspeechrecognitiontasksaswell(Jaitly\\nandHinton2013,).\\nInjectingnoiseintheinputtoaneuralnetwork(SietsmaandDow1991,)\\ncanalsobeseenasaformofdataaugmentation.Formanyclassiﬁcationand\\nevensomeregressiontasks,thetaskshouldstillbepossibletosolveevenifsmall\\nrandomnoiseisaddedtotheinput.Neuralnetworksprovenottobeveryrobust\\ntonoise,however(TangandEliasmith2010,).Onewaytoimprovetherobustness\\nofneuralnetworksissimplytotrainthemwithrandomnoiseappliedtotheir\\ninputs.Inputnoiseinjectionispartofsomeunsupervisedlearningalgorithmssuch\\nasthedenoisingautoencoder(Vincent2008etal.,).Noiseinjectionalsoworks\\nwhenthenoiseisappliedtothehiddenunits,whichcanbeseenasdoingdataset\\naugmentationatmultiplelevelsofabstraction.Poole2014etal.()recentlyshowed\\nthatthisapproachcanbehighlyeﬀectiveprovidedthatthemagnitudeofthe\\nnoiseiscarefullytuned.Dropout,apowerfulregularizationstrategythatwillbe\\ndescribedinsection,canbeseenasaprocessofconstructingnewinputsby 7.12\\nmultiplyingbynoise.\\nWhencomparingmachinelearningbenchmarkresults,itisimportanttotake\\ntheeﬀectofdatasetaugmentationintoaccount.Often,hand-designeddataset\\naugmentationschemescandramaticallyreducethegeneralization errorofamachine\\nlearningtechnique.Tocomparetheperformanceofonemachinelearningalgorithm\\ntoanother,itisnecessarytoperformcontrolledexperiments.Whencomparing\\nmachinelearningalgorithmAandmachinelearningalgorithmB,itisnecessary\\ntomakesurethatbothalgorithmswereevaluatedusingthesamehand-designed\\ndatasetaugmentationschemes.SupposethatalgorithmAperformspoorlywith\\nnodatasetaugmentationandalgorithmBperformswellwhencombinedwith\\nnumeroussynthetictransformationsoftheinput.Insuchacaseitislikelythe\\nsynthetictransformationscausedtheimprovedperformance,ratherthantheuse\\nofmachinelearningalgorithmB.Sometimesdecidingwhetheranexperiment\\nhasbeenproperlycontrolledrequiressubjectivejudgment.Forexample,machine\\nlearningalgorithmsthatinjectnoiseintotheinputareperformingaformofdataset\\naugmentation.Usually,operationsthataregenerallyapplicable(suchasadding\\nGaussiannoisetotheinput)areconsideredpartofthemachinelearningalgorithm,\\nwhileoperationsthatarespeciﬁctooneapplicationdomain(suchasrandomly\\ncroppinganimage)areconsideredtobeseparatepre-processingsteps.\\n2 4 1', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\n7.5NoiseRobustness\\nSectionhasmotivatedtheuseofnoiseappliedtotheinputsasadataset 7.4\\naugmentationstrategy.Forsomemodels,theadditionofnoisewithinﬁnitesimal\\nvarianceattheinputofthemodelisequivalenttoimposingapenaltyonthe\\nnormoftheweights(,,).Inthegeneralcase,itisimportantto Bishop1995ab\\nrememberthatnoiseinjectioncanbemuchmorepowerfulthansimplyshrinking\\ntheparameters,especiallywhenthenoiseisaddedtothehiddenunits.Noise\\nappliedtothehiddenunitsissuchanimportanttopicthatitmerititsownseparate\\ndiscussion;thedropoutalgorithmdescribedinsectionisthemaindevelopment 7.12\\nofthatapproach.\\nAnotherwaythatnoisehasbeenusedintheserviceofregularizingmodels\\nisbyaddingittotheweights.Thistechniquehasbeenusedprimarilyinthe\\ncontextofrecurrentneuralnetworks(,; Jimetal.1996Graves2011,).\\xa0Thiscan\\nbeinterpretedasa\\xa0stochasticimplementation of\\xa0Bayesianinference\\xa0overthe\\nweights.\\xa0TheBayesiantreatmentoflearningwouldconsiderthemodelweights\\ntobeuncertainandrepresentableviaaprobabilitydistributionthatreﬂectsthis\\nuncertainty.Addingnoisetotheweightsisapractical,stochasticwaytoreﬂect\\nthisuncertainty.\\nNoiseappliedtotheweightscanalsobeinterpretedasequivalent(undersome\\nassumptions)toamoretraditionalformofregularization, encouragingstabilityof\\nthefunctiontobelearned.Considertheregressionsetting,wherewewishtotrain\\nafunction ˆ y(x)thatmapsasetoffeaturesxtoascalarusingtheleast-squares\\ncostfunctionbetweenthemodelpredictions ˆ y()xandthetruevalues: y\\nJ= E p x , y ( )\\ue002(ˆ y y ()x−)2\\ue003\\n. (7.30)\\nThetrainingsetconsistsoflabeledexamples m {(x( 1 ), y( 1 )) ( , . . . ,x( ) m, y( ) m)}.\\nWenowassumethatwitheachinputpresentationwealsoincludearandom\\nperturbation \\ue00f W∼N(\\ue00f; 0 , ηI)ofthenetworkweights.Letusimaginethatwe\\nhaveastandard l-layerMLP.Wedenotetheperturbedmodelasˆ y \\ue00f W(x).Despite\\ntheinjectionofnoise,wearestillinterestedinminimizingthesquarederrorofthe\\noutputofthenetwork.Theobjectivefunctionthusbecomes:\\n˜ J W= E p , y , ( x \\ue00f W )\\ue068\\n(ˆ y \\ue00f W() )x− y2\\ue069\\n(7.31)\\n= E p , y , ( x \\ue00f W )\\ue002\\nˆ y2\\n\\ue00f W()2ˆx− y y \\ue00f W()+x y2\\ue003\\n.(7.32)\\nForsmall η,theminimization of Jwithaddedweightnoise(withcovariance\\nηI)isequivalenttominimization of Jwithanadditionalregularizationterm:\\n2 4 2', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nη E p , y ( x )\\ue002\\ue06b∇ Wˆ y()x\\ue06b2\\ue003\\n.Thisformofregularizationencouragestheparametersto\\ngotoregionsofparameterspacewheresmallperturbationsoftheweightshave\\narelativelysmallinﬂuenceontheoutput.Inotherwords,itpushesthemodel\\nintoregionswherethemodelisrelativelyinsensitivetosmallvariationsinthe\\nweights,ﬁndingpointsthatarenotmerelyminima,butminimasurroundedby\\nﬂatregions(HochreiterandSchmidhuber1995,).Inthesimpliﬁedcaseoflinear\\nregression(where,forinstance, ˆ y(x) =w\\ue03ex+ b),thisregularizationtermcollapses\\ninto η E p ( ) x\\ue002\\n\\ue06b\\ue06bx2\\ue003\\n,whichisnotafunctionofparametersandthereforedoesnot\\ncontributetothegradientof˜ J Wwithrespecttothemodelparameters.\\n7 . 5 . 1 In j ect i n g No i s e a t t h e O u t p u t T a rg et s\\nMostdatasetshavesomeamountofmistakesinthe ylabels.Itcanbeharmfulto\\nmaximize log p( y|x)when yisamistake.Onewaytopreventthisistoexplicitly\\nmodelthenoiseonthelabels.Forexample,wecanassumethatforsomesmall\\nconstant \\ue00f,thetrainingsetlabel yiscorrectwithprobability 1− \\ue00f,andotherwise\\nanyoftheotherpossiblelabelsmightbecorrect.Thisassumptioniseasyto\\nincorporateintothecostfunctionanalytically,ratherthanbyexplicitlydrawing\\nnoisesamples.Forexample,labelsmoothingregularizesamodelbasedona\\nsoftmaxwith koutputvaluesbyreplacingthehardandclassiﬁcationtargets 0 1\\nwithtargetsof\\ue00f\\nk− 1and1− \\ue00f,respectively.Thestandardcross-entropylossmay\\nthenbeusedwiththesesofttargets.Maximumlikelihoodlearningwithasoftmax\\nclassiﬁerandhardtargetsmayactuallyneverconverge—thesoftmaxcannever\\npredictaprobabilityofexactlyorexactly,soitwillcontinuetolearnlarger 0 1\\nandlargerweights,makingmoreextremepredictionsforever.Itispossibleto\\npreventthisscenariousingotherregularizationstrategieslikeweightdecay.Label\\nsmoothinghastheadvantageofpreventingthepursuitofhardprobabilitieswithout\\ndiscouragingcorrectclassiﬁcation.Thisstrategyhasbeenusedsincethe1980s\\nandcontinuestobefeaturedprominentlyinmodernneuralnetworks(Szegedy\\netal.,).2015\\n7.6Semi-SupervisedLearning\\nIntheparadigmofsemi-supervisedlearning,bothunlabeledexamplesfrom P( x)\\nandlabeledexamplesfrom P( x y ,)areusedtoestimate P( y x|)orpredict yfrom\\nx.\\nInthecontextofdeeplearning,semi-supervisedlearningusuallyrefersto\\nlearningarepresentationh= f(x) .Thegoalistolearnarepresentationso\\n2 4 3', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nthatexamplesfromthesameclasshavesimilarrepresentations.Unsupervised\\nlearningcanprovideusefulcuesforhowtogroupexamplesinrepresentation\\nspace.Examplesthatclustertightlyintheinputspaceshouldbemappedto\\nsimilarrepresentations.Alinearclassiﬁerinthenewspacemayachievebetter\\ngeneralization inmanycases(BelkinandNiyogi2002Chapelle2003 ,; etal.,).A\\nlong-standingvariantofthisapproachistheapplicationofprincipalcomponents\\nanalysisasapre-processingstepbeforeapplyingaclassiﬁer(ontheprojected\\ndata).\\nInsteadofhavingseparateunsupervisedandsupervisedcomponentsinthe\\nmodel,onecanconstructmodelsinwhichagenerativemodelofeither P( x)or\\nP( x y ,)sharesparameterswithadiscriminativemodelof P( y x|).Onecan\\nthentrade-oﬀthesupervisedcriterion −log P( y x|)withtheunsupervisedor\\ngenerativeone(suchas−log P( x)or−log P( x y ,)).Thegenerativecriterionthen\\nexpressesaparticularformofpriorbeliefaboutthesolutiontothesupervised\\nlearningproblem( ,),namelythatthestructureof Lasserreetal.2006 P( x)is\\nconnectedtothestructureof P( y x|)inawaythatiscapturedbytheshared\\nparametrization. Bycontrollinghowmuchofthegenerativecriterionisincluded\\ninthetotalcriterion,onecanﬁndabettertrade-oﬀthanwithapurelygenerative\\norapurelydiscriminativetrainingcriterion( ,; Lasserreetal.2006Larochelleand\\nBengio2008,).\\nSalakhutdinovandHinton2008()describeamethodforlearningthekernel\\nfunctionofakernelmachineusedforregression,inwhichtheusageofunlabeled\\nexamplesformodeling improvesquitesigniﬁcantly. P() x P( ) y x|\\nSee ()formoreinformationaboutsemi-supervisedlearning. Chapelle etal.2006\\n7.7Multi-TaskLearning\\nMulti-tasklearning(,)isawaytoimprovegeneralization bypooling Caruana1993\\ntheexamples(whichcanbeseenassoftconstraintsimposedontheparameters)\\narisingoutofseveraltasks.\\xa0Inthesamewaythatadditionaltrainingexamples\\nputmorepressureontheparametersofthemodeltowardsvaluesthatgeneralize\\nwell,whenpartofamodelissharedacrosstasks,thatpartofthemodelismore\\nconstrainedtowardsgoodvalues(assumingthesharingisjustiﬁed),oftenyielding\\nbettergeneralization.\\nFigureillustratesaverycommonformofmulti-tasklearning,inwhich 7.2\\ndiﬀerentsupervisedtasks(predicting y( ) igiven x)sharethesameinput x,aswell\\nassomeintermediate-lev elrepresentationh( s ha r e d)capturingacommonpoolof\\n2 4 4', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nfactors.Themodelcangenerallybedividedintotwokindsofpartsandassociated\\nparameters:\\n1.Task-speciﬁcparameters(whichonlybeneﬁtfromtheexamplesoftheirtask\\ntoachievegoodgeneralization). Thesearetheupperlayersoftheneural\\nnetworkinﬁgure.7.2\\n2.Genericparameters,sharedacrossallthetasks(whichbeneﬁtfromthe\\npooleddataofallthetasks).Thesearethelowerlayersoftheneuralnetwork\\ninﬁgure.7.2\\nh( 1 )h( 1 )h( 2 )h( 2 )h( 3 )h( 3 )y( 1 )y( 1 )y( 2 )y( 2 )\\nh( s h a r e d )h( s h a r e d )\\nxx\\nFigure7.2:Multi-tasklearningcanbecastinseveralwaysindeeplearningframeworks\\nandthisﬁgureillustratesthecommonsituationwherethetasksshareacommoninputbut\\ninvolvediﬀerenttargetrandomvariables.Thelowerlayersofadeepnetwork(whetherit\\nissupervisedandfeedforwardorincludesagenerativecomponentwithdownwardarrows)\\ncanbesharedacrosssuchtasks,whiletask-speciﬁcparameters(associatedrespectively\\nwiththeweightsintoandfromh(1)andh(2))canbelearnedontopofthoseyieldinga\\nsharedrepresentationh(shared).Theunderlyingassumptionisthatthereexistsacommon\\npooloffactorsthatexplainthevariationsintheinput x,whileeachtaskisassociated\\nwithasubsetofthesefactors.Inthisexample,itisadditionallyassumedthattop-level\\nhiddenunitsh(1)andh(2)arespecializedtoeachtask(respectivelypredicting y(1)and\\ny(2))whilesomeintermediate-levelrepresentationh(shared)issharedacrossalltasks.In\\ntheunsupervisedlearningcontext,itmakessenseforsomeofthetop-levelfactorstobe\\nassociatedwithnoneoftheoutputtasks(h(3)):thesearethefactorsthatexplainsomeof\\ntheinputvariationsbutarenotrelevantforpredicting y(1)or y(2).\\nImprovedgeneralization andgeneralization errorbounds(,)canbe Baxter1995\\nachievedbecauseofthesharedparameters,forwhichstatisticalstrengthcanbe\\n2 4 5', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\n0 50 100 150 200 250\\nTime(epochs)000 .005 .010 .015 .020 .Loss(negative log-likelihood)T r a i n i n g s e t l o s s\\nV a l i d a t i o n s e t l o s s\\nFigure7.3:Learningcurvesshowinghowthenegativelog-likelihoodlosschangesover\\ntime(indicatedasnumberoftrainingiterationsoverthedataset,or e p o c h s).Inthis\\nexample,wetrainamaxoutnetworkonMNIST.Observethatthetrainingobjective\\ndecreasesconsistentlyovertime,butthevalidationsetaveragelosseventuallybeginsto\\nincreaseagain,forminganasymmetricU-shapedcurve.\\ngreatlyimproved(inproportionwiththeincreasednumberofexamplesforthe\\nsharedparameters,comparedtothescenarioofsingle-taskmodels).Ofcoursethis\\nwillhappenonlyifsomeassumptionsaboutthestatisticalrelationshipbetween\\nthediﬀerenttasksarevalid,meaningthatthereissomethingsharedacrosssome\\nofthetasks.\\nFromthepointofviewofdeeplearning,theunderlyingpriorbeliefisthe\\nfollowing:amongthefactorsthat\\xa0explainthevariations\\xa0observed\\xa0inthedata\\nassociatedwiththediﬀerenttasks,somearesharedacrosstwoormoretasks.\\n7.8EarlyStopping\\nWhentraininglargemodelswithsuﬃcientrepresentationalcapacitytooverﬁt\\nthetask,weoftenobservethattrainingerrordecreasessteadilyovertime,but\\nvalidationseterrorbeginstoriseagain.Seeﬁgureforanexampleofthis 7.3\\nbehavior.Thisbehavioroccursveryreliably.\\nThismeanswecanobtainamodelwithbettervalidationseterror(andthus,\\nhopefullybettertestseterror)byreturningtotheparametersettingatthepointin\\ntimewiththelowestvalidationseterror.Everytimetheerroronthevalidationset\\nimproves,westoreacopyofthemodelparameters.Whenthetrainingalgorithm\\nterminates,wereturntheseparameters,ratherthanthelatestparameters.The\\n2 4 6', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nalgorithmterminateswhennoparametershaveimprovedoverthebestrecorded\\nvalidationerrorforsomepre-speciﬁednumberofiterations.Thisprocedureis\\nspeciﬁedmoreformallyinalgorithm .7.1\\nAlgorithm\\xa07.1Theearlystopping\\xa0meta-algorithmfor\\xa0determiningthe\\xa0best\\namountoftimetotrain.Thismeta-algorithm isageneralstrategythatworks\\nwellwithavarietyoftrainingalgorithmsandwaysofquantifyingerroronthe\\nvalidationset.\\nLetbethenumberofstepsbetweenevaluations. n\\nLet pbethe“patience,”thenumberoftimestoobserveworseningvalidationset\\nerrorbeforegivingup.\\nLetθ obetheinitialparameters.\\nθθ← o\\ni←0\\nj←0\\nv←∞\\nθ∗←θ\\ni∗← i\\nwhiledo j < p\\nUpdatebyrunningthetrainingalgorithmforsteps. θ n\\ni i n ←+\\nv\\ue030←ValidationSetError ()θ\\nif v\\ue030< vthen\\nj←0\\nθ∗←θ\\ni∗← i\\nv v←\\ue030\\nelse\\nj j←+1\\nendif\\nendwhile\\nBestparametersareθ∗,bestnumberoftrainingstepsis i∗\\nThisstrategyisknownasearlystopping.Itisprobablythemostcommonly\\nusedformofregularizationindeeplearning.Itspopularityisduebothtoits\\neﬀectivenessanditssimplicity.\\nOnewaytothinkofearlystoppingisasaveryeﬃcienthyperparameter selection\\nalgorithm.Inthisview,thenumberoftrainingstepsisjustanotherhyperparameter.\\nWecanseeinﬁgurethatthishyperparameter hasaU-shapedvalidationset 7.3\\n2 4 7', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nperformancecurve.Mosthyperparameters thatcontrolmodelcapacityhavesucha\\nU-shapedvalidationsetperformancecurve,asillustratedinﬁgure.Inthecaseof 5.3\\nearlystopping,wearecontrollingtheeﬀectivecapacityofthemodelbydetermining\\nhowmanystepsitcantaketoﬁtthetrainingset.Mosthyperparametersmustbe\\nchosenusinganexpensiveguessandcheckprocess,wherewesetahyperparameter\\natthestartoftraining,thenruntrainingforseveralstepstoseeitseﬀect.The\\n“trainingtime”\\xa0hyperparam eterisuniqueinthatbydeﬁnitionasinglerunof\\ntrainingtriesoutmanyvaluesofthehyperparameter.Theonlysigniﬁcantcost\\ntochoosingthishyperparameter automatically viaearlystoppingisrunningthe\\nvalidationsetevaluationperiodicallyduringtraining.Ideally,thisisdonein\\nparalleltothetrainingprocessonaseparatemachine,separateCPU,orseparate\\nGPUfromthemaintrainingprocess.Ifsuchresourcesarenotavailable,thenthe\\ncostoftheseperiodicevaluationsmaybereducedbyusingavalidationsetthatis\\nsmallcomparedtothetrainingsetorbyevaluatingthevalidationseterrorless\\nfrequentlyandobtainingalowerresolutionestimateoftheoptimaltrainingtime.\\nAnadditionalcosttoearlystoppingistheneedtomaintainacopyofthe\\nbestparameters.Thiscostisgenerallynegligible,becauseitisacceptabletostore\\ntheseparametersinaslowerandlargerformofmemory(forexample,trainingin\\nGPUmemory,butstoringtheoptimalparametersinhostmemoryoronadisk\\ndrive).Sincethebestparametersarewrittentoinfrequentlyandneverreadduring\\ntraining,theseoccasionalslowwriteshavelittleeﬀectonthetotaltrainingtime.\\nEarlystoppingisaveryunobtrusiveformofregularization, inthatitrequires\\nalmostnochangeintheunderlyingtrainingprocedure,theobjectivefunction,\\northesetofallowableparametervalues.Thismeansthatitiseasytouseearly\\nstoppingwithoutdamagingthelearningdynamics.Thisisincontrasttoweight\\ndecay,whereonemustbecarefulnottousetoomuchweightdecayandtrapthe\\nnetworkinabadlocalminimumcorrespondingtoasolutionwithpathologically\\nsmallweights.\\nEarlystoppingmaybeusedeitheraloneorinconjunctionwithotherregulariza-\\ntionstrategies.Evenwhenusingregularizationstrategiesthatmodifytheobjective\\nfunctiontoencouragebettergeneralization, itisrareforthebestgeneralization to\\noccuratalocalminimumofthetrainingobjective.\\nEarlystoppingrequiresavalidationset,whichmeanssometrainingdataisnot\\nfedtothemodel.Tobestexploitthisextradata,onecanperformextratraining\\naftertheinitialtrainingwithearlystoppinghascompleted.Inthesecond,extra\\ntrainingstep,allofthetrainingdataisincluded.Therearetwobasicstrategies\\nonecanuseforthissecondtrainingprocedure.\\nOnestrategy(algorithm )istoinitializethemodelagainandretrainonall 7.2\\n2 4 8', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nofthedata.Inthissecondtrainingpass,wetrainforthesamenumberofstepsas\\ntheearlystoppingproceduredeterminedwasoptimalintheﬁrstpass.Thereare\\nsomesubtletiesassociatedwiththisprocedure.Forexample,thereisnotagood\\nwayofknowingwhethertoretrainforthesamenumberofparameterupdatesor\\nthesamenumberofpassesthroughthedataset.Onthesecondroundoftraining,\\neachpassthroughthedatasetwillrequiremoreparameterupdatesbecausethe\\ntrainingsetisbigger.\\nAlgorithm7.2Ameta-algorithm forusingearlystoppingtodeterminehowlong\\ntotrain,thenretrainingonallthedata.\\nLetX( ) t r a i nandy( ) t r a i nbethetrainingset.\\nSplitX( ) t r a i nandy( ) t r a i ninto(X( ) s ubtr a i n,X( v a l i d )) (andy( ) s ubtr a i n,y( v a l i d ))\\nrespectively.\\nRunearlystopping(algorithm )startingfromrandom 7.1 θusingX( ) s ubtr a i nand\\ny( ) s ubtr a i nfortrainingdataandX( v a l i d )andy( v a l i d )forvalidationdata.This\\nreturns i∗,theoptimalnumberofsteps.\\nSettorandomvaluesagain. θ\\nTrainonX( ) t r a i nandy( ) t r a i nfor i∗steps.\\nAnotherstrategyforusingallofthedataistokeeptheparametersobtained\\nfromtheﬁrstroundoftrainingandthencontinuetrainingbutnowusingallof\\nthedata.Atthisstage,wenownolongerhaveaguideforwhentostopinterms\\nofanumberofsteps.\\xa0Instead,wecanmonitortheaveragelossfunctiononthe\\nvalidationset,andcontinuetraininguntilitfallsbelowthevalueofthetraining\\nsetobjectiveatwhichtheearlystoppingprocedurehalted.Thisstrategyavoids\\nthehighcostofretrainingthemodelfromscratch,butisnotaswell-behaved.For\\nexample,thereisnotanyguaranteethattheobjectiveonthevalidationsetwill\\neverreachthetargetvalue,sothisstrategyisnotevenguaranteedtoterminate.\\nThisprocedureispresentedmoreformallyinalgorithm .7.3\\nEarlystoppingisalsousefulbecauseitreducesthecomputational costofthe\\ntrainingprocedure.Besidestheobviousreductionincostduetolimitingthenumber\\noftrainingiterations,italsohasthebeneﬁtofprovidingregularizationwithout\\nrequiringtheadditionofpenaltytermstothecostfunctionorthecomputationof\\nthegradientsofsuchadditionalterms.\\nHowearlystoppingactsasaregularizer:Sofarwehavestatedthatearly\\nstoppingaregularizationstrategy,butwehavesupportedthisclaimonlyby is\\nshowinglearningcurveswherethevalidationseterrorhasaU-shapedcurve.What\\n2 4 9', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nAlgorithm7.3Meta-algorithm usingearlystoppingtodetermineatwhatobjec-\\ntivevaluewestarttooverﬁt,thencontinuetraininguntilthatvalueisreached.\\nLetX( ) t r a i nandy( ) t r a i nbethetrainingset.\\nSplitX( ) t r a i nandy( ) t r a i ninto(X( ) s ubtr a i n,X( v a l i d )) (andy( ) s ubtr a i n,y( v a l i d ))\\nrespectively.\\nRunearlystopping(algorithm )startingfromrandom 7.1 θusingX( ) s ubtr a i nand\\ny( ) s ubtr a i nfortrainingdataandX( v a l i d )andy( v a l i d )forvalidationdata.This\\nupdates.θ\\n\\ue00f J , ←(θX( ) s ubtr a i n,y( ) s ubtr a i n)\\nwhile J ,(θX( v a l i d ),y( v a l i d )) > \\ue00fdo\\nTrainonX( ) t r a i nandy( ) t r a i nforsteps. n\\nendwhile\\nistheactualmechanismbywhichearlystoppingregularizesthemodel?Bishop\\n()and ()arguedthatearlystoppinghastheeﬀectof 1995aSjöbergandLjung1995\\nrestrictingtheoptimization proceduretoarelativelysmallvolumeofparameter\\nspaceintheneighborhoodoftheinitialparametervalueθ o,asillustratedin\\nﬁgure.Morespeciﬁcally,imaginetaking 7.4 τoptimization steps(corresponding\\nto τtrainingiterations)andwithlearningrate \\ue00f.Wecanviewtheproduct \\ue00f τ\\nasameasureofeﬀectivecapacity.Assumingthegradientisbounded,restricting\\nboththenumberofiterationsandthelearningratelimitsthevolumeofparameter\\nspacereachablefromθ o.Inthissense, \\ue00f τbehavesasifitwerethereciprocalof\\nthecoeﬃcientusedforweightdecay.\\nIndeed,wecanshowhow—inthecaseofasimplelinearmodelwithaquadratic\\nerrorfunctionandsimplegradientdescent—earlystoppingisequivalentto L2\\nregularization.\\nInordertocomparewithclassical L2regularization, weexamineasimple\\nsettingwheretheonlyparametersarelinearweights(θ=w).Wecanmodel\\nthecostfunction Jwithaquadraticapproximationintheneighborhoodofthe\\nempiricallyoptimalvalueoftheweightsw∗:\\nˆ J J () = θ (w∗)+1\\n2(ww−∗)\\ue03eHww (−∗) , (7.33)\\nwhereHistheHessianmatrixof Jwithrespecttowevaluatedatw∗.Giventhe\\nassumptionthatw∗isaminimumof J(w),weknowthatHispositivesemideﬁnite.\\nUnderalocalTaylorseriesapproximation,thegradientisgivenby:\\n∇ wˆ J() = (wHww−∗) . (7.34)\\n2 5 0', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nw 1w 2w∗\\n˜ w\\nw 1w 2w∗\\n˜ w\\nFigure7.4:Anillustrationoftheeﬀectofearlystopping. ( L e f t )Thesolidcontourlines\\nindicatethecontoursofthenegativelog-likelihood.Thedashedlineindicatesthetrajectory\\ntakenbySGDbeginningfromtheorigin.Ratherthanstoppingatthepointw∗that\\nminimizesthecost,earlystoppingresultsinthetrajectorystoppingatanearlierpoint˜w.\\n( R i g h t )Anillustrationoftheeﬀectof L2regularizationforcomparison.Thedashedcircles\\nindicatethecontoursofthe L2penalty,whichcausestheminimumofthetotalcosttolie\\nnearertheoriginthantheminimumoftheunregularizedcost.\\nWearegoingtostudythetrajectoryfollowedbytheparametervectorduring\\ntraining.Forsimplicity,letussettheinitialparametervectortotheorigin,3that\\nisw( 0 )= 0.Letusstudytheapproximatebehaviorofgradientdescenton Jby\\nanalyzinggradientdescentonˆ J:\\nw( ) τ= w( 1 ) τ−−∇ \\ue00f wˆ J(w( 1 ) τ−) (7.35)\\n= w( 1 ) τ−− \\ue00fHw(( 1 ) τ−−w∗) (7.36)\\nw( ) τ−w∗= ( )(IH− \\ue00fw( 1 ) τ−−w∗) . (7.37)\\nLetusnowrewritethisexpressioninthespaceoftheeigenvectorsofH,exploiting\\ntheeigendecompositionofH:H=QQ Λ\\ue03e,where ΛisadiagonalmatrixandQ\\nisanorthonormalbasisofeigenvectors.\\nw( ) τ−w∗= (IQQ − \\ue00f Λ\\ue03e)(w( 1 ) τ−−w∗)(7.38)\\nQ\\ue03e(w( ) τ−w∗) = ( )I− \\ue00f ΛQ\\ue03e(w( 1 ) τ−−w∗) (7.39)\\n3F o r n e u ra l n e t w o rk s , t o o b t a i n s y m m e t ry b re a k i n g b e t w e e n h i d d e n u n i t s , w e c a n n o t i n i t i a l i z e\\na l l t h e p a ra m e t e rs t o 0 , a s d i s c u s s e d i n s e c t i o n . Ho w e v e r, t h e a rg u m e n t h o l d s f o r a n y o t h e r 6 . 2\\ni n i t i a l v a l u e w( 0 ).\\n2 5 1', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nAssumingthatw( 0 )=0andthat \\ue00fischosentobesmallenoughtoguarantee\\n|1− \\ue00f λ i| <1,theparametertrajectoryduringtrainingafter τparameterupdates\\nisasfollows:\\nQ\\ue03ew( ) τ= [ ( )I−I− \\ue00f Λτ]Q\\ue03ew∗. (7.40)\\nNow,theexpressionforQ\\ue03e˜winequationfor7.13 L2regularizationcanberear-\\nrangedas:\\nQ\\ue03e˜wI = (+ Λ α)− 1ΛQ\\ue03ew∗(7.41)\\nQ\\ue03e˜wII = [−(+ Λ α)− 1α]Q\\ue03ew∗(7.42)\\nComparingequationandequation,weseethatifthehyperparameters 7.40 7.42 \\ue00f,\\nα τ,andarechosensuchthat\\n( )I− \\ue00f Λτ= (+ ) Λ αI− 1α , (7.43)\\nthen L2regularizationandearlystoppingcanbeseentobeequivalent(atleast\\nunderthequadraticapproximation oftheobjectivefunction).Goingevenfurther,\\nbytakinglogarithmsandusingtheseriesexpansionforlog(1+ x),wecanconclude\\nthatifall λ iaresmall(thatis, \\ue00f λ i\\ue01c1and λ i /α\\ue01c1)then\\nτ≈1\\n\\ue00f α, (7.44)\\nα≈1\\nτ \\ue00f. (7.45)\\nThatis,undertheseassumptions,thenumberoftrainingiterations τplaysarole\\ninverselyproportionaltothe L2regularizationparameter,andtheinverseof τ \\ue00f\\nplaystheroleoftheweightdecaycoeﬃcient.\\nParametervaluescorrespondingtodirectionsofsigniﬁcantcurvature(ofthe\\nobjectivefunction)areregularizedlessthandirectionsoflesscurvature.Ofcourse,\\ninthecontextofearlystopping,thisreallymeansthatparametersthatcorrespond\\ntodirectionsofsigniﬁcantcurvaturetendtolearnearlyrelativetoparameters\\ncorrespondingtodirectionsoflesscurvature.\\nThederivationsinthissectionhaveshownthatatrajectoryoflength τends\\natapointthatcorrespondstoaminimumofthe L2-regularizedobjective.Early\\nstoppingisofcoursemorethanthemererestrictionofthetrajectorylength;\\ninstead,earlystoppingtypicallyinvolvesmonitoringthevalidationseterrorin\\nordertostopthetrajectoryataparticularlygoodpointinspace.Earlystopping\\nthereforehastheadvantageoverweightdecaythatearlystoppingautomatically\\ndeterminesthecorrectamountofregularizationwhileweightdecayrequiresmany\\ntrainingexperimentswithdiﬀerentvaluesofitshyperparameter.\\n2 5 2', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\n7.9ParameterTyingandParameterSharing\\nThusfar,inthischapter,whenwehavediscussedaddingconstraintsorpenalties\\ntotheparameters,wehavealwaysdonesowithrespecttoaﬁxedregionorpoint.\\nForexample, L2regularization(orweightdecay)penalizesmodelparametersfor\\ndeviatingfromtheﬁxedvalueofzero.However,sometimeswemayneedother\\nwaystoexpressourpriorknowledgeaboutsuitablevaluesofthemodelparameters.\\nSometimeswemightnotknowpreciselywhatvaluestheparametersshouldtake\\nbutweknow,fromknowledgeofthedomainandmodelarchitecture, thatthere\\nshouldbesomedependencies betweenthemodelparameters.\\nAcommontypeofdependencythatweoftenwanttoexpressisthatcertain\\nparametersshouldbeclosetooneanother.Considerthefollowingscenario:we\\nhavetwomodelsperformingthesameclassiﬁcationtask(withthesamesetof\\nclasses)butwithsomewhatdiﬀerentinputdistributions.Formally,wehavemodel\\nAwithparametersw( ) Aandmodel Bwithparametersw( ) B.Thetwomodels\\nmaptheinput\\xa0totwo\\xa0diﬀerent,\\xa0but\\xa0related outputs:ˆ y( ) A= f(w( ) A,x)and\\nˆ y( ) B= ( gw( ) B,x).\\nLetusimaginethatthetasksaresimilarenough(perhapswithsimilarinput\\nandoutputdistributions)thatwebelievethemodelparametersshouldbeclose\\ntoeachother: ∀ i, w( ) A\\nishouldbecloseto w( ) B\\ni.Wecanleveragethisinformation\\nthroughregularization. Speciﬁcally,wecanuseaparameternormpenaltyofthe\\nform: Ω(w( ) A,w( ) B)=\\ue06bw( ) A−w( ) B\\ue06b2\\n2.\\xa0Hereweusedan L2penalty,butother\\nchoicesarealsopossible.\\nThiskindofapproachwasproposedby (),whoregularized Lasserreetal.2006\\ntheparametersofonemodel,trainedasaclassiﬁerinasupervisedparadigm,to\\nbeclosetotheparametersofanothermodel,trainedinanunsupervisedparadigm\\n(tocapturethedistributionoftheobservedinputdata).Thearchitectures were\\nconstructedsuchthatmanyoftheparametersintheclassiﬁermodelcouldbe\\npairedtocorrespondingparametersintheunsupervisedmodel.\\nWhileaparameternormpenaltyisonewaytoregularizeparameterstobe\\nclosetooneanother,themorepopularwayistouseconstraints:toforcesets\\nofparameterstobeequal.Thismethodofregularizationisoftenreferredtoas\\nparametersharing,becauseweinterpretthevariousmodelsormodelcomponents\\nassharingauniquesetofparameters.Asigniﬁcantadvantageofparametersharing\\noverregularizingtheparameterstobeclose(viaanormpenalty)isthatonlya\\nsubsetoftheparameters(theuniqueset)needtobestoredinmemory.Incertain\\nmodels—suchastheconvolutionalneuralnetwork—thiscanleadtosigniﬁcant\\nreductioninthememoryfootprintofthemodel.\\n2 5 3', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nConvolutionalNeuralNetworksByfarthemostpopularandextensiveuse\\nofparametersharingoccursinconvolutionalneuralnetworks(CNNs)applied\\ntocomputervision.\\nNaturalimageshavemanystatisticalpropertiesthatareinvarianttotranslation.\\nForexample,aphotoofacatremainsaphotoofacatifitistranslatedonepixel\\ntotheright.CNNstakethispropertyintoaccountbysharingparametersacross\\nmultipleimagelocations.Thesamefeature(ahiddenunitwiththesameweights)\\niscomputedoverdiﬀerentlocationsintheinput.Thismeansthatwecanﬁnda\\ncatwiththesamecatdetectorwhetherthecatappearsatcolumn iorcolumn\\ni+1intheimage.\\nParametersharinghasallowedCNNstodramaticallylowerthenumberofunique\\nmodelparametersandtosigniﬁcantlyincreasenetworksizeswithoutrequiringa\\ncorrespondingincreaseintrainingdata.\\xa0Itremainsoneofthebestexamplesof\\nhowtoeﬀectivelyincorporatedomainknowledgeintothenetworkarchitecture.\\nCNNswillbediscussedinmoredetailinchapter.9\\n7.10SparseRepresentations\\nWeightdecayactsbyplacingapenaltydirectlyonthemodelparameters.Another\\nstrategyistoplaceapenaltyontheactivationsoftheunitsinaneuralnetwork,\\nencouragingtheiractivationstobesparse.Thisindirectlyimposesacomplicated\\npenaltyonthemodelparameters.\\nWehave\\xa0alreadydiscussed\\xa0(insection)how7.1.2 L1penalizationinduces\\nasparseparametrization—meaning thatmanyoftheparametersbecomezero\\n(orcloseto\\xa0zero).Representationalsparsity,\\xa0on\\xa0theother\\xa0hand,\\xa0des cribesa\\nrepresentationwheremanyoftheelementsoftherepresentationarezero(orclose\\ntozero).Asimpliﬁedviewofthisdistinctioncanbeillustratedinthecontextof\\nlinearregression:\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f018\\n5\\n15\\n−9\\n−3\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0400 20 0 −\\n00 10 3 0 −\\n050 0 0 0\\n100 10 4 − −\\n100 0 50 −\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f02\\n3\\n−2\\n−5\\n1\\n4\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\ny∈ RmA∈ Rm n×x∈ Rn(7.46)\\n2 5 4', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0−14\\n1\\n19\\n2\\n23\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f03 12 54 1 − −\\n4 2 3 11 3 − −\\n− − − 15 4 2 3 2\\n3 1 2 30 3 − −\\n− − − − 54 22 5 1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f00\\n2\\n0\\n0\\n−3\\n0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\ny∈ RmB∈ Rm n×h∈ Rn(7.47)\\nIntheﬁrstexpression,wehaveanexampleofasparselyparametrized linear\\nregressionmodel.Inthesecond,wehavelinearregressionwithasparserepresenta-\\ntionhofthedatax.Thatis,hisafunctionofxthat,insomesense,represents\\ntheinformationpresentin,butdoessowithasparsevector. x\\nRepresentationalregularizationisaccomplishedbythesamesortsofmechanisms\\nthatwehaveusedinparameterregularization.\\nNormpenaltyregularizationofrepresentationsisperformedbyaddingtothe\\nlossfunction Janormpenaltyontherepresentation.Thispenaltyisdenoted\\nΩ()h.Asbefore,wedenotetheregularizedlossfunctionby˜ J:\\n˜ J , J , α (;θXy) = (;θXy)+Ω()h (7.48)\\nwhere α∈[0 ,∞)weightstherelativecontributionofthenormpenaltyterm,with\\nlargervaluesofcorrespondingtomoreregularization. α\\nJustasan L1penaltyontheparametersinducesparametersparsity,an L1\\npenaltyontheelementsoftherepresentationinducesrepresentationalsparsity:\\nΩ(h) =||||h 1=\\ue050\\ni| h i|.\\xa0Ofcourse,the L1penaltyisonlyonechoiceofpenalty\\nthatcanresultinasparserepresentation.Othersincludethepenaltyderivedfrom\\naStudent- tpriorontherepresentation( ,;,) OlshausenandField1996Bergstra2011\\nandKLdivergencepenalties( ,)thatareespecially LarochelleandBengio2008\\nusefulforrepresentationswithelementsconstrainedtolieontheunitinterval.\\nLee2008Goodfellow 2009 etal.()and etal.()bothprovideexamplesofstrategies\\nbasedonregularizingtheaverageactivationacrossseveralexamples,1\\nm\\ue050\\nih( ) i,to\\nbenearsometargetvalue,suchasavectorwith.01foreachentry.\\nOtherapproachesobtainrepresentationalsparsitywithahardconstrainton\\ntheactivationvalues.Forexample,orthogonalmatchingpursuit(Patietal.,\\n1993)encodesaninputxwiththerepresentationhthatsolvestheconstrained\\noptimization problem\\nargmin\\nh h ,\\ue06b\\ue06b 0 < k\\ue06b− \\ue06bxWh2, (7.49)\\nwhere \\ue06b\\ue06bh 0isthenumberofnon-zeroentriesofh.\\xa0Thisproblemcanbesolved\\neﬃcientlywhenWisconstrainedtobeorthogonal.Thismethodisoftencalled\\n2 5 5', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nOMP- kwiththevalueof kspeciﬁedtoindicatethenumberofnon-zerofeatures\\nallowed. ()demonstratedthatOMP-canbeaveryeﬀective CoatesandNg2011 1\\nfeatureextractorfordeeparchitectures.\\nEssentiallyanymodelthathashiddenunitscanbemadesparse.Throughout\\nthisbook,wewillseemanyexamplesofsparsityregularizationusedinavarietyof\\ncontexts.\\n7.11BaggingandOtherEnsembleMethods\\nBagging(shortforbootstrapaggregating)isatechniqueforreducinggen-\\neralizationerrorbycombiningseveralmodels(,).Theideaisto Breiman1994\\ntrainseveraldiﬀerentmodelsseparately,thenhaveallofthemodelsvoteonthe\\noutputfortestexamples.Thisisanexampleofageneralstrategyinmachine\\nlearningcalledmodelaveraging.Techniquesemployingthisstrategyareknown\\nasensemblemethods.\\nThereasonthatmodelaveragingworksisthatdiﬀerentmodelswillusually\\nnotmakeallthesameerrorsonthetestset.\\nConsiderforexampleasetof kregressionmodels.Supposethateachmodel\\nmakesanerror \\ue00f ioneachexample,\\xa0withtheerrorsdrawnfromazero-mean\\nmultivariatenormaldistributionwithvariances E[ \\ue00f2\\ni] = vandcovariances E[ \\ue00f i \\ue00f j] =\\nc.\\xa0Thentheerrormadebytheaveragepredictionofalltheensemblemodelsis\\n1\\nk\\ue050\\ni \\ue00f i.Theexpectedsquarederroroftheensemblepredictoris\\nE\\uf8ee\\n\\uf8f0\\ue020\\n1\\nk\\ue058\\ni\\ue00f i\\ue0212\\uf8f9\\n\\uf8fb=1\\nk2E\\uf8ee\\n\\uf8f0\\ue058\\ni\\uf8eb\\n\\uf8ed \\ue00f2\\ni+\\ue058\\nj i\\ue036=\\ue00f i \\ue00f j\\uf8f6\\n\\uf8f8\\uf8f9\\n\\uf8fb(7.50)\\n=1\\nkv+k−1\\nkc . (7.51)\\nInthecasewheretheerrorsareperfectlycorrelatedand c= v,themeansquared\\nerrorreducesto v,sothemodelaveragingdoesnothelpatall.Inthecasewhere\\ntheerrorsareperfectlyuncorrelated and c= 0,theexpectedsquarederrorofthe\\nensembleisonly1\\nkv.Thismeansthattheexpectedsquarederroroftheensemble\\ndecreaseslinearlywiththeensemblesize.Inotherwords,onaverage,theensemble\\nwillperformatleastaswellasanyofitsmembers,andifthemembersmake\\nindependenterrors,theensemblewillperformsigniﬁcantlybetterthanitsmembers.\\nDiﬀerentensemblemethodsconstructtheensembleofmodelsindiﬀerentways.\\nForexample,eachmemberoftheensemblecouldbeformedbytrainingacompletely\\n2 5 6', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\n8\\n8F i r s t \\xa0 e nse m b l e \\xa0 m e m b e r\\nSe c ond\\xa0e nse m b l e \\xa0 m e m b e rO r i gi nal \\xa0 data s e t\\nF i r s t \\xa0 r e s am pl e d \\xa0 d a t a s e t\\nSe c ond\\xa0re s am p l e d \\xa0 d a t a s e t\\nFigure7.5:Acartoondepictionofhowbaggingworks.Supposewetrainan8detectoron\\nthedatasetdepictedabove,containingan8,a6anda9.Supposewemaketwodiﬀerent\\nresampleddatasets.Thebaggingtrainingprocedureistoconstructeachofthesedatasets\\nbysamplingwithreplacement.Theﬁrstdatasetomitsthe9andrepeatsthe8.Onthis\\ndataset,thedetectorlearnsthataloopontopofthedigitcorrespondstoan8.On\\ntheseconddataset,werepeatthe9andomitthe6.Inthiscase,thedetectorlearns\\nthatalooponthebottomofthedigitcorrespondstoan8.Eachoftheseindividual\\nclassiﬁcationrulesisbrittle,butifweaveragetheiroutputthenthedetectorisrobust,\\nachievingmaximalconﬁdenceonlywhenbothloopsofthe8arepresent.\\ndiﬀerentkindofmodelusingadiﬀerentalgorithmorobjectivefunction.Bagging\\nisamethodthatallowsthesamekindofmodel,trainingalgorithmandobjective\\nfunctiontobereusedseveraltimes.\\nSpeciﬁcally,bagginginvolvesconstructing kdiﬀerentdatasets.Eachdataset\\nhasthesamenumberofexamplesastheoriginaldataset,buteachdatasetis\\nconstructedbysamplingwithreplacementfromtheoriginaldataset.Thismeans\\nthat,withhighprobability,eachdatasetismissingsomeoftheexamplesfromthe\\noriginaldatasetandalsocontainsseveralduplicateexamples(onaveragearound\\n2/3oftheexamplesfromtheoriginaldatasetarefoundintheresultingtraining\\nset,ifithasthesamesizeastheoriginal).Model iisthentrainedondataset\\ni.Thediﬀerencesbetweenwhichexamplesareincludedineachdatasetresultin\\ndiﬀerencesbetweenthetrainedmodels.Seeﬁgureforanexample.7.5\\nNeuralnetworksreachawideenoughvarietyofsolutionpointsthattheycan\\noftenbeneﬁtfrommodelaveragingevenifallofthemodelsaretrainedonthesame\\ndataset.Diﬀerencesinrandominitialization, randomselectionofminibatches,\\ndiﬀerencesinhyperparameters,ordiﬀerentoutcomesofnon-determinis ticimple-\\nmentationsofneuralnetworksareoftenenoughtocausediﬀerentmembersofthe\\n2 5 7', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nensembletomakepartiallyindependenterrors.\\nModelaveragingisanextremelypowerfulandreliablemethodforreducing\\ngeneralization error.Itsuseisusuallydiscouragedwhenbenchmarkingalgorithms\\nforscientiﬁcpapers,becauseanymachinelearningalgorithmcanbeneﬁtsubstan-\\ntiallyfrommodelaveragingatthepriceofincreasedcomputationandmemory.\\nForthisreason,benchmarkcomparisonsareusuallymadeusingasinglemodel.\\nMachinelearningcontestsareusuallywonbymethodsusingmodelaverag-\\ningoverdozensofmodels.ArecentprominentexampleistheNetﬂixGrand\\nPrize(Koren2009,).\\nNotalltechniquesforconstructingensemblesaredesignedtomaketheensemble\\nmoreregularizedthantheindividualmodels.Forexample,atechniquecalled\\nboosting(FreundandSchapire1996ba,,)constructsanensemblewithhigher\\ncapacitythantheindividualmodels.Boostinghasbeenappliedtobuildensembles\\nofneuralnetworks(SchwenkandBengio1998,)byincrementallyaddingneural\\nnetworkstotheensemble.Boostinghasalsobeenappliedinterpretinganindividual\\nneuralnetworkasanensemble( ,),incrementallyaddinghidden Bengioetal.2006a\\nunitstotheneuralnetwork.\\n7.12Dropout\\nDropout(Srivastava2014etal.,)providesacomputationally inexpensivebut\\npowerfulmethodofregularizingabroadfamilyofmodels.Toaﬁrstapproximation,\\ndropoutcanbethoughtofasamethodofmakingbaggingpracticalforensembles\\nofverymanylargeneuralnetworks.Bagginginvolvestrainingmultiplemodels,\\nandevaluatingmultiplemodelsoneachtestexample.Thisseemsimpractical\\nwheneachmodelisalargeneuralnetwork,sincetrainingandevaluatingsuch\\nnetworksiscostlyintermsofruntimeandmemory.Itiscommontouseensembles\\nofﬁvetotenneuralnetworks— ()usedsixtowintheILSVRC— Szegedy etal.2014a\\nbutmorethanthisrapidlybecomesunwieldy.Dropoutprovidesaninexpensive\\napproximationtotrainingandevaluatingabaggedensembleofexponentiallymany\\nneuralnetworks.\\nSpeciﬁcally,dropouttrainstheensembleconsistingofallsub-networksthat\\ncanbeformedbyremovingnon-outputunitsfromanunderlyingbasenetwork,\\nasillustratedinﬁgure.Inmostmodernneuralnetworks,basedonaseriesof 7.6\\naﬃnetransformationsandnonlinearities, wecaneﬀectivelyremoveaunitfroma\\nnetworkbymultiplyingitsoutputvaluebyzero.\\xa0Thisprocedurerequiressome\\nslightmodiﬁcationformodelssuchasradialbasisfunctionnetworks,whichtake\\n2 5 8', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nthediﬀerencebetweentheunit’sstateandsomereferencevalue.Here,wepresent\\nthedropoutalgorithmintermsofmultiplication byzeroforsimplicity,butitcan\\nbetriviallymodiﬁedtoworkwithotheroperationsthatremoveaunitfromthe\\nnetwork.\\nRecallthattolearnwithbagging,wedeﬁne kdiﬀerentmodels,construct k\\ndiﬀerentdatasetsbysamplingfromthetrainingsetwithreplacement,andthen\\ntrainmodel iondataset i.Dropoutaimstoapproximatethisprocess,butwithan\\nexponentiallylargenumberofneuralnetworks.Speciﬁcally,totrainwithdropout,\\nweuseaminibatch-bas edlearningalgorithmthatmakessmallsteps,suchas\\nstochasticgradientdescent.Eachtimeweloadanexampleintoaminibatch,we\\nrandomlysampleadiﬀerentbinarymasktoapplytoalloftheinputandhidden\\nunitsinthenetwork.Themaskforeachunitissampledindependentlyfromallof\\ntheothers.Theprobabilityofsamplingamaskvalueofone(causingaunittobe\\nincluded)isahyperparameter ﬁxedbeforetrainingbegins.\\xa0Itisnotafunction\\nofthecurrentvalueofthemodelparametersortheinputexample.Typically,\\naninputunitisincludedwithprobability0.8andahiddenunitisincludedwith\\nprobability0.5.Wethenrunforwardpropagation, back-propagation,andthe\\nlearningupdateasusual.Figureillustrateshowtorunforwardpropagation 7.7\\nwithdropout.\\nMoreformally,supposethatamaskvectorµspeciﬁeswhichunitstoinclude,\\nand J(θµ ,)deﬁnesthecostofthemodeldeﬁnedbyparametersθandmaskµ.\\nThendropouttrainingconsistsinminimizing E µ J(θµ ,).Theexpectationcontains\\nexponentiallymanytermsbutwecanobtainanunbiasedestimateofitsgradient\\nbysamplingvaluesof.µ\\nDropouttrainingisnotquitethesameasbaggingtraining.Inthecaseof\\nbagging,themodelsareallindependent.Inthecaseofdropout,themodelsshare\\nparameters,witheachmodelinheritingadiﬀerentsubsetofparametersfromthe\\nparentneuralnetwork.Thisparametersharingmakesitpossibletorepresentan\\nexponentialnumberofmodelswithatractableamountofmemory.Inthecaseof\\nbagging,eachmodelistrainedtoconvergenceonitsrespectivetrainingset.Inthe\\ncaseofdropout,typicallymostmodelsarenotexplicitlytrainedatall—usually,\\nthemodelislargeenoughthatitwouldbeinfeasibletosampleallpossiblesub-\\nnetworkswithinthelifetimeoftheuniverse.Instead,atinyfractionofthepossible\\nsub-networksareeachtrainedforasinglestep,andtheparametersharingcauses\\ntheremainingsub-networkstoarriveatgoodsettingsoftheparameters.These\\naretheonlydiﬀerences.Beyondthese,dropoutfollowsthebaggingalgorithm.For\\nexample,thetrainingsetencounteredbyeachsub-networkisindeedasubsetof\\ntheoriginaltrainingsetsampledwithreplacement.\\n2 5 9', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nyy\\nh 1 h 1 h 2 h 2\\nx 1 x 1 x 2 x 2yy\\nh 1 h 1 h 2 h 2\\nx 1 x 1 x 2 x 2yy\\nh 1 h 1 h 2 h 2\\nx 2 x 2yy\\nh 1 h 1 h 2 h 2\\nx 1 x 1yy\\nh 2 h 2\\nx 1 x 1 x 2 x 2\\nyy\\nh 1 h 1\\nx 1 x 1 x 2 x 2yy\\nh 1 h 1 h 2 h 2yy\\nx 1 x 1 x 2 x 2yy\\nh 2 h 2\\nx 2 x 2\\nyy\\nh 1 h 1\\nx 1 x 1yy\\nh 1 h 1\\nx 2 x 2yy\\nh 2 h 2\\nx 1 x 1yy\\nx 1 x 1\\nyy\\nx 2 x 2yy\\nh 2 h 2yy\\nh 1 h 1yyB ase \\xa0 ne t w or k\\nE nse m bl e \\xa0 of \\xa0 s u b n e t w or k s\\nFigure\\xa07.6:Dropout\\xa0trainsan\\xa0ensemble\\xa0consistingof\\xa0allsub-networks\\xa0that\\xa0canbe\\nconstructedbyremovingnon-outputunitsfromanunderlyingbasenetwork.Here,we\\nbeginwithabasenetworkwithtwovisibleunitsandtwohiddenunits.Therearesixteen\\npossiblesubsetsofthesefourunits.Weshowallsixteensubnetworksthatmaybeformed\\nbydroppingoutdiﬀerentsubsetsofunitsfromtheoriginalnetwork.Inthissmallexample,\\nalargeproportionoftheresultingnetworkshavenoinputunitsornopathconnecting\\ntheinputtotheoutput.Thisproblembecomesinsigniﬁcantfornetworkswithwider\\nlayers,wheretheprobabilityofdroppingallpossiblepathsfrominputstooutputsbecomes\\nsmaller.\\n2 6 0', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nˆ x 1ˆ x 1\\nµ x 1 µ x 1 x 1 x 1ˆ x 2ˆ x 2\\nx 2 x 2 µ x 2 µ x 2h 1 h 1 h 2 h 2µ h 1 µ h 1 µ h 2 µ h 2ˆ h 1ˆ h 1ˆ h 2ˆ h 2yyyy\\nh 1 h 1 h 2 h 2\\nx 1 x 1 x 2 x 2\\nFigure7.7:Anexampleofforwardpropagationthroughafeedforwardnetworkusing\\ndropout. ( T o p )Inthisexample,weuseafeedforwardnetworkwithtwoinputunits,one\\nhiddenlayerwithtwohiddenunits,andoneoutputunit.Toperformforward ( Bottom )\\npropagationwithdropout,werandomlysampleavectorµwithoneentryforeachinput\\norhiddenunitinthenetwork.Theentriesofµarebinaryandaresampledindependently\\nfromeachother.Theprobabilityofeachentrybeingisahyperparameter,usually 1 0 .5\\nforthehiddenlayersand0 .8fortheinput.Eachunitinthenetworkismultipliedby\\nthecorrespondingmask,andthenforwardpropagationcontinuesthroughtherestofthe\\nnetworkasusual.Thisisequivalenttorandomlyselectingoneofthesub-networksfrom\\nﬁgureandrunningforwardpropagationthroughit. 7.6\\n2 6 1', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nTomakeaprediction,abaggedensemblemustaccumulatevotesfromallof\\nitsmembers.Werefertothisprocessasinferenceinthiscontext.\\xa0Sofar,our\\ndescriptionofbagginganddropouthasnotrequiredthatthemodelbeexplicitly\\nprobabilistic.Now,weassumethatthemodel’sroleistooutputaprobability\\ndistribution.Inthecaseofbagging,eachmodel iproducesaprobabilitydistribution\\np( ) i( y|x).Thepredictionoftheensembleisgivenbythearithmeticmeanofall\\nofthesedistributions,\\n1\\nkk\\ue058\\ni = 1p( ) i( ) y|x . (7.52)\\nInthecaseofdropout,eachsub-modeldeﬁnedbymaskvectorµdeﬁnesaprob-\\nabilitydistribution p( y ,|xµ).Thearithmeticmeanoverallmasksisgiven\\nby\\ue058\\nµp p y , ()µ(|xµ) (7.53)\\nwhere p(µ)istheprobabilitydistributionthatwasusedtosampleµattraining\\ntime.\\nBecausethissumincludesanexponentialnumberofterms,itisintractable\\ntoevaluateexceptincaseswherethestructureofthemodelpermitssomeform\\nofsimpliﬁcation.Sofar,deepneuralnetsarenotknowntopermitanytractable\\nsimpliﬁcation.Instead,\\xa0wecan\\xa0approximatetheinferencewithsampling,\\xa0by\\naveragingtogethertheoutputfrommanymasks.Even10-20masksareoften\\nsuﬃcienttoobtaingoodperformance.\\nHowever,thereisanevenbetterapproach,thatallowsustoobtainagood\\napproximationtothepredictionsoftheentireensemble,atthecostofonlyone\\nforwardpropagation. Todoso,wechangetousingthegeometricmeanratherthan\\nthearithmeticmeanoftheensemblemembers’predicteddistributions.Warde-\\nFarley2014etal.()presentargumentsandempiricalevidencethatthegeometric\\nmeanperformscomparablytothearithmeticmeaninthiscontext.\\nThegeometricmeanofmultipleprobabilitydistributionsisnotguaranteedtobe\\naprobabilitydistribution.Toguaranteethattheresultisaprobabilitydistribution,\\nweimposetherequirementthatnoneofthesub-modelsassignsprobability0toany\\nevent,andwerenormalizetheresultingdistribution.Theunnormalized probability\\ndistributiondeﬁneddirectlybythegeometricmeanisgivenby\\n˜ p e nse m bl e( ) = y|x 2d\\ue073\\ue059\\nµp y , (|xµ) (7.54)\\nwhere disthenumberofunitsthatmaybedropped.Hereweuseauniform\\ndistributionoverµtosimplifythepresentation,butnon-uniformdistributionsare\\n2 6 2', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nalsopossible.Tomakepredictionswemustre-normalizetheensemble:\\np e nse m bl e( ) = y|x˜ p e nse m bl e( ) y|x\\ue050\\ny\\ue030˜ p e nse m bl e( y\\ue030|x). (7.55)\\nAkeyinsight( ,)involvedindropoutisthatwecanapproxi- Hintonetal.2012c\\nmate p e nse m bl ebyevaluating p( y|x)inonemodel:themodelwithallunits,but\\nwiththeweightsgoingoutofunit imultipliedbytheprobabilityofincludingunit\\ni.Themotivationforthismodiﬁcationistocapturetherightexpectedvalueofthe\\noutputfromthatunit.Wecallthisapproachtheweightscalinginferencerule.\\nThereisnotyetanytheoreticalargumentfortheaccuracyofthisapproximate\\ninferenceruleindeepnonlinearnetworks,butempiricallyitperformsverywell.\\nBecauseweusuallyuseaninclusionprobabilityof1\\n2,theweightscalingrule\\nusuallyamountstodividingtheweightsbyattheendoftraining,andthenusing 2 \\nthemodelasusual.Anotherwaytoachievethesameresultistomultiplythe\\nstatesoftheunitsbyduringtraining.Eitherway,thegoalistomakesurethat 2\\ntheexpectedtotalinputtoaunitattesttimeisroughlythesameastheexpected\\ntotalinputtothatunitattraintime,eventhoughhalftheunitsattraintimeare\\nmissingonaverage.\\nFormanyclassesofmodelsthatdonothavenonlinearhiddenunits,theweight\\nscalinginferenceruleisexact.Forasimpleexample,considerasoftmaxregression\\nclassiﬁerwithinputvariablesrepresentedbythevector: n v\\nP y (= y | v) = softmax\\ue010\\nW\\ue03ev+b\\ue011\\ny. (7.56)\\nWecanindexintothefamilyofsub-modelsbyelement-wisemultiplicationofthe\\ninputwithabinaryvector: d\\nP y (= y | v;) = dsoftmax\\ue010\\nW\\ue03e( )+d\\ue00c vb\\ue011\\ny.(7.57)\\nTheensemblepredictorisdeﬁnedbyre-normalizingthegeometricmeanoverall\\nensemblemembers’predictions:\\nP e nse m bl e(= ) =y y| v˜ P e nse m bl e(= )y y| v\\ue050\\ny\\ue030˜ P e nse m bl e(= y y\\ue030| v)(7.58)\\nwhere\\n˜ P e nse m bl e(= ) =y y| v2n\\ue073\\ue059\\nd∈{} 0 1 ,nP y . (= y | v;)d (7.59)\\n2 6 3', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nToseethattheweightscalingruleisexact,wecansimplify ˜ P e nse m bl e:\\n˜ P e nse m bl e(= ) =y y| v2n\\ue073\\ue059\\nd∈{} 0 1 ,nP y (= y | v;)d(7.60)\\n= 2n\\ue073\\ue059\\nd∈{} 0 1 ,nsoftmax (W\\ue03e( )+)d\\ue00c vby (7.61)\\n= 2n\\ue076\\ue075\\ue075\\ue074\\ue059\\nd∈{} 0 1 ,nexp\\ue000\\nW\\ue03ey , :( )+d\\ue00c v b y\\ue001\\n\\ue050\\ny\\ue030exp\\ue010\\nW\\ue03e\\ny\\ue030 , :( )+d\\ue00c v b y\\ue030\\ue011 (7.62)\\n=2n\\ue071\\ue051\\nd∈{} 0 1 ,nexp\\ue000\\nW\\ue03ey , :( )+d\\ue00c v b y\\ue001\\n2n\\ue072\\ue051\\nd∈{} 0 1 ,n\\ue050\\ny\\ue030exp\\ue010\\nW\\ue03e\\ny\\ue030 , :( )+d\\ue00c v b y\\ue030\\ue011(7.63)\\nBecause˜ Pwillbenormalized,wecansafelyignoremultiplication byfactorsthat\\nareconstantwithrespectto: y\\n˜ P e nse m bl e(= ) y y| v∝2n\\ue073\\ue059\\nd∈{} 0 1 ,nexp\\ue000\\nW\\ue03ey , :( )+d\\ue00c v b y\\ue001\\n(7.64)\\n= exp\\uf8eb\\n\\uf8ed1\\n2n\\ue058\\nd∈{} 0 1 ,nW\\ue03e\\ny , :( )+d\\ue00c v b y\\uf8f6\\n\\uf8f8 (7.65)\\n= exp\\ue0121\\n2W\\ue03e\\ny , : v+ b y\\ue013\\n. (7.66)\\nSubstitutingthisbackintoequationweobtainasoftmaxclassiﬁerwithweights 7.58\\n1\\n2W.\\nTheweightscalingruleisalsoexactinothersettings,includingregression\\nnetworkswithconditionallynormaloutputs,anddeepnetworksthathavehidden\\nlayerswithoutnonlinearities. However,theweightscalingruleisonlyanapproxi-\\nmationfordeepmodelsthathavenonlinearities. Thoughtheapproximationhas\\nnotbeentheoreticallycharacterized, itoftenworkswell,empirically.Goodfellow\\netal.()foundexperimentallythattheweightscalingapproximationcanwork 2013a\\nbetter(intermsofclassiﬁcationaccuracy)thanMonteCarloapproximations tothe\\nensemblepredictor.ThisheldtrueevenwhentheMonteCarloapproximationwas\\nallowedtosampleupto1,000sub-networks. ()found GalandGhahramani2015\\nthatsomemodelsobtainbetterclassiﬁcationaccuracyusingtwentysamplesand\\n2 6 4', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\ntheMonteCarloapproximation.Itappearsthattheoptimalchoiceofinference\\napproximationisproblem-dependent.\\nSrivastava2014etal.()showedthatdropoutismoreeﬀectivethanother\\nstandardcomputationally inexpensiveregularizers,suchasweightdecay,ﬁlter\\nnormconstraintsandsparseactivityregularization. Dropoutmayalsobecombined\\nwithotherformsofregularizationtoyieldafurtherimprovement.\\nOneadvantageofdropoutisthatitisverycomputationally cheap.Using\\ndropoutduringtrainingrequiresonly O( n)computationperexampleperupdate,\\ntogenerate nrandombinarynumbersandmultiplythembythestate.Depending\\nontheimplementation,itmayalsorequire O( n)memorytostorethesebinary\\nnumbersuntiltheback-propagationstage.Runninginferenceinthetrainedmodel\\nhasthesamecostper-exampleasifdropoutwerenotused,thoughwemustpay\\nthecostofdividingtheweightsby2oncebeforebeginningtoruninferenceon\\nexamples.\\nAnothersigniﬁcantadvantageofdropoutisthatitdoesnotsigniﬁcantlylimit\\nthetypeofmodelortrainingprocedurethatcanbeused.Itworkswellwithnearly\\nanymodelthatusesadistributedrepresentationandcanbetrainedwithstochastic\\ngradientdescent.Thisincludesfeedforwardneuralnetworks,probabilisticmodels\\nsuchasrestrictedBoltzmannmachines(Srivastava2014etal.,),andrecurrent\\nneuralnetworks(BayerandOsendorfer2014Pascanu2014a ,; etal.,).Manyother\\nregularizationstrategiesofcomparablepowerimposemoresevererestrictionson\\nthearchitectureofthemodel.\\nThoughthecostper-stepofapplyingdropouttoaspeciﬁcmodelisnegligible,\\nthecostofusingdropoutinacompletesystemcanbesigniﬁcant.Becausedropout\\nisaregularizationtechnique,itreducestheeﬀectivecapacityofamodel.Tooﬀset\\nthiseﬀect,wemustincreasethesizeofthemodel.Typicallytheoptimalvalidation\\nseterrorismuchlowerwhenusingdropout,butthiscomesatthecostofamuch\\nlargermodelandmanymoreiterationsofthetrainingalgorithm.Forverylarge\\ndatasets,regularizationconferslittlereductioningeneralization error.\\xa0Inthese\\ncases,thecomputational costofusingdropoutandlargermodelsmayoutweigh\\nthebeneﬁtofregularization.\\nWhenextremelyfewlabeledtrainingexamplesareavailable,dropoutisless\\neﬀective.Bayesian\\xa0neuralnetworks(,\\xa0)outperform\\xa0dropout\\xa0onthe Neal1996\\nAlternativeSplicingDataset(,)wherefewerthan5,000examples Xiongetal.2011\\nareavailable(Srivastava2014etal.,).Whenadditionalunlabeleddataisavailable,\\nunsupervisedfeaturelearningcangainanadvantageoverdropout.\\nWager2013etal.()showedthat,whenappliedtolinearregression,dropout\\nisequivalentto L2weightdecay,withadiﬀerentweightdecaycoeﬃcientfor\\n2 6 5', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\neachinputfeature.Themagnitudeofeachfeature’sweightdecaycoeﬃcientis\\ndeterminedbyitsvariance.Similarresultsholdforotherlinearmodels.Fordeep\\nmodels,dropoutisnotequivalenttoweightdecay.\\nThestochasticityusedwhiletrainingwithdropoutisnotnecessaryforthe\\napproach’ssuccess.Itisjustameansofapproximating thesumoverallsub-\\nmodels.WangandManning2013()derivedanalyticalapproximationstothis\\nmarginalization. Theirapproximation,knownasfastdropoutresultedinfaster\\nconvergencetimeduetothereducedstochasticityinthecomputationofthe\\ngradient.Thismethodcanalsobeappliedattesttime,asamoreprincipled\\n(butalsomorecomputationally expensive)approximation totheaverageoverall\\nsub-networksthantheweightscalingapproximation.Fastdropouthasbeenused\\ntonearlymatchtheperformanceofstandarddropoutonsmallneuralnetwork\\nproblems,buthasnotyetyieldedasigniﬁcantimprovementorbeenappliedtoa\\nlargeproblem.\\nJustasstochasticityisnotnecessarytoachievetheregularizing\\xa0eﬀect of\\ndropout,itisalsonotsuﬃcient.Todemonstratethis,Warde-Farley2014etal.()\\ndesignedcontrolexperimentsusingamethodcalleddropoutboostingthatthey\\ndesignedtouseexactlythesamemasknoiseastraditionaldropoutbutlack\\nitsregularizingeﬀect.Dropoutboostingtrainstheentireensembletojointly\\nmaximizethelog-likelihoodonthetrainingset.Inthesamesensethattraditional\\ndropoutisanalogoustobagging,\\xa0this approachisanalogoustoboosting.As\\nintended,experimentswithdropoutboostingshowalmostnoregularizationeﬀect\\ncomparedtotrainingtheentirenetworkasasinglemodel.Thisdemonstratesthat\\ntheinterpretationofdropoutasbagginghasvaluebeyondtheinterpretationof\\ndropoutasrobustnesstonoise.Theregularizationeﬀectofthebaggedensembleis\\nonlyachievedwhenthestochasticallysampledensemblemembersaretrainedto\\nperformwellindependently ofeachother.\\nDropouthasinspiredotherstochasticapproachestotrainingexponentially\\nlargeensemblesofmodelsthatshareweights.\\xa0DropConnectisaspecialcaseof\\ndropoutwhereeachproductbetweenasinglescalarweightandasinglehidden\\nunitstateisconsideredaunitthatcanbedropped(Wan2013etal.,).Stochastic\\npoolingisaformofrandomizedpooling(seesection)forbuildingensembles 9.3\\nofconvolutionalnetworkswitheachconvolutionalnetworkattendingtodiﬀerent\\nspatiallocationsofeachfeaturemap.\\xa0Sofar,dropoutremainsthemostwidely\\nusedimplicitensemblemethod.\\nOneofthekeyinsightsofdropoutisthattraininganetworkwithstochastic\\nbehaviorandmakingpredictionsbyaveragingovermultiplestochasticdecisions\\nimplementsaformofbaggingwithparametersharing.Earlier,\\xa0wedescribed\\n2 6 6', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\ndropoutas\\xa0bagginganensembleofmodelsformedbyincludingor\\xa0excluding\\nunits.However,thereisnoneedforthismodelaveragingstrategytobebasedon\\ninclusionandexclusion.Inprinciple,anykindofrandommodiﬁcationisadmissible.\\nInpractice,wemustchoosemodiﬁcationfamiliesthatneuralnetworksareable\\ntolearntoresist.Ideally,weshouldalsousemodelfamiliesthatallowafast\\napproximateinferencerule.Wecanthinkofanyformofmodiﬁcationparametrized\\nbyavectorµastraininganensembleconsistingof p( y ,|xµ)forallpossible\\nvaluesofµ.Thereisnorequirementthatµhaveaﬁnitenumberofvalues.For\\nexample,µcanbereal-valued.Srivastava2014etal.()showedthatmultiplyingthe\\nweightsbyµ∼N( 1 , I)canoutperformdropoutbasedonbinarymasks.Because\\nE[µ] = 1thestandardnetworkautomatically implementsapproximate inference\\nintheensemble,withoutneedinganyweightscaling.\\nSofarwehavedescribeddropoutpurelyasameansofperformingeﬃcient,\\napproximatebagging.However,thereisanotherviewofdropoutthatgoesfurther\\nthanthis.Dropouttrainsnotjustabaggedensembleofmodels,butanensemble\\nofmodelsthatsharehiddenunits.Thismeanseachhiddenunitmustbeableto\\nperformwellregardlessofwhichotherhiddenunitsareinthemodel.Hiddenunits\\nmustbepreparedtobeswappedandinterchangedbetweenmodels.Hintonetal.\\n()wereinspiredbyanideafrombiology:sexualreproduction,whichinvolves 2012c\\nswappinggenesbetweentwodiﬀerentorganisms,createsevolutionarypressurefor\\ngenestobecomenotjustgood,buttobecomereadilyswappedbetweendiﬀerent\\norganisms.Suchgenesandsuchfeaturesareveryrobusttochangesintheir\\nenvironmentbecausetheyarenotabletoincorrectlyadapttounusualfeatures\\nofanyoneorganismormodel.Dropoutthusregularizeseachhiddenunittobe\\nnotmerelyagoodfeaturebutafeaturethatisgoodinmanycontexts.\\xa0Warde-\\nFarley2014etal.()compareddropouttrainingtotrainingoflargeensemblesand\\nconcludedthatdropoutoﬀersadditionalimprovementstogeneralization error\\nbeyondthoseobtainedbyensemblesofindependentmodels.\\nItisimportanttounderstandthatalargeportionofthepowerofdropout\\narisesfromthefactthatthemaskingnoiseisappliedtothehiddenunits.This\\ncanbeseenasaformofhighlyintelligent,adaptivedestructionoftheinformation\\ncontentoftheinputratherthandestructionoftherawvaluesoftheinput.For\\nexample,ifthemodellearnsahiddenunit h ithatdetectsafacebyﬁndingthenose,\\nthendropping h icorrespondstoerasingtheinformationthatthereisanosein\\ntheimage.Themodelmustlearnanother h i,eitherthatredundantlyencodesthe\\npresenceofanose,orthatdetectsthefacebyanotherfeature,suchasthemouth.\\nTraditionalnoiseinjectiontechniquesthataddunstructurednoiseattheinputare\\nnotabletorandomlyerasetheinformationaboutanosefromanimageofaface\\nunlessthemagnitudeofthenoiseissogreatthatnearlyalloftheinformationin\\n2 6 7', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\ntheimageisremoved.Destroyingextractedfeaturesratherthanoriginalvalues\\nallowsthedestructionprocesstomakeuseofalloftheknowledgeabouttheinput\\ndistributionthatthemodelhasacquiredsofar.\\nAnotherimportantaspectofdropoutisthatthenoiseismultiplicative. Ifthe\\nnoisewereadditivewithﬁxedscale,thenarectiﬁedlinearhiddenunit h iwith\\naddednoise \\ue00fcouldsimplylearntohave h ibecomeverylargeinordertomake\\ntheaddednoise \\ue00finsigniﬁcantbycomparison.Multiplicativenoisedoesnotallow\\nsuchapathologicalsolutiontothenoiserobustnessproblem.\\nAnotherdeeplearningalgorithm,batchnormalization, reparametrizes themodel\\ninawaythatintroducesbothadditiveandmultiplicativenoiseonthehidden\\nunitsattrainingtime.Theprimarypurposeofbatchnormalization istoimprove\\noptimization, butthenoisecanhavearegularizingeﬀect,andsometimesmakes\\ndropoutunnecessary.Batchnormalization isdescribedfurtherinsection.8.7.1\\n7.13AdversarialTraining\\nInmanycases,neuralnetworkshavebeguntoreachhumanperformancewhen\\nevaluatedonani.i.d.testset.Itisnaturalthereforetowonderwhetherthese\\nmodelshaveobtainedatruehuman-levelunderstandingofthesetasks.Inorder\\ntoprobethelevelofunderstandinganetworkhasoftheunderlyingtask,wecan\\nsearchforexamplesthatthemodelmisclassiﬁes. ()foundthat Szegedy etal.2014b\\nevenneuralnetworksthatperformathumanlevelaccuracyhaveanearly100%\\nerrorrateonexamplesthatareintentionallyconstructedbyusinganoptimization\\nproceduretosearchforaninputx\\ue030nearadatapointxsuchthatthemodel\\noutputisverydiﬀerentatx\\ue030.Inmanycases,x\\ue030canbesosimilartoxthata\\nhumanobservercannottellthediﬀerencebetweentheoriginalexampleandthe\\nadversarialexample,butthenetworkcanmakehighlydiﬀerentpredictions.See\\nﬁgureforanexample.7.8\\nAdversarialexampleshavemanyimplications,forexample,incomputersecurity,\\nthatarebeyondthescopeofthischapter.\\xa0However,theyareinterestinginthe\\ncontextofregularizationbecauseonecanreducetheerrorrateontheoriginali.i.d.\\ntestsetviaadversarialtraining—trainingonadversariallyperturbedexamples\\nfromthetrainingset( ,; Szegedy etal.2014bGoodfellow2014betal.,).\\nGoodfellow2014betal.()showedthatoneoftheprimarycausesofthese\\nadversarial\\xa0examplesis\\xa0excessive\\xa0linearity.Neural\\xa0networks\\xa0arebuilt\\xa0out\\xa0of\\nprimarilylinearbuildingblocks.\\xa0Insomeexperimentstheoverallfunctionthey\\nimplementprovestobehighlylinearasaresult.Theselinearfunctionsareeasy\\n2 6 8', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\n+ .007× =\\nx sign(∇ x J(θx , , y))x+\\n\\ue00fsign(∇ x J(θx , , y))\\ny=“panda” “nematode”“gibbon”\\nw/57.7%\\nconﬁdencew/8.2%\\nconﬁdencew/99.3%\\nconﬁdence\\nFigure7.8:\\xa0AdemonstrationofadversarialexamplegenerationappliedtoGoogLeNet\\n( ,)onImageNet.Byaddinganimperceptiblysmallvectorwhose Szegedy e t a l .2014a\\nelementsareequaltothesignoftheelementsofthegradientofthecostfunctionwith\\nrespecttotheinput,wecanchangeGoogLeNet’sclassiﬁcationoftheimage.Reproduced\\nwithpermissionfrom (). Goodfellow e t a l .2014b\\ntooptimize.Unfortunately,thevalueofalinearfunctioncanchangeveryrapidly\\nifithasnumerousinputs.Ifwechangeeachinputby \\ue00f,thenalinearfunction\\nwithweightswcanchangebyasmuchas \\ue00f||||w 1,whichcanbeaverylarge\\namountifwishigh-dimensional.Adversarialtrainingdiscouragesthishighly\\nsensitivelocallylinearbehaviorbyencouragingthenetworktobelocallyconstant\\nintheneighborhoodofthetrainingdata.Thiscanbeseenasawayofexplicitly\\nintroducingalocalconstancypriorintosupervisedneuralnets.\\nAdversarialtraininghelpstoillustratethepowerofusingalargefunction\\nfamilyincombinationwithaggressiveregularization. Purelylinearmodels,like\\nlogisticregression,arenotabletoresistadversarialexamplesbecausetheyare\\nforcedtobelinear.Neuralnetworksareabletorepresentfunctionsthatcanrange\\nfromnearlylineartonearlylocallyconstantandthushavetheﬂexibilitytocapture\\nlineartrendsinthetrainingdatawhilestilllearningtoresistlocalperturbation.\\nAdversarialexamplesalsoprovideameansofaccomplishingsemi-supervised\\nlearning.Atapointxthatisnotassociatedwithalabelinthedataset,the\\nmodelitselfassignssomelabel ˆ y.Themodel’slabel ˆ ymaynotbethetruelabel,\\nbutifthemodelishighquality,thenˆ yhasahighprobabilityofprovidingthe\\ntruelabel.Wecanseekanadversarialexamplex\\ue030thatcausestheclassiﬁerto\\noutputalabel y\\ue030with y\\ue030\\ue036=ˆ y.Adversarialexamplesgeneratedusingnotthetrue\\nlabelbutalabelprovidedbyatrainedmodelarecalledvirtualadversarial\\nexamples(Miyato2015etal.,).Theclassiﬁermaythenbetrainedtoassignthe\\nsamelabeltoxandx\\ue030.Thisencouragestheclassiﬁertolearnafunctionthatis\\n2 6 9', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nrobusttosmallchangesanywherealongthemanifoldwheretheunlabeleddata\\nlies.Theassumptionmotivatingthisapproachisthatdiﬀerentclassesusuallylie\\nondisconnectedmanifolds,andasmallperturbationshouldnotbeabletojump\\nfromoneclassmanifoldtoanotherclassmanifold.\\n7.14Tangent\\xa0Distance,\\xa0TangentProp,and\\xa0Manifold\\nTangentClassiﬁer\\nManymachinelearningalgorithmsaimtoovercomethecurseofdimensionality\\nbyassumingthatthedataliesnearalow-dimensional manifold,asdescribedin\\nsection.5.11.3\\nOneoftheearlyattemptstotakeadvantageofthemanifoldhypothesisisthe\\ntangentdistancealgorithm( ,,).Itisanon-parametric Simard etal.19931998\\nnearest-neighboralgorithminwhichthemetricusedisnotthegenericEuclidean\\ndistancebutonethatisderivedfromknowledgeofthemanifoldsnearwhich\\nprobabilityconcentrates.Itisassumedthatwearetryingtoclassifyexamplesand\\nthatexamplesonthesamemanifoldsharethesamecategory.Sincetheclassiﬁer\\nshouldbeinvarianttothelocalfactorsofvariationthatcorrespondtomovement\\nonthemanifold,itwouldmakesensetouseasnearest-neighbordistancebetween\\npointsx 1andx 2thedistancebetweenthemanifolds M 1and M 2towhichthey\\nrespectivelybelong.Althoughthatmaybecomputationally diﬃcult(itwould\\nrequiresolvinganoptimization problem,toﬁndthenearestpairofpointson M 1\\nand M 2),acheapalternativethatmakessenselocallyistoapproximate M ibyits\\ntangentplaneatx iandmeasurethedistancebetweenthetwotangents,orbetween\\natangentplaneandapoint.Thatcanbeachievedbysolvingalow-dimensional\\nlinearsystem(inthedimensionofthemanifolds).Ofcourse,thisalgorithmrequires\\nonetospecifythetangentvectors.\\nInarelatedspirit,thetangentpropalgorithm( ,)(ﬁgure) Simardetal.19927.9\\ntrainsaneuralnetclassiﬁerwithanextrapenaltytomakeeachoutput f(x)of\\ntheneuralnetlocallyinvarianttoknownfactorsofvariation.Thesefactorsof\\nvariationcorrespondtomovementalongthemanifoldnearwhichexamplesofthe\\nsameclassconcentrate.Localinvarianceisachievedbyrequiring ∇ x f(x)tobe\\northogonaltotheknownmanifoldtangentvectorsv( ) iatx,orequivalentlythat\\nthedirectionalderivativeof fatxinthedirectionsv( ) ibesmallbyaddinga\\nregularizationpenalty:Ω\\nΩ() = f\\ue058\\ni\\ue010\\n(∇ x f())x\\ue03ev( ) i\\ue0112\\n. (7.67)\\n2 7 0', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nThisregularizercanofcoursebescaledbyanappropriatehyperparameter, and,for\\nmostneuralnetworks,wewouldneedtosumovermanyoutputsratherthanthelone\\noutput f(x)describedhereforsimplicity.Aswiththetangentdistancealgorithm,\\nthetangentvectorsarederivedapriori,usuallyfromtheformalknowledgeof\\ntheeﬀectoftransformationssuchastranslation,rotation,andscalinginimages.\\nTangentprophasbeenusednotjustforsupervisedlearning( ,) Simardetal.1992\\nbutalsointhecontextofreinforcementlearning(,). Thrun1995\\nTangentpropagation is\\xa0closelyrelated\\xa0todataset\\xa0augmentation.In\\xa0both\\ncases,theuserofthealgorithmencodeshisorherpriorknowledgeofthetask\\nbyspecifyingasetoftransformationsthatshouldnotaltertheoutputofthe\\nnetwork.Thediﬀerenceisthatinthecaseofdatasetaugmentation, thenetworkis\\nexplicitlytrainedtocorrectlyclassifydistinctinputsthatwerecreatedbyapplying\\nmorethananinﬁnitesimalamountofthesetransformations.Tangentpropagation\\ndoesnotrequireexplicitlyvisitinganewinputpoint.Instead,itanalytically\\nregularizesthemodeltoresistperturbationinthedirectionscorrespondingto\\nthe\\xa0speciﬁed\\xa0transformation.While\\xa0thisanalytical\\xa0approac h\\xa0isintellectually\\nelegant,ithastwomajordrawbacks.First,itonlyregularizesthemodeltoresist\\ninﬁnitesimalperturbation.Explicitdatasetaugmentationconfersresistanceto\\nlargerperturbations.Second,theinﬁnitesimalapproachposesdiﬃcultiesformodels\\nbasedonrectiﬁedlinearunits.Thesemodelscanonlyshrinktheirderivatives\\nbyturningunitsoﬀorshrinkingtheirweights.Theyarenotabletoshrinktheir\\nderivativesbysaturatingatahighvaluewithlargeweights,assigmoidortanh\\nunitscan.Datasetaugmentation workswellwithrectiﬁedlinearunitsbecause\\ndiﬀerentsubsetsofrectiﬁedunitscanactivatefordiﬀerenttransformedversionsof\\neachoriginalinput.\\nTangentpropagationisalsorelatedtodoublebackprop(DruckerandLeCun,\\n1992)andadversarialtraining( ,; ,). Szegedy etal.2014bGoodfellowetal.2014b\\nDoublebackpropregularizestheJacobiantobesmall,whileadversarialtraining\\nﬁndsinputsneartheoriginalinputsandtrainsthemodeltoproducethesame\\noutputontheseasontheoriginalinputs.Tangentpropagation anddataset\\naugmentationusingmanuallyspeciﬁedtransformationsbothrequirethatthe\\nmodelshouldbeinvarianttocertainspeciﬁeddirectionsofchangeintheinput.\\nDoublebackpropandadversarialtrainingbothrequirethatthemodelshouldbe\\ninvarianttodirectionsofchangeintheinputsolongasthechangeissmall.Just all\\nasdatasetaugmentationisthenon-inﬁnitesimalversionoftangentpropagation,\\nadversarialtrainingisthenon-inﬁnitesimalversionofdoublebackprop.\\nThemanifoldtangentclassiﬁer(,),eliminatestheneedto Rifaietal.2011c\\nknowthetangentvectorsapriori.Aswewillseeinchapter,autoencoderscan 14\\n2 7 1', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nx 1x 2N o r m a lT a ng e nt\\nFigure7.9:\\xa0Illustrationofthemainideaofthetangentpropalgorithm( , Simard e t a l .\\n1992 Rifai2011c )andmanifoldtangentclassiﬁer( e t a l .,),whichbothregularizethe\\nclassiﬁeroutputfunction f(x).Eachcurverepresentsthemanifoldforadiﬀerentclass,\\nillustratedhereasaone-dimensionalmanifoldembeddedinatwo-dimensionalspace.\\nOnonecurve,wehavechosenasinglepointanddrawnavectorthatistangenttothe\\nclassmanifold(paralleltoandtouchingthemanifold)andavectorthatisnormaltothe\\nclassmanifold(orthogonaltothemanifold).Inmultipledimensionstheremaybemany\\ntangentdirectionsandmanynormaldirections.Weexpecttheclassiﬁcationfunctionto\\nchangerapidlyasitmovesinthedirectionnormaltothemanifold,andnottochangeas\\nitmovesalongtheclassmanifold.Bothtangentpropagationandthemanifoldtangent\\nclassiﬁerregularize f(x) tonotchangeverymuchasxmovesalongthemanifold.Tangent\\npropagationrequirestheusertomanuallyspecifyfunctionsthatcomputethetangent\\ndirections(suchasspecifyingthatsmalltranslationsofimagesremaininthesameclass\\nmanifold)whilethemanifoldtangentclassiﬁerestimatesthemanifoldtangentdirections\\nbytraininganautoencodertoﬁtthetrainingdata.Theuseofautoencoderstoestimate\\nmanifoldswillbedescribedinchapter.14\\nestimatethemanifoldtangentvectors.Themanifoldtangentclassiﬁermakesuse\\nofthistechniquetoavoidneedinguser-speciﬁedtangentvectors.\\xa0Asillustrated\\ninﬁgure,theseestimatedtangentvectorsgobeyondtheclassicalinvariants 14.10\\nthatariseoutofthegeometryofimages(suchastranslation,rotationandscaling)\\nandincludefactorsthatmustbelearnedbecausetheyareobject-speciﬁc(suchas\\nmovingbodyparts).Thealgorithmproposedwiththemanifoldtangentclassiﬁer\\nisthereforesimple:(1)useanautoencodertolearnthemanifoldstructureby\\nunsupervisedlearning,and(2)usethesetangentstoregularizeaneuralnetclassiﬁer\\nasintangentprop(equation).7.67\\nThischapterhasdescribedmostofthegeneralstrategiesusedtoregularize\\nneuralnetworks.Regularizationisacentralthemeofmachinelearningandassuch\\n2 7 2', 'CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nwillberevisitedperiodicallybymostoftheremainingchapters.Anothercentral\\nthemeofmachinelearningisoptimization, describednext.\\n2 7 3']}, 'DeepLearning-chapter-6.pdf': {'text': ['C h a p t e r 6\\nD e e p F e e d f orw ard N e t w orks\\nDeepfeedforwardnetworks,alsooftencalledfeedforwardneuralnetworks,\\normultilayerperceptrons(MLPs),arethequintessentialdeeplearningmodels.\\nThegoalofafeedforwardnetworkistoapproximatesomefunction f∗.Forexample,\\nforaclassiﬁer, y= f∗(x)mapsaninputxtoacategory y.Afeedforwardnetwork\\ndeﬁnesamappingy= f(x;θ)andlearnsthevalueoftheparametersθthatresult\\ninthebestfunctionapproximation.\\nThesemodelsarecalledfeedforwardbecauseinformationﬂowsthroughthe\\nfunctionbeingevaluatedfromx,throughtheintermediate computations usedto\\ndeﬁne f,andﬁnallytotheoutputy.Therearenofeedbackconnectionsinwhich\\noutputsofthemodelarefedbackintoitself.Whenfeedforwardneuralnetworks\\nareextendedtoincludefeedbackconnections,theyarecalledrecurrentneural\\nnetworks,presentedinchapter.10\\nFeedforwardnetworksareofextremeimportancetomachinelearningpracti-\\ntioners.Theyformthebasisofmanyimportantcommercialapplications.For\\nexample,theconvolutionalnetworksusedforobjectrecognitionfromphotosarea\\nspecializedkindoffeedforwardnetwork.Feedforwardnetworksareaconceptual\\nsteppingstoneonthepathtorecurrentnetworks,whichpowermanynatural\\nlanguageapplications.\\nFeedforwardneuralnetworksarecallednetworksbecausetheyaretypically\\nrepresentedbycomposingtogethermanydiﬀerentfunctions.Themodelisasso-\\nciatedwithadirectedacyclicgraphdescribinghowthefunctionsarecomposed\\ntogether.Forexample,wemighthavethreefunctions f( 1 ), f( 2 ),and f( 3 )connected\\ninachain,toform f(x) = f( 3 )( f( 2 )( f( 1 )(x))).Thesechainstructuresarethemost\\ncommonlyusedstructuresofneuralnetworks.Inthiscase, f( 1 )iscalledtheﬁrst\\nlayerofthenetwork, f( 2 )iscalledthesecondlayer,andsoon.Theoverall\\n168', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nlengthofthechaingivesthedepthofthemodel.Itisfromthisterminologythat\\nthename“deeplearning”arises.Theﬁnallayerofafeedforwardnetworkiscalled\\ntheoutputlayer.Duringneuralnetworktraining,wedrive f(x)tomatch f∗(x).\\nThetrainingdataprovidesuswithnoisy,approximateexamplesof f∗(x) evaluated\\natdiﬀerenttrainingpoints.Eachexamplexisaccompanied byalabel y f≈∗(x).\\nThetrainingexamplesspecifydirectlywhattheoutputlayermustdoateachpoint\\nx;itmustproduceavaluethatiscloseto y.Thebehavioroftheotherlayersis\\nnotdirectlyspeciﬁedbythetrainingdata.\\xa0Thelearningalgorithmmustdecide\\nhowtousethoselayerstoproducethedesiredoutput,butthetrainingdatadoes\\nnotsaywhateachindividuallayershoulddo.Instead,thelearningalgorithmmust\\ndecidehowtousetheselayerstobestimplementanapproximation of f∗.Because\\nthetrainingdatadoesnotshowthedesiredoutputforeachoftheselayers,these\\nlayersarecalledhiddenlayers.\\nFinally,thesenetworksarecalled ne u r a lbecausetheyarelooselyinspiredby\\nneuroscience.Eachhiddenlayerofthenetworkistypicallyvector-valued.The\\ndimensionalityofthesehiddenlayersdeterminesthewidthofthemodel.Each\\nelementofthevectormaybeinterpretedasplayingaroleanalogoustoaneuron.\\nRatherthanthinkingofthelayerasrepresentingasinglevector-to-vectorfunction,\\nwecanalsothinkofthelayerasconsistingofmanyunitsthatactinparallel,\\neachrepresentingavector-to-scalarfunction.Eachunitresemblesaneuronin\\nthesensethatitreceivesinputfrommanyotherunitsandcomputesitsown\\nactivationvalue.\\xa0Theideaofusingmanylayersofvector-valuedrepresentation\\nisdrawnfromneuroscience.Thechoiceofthefunctions f( ) i(x)usedtocompute\\ntheserepresentationsisalsolooselyguidedbyneuroscientiﬁcobservationsabout\\nthefunctionsthatbiologicalneuronscompute.However,modernneuralnetwork\\nresearchisguidedbymanymathematical andengineeringdisciplines,andthe\\ngoalofneuralnetworksisnottoperfectlymodelthebrain.Itisbesttothinkof\\nfeedforwardnetworksasfunctionapproximation machinesthataredesignedto\\nachievestatisticalgeneralization, occasionallydrawingsomeinsightsfromwhatwe\\nknowaboutthebrain,ratherthanasmodelsofbrainfunction.\\nOnewaytounderstandfeedforwardnetworksistobeginwithlinearmodels\\nandconsiderhowtoovercometheirlimitations.\\xa0Linearmodels,suchaslogistic\\nregressionandlinearregression,areappealingbecausetheymaybeﬁteﬃciently\\nandreliably,eitherinclosedformorwithconvexoptimization. Linearmodelsalso\\nhavetheobviousdefectthatthemodelcapacityislimitedtolinearfunctions,so\\nthemodelcannotunderstandtheinteractionbetweenanytwoinputvariables.\\nToextendlinearmodelstorepresentnonlinearfunctionsofx,wecanapply\\nthelinearmodelnottoxitselfbuttoatransformedinput φ(x),where φisa\\n1 6 9', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nnonlineartransformation.Equivalently,wecanapplythekerneltrickdescribedin\\nsection,toobtainanonlinearlearningalgorithmbasedonimplicitlyapplying 5.7.2\\nthe φmapping.Wecanthinkof φasprovidingasetoffeaturesdescribingx,or\\nasprovidinganewrepresentationfor.x\\nThequestionisthenhowtochoosethemapping. φ\\n1.Oneoptionistouseaverygeneric φ,suchastheinﬁnite-dimens ional φthat\\nisimplicitlyusedbykernelmachinesbasedontheRBFkernel.\\xa0If φ(x)is\\nofhighenoughdimension,wecanalwayshaveenoughcapacitytoﬁtthe\\ntrainingset,butgeneralization tothetestsetoftenremainspoor.Very\\ngenericfeaturemappingsareusuallybasedonlyontheprincipleoflocal\\nsmoothnessanddonotencodeenoughpriorinformationtosolveadvanced\\nproblems.\\n2.Anotheroptionistomanuallyengineer φ.Untiltheadventofdeeplearning,\\nthiswasthedominantapproach.Thisapproachrequiresdecadesofhuman\\neﬀortfor\\xa0eachseparate\\xa0task,\\xa0withpractitioners\\xa0specializing\\xa0in diﬀerent\\ndomainssuchasspeech\\xa0recognition or\\xa0computer vision,\\xa0and\\xa0with little\\ntransferbetweendomains.\\n3.Thestrategyofdeeplearningistolearn φ.Inthisapproach,wehaveamodel\\ny= f(x;θw ,) = φ(x;θ)\\ue03ew.Wenowhaveparametersθthatweusetolearn\\nφfromabroadclassoffunctions,andparameterswthatmapfrom φ(x)to\\nthedesiredoutput.Thisisanexampleofadeepfeedforwardnetwork,with\\nφdeﬁningahiddenlayer.\\xa0Thisapproachistheonlyoneofthethreethat\\ngivesupontheconvexityofthetrainingproblem,butthebeneﬁtsoutweigh\\ntheharms.Inthisapproach,weparametrizetherepresentationas φ(x;θ)\\nandusetheoptimization algorithmtoﬁndtheθthatcorrespondstoagood\\nrepresentation.Ifwewish,thisapproachcancapturethebeneﬁtoftheﬁrst\\napproachbybeinghighlygeneric—wedosobyusingaverybroadfamily\\nφ(x;θ).Thisapproachcanalsocapturethebeneﬁtofthesecondapproach.\\nHumanpractitioners canencodetheirknowledgetohelpgeneralization by\\ndesigningfamilies φ(x;θ)thattheyexpectwillperformwell.Theadvantage\\nisthatthehumandesigneronlyneedstoﬁndtherightgeneralfunction\\nfamilyratherthanﬁndingpreciselytherightfunction.\\nThisgeneralprincipleofimprovingmodelsbylearningfeaturesextendsbeyond\\nthefeedforwardnetworksdescribedinthischapter.Itisarecurringthemeofdeep\\nlearningthatappliestoallofthekindsofmodelsdescribedthroughoutthisbook.\\nFeedforwardnetworksaretheapplicationofthisprincipletolearningdeterministic\\n1 7 0', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nmappingsfromxtoythatlackfeedbackconnections.\\xa0Othermodelspresented\\nlaterwillapplytheseprinciplestolearningstochasticmappings,learningfunctions\\nwithfeedback,andlearningprobabilitydistributionsoverasinglevector.\\nWebeginthischapterwithasimpleexampleofafeedforwardnetwork.Next,\\nweaddresseachofthedesigndecisionsneededtodeployafeedforwardnetwork.\\nFirst,trainingafeedforwardnetworkrequiresmakingmanyofthesamedesign\\ndecisionsasarenecessaryforalinearmodel:choosingtheoptimizer,thecost\\nfunction,andtheformoftheoutputunits.Wereviewthesebasicsofgradient-based\\nlearning,thenproceedtoconfrontsomeofthedesigndecisionsthatareunique\\ntofeedforwardnetworks.Feedforwardnetworkshaveintroducedtheconceptofa\\nhiddenlayer,andthisrequiresustochoosetheactivationfunctionsthatwill\\nbeusedtocomputethehiddenlayervalues.Wemustalsodesignthearchitecture\\nofthenetwork,includinghowmanylayersthenetworkshouldcontain,howthese\\nlayersshould\\xa0beconnectedto\\xa0each\\xa0other,\\xa0and howmanyunitsshould\\xa0bein\\neachlayer.Learningindeepneuralnetworksrequirescomputingthegradients\\nofcomplicatedfunctions.Wepresenttheback-propagationalgorithmandits\\nmoderngeneralizations ,whichcanbeusedtoeﬃcientlycomputethesegradients.\\nFinally,weclosewithsomehistoricalperspective.\\n6. 1 E x am p l e: L earni n g X O R\\nTomaketheideaofafeedforwardnetworkmoreconcrete,webeginwithan\\nexampleofafullyfunctioningfeedforwardnetworkonaverysimpletask:learning\\ntheXORfunction.\\nTheXORfunction(“exclusiveor”)isanoperationontwobinaryvalues, x 1\\nand x 2.Whenexactlyoneofthesebinaryvaluesisequalto,theXORfunction 1\\nreturns.Otherwise,itreturns0.TheXORfunctionprovidesthetargetfunction 1\\ny= f∗(x)thatwewanttolearn.Ourmodelprovidesafunction y= f(x;θ)and\\nourlearningalgorithmwilladapttheparametersθtomake fassimilaraspossible\\nto f∗.\\nInthissimpleexample,wewillnotbeconcernedwithstatisticalgeneralization.\\nWewantournetworktoperformcorrectlyonthefourpoints X={[0 ,0]\\ue03e,[0 ,1]\\ue03e,\\n[1 ,0]\\ue03e,and[1 ,1]\\ue03e}.\\xa0Wewilltrainthenetworkonallfourofthesepoints.\\xa0The\\nonlychallengeistoﬁtthetrainingset.\\nWecantreatthisproblemasaregressionproblemanduseameansquared\\nerrorlossfunction.Wechoosethislossfunctiontosimplifythemathforthis\\nexampleasmuchaspossible.Inpracticalapplications,MSEisusuallynotan\\n1 7 1', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nappropriatecostfunctionformodelingbinarydata.Moreappropriateapproaches\\naredescribedinsection.6.2.2.2\\nEvaluatedonourwholetrainingset,theMSElossfunctionis\\nJ() =θ1\\n4\\ue058\\nx∈ X( f∗() (;))x− fxθ2. (6.1)\\nNowwemustchoosetheformofourmodel, f(x;θ).Supposethatwechoose\\nalinearmodel,withconsistingofand.Ourmodelisdeﬁnedtobe θw b\\nf , b (;xw) = x\\ue03ew+ b . (6.2)\\nWecanminimize J(θ)inclosedformwithrespecttowand busingthenormal\\nequations.\\nAftersolvingthenormalequations,weobtainw= 0and b=1\\n2.\\xa0Thelinear\\nmodelsimplyoutputs 0 .5everywhere.Whydoesthishappen?Figureshows6.1\\nhowalinearmodelisnotabletorepresenttheXORfunction.Onewaytosolve\\nthisproblemistouseamodelthatlearnsadiﬀerentfeaturespaceinwhicha\\nlinearmodelisabletorepresentthesolution.\\nSpeciﬁcally,wewillintroduceaverysimplefeedforwardnetworkwithone\\nhiddenlayercontainingtwohiddenunits.Seeﬁgureforanillustrationof 6.2\\nthismodel.Thisfeedforwardnetworkhasavectorofhiddenunitshthatare\\ncomputedbyafunction f( 1 )(x;Wc ,).Thevaluesofthesehiddenunitsarethen\\nusedastheinputforasecondlayer.Thesecondlayeristheoutputlayerofthe\\nnetwork.Theoutputlayerisstilljustalinearregressionmodel,butnowitis\\nappliedtohratherthantox.Thenetworknowcontainstwofunctionschained\\ntogether:h= f( 1 )(x;Wc ,)and y= f( 2 )(h;w , b),withthecompletemodelbeing\\nf , , , b f (;xWcw) = ( 2 )( f( 1 )())x .\\nWhatfunctionshould f( 1 )compute?Linearmodelshaveserveduswellsofar,\\nanditmaybetemptingtomake f( 1 )belinearaswell.Unfortunately,if f( 1 )were\\nlinear,thenthefeedforwardnetworkasawholewouldremainalinearfunctionof\\nitsinput.Ignoringtheintercepttermsforthemoment,suppose f( 1 )(x) =W\\ue03ex\\nand f( 2 )(h) =h\\ue03ew.Then f(x) =w\\ue03eW\\ue03ex.Wecouldrepresentthisfunctionas\\nf() = xx\\ue03ew\\ue030wherew\\ue030= Ww.\\nClearly,wemustuseanonlinearfunctiontodescribethefeatures.Mostneural\\nnetworksdosousinganaﬃnetransformationcontrolledbylearnedparameters,\\nfollowedbyaﬁxed,nonlinearfunctioncalledanactivationfunction.Weusethat\\nstrategyhere,bydeﬁningh= g(W\\ue03ex+c) ,whereWprovidestheweightsofa\\nlineartransformationandcthebiases.Previously,todescribealinearregression\\n1 7 2', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\n0 1\\nx 101x 2O r i g i n a l s p a c e x\\n0 1 2\\nh 101h 2L e a r n e d s p a c e h\\nFigure6.1:SolvingtheXORproblembylearningarepresentation.Theboldnumbers\\nprintedontheplotindicatethevaluethatthelearnedfunctionmustoutputateachpoint.\\n( L e f t )AlinearmodelapplieddirectlytotheoriginalinputcannotimplementtheXOR\\nfunction.When x1= 0,themodel’soutputmustincreaseas x2increases.When x1= 1,\\nthemodel’soutputmustdecreaseas x 2increases.Alinearmodelmustapplyaﬁxed\\ncoeﬃcient w 2to x 2.Thelinearmodelthereforecannotusethevalueof x 1tochange\\nthecoeﬃcienton x 2andcannotsolvethisproblem. ( R i g h t )Inthetransformedspace\\nrepresentedbythefeaturesextractedbyaneuralnetwork,alinearmodelcannowsolve\\ntheproblem.Inourexamplesolution,thetwopointsthatmusthaveoutputhavebeen 1\\ncollapsedintoasinglepointinfeaturespace.Inotherwords,thenonlinearfeatureshave\\nmappedbothx= [1 ,0]\\ue03eandx= [0 ,1]\\ue03etoasinglepointinfeaturespace,h= [1 ,0]\\ue03e.\\nThelinearmodelcannowdescribethefunctionasincreasingin h1anddecreasingin h2.\\nInthisexample,themotivationforlearningthefeaturespaceisonlytomakethemodel\\ncapacitygreatersothatitcanﬁtthetrainingset.Inmorerealisticapplications,learned\\nrepresentationscanalsohelpthemodeltogeneralize.\\n1 7 3', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nyy\\nhh\\nx xWwyy\\nh 1 h 1\\nx 1 x 1h 2 h 2\\nx 2 x 2\\nFigure6.2:Anexampleofafeedforwardnetwork,drawnintwodiﬀerentstyles.Speciﬁcally,\\nthisisthefeedforwardnetworkweusetosolvetheXORexample.Ithasasinglehidden\\nlayercontainingtwounits. ( L e f t )Inthisstyle,wedraweveryunitasanodeinthegraph.\\nThisstyleisveryexplicitandunambiguousbutfornetworkslargerthanthisexample\\nitcanconsumetoomuchspace. Inthisstyle,wedrawanodeinthegraphfor ( R i g h t )\\neachentirevectorrepresentingalayer’sactivations.\\xa0Thisstyleismuchmorecompact.\\nSometimesweannotatetheedgesinthisgraphwiththenameoftheparametersthat\\ndescribetherelationshipbetweentwolayers.Here,weindicatethatamatrixWdescribes\\nthemappingfromxtoh,andavectorwdescribesthemappingfromhto y.We\\ntypicallyomittheinterceptparametersassociatedwitheachlayerwhenlabelingthiskind\\nofdrawing.\\nmodel,weusedavectorofweightsandascalarbiasparametertodescribean\\naﬃnetransformationfromaninputvectortoanoutputscalar.Now,wedescribe\\nanaﬃnetransformationfromavectorxtoavectorh,soanentirevectorofbias\\nparametersisneeded.Theactivationfunction gistypicallychosentobeafunction\\nthatisappliedelement-wise,with h i= g(x\\ue03eW : , i+ c i).Inmodernneuralnetworks,\\nthedefaultrecommendation istousetherectiﬁedlinearunitorReLU(Jarrett\\ne t a l . e t a l . ,; ,; 2009NairandHinton2010Glorot,)deﬁnedbytheactivation 2011a\\nfunction depictedinﬁgure. g z , z () = max0{} 6.3\\nWecannowspecifyourcompletenetworkas\\nf , , , b (;xWcw) = w\\ue03emax0{ ,W\\ue03exc+}+ b . (6.3)\\nWecannowspecifyasolutiontotheXORproblem.Let\\nW=\\ue01411\\n11\\ue015\\n, (6.4)\\nc=\\ue014\\n0\\n−1\\ue015\\n, (6.5)\\n1 7 4', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\n0\\nz0g z ( ) = m a x 0{ , z}\\nFigure6.3:Therectiﬁedlinearactivationfunction.Thisactivationfunctionisthedefault\\nactivationfunctionrecommendedforusewithmostfeedforwardneuralnetworks.Applying\\nthisfunctiontotheoutputofalineartransformationyieldsanonlineartransformation.\\nHowever,thefunctionremainsveryclosetolinear,inthesensethatisapiecewiselinear\\nfunctionwithtwolinearpieces.Becauserectiﬁedlinearunitsarenearlylinear,they\\npreservemanyofthepropertiesthatmakelinearmodelseasytooptimizewithgradient-\\nbasedmethods.Theyalsopreservemanyofthepropertiesthatmakelinearmodels\\ngeneralizewell.Acommonprinciplethroughoutcomputerscienceisthatwecanbuild\\ncomplicatedsystemsfromminimalcomponents.\\xa0MuchasaTuringmachine’smemory\\nneedsonlytobeabletostore0or1states,wecanbuildauniversalfunctionapproximator\\nfromrectiﬁedlinearfunctions.\\n1 7 5', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nw=\\ue0141\\n−2\\ue015\\n, (6.6)\\nand. b= 0\\nWecannowwalkthroughthewaythatthemodelprocessesabatchofinputs.\\nLetXbethedesignmatrixcontainingallfourpointsinthebinaryinputspace,\\nwithoneexampleperrow:\\nX=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f000\\n01\\n10\\n11\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb. (6.7)\\nTheﬁrststepintheneuralnetworkistomultiplytheinputmatrixbytheﬁrst\\nlayer’sweightmatrix:\\nXW=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f000\\n11\\n11\\n22\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb. (6.8)\\nNext,weaddthebiasvector,toobtainc\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f00 1−\\n10\\n10\\n21\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb. (6.9)\\nInthisspace,alloftheexamplesliealongalinewithslope.Aswemovealong 1\\nthisline,theoutputneedstobeginat,thenriseto,thendropbackdownto. 0 1 0\\nAlinearmodelcannotimplementsuchafunction.Toﬁnishcomputingthevalue\\nofforeachexample,weapplytherectiﬁedlineartransformation: h\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f000\\n10\\n10\\n21\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb. (6.10)\\nThistransformationhaschangedtherelationshipbetweentheexamples.Theyno\\nlongerlieonasingleline.Asshowninﬁgure,theynowlieinaspacewherea 6.1\\nlinearmodelcansolvetheproblem.\\nWeﬁnishbymultiplyingbytheweightvector:w\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f00\\n1\\n1\\n0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb. (6.11)\\n1 7 6', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nTheneuralnetworkhasobtainedthecorrectanswerforeveryexampleinthebatch.\\nInthisexample,wesimplyspeciﬁedthesolution,thenshowedthatitobtained\\nzeroerror.\\xa0Inarealsituation,theremightbebillionsofmodelparametersand\\nbillionsoftrainingexamples,soonecannotsimplyguessthesolutionaswedid\\nhere.Instead,agradient-basedoptimization algorithmcanﬁndparametersthat\\nproduceverylittleerror.ThesolutionwedescribedtotheXORproblemisata\\nglobalminimumofthelossfunction,sogradientdescentcouldconvergetothis\\npoint.ThereareotherequivalentsolutionstotheXORproblemthatgradient\\ndescentcouldalsoﬁnd.Theconvergencepointofgradientdescentdependsonthe\\ninitialvaluesoftheparameters.Inpractice,gradientdescentwouldusuallynot\\nﬁndclean,easilyunderstood,integer-valuedsolutionsliketheonewepresented\\nhere.\\n6. 2 Gradi en t - Bas e d L earni n g\\nDesigningandtraininganeuralnetworkisnotmuchdiﬀerentfromtrainingany\\nothermachinelearningmodelwithgradientdescent.Insection,wedescribed 5.10\\nhowtobuildamachinelearningalgorithmbyspecifyinganoptimizationprocedure,\\nacostfunction,andamodelfamily.\\nThelargestdiﬀerencebetweenthelinearmodelswehaveseensofarandneural\\nnetworksisthatthenonlinearityofaneuralnetworkcausesmostinterestingloss\\nfunctionstobecomenon-convex.Thismeansthatneuralnetworksareusually\\ntrainedbyusingiterative,gradient-basedoptimizersthatmerelydrivethecost\\nfunctiontoaverylowvalue,ratherthanthelinearequationsolversusedtotrain\\nlinearregressionmodelsortheconvexoptimization algorithmswithglobalconver-\\ngenceguaranteesusedtotrainlogisticregressionorSVMs.Convexoptimization\\nconvergesstartingfromanyinitialparameters(intheory—inpracticeitisvery\\nrobustbutcanencounternumericalproblems).Stochasticgradientdescentapplied\\ntonon-convexlossfunctionshasnosuchconvergenceguarantee,andissensitive\\ntothevaluesoftheinitialparameters.Forfeedforwardneuralnetworks,itis\\nimportanttoinitializeallweightstosmallrandomvalues.Thebiasesmaybe\\ninitializedtozeroortosmallpositivevalues.Theiterativegradient-basedopti-\\nmizationalgorithmsusedtotrainfeedforwardnetworksandalmostallotherdeep\\nmodelswillbedescribedindetailinchapter,withparameterinitialization in 8\\nparticulardiscussedinsection.Forthemoment,itsuﬃcestounderstandthat 8.4\\nthetrainingalgorithmisalmostalwaysbasedonusingthegradienttodescendthe\\ncostfunctioninonewayoranother.\\xa0The speciﬁcalgorithmsareimprovements\\nandreﬁnementsontheideasofgradientdescent,introducedinsection,and,4.3\\n1 7 7', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nmorespeciﬁcally,aremostoftenimprovementsofthestochasticgradientdescent\\nalgorithm,introducedinsection.5.9\\nWecanofcourse,trainmodelssuchaslinearregressionandsupportvector\\nmachineswithgradientdescenttoo,andinfactthisiscommonwhenthetraining\\nsetisextremelylarge.Fromthispointofview,traininganeuralnetworkisnot\\nmuchdiﬀerentfromtraininganyothermodel.Computingthegradientisslightly\\nmorecomplicatedforaneuralnetwork,butcanstillbedoneeﬃcientlyandexactly.\\nSectionwilldescribehowtoobtainthegradientusingtheback-propagation 6.5\\nalgorithmandmoderngeneralizations oftheback-propagationalgorithm.\\nAswithothermachinelearningmodels,toapplygradient-basedlearningwe\\nmustchooseacostfunction,andwemustchoosehowtorepresenttheoutputof\\nthemodel.Wenowrevisitthesedesignconsiderationswithspecialemphasison\\ntheneuralnetworksscenario.\\n6.2.1CostFunctions\\nAnimportantaspectofthedesignofadeepneuralnetworkisthechoiceofthe\\ncostfunction.Fortunately,thecostfunctionsforneuralnetworksaremoreorless\\nthesameasthoseforotherparametricmodels,suchaslinearmodels.\\nInmostcases,ourparametricmodeldeﬁnesadistribution p(yx|;θ)and\\nwesimplyuse\\xa0theprinciple\\xa0ofmaximumlikelihood.Thismeansweusethe\\ncross-entropybetweenthetrainingdataandthemodel’spredictionsasthecost\\nfunction.\\nSometimes,wetakeasimplerapproach,whereratherthanpredictingacomplete\\nprobabilitydistributionovery,wemerelypredictsomestatisticofyconditioned\\non.Specializedlossfunctionsallowustotrainapredictoroftheseestimates. x\\nThetotalcostfunctionusedtotrainaneuralnetworkwilloftencombineone\\noftheprimarycostfunctionsdescribedherewitharegularizationterm.Wehave\\nalreadyseensomesimpleexamplesofregularizationappliedtolinearmodelsin\\nsection.Theweightdecayapproachusedforlinearmodelsisalsodirectly 5.2.2\\napplicabletodeepneuralnetworksandisamongthemostpopularregularization\\nstrategies.Moreadvancedregularizationstrategiesforneuralnetworkswillbe\\ndescribedinchapter.7\\n6.2.1.1LearningConditionalDistributionswithMaximumLikelihood\\nMostmodernneuralnetworksaretrainedusingmaximumlikelihood.Thismeans\\nthatthecostfunctionissimplythenegativelog-likelihood,equivalentlydescribed\\n1 7 8', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nasthecross-entropybetweenthetrainingdataandthemodeldistribution.This\\ncostfunctionisgivenby\\nJ() = θ − E x y ,∼ ˆ pdatalog p m o de l( )yx| . (6.12)\\nThespeciﬁcformofthecostfunctionchangesfrommodeltomodel,depending\\nonthespeciﬁcformoflog p m o de l.Theexpansionoftheaboveequationtypically\\nyieldssometermsthatdonotdependonthemodelparametersandmaybedis-\\ncarded.Forexample,aswesawinsection,if5.5.1 p m o de l(yx|) =N(y; f(x;θ) ,I),\\nthenwerecoverthemeansquarederrorcost,\\nJ θ() =1\\n2E x y ,∼ ˆ pdata||− ||y f(;)xθ2+const , (6.13)\\nuptoascalingfactorof1\\n2andatermthatdoesnotdependon.Thediscardedθ\\nconstantisbasedonthevarianceoftheGaussiandistribution,whichinthiscase\\nwechosenottoparametrize. Previously,wesawthattheequivalencebetween\\nmaximumlikelihoodestimationwithanoutputdistributionandminimization of\\nmeansquarederrorholdsforalinearmodel,butinfact,theequivalenceholds\\nregardlessoftheusedtopredictthemeanoftheGaussian. f(;)xθ\\nAnadvantageofthisapproachofderivingthecostfunctionfrommaximum\\nlikelihoodisthatitremovestheburdenofdesigningcostfunctionsforeachmodel.\\nSpecifyingamodel p(yx|)automatically determinesacostfunction log p(yx|).\\nOnerecurringthemethroughoutneuralnetworkdesignisthatthegradientof\\nthecostfunctionmustbelargeandpredictableenoughtoserveasagoodguide\\nforthelearningalgorithm.Functionsthatsaturate(becomeveryﬂat)undermine\\nthisobjectivebecausetheymakethegradientbecomeverysmall.Inmanycases\\nthishappensbecausetheactivationfunctionsusedtoproducetheoutputofthe\\nhiddenunitsortheoutputunitssaturate.\\xa0Thenegativelog-likelihoodhelpsto\\navoidthisproblemformanymodels.Manyoutputunitsinvolveanexpfunction\\nthatcansaturatewhenitsargumentisverynegative.The logfunctioninthe\\nnegativelog-likelihoodcostfunctionundoestheexpofsomeoutputunits.Wewill\\ndiscusstheinteractionbetweenthecostfunctionandthechoiceofoutputunitin\\nsection.6.2.2\\nOneunusualpropertyofthecross-entropycostusedtoperformmaximum\\nlikelihoodestimationisthatitusuallydoesnothaveaminimumvaluewhenapplied\\ntothemodelscommonlyusedinpractice.Fordiscreteoutputvariables,most\\nmodelsareparametrized insuchawaythattheycannotrepresentaprobability\\nofzeroorone,butcancomearbitrarilyclosetodoingso.Logisticregression\\nisanexampleofsuchamodel.Forreal-valuedoutputvariables,ifthemodel\\n1 7 9', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\ncancontrolthedensityoftheoutputdistribution(forexample,bylearningthe\\nvarianceparameterofaGaussianoutputdistribution)thenitbecomespossible\\ntoassignextremelyhighdensitytothecorrecttrainingsetoutputs,resultingin\\ncross-entropyapproachingnegativeinﬁnity.Regularizationtechniquesdescribed\\ninchapterprovideseveraldiﬀerentwaysofmodifyingthelearningproblemso 7\\nthatthemodelcannotreapunlimitedrewardinthisway.\\n6.2.1.2LearningConditionalStatistics\\nInsteadoflearningafullprobabilitydistribution p(yx|;θ)weoftenwanttolearn\\njustoneconditionalstatisticofgiven.yx\\nForexample,wemayhaveapredictor f(x;θ) thatwewishtopredictthemean\\nof.y\\nIfweuseasuﬃcientlypowerfulneuralnetwork,wecanthinkoftheneural\\nnetworkasbeingabletorepresentanyfunction ffromawideclassoffunctions,\\nwiththisclassbeinglimitedonlybyfeaturessuchascontinuityandboundedness\\nratherthanbyhavingaspeciﬁcparametricform.Fromthispointofview,we\\ncanviewthecostfunctionasbeingafunctionalratherthanjustafunction.A\\nfunctionalisamappingfromfunctionstorealnumbers.Wecanthusthinkof\\nlearningaschoosingafunctionratherthanmerelychoosingasetofparameters.\\nWecandesignourcostfunctionaltohaveitsminimumoccuratsomespeciﬁc\\nfunctionwedesire.Forexample,wecandesignthecostfunctionaltohaveits\\nminimumlieonthefunctionthatmapsxtotheexpectedvalueofygivenx.\\nSolvinganoptimizationproblemwithrespecttoafunctionrequiresamathematical\\ntoolcalledcalculusofvariations,describedinsection.Itisnotnecessary 19.4.2\\ntounderstandcalculusofvariationstounderstandthecontentofthischapter.At\\nthemoment,itisonlynecessarytounderstandthatcalculusofvariationsmaybe\\nusedtoderivethefollowingtworesults.\\nOurﬁrstresultderivedusingcalculusofvariationsisthatsolvingtheoptimiza-\\ntionproblem\\nf∗= argmin\\nfE x y ,∼ pdata||− ||y f()x2(6.14)\\nyields\\nf∗() = x E y∼ pdata ( ) y x|[]y , (6.15)\\nsolongasthisfunctionlieswithintheclassweoptimizeover.Inotherwords,ifwe\\ncouldtrainoninﬁnitelymanysamplesfromthetruedatageneratingdistribution,\\nminimizingthemeansquarederrorcostfunctiongivesafunctionthatpredictsthe\\nmeanofforeachvalueof. y x\\n1 8 0', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nDiﬀerentcostfunctionsgivediﬀerentstatistics.Asecondresultderivedusing\\ncalculusofvariationsisthat\\nf∗= argmin\\nfE x y ,∼ pdata||− ||y f()x 1 (6.16)\\nyieldsafunctionthatpredictsthe m e d i a nvalueofyforeachx,solongassucha\\nfunctionmaybedescribedbythefamilyoffunctionsweoptimizeover.Thiscost\\nfunctioniscommonlycalled . meanabsoluteerror\\nUnfortunately,meansquarederrorandmeanabsoluteerroroftenleadtopoor\\nresultswhenusedwithgradient-basedoptimization. Someoutputunitsthat\\nsaturateproduceverysmallgradientswhencombinedwiththesecostfunctions.\\nThisisonereasonthatthecross-entropycostfunctionismorepopularthanmean\\nsquarederrorormeanabsoluteerror,evenwhenitisnotnecessarytoestimatean\\nentiredistribution. p( )yx|\\n6.2.2OutputUnits\\nThechoiceofcostfunctionistightlycoupledwiththechoiceofoutputunit.Most\\nofthetime,wesimplyusethecross-entropybetweenthedatadistributionandthe\\nmodeldistribution.\\xa0Thechoiceofhowtorepresenttheoutputthendetermines\\ntheformofthecross-entropyfunction.\\nAnykindofneuralnetworkunitthatmaybeusedasanoutputcanalsobe\\nusedasahiddenunit.Here,wefocusontheuseoftheseunitsasoutputsofthe\\nmodel,butinprincipletheycanbeusedinternallyaswell.Werevisittheseunits\\nwithadditionaldetailabouttheiruseashiddenunitsinsection.6.3\\nThroughoutthissection,wesupposethatthefeedforwardnetworkprovidesa\\nsetofhiddenfeaturesdeﬁnedbyh= f(x;θ).Theroleoftheoutputlayeristhen\\ntoprovidesomeadditionaltransformationfromthefeaturestocompletethetask\\nthatthenetworkmustperform.\\n6.2.2.1LinearUnitsforGaussianOutputDistributions\\nOnesimplekindofoutputunitisanoutputunitbasedonanaﬃnetransformation\\nwithnononlinearity.Theseareoftenjustcalledlinearunits.\\nGivenfeaturesh,alayeroflinearoutputunitsproducesavectorˆy=W\\ue03eh+b.\\nLinearoutputlayersareoftenusedtoproducethemeanofaconditional\\nGaussiandistribution:\\np( ) = (;yx| NyˆyI ,) . (6.17)\\n1 8 1', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nMaximizingthelog-likelihoodisthenequivalenttominimizingthemeansquared\\nerror.\\nThemaximumlikelihoodframeworkmakesitstraightforwardtolearnthe\\ncovarianceoftheGaussiantoo,ortomakethecovarianceoftheGaussianbea\\nfunctionoftheinput.However,thecovariancemustbeconstrainedtobeapositive\\ndeﬁnitematrixforallinputs.Itisdiﬃculttosatisfysuchconstraintswithalinear\\noutputlayer,sotypicallyotheroutputunitsareusedtoparametrizethecovariance.\\nApproachestomodelingthecovariancearedescribedshortly,insection.6.2.2.4\\nBecauselinearunitsdonotsaturate,theyposelittlediﬃcultyforgradient-\\nbasedoptimizationalgorithmsandmaybeusedwithawidevarietyofoptimization\\nalgorithms.\\n6.2.2.2SigmoidUnitsforBernoulliOutputDistributions\\nManytasksrequirepredictingthevalueofabinaryvariable y.Classiﬁcation\\nproblemswithtwoclassescanbecastinthisform.\\nThemaximum-likelihoodapproachistodeﬁneaBernoullidistributionover y\\nconditionedon.x\\nABernoullidistributionisdeﬁnedbyjustasinglenumber.Theneuralnet\\nneedstopredictonly P( y= 1|x).Forthisnumbertobeavalidprobability,it\\nmustlieintheinterval[0,1].\\nSatisfyingthisconstraintrequiressomecarefuldesigneﬀort.Supposewewere\\ntousealinearunit,andthresholditsvaluetoobtainavalidprobability:\\nP y(= 1 ) = max |x\\ue06e\\n0min ,\\ue06e\\n1 ,w\\ue03eh+ b\\ue06f\\ue06f\\n.(6.18)\\nThiswouldindeeddeﬁneavalidconditionaldistribution,butwewouldnotbeable\\ntotrainitveryeﬀectivelywithgradientdescent.Anytimethatw\\ue03eh+ bstrayed\\noutsidetheunitinterval,thegradientoftheoutputofthemodelwithrespectto\\nitsparameterswouldbe 0.Agradientof 0istypicallyproblematicbecausethe\\nlearningalgorithmnolongerhasaguideforhowtoimprovethecorresponding\\nparameters.\\nInstead,itisbettertouseadiﬀerentapproachthatensuresthereisalwaysa\\nstronggradientwheneverthemodelhasthewronganswer.Thisapproachisbased\\nonusingsigmoidoutputunitscombinedwithmaximumlikelihood.\\nAsigmoidoutputunitisdeﬁnedby\\nˆ y σ= \\ue010\\nw\\ue03eh+ b\\ue011\\n(6.19)\\n1 8 2', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nwhereisthelogisticsigmoidfunctiondescribedinsection. σ 3.10\\nWecanthinkofthesigmoidoutputunitashavingtwocomponents.First,it\\nusesalinearlayertocompute z=w\\ue03eh+ b.Next,itusesthesigmoidactivation\\nfunctiontoconvertintoaprobability. z\\nWeomitthedependenceonxforthemomenttodiscusshowtodeﬁnea\\nprobabilitydistributionover yusingthevalue z.Thesigmoidcanbemotivated\\nbyconstructinganunnormalized probabilitydistribution˜ P( y),whichdoesnot\\nsumto1.Wecanthendividebyanappropriateconstanttoobtainavalid\\nprobabilitydistribution.Ifwebeginwiththeassumptionthattheunnormalized log\\nprobabilitiesarelinearin yand z,wecanexponentiatetoobtaintheunnormalized\\nprobabilities. WethennormalizetoseethatthisyieldsaBernoullidistribution\\ncontrolledbyasigmoidaltransformationof: z\\nlog˜ P y y z () = (6.20)\\n˜ P y y z () = exp() (6.21)\\nP y() =exp() y z\\ue0501\\ny\\ue030= 0exp( y\\ue030z)(6.22)\\nP y σ y z . () = ((2−1)) (6.23)\\nProbabilitydistributionsbasedonexponentiationandnormalization arecommon\\nthroughoutthestatisticalmodelingliterature.The zvariabledeﬁningsucha\\ndistributionoverbinaryvariablesiscalleda.logit\\nThisapproachtopredictingtheprobabilities inlog-spaceisnaturaltouse\\nwithmaximumlikelihoodlearning.Becausethecostfunctionusedwithmaximum\\nlikelihoodis−log P( y|x),theloginthecostfunctionundoestheexpofthe\\nsigmoid.Withoutthiseﬀect,thesaturationofthesigmoidcouldpreventgradient-\\nbased\\xa0learningfrom\\xa0makinggoodprogress.Theloss\\xa0functionfor\\xa0maximum\\nlikelihoodlearningofaBernoulliparametrized byasigmoidis\\nJ P y () = logθ − (|x) (6.24)\\n= log((2 1)) − σ y− z (6.25)\\n= ((12)) ζ − y z . (6.26)\\nThisderivationmakesuseofsomepropertiesfromsection.Byrewriting3.10\\nthelossintermsofthesoftplusfunction,wecanseethatitsaturatesonlywhen\\n(1−2 y) zisverynegative.Saturationthusoccursonlywhenthemodelalready\\nhastherightanswer—when y= 1and zisverypositive,or y= 0and zisvery\\nnegative.When zhasthewrongsign,theargumenttothesoftplusfunction,\\n1 8 3', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\n(1−2 y) z,maybesimpliﬁedto|| z.As|| zbecomeslargewhile zhasthewrongsign,\\nthesoftplusfunctionasymptotestowardsimplyreturningitsargument || z.The\\nderivativewithrespectto zasymptotestosign( z),so,inthelimitofextremely\\nincorrect z,thesoftplusfunctiondoesnotshrinkthegradientatall.Thisproperty\\nisveryusefulbecauseitmeansthatgradient-basedlearningcanacttoquickly\\ncorrectamistaken. z\\nWhenweuseotherlossfunctions,suchasmeansquarederror,thelosscan\\nsaturateanytime σ( z)saturates.Thesigmoidactivationfunctionsaturatesto0\\nwhen zbecomesverynegativeandsaturatestowhen1 zbecomesverypositive.\\nThegradientcanshrinktoosmalltobeusefulforlearningwheneverthishappens,\\nwhetherthemodelhasthecorrectanswerortheincorrectanswer.Forthisreason,\\nmaximumlikelihoodisalmostalwaysthepreferredapproachtotrainingsigmoid\\noutputunits.\\nAnalytically,thelogarithmofthesigmoidisalwaysdeﬁnedandﬁnite,because\\nthesigmoidreturnsvaluesrestrictedtotheopeninterval(0 ,1),ratherthanusing\\ntheentireclosedintervalofvalidprobabilities [0 ,1].Insoftwareimplementations,\\ntoavoidnumericalproblems,itisbesttowritethenegativelog-likelihoodasa\\nfunctionof z,ratherthanasafunctionofˆ y= σ( z).Ifthesigmoidfunction\\nunderﬂowstozero,thentakingthelogarithmofˆ yyieldsnegativeinﬁnity.\\n6.2.2.3SoftmaxUnitsforMultinoulliOutputDistributions\\nAnytimewewishtorepresentaprobabilitydistributionoveradiscretevariable\\nwith npossiblevalues,wemayusethesoftmaxfunction.Thiscanbeseenasa\\ngeneralization ofthesigmoidfunctionwhichwasusedtorepresentaprobability\\ndistributionoverabinaryvariable.\\nSoftmaxfunctionsaremostoftenusedastheoutputofaclassiﬁer,torepresent\\ntheprobabilitydistributionover ndiﬀerentclasses.Morerarely,softmaxfunctions\\ncanbeusedinsidethemodelitself,ifwewishthemodeltochoosebetweenoneof\\nndiﬀerentoptionsforsomeinternalvariable.\\nInthecaseofbinaryvariables,wewishedtoproduceasinglenumber\\nˆ y P y . = (= 1 )|x (6.27)\\nBecausethisnumberneededtoliebetweenand,andbecausewewantedthe 0 1\\nlogarithmofthenumbertobewell-behavedforgradient-basedoptimization of\\nthelog-likelihood,wechosetoinsteadpredictanumber z=log˜ P( y=1|x).\\nExponentiatingandnormalizinggaveusaBernoullidistributioncontrolledbythe\\nsigmoidfunction.\\n1 8 4', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nTogeneralizetothecaseofadiscretevariablewith nvalues,wenowneed\\ntoproduceavectorˆy,with ˆ y i= P( y= i|x).Werequirenotonlythateach\\nelementofˆ y ibebetweenand,butalsothattheentirevectorsumstosothat 0 1 1\\nitrepresentsavalidprobabilitydistribution.Thesameapproachthatworkedfor\\ntheBernoullidistributiongeneralizestothemultinoullidistribution.First,alinear\\nlayerpredictsunnormalized logprobabilities:\\nzW= \\ue03ehb+ , (6.28)\\nwhere z i=log˜ P( y= i|x) .Thesoftmaxfunctioncanthenexponentiateand\\nnormalizetoobtainthedesired z ˆy.Formally,thesoftmaxfunctionisgivenby\\nsoftmax()z i=exp( z i)\\ue050\\njexp( z j). (6.29)\\nAswiththelogisticsigmoid,theuseoftheexpfunctionworksverywellwhen\\ntrainingthesoftmaxtooutputatargetvalueyusingmaximumlog-likelihood.In\\nthiscase,wewishtomaximize log P(y= i;z)=logsoftmax(z) i.Deﬁningthe\\nsoftmaxintermsofexpisnaturalbecausetheloginthelog-likelihoodcanundo\\ntheofthesoftmax: exp\\nlogsoftmax()z i= z i−log\\ue058\\njexp( z j) . (6.30)\\nTheﬁrsttermofequationshowsthattheinput 6.30 z ialwayshasadirect\\ncontributiontothecostfunction.Becausethistermcannotsaturate,weknow\\nthatlearningcanproceed,evenifthecontributionof z itothesecondtermof\\nequationbecomesverysmall.Whenmaximizingthelog-likelihood,theﬁrst 6.30\\ntermencourages z itobepushedup,whilethesecondtermencouragesallofztobe\\npusheddown.Togainsomeintuitionforthesecondterm,log\\ue050\\njexp( z j),observe\\nthatthistermcanberoughlyapproximatedbymax j z j.Thisapproximation is\\nbasedontheideathatexp( z k) isinsigniﬁcantforany z kthatisnoticeablylessthan\\nmax j z j.Theintuitionwecangainfromthisapproximation isthatthenegative\\nlog-likelihoodcostfunctionalwaysstronglypenalizesthemostactiveincorrect\\nprediction.Ifthecorrectansweralreadyhasthelargestinputtothesoftmax,then\\nthe− z itermandthelog\\ue050\\njexp( z j)≈max j z j= z itermswillroughlycancel.\\nThisexamplewillthencontributelittletotheoveralltrainingcost,whichwillbe\\ndominatedbyotherexamplesthatarenotyetcorrectlyclassiﬁed.\\nSofarwehavediscussedonlyasingleexample.Overall,unregularized maximum\\nlikelihoodwilldrivethemodeltolearnparametersthatdrivethesoftmaxtopredict\\n1 8 5', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nthefractionofcountsofeachoutcomeobservedinthetrainingset:\\nsoftmax((;))zxθ i≈\\ue050m\\nj = 1 1y() j= i , x() j= x\\ue050m\\nj = 1 1x() j = x. (6.31)\\nBecausemaximumlikelihoodisaconsistentestimator,thisisguaranteedtohappen\\nsolongasthemodelfamilyiscapableofrepresentingthetrainingdistribution.In\\npractice,limitedmodelcapacityandimperfectoptimization willmeanthatthe\\nmodelisonlyabletoapproximatethesefractions.\\nManyobjectivefunctionsotherthanthelog-likelihooddonotworkaswell\\nwiththesoftmaxfunction.Speciﬁcally,objectivefunctionsthatdonotusealogto\\nundotheexpofthesoftmaxfailtolearnwhentheargumenttotheexpbecomes\\nverynegative,causingthegradienttovanish.Inparticular,squarederrorisa\\npoorlossfunctionforsoftmaxunits,andcanfailtotrainthemodeltochangeits\\noutput,evenwhenthemodelmakeshighlyconﬁdentincorrectpredictions(,Bridle\\n1990).Tounderstandwhytheseotherlossfunctionscanfail,weneedtoexamine\\nthesoftmaxfunctionitself.\\nLikethesigmoid,thesoftmaxactivationcansaturate.Thesigmoidfunctionhas\\nasingleoutputthatsaturateswhenitsinputisextremelynegativeorextremely\\npositive.Inthecaseofthesoftmax,therearemultipleoutputvalues.These\\noutputvaluescansaturatewhenthediﬀerencesbetweeninputvaluesbecome\\nextreme.Whenthesoftmaxsaturates,manycostfunctionsbasedonthesoftmax\\nalsosaturate,unlesstheyareabletoinvertthesaturatingactivatingfunction.\\nToseethatthesoftmaxfunctionrespondstothediﬀerencebetweenitsinputs,\\nobservethatthesoftmaxoutputisinvarianttoaddingthesamescalartoallofits\\ninputs:\\nsoftmax() = softmax(+) zz c . (6.32)\\nUsingthisproperty,wecanderiveanumericallystablevariantofthesoftmax:\\nsoftmax() = softmax( max zz−\\niz i) . (6.33)\\nThereformulatedversionallowsustoevaluatesoftmaxwithonlysmallnumerical\\nerrorsevenwhen zcontainsextremelylargeorextremelynegativenumbers.Ex-\\naminingthenumericallystablevariant,weseethatthesoftmaxfunctionisdriven\\nbytheamountthatitsargumentsdeviatefrommax i z i.\\nAnoutput softmax(z) isaturatestowhenthecorrespondinginputismaximal 1\\n( z i=max i z i)and z iismuchgreaterthanalloftheotherinputs.Theoutput\\nsoftmax(z) icanalsosaturatetowhen0 z iisnotmaximalandthemaximumis\\nmuchgreater.Thisisageneralization ofthewaythatsigmoidunitssaturate,and\\n1 8 6', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\ncancausesimilardiﬃcultiesforlearningifthelossfunctionisnotdesignedto\\ncompensateforit.\\nTheargumentztothesoftmaxfunctioncanbeproducedintwodiﬀerentways.\\nThemostcommonissimplytohaveanearlierlayeroftheneuralnetworkoutput\\neveryelementofz,asdescribedaboveusingthelinearlayerz=W\\ue03eh+b.While\\nstraightforward,thisapproachactuallyoverparametrizes thedistribution.The\\nconstraintthatthe noutputsmustsumtomeansthatonly 1 n−1parametersare\\nnecessary;theprobabilityofthe n-thvaluemaybeobtainedbysubtractingthe\\nﬁrst n−1 1 probabilitiesfrom.Wecanthusimposearequirementthatoneelement\\nofzbeﬁxed.Forexample,wecanrequirethat z n=0.Indeed,thisisexactly\\nwhatthesigmoidunitdoes.Deﬁning P( y= 1|x) = σ( z)isequivalenttodeﬁning\\nP( y= 1|x) =softmax(z) 1withatwo-dimensionalzand z 1= 0.Boththe n−1\\nargumentandthe nargumentapproachestothesoftmaxcandescribethesame\\nsetofprobabilitydistributions,buthavediﬀerentlearningdynamics.Inpractice,\\nthereisrarelymuchdiﬀerencebetweenusingtheoverparametrized versionorthe\\nrestrictedversion,anditissimplertoimplementtheoverparametrized version.\\nFromaneuroscientiﬁcpointofview,itisinterestingtothinkofthesoftmaxas\\nawaytocreateaformofcompetitionbetweentheunitsthatparticipateinit:the\\nsoftmaxoutputsalwayssumto1soanincreaseinthevalueofoneunitnecessarily\\ncorrespondstoadecreaseinthevalueofothers.Thisisanalogoustothelateral\\ninhibitionthatisbelievedtoexistbetweennearbyneuronsinthecortex.Atthe\\nextreme(whenthediﬀerencebetweenthemaximal a iandtheothersislargein\\nmagnitude)itbecomesaformofwinner-take-all(oneoftheoutputsisnearly1\\nandtheothersarenearly0).\\nThename“softmax”canbesomewhatconfusing.Thefunctionismoreclosely\\nrelatedtotheargmaxfunctionthanthemaxfunction.\\xa0Theterm“soft”derives\\nfromthefactthatthesoftmaxfunctioniscontinuousanddiﬀerentiable. The\\nargmaxfunction,withitsresultrepresentedasaone-hotvector,isnotcontinuous\\nordiﬀerentiable. Thesoftmaxfunctionthusprovidesa“softened”versionofthe\\nargmax.Thecorrespondingsoftversionofthemaximumfunctionissoftmax(z)\\ue03ez.\\nItwouldperhapsbebettertocallthesoftmaxfunction“softargmax,”\\xa0butthe\\ncurrentnameisanentrenchedconvention.\\n6.2.2.4OtherOutputTypes\\nThelinear,\\xa0sigmoid,\\xa0andsoftmaxoutputunitsdescribedabovearethemost\\ncommon.Neuralnetworkscangeneralizetoalmostanykindofoutputlayerthat\\nwewish.Theprincipleofmaximumlikelihoodprovidesaguideforhowtodesign\\n1 8 7', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nagoodcostfunctionfornearlyanykindofoutputlayer.\\nIngeneral,ifwedeﬁneaconditionaldistribution p(yx|;θ),theprincipleof\\nmaximumlikelihoodsuggestsweuse asourcostfunction. − | log( pyxθ;)\\nIngeneral,wecanthinkoftheneuralnetworkasrepresentingafunction f(x;θ).\\nTheoutputsofthisfunctionarenotdirectpredictionsofthevaluey.Instead,\\nf(x;θ) =ωprovidestheparametersforadistributionover y.Ourlossfunction\\ncanthenbeinterpretedas . −log(;()) p yωx\\nForexample,wemaywishtolearnthevarianceofaconditionalGaussianfor y,\\ngiven x.Inthesimplecase,wherethevariance σ2isaconstant,thereisaclosed\\nformexpressionbecausethemaximumlikelihoodestimatorofvarianceissimplythe\\nempiricalmeanofthesquareddiﬀerencebetweenobservations yandtheirexpected\\nvalue.Acomputationally moreexpensiveapproachthatdoesnotrequirewriting\\nspecial-casecodeistosimplyincludethevarianceasoneofthepropertiesofthe\\ndistribution p( y|x)thatiscontrolledbyω= f(x;θ).Thenegativelog-likelihood\\n−log p(y;ω(x))willthenprovideacostfunctionwiththeappropriateterms\\nnecessarytomakeouroptimization procedureincrementally learnthevariance.In\\nthesimplecasewherethestandarddeviationdoesnotdependontheinput,we\\ncanmakeanewparameterinthenetworkthatiscopieddirectlyintoω.Thisnew\\nparametermightbe σitselforcouldbeaparameter vrepresenting σ2oritcould\\nbeaparameter βrepresenting1\\nσ2,dependingonhowwechoosetoparametrize\\nthedistribution.Wemaywishourmodeltopredictadiﬀerentamountofvariance\\nin yfordiﬀerentvaluesof x.Thisiscalledaheteroscedasticmodel.Inthe\\nheteroscedasticcase,wesimplymakethespeciﬁcationofthevariancebeoneof\\nthevaluesoutputby f( x;θ).AtypicalwaytodothisistoformulatetheGaussian\\ndistributionusingprecision,ratherthanvariance,asdescribedinequation.3.22\\nInthemultivariatecaseitismostcommontouseadiagonalprecisionmatrix\\ndiag (6.34) ()β .\\nThisformulationworkswellwithgradientdescentbecausetheformulaforthe\\nlog-likelihoodoftheGaussiandistributionparametrized byβinvolvesonlymul-\\ntiplicationby β iandadditionoflogβ i.Thegradientofmultiplication, addition,\\nandlogarithmoperationsiswell-behaved.Bycomparison,ifweparametrized the\\noutputintermsofvariance,wewouldneedtousedivision.Thedivisionfunction\\nbecomesarbitrarilysteepnearzero.Whilelargegradientscanhelplearning,\\narbitrarilylargegradientsusuallyresultininstability.Ifweparametrized the\\noutputintermsofstandarddeviation,thelog-likelihoodwouldstillinvolvedivision,\\nandwouldalsoinvolvesquaring.Thegradientthroughthesquaringoperation\\ncanvanishnearzero,makingitdiﬃculttolearnparametersthataresquared.\\n1 8 8', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nRegardlessofwhetherweusestandarddeviation,variance,orprecision,wemust\\nensurethatthecovariancematrixoftheGaussianispositivedeﬁnite.\\xa0Because\\ntheeigenvaluesoftheprecisionmatrixarethereciprocalsoftheeigenvaluesof\\nthecovariancematrix,thisisequivalenttoensuringthattheprecisionmatrixis\\npositivedeﬁnite.Ifweuseadiagonalmatrix,orascalartimesthediagonalmatrix,\\nthentheonlyconditionweneedtoenforceontheoutputofthemodelispositivity.\\nIfwesupposethataistherawactivationofthemodelusedtodeterminethe\\ndiagonalprecision,wecanusethesoftplusfunctiontoobtainapositiveprecision\\nvector:β= ζ(a) .Thissamestrategyappliesequallyifusingvarianceorstandard\\ndeviationratherthanprecisionorifusingascalartimesidentityratherthan\\ndiagonalmatrix.\\nItisraretolearnacovarianceorprecisionmatrixwithricherstructurethan\\ndiagonal.\\xa0Ifthecovarianceisfullandconditional,thenaparametrization must\\nbechosenthatguaranteespositive-deﬁnitenessofthepredictedcovariancematrix.\\nThiscanbeachievedbywriting Σ() = ()xBxB\\ue03e()x,whereBisanunconstrained\\nsquarematrix.Onepracticalissueifthematrixisfullrankisthatcomputingthe\\nlikelihoodisexpensive,witha d d×matrixrequiring O( d3)computationforthe\\ndeterminantandinverseof Σ(x)(orequivalently,andmorecommonlydone,its\\neigendecompositionorthatof).Bx()\\nWeoftenwanttoperformmultimodalregression,thatis,topredictrealvalues\\nthatcomefromaconditionaldistribution p(yx|)thatcanhaveseveraldiﬀerent\\npeaksinyspaceforthesamevalueofx.Inthiscase,aGaussianmixtureis\\nanaturalrepresentationfortheoutput( ,;,). Jacobs e t a l .1991Bishop1994\\nNeuralnetworkswithGaussianmixturesastheiroutputareoftencalledmixture\\ndensitynetworks.AGaussianmixtureoutputwith ncomponentsisdeﬁnedby\\ntheconditionalprobabilitydistribution\\np( ) =yx|n\\ue058\\ni = 1p i (= c |Nx)(;yµ( ) i()x , Σ( ) i())x .(6.35)\\nTheneuralnetworkmusthavethreeoutputs:avectordeﬁning p(c= i|x),a\\nmatrixprovidingµ( ) i(x)forall i,andatensorproviding Σ( ) i(x)forall i.These\\noutputsmustsatisfydiﬀerentconstraints:\\n1.Mixturecomponents p(c= i|x):theseformamultinoullidistribution\\noverthe ndiﬀerentcomponentsassociatedwithlatentvariable1c,andcan\\n1W e c o n s i d e r c t o b e l a t e n t b e c a u s e we d o n o t o b s e rv e i t i n t h e d a t a : g i v e n i n p u t x a n d t a rg e t\\ny , i t i s n o t p o s s i b l e t o k n o w with c e rta i n t y wh i c h Ga u s s i a n c o m p o n e n t wa s re s p o n s i b l e f o r y , b u t\\nw e c a n i m a g i n e t h a t y w a s g e n e ra t e d b y p i c k i n g o n e o f t h e m , a n d m a k e t h a t u n o b s e rv e d c h o i c e a\\nra n d o m v a ria b l e .\\n1 8 9', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\ntypicallybeobtainedbyasoftmaxoveran n-dimensionalvector,toguarantee\\nthattheseoutputsarepositiveandsumto1.\\n2.Meansµ( ) i(x):theseindicatethecenterormeanassociatedwiththe i-th\\nGaussiancomponent,andareunconstrained(typicallywithnononlinearity\\natallfortheseoutputunits).If yisa d-vector,thenthenetworkmustoutput\\nan n d×matrixcontainingall nofthese d-dimensionalvectors.\\xa0Learning\\nthesemeanswithmaximumlikelihoodisslightlymorecomplicatedthan\\nlearningthemeansofadistributionwithonlyoneoutputmode.Weonly\\nwanttoupdatethemeanforthecomponentthatactuallyproducedthe\\nobservation.Inpractice,wedonotknowwhichcomponentproducedeach\\nobservation.Theexpressionforthenegativelog-likelihoodnaturallyweights\\neachexample’scontributiontothelossforeachcomponentbytheprobability\\nthatthecomponentproducedtheexample.\\n3.Covariances Σ( ) i(x):thesespecifythecovariancematrixforeachcomponent\\ni.AswhenlearningasingleGaussiancomponent,wetypicallyuseadiagonal\\nmatrixtoavoidneedingtocomputedeterminants. Aswithlearningthemeans\\nofthemixture,maximumlikelihoodiscomplicatedbyneedingtoassign\\npartialresponsibilityforeachpointtoeachmixturecomponent.Gradient\\ndescentwillautomatically followthecorrectprocessifgiventhecorrect\\nspeciﬁcationofthenegativelog-likelihoodunderthemixturemodel.\\nIthasbeenreportedthatgradient-basedoptimization ofconditionalGaussian\\nmixtures(ontheoutputofneuralnetworks)canbeunreliable,inpartbecauseone\\ngetsdivisions(bythevariance)whichcanbenumericallyunstable(whensome\\nvariancegetstobesmallforaparticularexample,yieldingverylargegradients).\\nOnesolutionistoclipgradients(seesection)whileanotheristoscale 10.11.1\\nthegradientsheuristically( ,). MurrayandLarochelle2014\\nGaussianmixtureoutputsareparticularlyeﬀectiveingenerativemodelsof\\nspeech(Schuster1999,)ormovementsofphysicalobjects(Graves2013,).The\\nmixturedensitystrategygivesawayforthenetworktorepresentmultipleoutput\\nmodesandtocontrolthevarianceofitsoutput,whichiscrucialforobtaining\\nahighdegreeofqualityinthesereal-valueddomains.Anexampleofamixture\\ndensitynetworkisshowninﬁgure.6.4\\nIngeneral,wemaywishtocontinuetomodellargervectorsycontainingmore\\nvariables,andtoimposericherandricherstructuresontheseoutputvariables.For\\nexample,wemaywishforourneuralnetworktooutputasequenceofcharacters\\nthatformsasentence.Inthese\\xa0cases,wemaycontinuetousetheprinciple\\nofmaximumlikelihoodappliedtoourmodel p(y;ω(x)),butthemodelweuse\\n1 9 0', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nxy\\nFigure6.4:Samplesdrawnfromaneuralnetworkwithamixturedensityoutputlayer.\\nTheinput xissampledfromauniformdistributionandtheoutput yissampledfrom\\np m o d e l( y x|).Theneuralnetworkisabletolearnnonlinearmappingsfromtheinputto\\ntheparametersoftheoutputdistribution.Theseparametersincludetheprobabilities\\ngoverningwhichofthreemixturecomponentswillgeneratetheoutputaswellasthe\\nparametersforeachmixturecomponent.EachmixturecomponentisGaussianwith\\npredictedmeanandvariance.Alloftheseaspectsoftheoutputdistributionareableto\\nvarywithrespecttotheinput,andtodosoinnonlinearways. x\\ntodescribeybecomescomplexenoughtobebeyondthescopeofthischapter.\\nChapterdescribeshowtouserecurrentneuralnetworkstodeﬁnesuchmodels 10\\noversequences,andpartdescribesadvancedtechniquesformodelingarbitrary III\\nprobabilitydistributions.\\n6. 3 Hi d d en Un i t s\\nSofarwehavefocusedourdiscussionondesignchoicesforneuralnetworksthat\\narecommontomostparametricmachinelearningmodelstrainedwithgradient-\\nbasedoptimization. Nowweturntoanissuethatisuniquetofeedforwardneural\\nnetworks:howtochoosethetypeofhiddenunittouseinthehiddenlayersofthe\\nmodel.\\nThedesignofhiddenunitsisanextremelyactiveareaofresearchanddoesnot\\nyethavemanydeﬁnitiveguidingtheoreticalprinciples.\\nRectiﬁedlinearunitsareanexcellentdefaultchoiceofhiddenunit.Manyother\\ntypesofhiddenunitsareavailable.Itcanbediﬃculttodeterminewhentouse\\nwhichkind(thoughrectiﬁedlinearunitsareusuallyanacceptablechoice).\\xa0We\\n1 9 1', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\ndescribeheresomeofthebasicintuitionsmotivatingeachtypeofhiddenunits.\\nTheseintuitionscanhelpdecidewhentotryouteachoftheseunits.Itisusually\\nimpossibletopredictinadvancewhichwillworkbest.Thedesignprocessconsists\\noftrialanderror,intuitingthatakindofhiddenunitmayworkwell,andthen\\ntraininganetworkwiththatkindofhiddenunitandevaluatingitsperformance\\nonavalidationset.\\nSomeofthehiddenunitsincludedinthislistarenotactuallydiﬀerentiableat\\nallinputpoints.Forexample,therectiﬁedlinearfunction g( z) =max{0 , z}isnot\\ndiﬀerentiableat z= 0.Thismayseemlikeitinvalidates gforusewithagradient-\\nbasedlearningalgorithm.Inpractice,gradientdescentstillperformswellenough\\nforthesemodelstobeusedformachinelearningtasks.\\xa0Thisisinpartbecause\\nneuralnetworktrainingalgorithmsdonotusuallyarriveatalocalminimumof\\nthecostfunction,butinsteadmerelyreduceitsvaluesigniﬁcantly,asshownin\\nﬁgure.Theseideaswillbedescribedfurtherinchapter.Becausewedonot 4.3 8\\nexpecttrainingtoactuallyreachapointwherethegradientis 0,itisacceptable\\nfortheminimaofthecostfunctiontocorrespondtopointswithundeﬁnedgradient.\\nHiddenunitsthatarenotdiﬀerentiableareusuallynon-diﬀerentiable atonlya\\nsmallnumberofpoints.Ingeneral,afunction g( z)hasaleftderivativedeﬁned\\nbytheslopeofthefunctionimmediately totheleftof zandarightderivative\\ndeﬁnedbytheslopeofthefunctionimmediately totherightof z.Afunction\\nisdiﬀerentiableat zonlyifboththeleftderivativeandtherightderivativeare\\ndeﬁnedandequaltoeachother.Thefunctionsusedinthecontextofneural\\nnetworksusuallyhavedeﬁnedleftderivativesanddeﬁnedrightderivatives.Inthe\\ncaseof g( z) =max{0 , z},theleftderivativeat z= 00isandtherightderivative\\nis.Softwareimplementations ofneuralnetworktrainingusuallyreturnoneof 1\\ntheone-sidedderivativesratherthanreportingthatthederivativeisundeﬁnedor\\nraisinganerror.\\xa0Thismaybeheuristicallyjustiﬁedbyobservingthatgradient-\\nbasedoptimization onadigitalcomputerissubjecttonumericalerroranyway.\\nWhenafunctionisaskedtoevaluate g(0),itisveryunlikelythattheunderlying\\nvaluetrulywas.Instead,itwaslikelytobesomesmallvalue 0 \\ue00fthatwasrounded\\nto.Insomecontexts,moretheoreticallypleasingjustiﬁcationsareavailable,but 0\\ntheseusuallydonotapplytoneuralnetworktraining.Theimportantpointisthat\\ninpracticeonecansafelydisregardthenon-diﬀerentiabilityofthehiddenunit\\nactivationfunctionsdescribedbelow.\\nUnlessindicatedotherwise,mosthiddenunitscanbedescribedasaccepting\\navectorofinputsx,computinganaﬃnetransformationz=W\\ue03ex+b,and\\nthenapplyinganelement-wisenonlinearfunction g(z).Mosthiddenunitsare\\ndistinguishedfromeachotheronlybythechoiceoftheformoftheactivation\\nfunction. g()z\\n1 9 2', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\n6.3.1RectiﬁedLinearUnitsandTheirGeneralizations\\nRectiﬁedlinearunitsusetheactivationfunction . g z , z () = max0{}\\nRectiﬁedlinearunitsareeasytooptimizebecausetheyaresosimilartolinear\\nunits.Theonlydiﬀerencebetweenalinearunitandarectiﬁedlinearunitis\\nthatarectiﬁedlinearunitoutputszeroacrosshalfitsdomain.\\xa0This makesthe\\nderivativesthrougharectiﬁedlinearunitremainlargewhenevertheunitisactive.\\nThegradientsarenotonlylargebutalsoconsistent.Thesecondderivativeofthe\\nrectifyingoperationisalmosteverywhere,andthederivativeoftherectifying 0\\noperationiseverywherethattheunitisactive.Thismeansthatthegradient 1\\ndirectionisfarmoreusefulforlearningthanitwouldbewithactivationfunctions\\nthatintroducesecond-ordereﬀects.\\nRectiﬁedlinearunitsaretypicallyusedontopofanaﬃnetransformation:\\nhW= ( g\\ue03exb+) . (6.36)\\nWheninitializingtheparametersoftheaﬃnetransformation,itcanbeagood\\npracticetosetallelementsofbtoasmall,positivevalue,suchas0 .1.Thismakes\\nitverylikelythattherectiﬁedlinearunitswillbeinitiallyactiveformostinputs\\ninthetrainingsetandallowthederivativestopassthrough.\\nSeveralgeneralizations ofrectiﬁedlinearunitsexist.Mostofthesegeneral-\\nizationsperformcomparablytorectiﬁedlinearunitsandoccasionallyperform\\nbetter.\\nOnedrawbacktorectiﬁedlinearunitsisthattheycannotlearnviagradient-\\nbased\\xa0methods\\xa0onexamples\\xa0for\\xa0which\\xa0their\\xa0activ ation\\xa0iszero.Avariety\\xa0of\\ngeneralizations ofrectiﬁedlinearunitsguaranteethattheyreceivegradientevery-\\nwhere.\\nThreegeneralizations ofrectiﬁedlinearunitsarebasedonusinganon-zero\\nslope α iwhen z i <0: h i= g(zα ,) i=max(0 , z i)+ α imin(0 , z i).Absolutevalue\\nrectiﬁcationﬁxes α i=−1toobtain g( z) =|| z.Itisusedforobjectrecognition\\nfromimages( ,),whereitmakessensetoseekfeaturesthatare Jarrett e t a l .2009\\ninvariantunderapolarityreversaloftheinputillumination. Othergeneralizations\\nofrectiﬁedlinearunitsaremorebroadlyapplicable.AleakyReLU(,Maas e t a l .\\n2013)ﬁxes α itoasmallvaluelike0.01whileaparametricReLUorPReLU\\ntreats α iasalearnableparameter(,). He e t a l .2015\\nMaxoutunits( ,)generalizerectiﬁedlinearunits Goodfellow e t a l .2013a\\nfurther.Insteadofapplyinganelement-wisefunction g( z),maxoutunitsdividez\\nintogroupsof kvalues.Eachmaxoutunitthenoutputsthemaximumelementof\\n1 9 3', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\noneofthesegroups:\\ng()z i=max\\nj∈ G() iz j (6.37)\\nwhere G( ) iisthesetofindicesintotheinputsforgroup i,{( i−1) k+1 , . . . , i k}.\\nThisprovidesawayoflearningapiecewiselinearfunctionthatrespondstomultiple\\ndirectionsintheinputspace.x\\nAmaxoutunitcanlearnapiecewiselinear,convexfunctionwithupto kpieces.\\nMaxoutunitscanthusbeseenas l e a r ning t h e a c t i v a t i o n f u nc t i o nitselfrather\\nthanjusttherelationshipbetweenunits.Withlargeenough k,amaxoutunitcan\\nlearntoapproximateanyconvexfunctionwitharbitraryﬁdelity.Inparticular,\\namaxoutlayerwithtwopiecescanlearntoimplementthesamefunctionofthe\\ninputxasatraditionallayerusingtherectiﬁedlinearactivationfunction,absolute\\nvaluerectiﬁcationfunction,ortheleakyorparametricReLU,orcanlearnto\\nimplementatotallydiﬀerentfunctionaltogether.Themaxoutlayerwillofcourse\\nbeparametrized diﬀerentlyfromanyoftheseotherlayertypes,sothelearning\\ndynamicswillbediﬀerenteveninthecaseswheremaxoutlearnstoimplementthe\\nsamefunctionofasoneoftheotherlayertypes. x\\nEachmaxoutunitisnowparametrized by kweightvectorsinsteadofjustone,\\nsomaxoutunitstypicallyneedmoreregularizationthanrectiﬁedlinearunits.They\\ncanworkwellwithoutregularizationifthetrainingsetislargeandthenumberof\\npiecesperunitiskeptlow(,). Cai e t a l .2013\\nMaxoutunitshaveafewotherbeneﬁts.Insomecases,onecangainsomesta-\\ntisticalandcomputational advantagesbyrequiringfewerparameters.Speciﬁcally,\\nifthefeaturescapturedby ndiﬀerentlinearﬁlterscanbesummarizedwithout\\nlosinginformationbytakingthemaxovereachgroupof kfeatures,thenthenext\\nlayercangetbywithtimesfewerweights. k\\nBecauseeachunitisdrivenbymultipleﬁlters,maxoutunitshavesomeredun-\\ndancythathelpsthemtoresistaphenomenon calledcatastrophicforgetting\\ninwhichneuralnetworksforgethowtoperformtasksthattheyweretrainedonin\\nthepast( ,). Goodfellow e t a l .2014a\\nRectiﬁedlinearunitsandallofthesegeneralizations ofthemarebasedonthe\\nprinciplethatmodelsareeasiertooptimizeiftheirbehaviorisclosertolinear.\\nThissamegeneralprincipleofusinglinearbehaviortoobtaineasieroptimization\\nalsoappliesinothercontextsbesidesdeeplinearnetworks.Recurrentnetworkscan\\nlearnfromsequencesandproduceasequenceofstatesandoutputs.Whentraining\\nthem,oneneedstopropagateinformationthroughseveraltimesteps,whichismuch\\neasierwhensomelinearcomputations (withsomedirectionalderivativesbeingof\\nmagnitudenear1)areinvolved.Oneofthebest-performingrecurrentnetwork\\n1 9 4', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\narchitectures,theLSTM,propagatesinformationthroughtimeviasummation—a\\nparticularstraightforwardkindofsuchlinearactivation.Thisisdiscussedfurther\\ninsection.10.10\\n6.3.2LogisticSigmoidandHyperbolicTangent\\nPriortotheintroduction ofrectiﬁedlinearunits,mostneuralnetworksusedthe\\nlogisticsigmoidactivationfunction\\ng z σ z () = () (6.38)\\northehyperbolictangentactivationfunction\\ng z z . () = tanh( ) (6.39)\\nTheseactivationfunctionsarecloselyrelatedbecause . tanh( ) = 2(2)1 z σ z−\\nWe\\xa0havealready\\xa0seen sigmoid\\xa0unitsasoutput\\xa0units,\\xa0usedto\\xa0predictthe\\nprobabilitythatabinaryvariableis.Unlikepiecewiselinearunits,sigmoidal 1\\nunitssaturateacrossmostoftheirdomain—they saturatetoahighvaluewhen\\nzisverypositive,saturatetoalowvaluewhen zisverynegative,andareonly\\nstronglysensitivetotheirinputwhen zisnear0.Thewidespreadsaturationof\\nsigmoidalunitscanmakegradient-basedlearningverydiﬃcult.Forthisreason,\\ntheiruseashiddenunitsinfeedforwardnetworksisnowdiscouraged.Theiruse\\nasoutputunitsiscompatiblewiththeuseofgradient-basedlearningwhenan\\nappropriatecostfunctioncanundothesaturationofthesigmoidintheoutput\\nlayer.\\nWhenasigmoidalactivationfunctionmustbeused,thehyperbolictangent\\nactivationfunctiontypicallyperformsbetterthanthelogisticsigmoid.Itresembles\\ntheidentityfunctionmoreclosely,inthesensethattanh(0) = 0while σ(0) =1\\n2.\\nBecausetanhissimilartotheidentityfunctionnear,trainingadeepneural 0\\nnetworkˆ y=w\\ue03etanh(U\\ue03etanh(V\\ue03ex))resemblestrainingalinearmodelˆ y=\\nw\\ue03eU\\ue03eV\\ue03exsolongastheactivationsofthenetworkcanbekeptsmall.This\\nmakestrainingthenetworkeasier. tanh\\nSigmoidalactivationfunctionsaremorecommoninsettingsotherthanfeed-\\nforwardnetworks.Recurrentnetworks,manyprobabilisticmodels,andsome\\nautoencodershaveadditionalrequirementsthatruleouttheuseofpiecewise\\nlinearactivationfunctionsandmakesigmoidalunitsmoreappealingdespitethe\\ndrawbacksofsaturation.\\n1 9 5', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\n6.3.3OtherHiddenUnits\\nManyothertypesofhiddenunitsarepossible,butareusedlessfrequently.\\nIngeneral,awidevarietyofdiﬀerentiable functionsperformperfectlywell.\\nManyunpublishedactivationfunctionsperformjustaswellasthepopularones.\\nToprovideaconcreteexample,theauthorstestedafeedforwardnetworkusing\\nh=cos(Wx+b)ontheMNISTdatasetandobtainedanerrorrateoflessthan\\n1%,whichiscompetitivewithresultsobtainedusingmoreconventionalactivation\\nfunctions.Duringresearchanddevelopmentofnewtechniques,itiscommon\\ntotestmanydiﬀerentactivationfunctionsandﬁndthatseveralvariationson\\nstandardpracticeperformcomparably.Thismeansthatusuallynewhiddenunit\\ntypesarepublishedonlyiftheyareclearlydemonstratedtoprovideasigniﬁcant\\nimprovement.Newhiddenunittypesthatperformroughlycomparablytoknown\\ntypesaresocommonastobeuninteresting.\\nItwouldbeimpracticaltolistallofthehiddenunittypesthathaveappeared\\nintheliterature.Wehighlightafewespeciallyusefulanddistinctiveones.\\nOnepossibilityistonothaveanactivation g( z)atall.Onecanalsothinkof\\nthisasusingtheidentityfunctionastheactivationfunction.Wehavealready\\nseenthatalinearunitcanbeusefulastheoutputofaneuralnetwork.Itmay\\nalsobeusedasahiddenunit.Ifeverylayeroftheneuralnetworkconsistsofonly\\nlineartransformations,thenthenetworkasawholewillbelinear.However,it\\nisacceptableforsomelayersoftheneuralnetworktobepurelylinear.Consider\\naneuralnetworklayerwith ninputsand poutputs,h= g(W\\ue03ex+b).Wemay\\nreplacethiswithtwolayers,withonelayerusingweightmatrixUandtheother\\nusingweightmatrixV.Iftheﬁrstlayerhasnoactivationfunction,thenwehave\\nessentiallyfactoredtheweightmatrixoftheoriginallayerbasedonW.The\\nfactoredapproachistocomputeh= g(V\\ue03eU\\ue03ex+b).IfUproduces qoutputs,\\nthenUandVtogethercontainonly ( n+ p) qparameters,whileWcontains n p\\nparameters.Forsmall q,thiscanbeaconsiderablesavinginparameters.It\\ncomesatthecostofconstrainingthelineartransformationtobelow-rank,but\\ntheselow-rankrelationshipsareoftensuﬃcient.Linearhiddenunitsthusoﬀeran\\neﬀectivewayofreducingthenumberofparametersinanetwork.\\nSoftmaxunitsareanotherkindofunitthatisusuallyusedasanoutput(as\\ndescribedinsection)butmaysometimesbeusedasahiddenunit.Softmax 6.2.2.3\\nunitsnaturallyrepresentaprobabilitydistributionoveradiscretevariablewith k\\npossiblevalues,sotheymaybeusedasakindofswitch.Thesekindsofhidden\\nunitsareusuallyonlyusedinmoreadvancedarchitectures thatexplicitlylearnto\\nmanipulatememory,describedinsection.10.12\\n1 9 6', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nAfewotherreasonablycommonhiddenunittypesinclude:\\n•RadialbasisfunctionorRBFunit: h i=exp\\ue010\\n−1\\nσ2\\ni||W : , i−||x2\\ue011\\n.This\\nfunctionbecomesmoreactiveasxapproachesatemplateW : , i.Becauseit\\nsaturatestoformost,itcanbediﬃculttooptimize. 0x\\n•Softplus: g( a) = ζ( a) =log(1+ ea).Thisisasmoothversionoftherectiﬁer,\\nintroducedby ()forfunctionapproximationandby Dugas e t a l .2001 Nair\\nandHinton2010()fortheconditionaldistributionsofundirectedprobabilistic\\nmodels. ()comparedthesoftplusandrectiﬁerandfound Glorot e t a l .2011a\\nbetterresultswiththelatter.Theuseofthesoftplusisgenerallydiscouraged.\\nThesoftplusdemonstratesthattheperformanceofhiddenunittypescan\\nbeverycounterintuitive—onemightexpectittohaveanadvantageover\\ntherectiﬁerduetobeingdiﬀerentiableeverywhereorduetosaturatingless\\ncompletely,butempiricallyitdoesnot.\\n•Hardtanh:thisisshapedsimilarlytothetanhandtherectiﬁerbutunlike\\nthelatter,itisbounded, g( a)=max(−1 ,min(1 , a)).Itwasintroduced\\nby(). Collobert2004\\nHiddenunitdesignremainsanactiveareaofresearchandmanyusefulhidden\\nunittypesremaintobediscovered.\\n6. 4 A rc h i t ec t u re D es i gn\\nAnotherkeydesignconsiderationforneuralnetworksisdeterminingthearchitecture.\\nThewordarchitecturereferstotheoverallstructureofthenetwork:howmany\\nunitsitshouldhaveandhowtheseunitsshouldbeconnectedtoeachother.\\nMostneuralnetworksareorganizedintogroupsofunitscalledlayers.\\xa0Most\\nneuralnetworkarchitectures arrangetheselayersinachainstructure,witheach\\nlayerbeingafunctionofthelayerthatprecededit.Inthisstructure,theﬁrstlayer\\nisgivenby\\nh( 1 )= g( 1 )\\ue010\\nW( 1 )\\ue03exb+( 1 )\\ue011\\n, (6.40)\\nthesecondlayerisgivenby\\nh( 2 )= g( 2 )\\ue010\\nW( 2 )\\ue03eh( 1 )+b( 2 )\\ue011\\n, (6.41)\\nandsoon.\\n1 9 7', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nInthesechain-basedarchitectures,themainarchitecturalconsiderationsare\\ntochoosethedepthofthenetworkandthewidthofeachlayer.Aswewillsee,\\nanetworkwithevenonehiddenlayerissuﬃcienttoﬁtthetrainingset.Deeper\\nnetworksoftenareabletousefarfewerunitsperlayerandfarfewerparameters\\nandoftengeneralizetothetestset,butarealsooftenhardertooptimize.\\xa0The\\nidealnetworkarchitectureforataskmustbefoundviaexperimentationguidedby\\nmonitoringthevalidationseterror.\\n6.4.1UniversalApproximationPropertiesandDepth\\nAlinearmodel,mappingfromfeaturestooutputsviamatrixmultiplication, can\\nbydeﬁnitionrepresentonlylinearfunctions.Ithastheadvantageofbeingeasyto\\ntrainbecausemanylossfunctionsresultinconvexoptimization problemswhen\\nappliedtolinearmodels.Unfortunately,weoftenwanttolearnnonlinearfunctions.\\nAtﬁrstglance,wemightpresumethatlearninganonlinearfunctionrequires\\ndesigningaspecializedmodelfamilyforthekindofnonlinearitywewanttolearn.\\nFortunately,feedforwardnetworkswithhiddenlayersprovideauniversalapproxi-\\nmationframework.Speciﬁcally,theuniversalapproximationtheorem(Hornik\\ne t a l .,;,)statesthatafeedforwardnetworkwithalinearoutput 1989Cybenko1989\\nlayerandatleastonehiddenlayerwithany“squashing”activationfunction(such\\nasthelogisticsigmoidactivationfunction)canapproximateanyBorelmeasurable\\nfunctionfromoneﬁnite-dimensional spacetoanotherwithanydesirednon-zero\\namountoferror,providedthatthenetworkisgivenenoughhiddenunits.The\\nderivativesofthefeedforwardnetworkcanalsoapproximate thederivativesofthe\\nfunctionarbitrarilywell( ,).TheconceptofBorelmeasurability Hornik e t a l .1990\\nisbeyondthescopeofthisbook;\\xa0forourpurposesitsuﬃcestosaythatany\\ncontinuousfunctiononaclosedandboundedsubsetof RnisBorelmeasurable\\nandthereforemaybeapproximatedbyaneuralnetwork.Aneuralnetworkmay\\nalsoapproximateanyfunctionmappingfromanyﬁnitedimensionaldiscretespace\\ntoanother.Whiletheoriginaltheoremswereﬁrststatedintermsofunitswith\\nactivationfunctionsthatsaturatebothforverynegativeandforverypositive\\narguments,universalapproximation theoremshavealsobeenprovedforawider\\nclassofactivationfunctions,whichincludesthenowcommonlyusedrectiﬁedlinear\\nunit( ,). Leshno e t a l .1993\\nTheuniversalapproximationtheoremmeansthatregardlessofwhatfunction\\nwearetryingtolearn,weknowthatalargeMLPwillbeableto r e p r e s e ntthis\\nfunction.However,wearenotguaranteedthatthetrainingalgorithmwillbeable\\nto l e a r nthatfunction.EveniftheMLPisabletorepresentthefunction,learning\\ncanfailfortwodiﬀerentreasons.First,theoptimizationalgorithmusedfortraining\\n1 9 8', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nmaynotbeabletoﬁndthevalueoftheparametersthatcorrespondstothedesired\\nfunction.Second,thetrainingalgorithmmightchoosethewrongfunctiondueto\\noverﬁtting.Recallfromsectionthatthe“nofreelunch”theoremshowsthat 5.2.1\\nthereisnouniversallysuperiormachinelearningalgorithm.Feedforwardnetworks\\nprovideauniversalsystemforrepresentingfunctions,inthesensethat,givena\\nfunction,thereexistsafeedforwardnetworkthatapproximatesthefunction.There\\nisnouniversalprocedureforexaminingatrainingsetofspeciﬁcexamplesand\\nchoosingafunctionthatwillgeneralizetopointsnotinthetrainingset.\\nTheuniversalapproximationtheoremsaysthatthereexistsanetworklarge\\nenoughtoachieveanydegreeofaccuracywedesire,butthetheoremdoesnot\\nsayhowlargethisnetworkwillbe.()providessomeboundsonthe Barron1993\\nsizeofasingle-layernetworkneededtoapproximate abroadclassoffunctions.\\nUnfortunately,intheworsecase,anexponentialnumberofhiddenunits(possibly\\nwithonehiddenunitcorrespondingtoeachinputconﬁgurationthatneedstobe\\ndistinguished)mayberequired.Thisiseasiesttoseeinthebinarycase:the\\nnumberofpossiblebinaryfunctionsonvectorsv∈{0 ,1}nis22nandselecting\\nonesuchfunctionrequires 2nbits,whichwillingeneralrequire O(2n)degreesof\\nfreedom.\\nInsummary,afeedforwardnetworkwithasinglelayerissuﬃcienttorepresent\\nanyfunction,butthelayermaybeinfeasiblylargeandmayfailtolearnand\\ngeneralizecorrectly.Inmanycircumstances,usingdeepermodelscanreducethe\\nnumberofunitsrequiredtorepresentthedesiredfunctionandcanreducethe\\namountofgeneralization error.\\nThereexistfamiliesoffunctionswhichcanbeapproximated eﬃcientlybyan\\narchitecturewithdepthgreaterthansomevalue d,butwhichrequireamuchlarger\\nmodelifdepthisrestrictedtobelessthanorequalto d.Inmanycases,thenumber\\nofhiddenunitsrequiredbytheshallowmodelisexponentialin n.\\xa0Suchresults\\nwereﬁrstprovedformodelsthatdonotresemblethecontinuous,diﬀerentiable\\nneuralnetworksusedformachinelearning,buthavesincebeenextendedtothese\\nmodels.Theﬁrstresultswereforcircuitsoflogicgates(,).Later Håstad1986\\nworkextendedtheseresultstolinearthresholdunitswithnon-negativeweights\\n( ,; ,),andthentonetworkswith HåstadandGoldmann1991Hajnal e t a l .1993\\ncontinuous-valuedactivations(,; ,).\\xa0Manymodern Maass1992Maass e t a l .1994\\nneuralnetworksuserectiﬁedlinearunits. ()demonstrated Leshno e t a l .1993\\nthatshallownetworkswithabroadfamilyofnon-polynomialactivationfunctions,\\nincludingrectiﬁedlinearunits,haveuniversalapproximation properties,butthese\\nresultsdonotaddressthequestionsofdepthoreﬃciency—theyspecifyonlythat\\nasuﬃcientlywiderectiﬁernetworkcouldrepresentanyfunction.Montufar e t a l .\\n1 9 9', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\n()showedthatfunctionsrepresentablewithadeeprectiﬁernetcanrequire 2014\\nanexponentialnumberofhiddenunitswithashallow(onehiddenlayer)network.\\nMoreprecisely,theyshowedthatpiecewiselinearnetworks(whichcanbeobtained\\nfromrectiﬁernonlinearities ormaxoutunits)canrepresentfunctionswithanumber\\nofregionsthatisexponentialinthedepthofthenetwork.Figureillustrateshow 6.5\\nanetworkwithabsolutevaluerectiﬁcationcreatesmirrorimagesofthefunction\\ncomputedontopofsomehiddenunit,withrespecttotheinputofthathidden\\nunit.Eachhiddenunitspeciﬁeswheretofoldtheinputspaceinordertocreate\\nmirrorresponses(onbothsidesoftheabsolutevaluenonlinearity). Bycomposing\\nthesefoldingoperations,weobtainanexponentiallylargenumberofpiecewise\\nlinearregionswhichcancaptureallkindsofregular(e.g.,repeating)patterns.\\nFigure6.5:Anintuitive,geometricexplanationoftheexponentialadvantageofdeeper\\nrectiﬁernetworksformallyby (). Montufar e t a l .2014 ( L e f t )Anabsolutevaluerectiﬁcation\\nunithasthesameoutputforeverypairofmirrorpointsinitsinput.Themirroraxis\\nofsymmetryisgivenbythehyperplanedeﬁnedbytheweightsandbiasoftheunit.A\\nfunctioncomputedontopofthatunit(thegreendecisionsurface)willbeamirrorimage\\nofasimplerpatternacrossthataxisofsymmetry.Thefunctioncanbeobtained ( C e n t e r )\\nbyfoldingthespacearoundtheaxisofsymmetry.Anotherrepeatingpatterncan ( R i g h t )\\nbefoldedontopoftheﬁrst(byanotherdownstreamunit)toobtainanothersymmetry\\n(whichisnowrepeatedfourtimes,withtwohiddenlayers).Figurereproducedwith\\npermissionfrom (). Montufar e t a l .2014\\nMoreprecisely,themaintheoremin ()statesthatthe Montufar e t a l .2014\\nnumberoflinearregionscarvedoutbyadeeprectiﬁernetworkwith dinputs,\\ndepth,andunitsperhiddenlayer,is l n\\nO\\ue020\\ue012n\\nd\\ue013d l (− 1 )\\nnd\\ue021\\n, (6.42)\\ni.e.,exponentialinthedepth.Inthecaseofmaxoutnetworkswithﬁltersper l k\\nunit,thenumberoflinearregionsis\\nO\\ue010\\nk( 1 ) + l− d\\ue011\\n. (6.43)\\n2 0 0', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nOfcourse,thereisnoguaranteethatthekindsoffunctionswewanttolearnin\\napplicationsofmachinelearning(andinparticularforAI)sharesuchaproperty.\\nWemayalsowanttochooseadeepmodelforstatisticalreasons.\\xa0Anytime\\nwechooseaspeciﬁcmachinelearningalgorithm,weareimplicitlystatingsome\\nsetofpriorbeliefswehaveaboutwhatkindoffunctionthealgorithmshould\\nlearn.Choosingadeepmodelencodesaverygeneralbeliefthatthefunctionwe\\nwanttolearnshouldinvolvecompositionofseveralsimplerfunctions.Thiscanbe\\ninterpretedfromarepresentationlearningpointofviewassayingthatwebelieve\\nthelearningproblemconsistsofdiscoveringasetofunderlyingfactorsofvariation\\nthatcaninturnbedescribedintermsofother,simplerunderlyingfactorsof\\nvariation.Alternately,wecaninterprettheuseofadeeparchitectureasexpressing\\nabeliefthatthefunctionwewanttolearnisacomputerprogramconsistingof\\nmultiplesteps,whereeachstepmakesuseofthepreviousstep’soutput.\\xa0These\\nintermediateoutputsarenotnecessarilyfactorsofvariation,butcaninsteadbe\\nanalogoustocountersorpointersthatthenetworkusestoorganizeitsinternal\\nprocessing.Empirically,greaterdepthdoesseemtoresultinbettergeneralization\\nforawidevarietyoftasks( ,; ,;,; Bengio e t a l .2007Erhan e t a l .2009Bengio2009\\nMesnil2011Ciresan2012Krizhevsky2012Sermanet e t a l .,; e t a l .,; e t a l .,; e t a l .,\\n2013Farabet2013Couprie 2013Kahou 2013Goodfellow ; e t a l .,; e t a l .,; e t a l .,;\\ne t a l . e t a l . ,;2014dSzegedy ,).Seeﬁgureandﬁgureforexamplesof 2014a 6.6 6.7\\nsomeoftheseempiricalresults.Thissuggeststhatusingdeeparchitecturesdoes\\nindeedexpressausefulprioroverthespaceoffunctionsthemodellearns.\\n6.4.2OtherArchitecturalConsiderations\\nSofarwehavedescribedneuralnetworksasbeingsimplechainsoflayers,withthe\\nmainconsiderationsbeingthedepthofthenetworkandthewidthofeachlayer.\\nInpractice,neuralnetworksshowconsiderablymorediversity.\\nManyneuralnetworkarchitectures havebeendevelopedforspeciﬁctasks.\\nSpecializedarchitecturesforcomputervisioncalledconvolutionalnetworksare\\ndescribedinchapter.Feedforwardnetworksmayalsobegeneralizedtothe 9\\nrecurrentneuralnetworksforsequenceprocessing,describedinchapter,which10\\nhavetheirownarchitecturalconsiderations.\\nIngeneral,thelayersneednotbeconnectedinachain,eventhoughthisisthe\\nmostcommonpractice.Manyarchitecturesbuildamainchainbutthenaddextra\\narchitecturalfeaturestoit,suchasskipconnectionsgoingfromlayer itolayer\\ni+2orhigher.Theseskipconnectionsmakeiteasierforthegradienttoﬂowfrom\\noutputlayerstolayersnearertheinput.\\n2 0 1', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\n3 4 5 6 7 8 9 1 0 1 1\\nN u m b e r o f h i d d e n l a y e r s9 2 0 .9 2 5 .9 3 0 .9 3 5 .9 4 0 .9 4 5 .9 5 0 .9 5 5 .9 6 0 .9 6 5 .T e s t a c c u r a c y ( p e r c e n t )\\nFigure6.6:Empiricalresultsshowingthatdeepernetworksgeneralizebetterwhenused\\ntotranscribemulti-digitnumbersfromphotographsofaddresses.DatafromGoodfellow\\ne t a l .().\\xa0Thetestsetaccuracyconsistentlyincreaseswithincreasingdepth.\\xa0See 2014d\\nﬁgureforacontrolexperimentdemonstratingthatotherincreasestothemodelsize 6.7\\ndonotyieldthesameeﬀect.\\nAnotherkeyconsiderationofarchitecturedesignisexactlyhowtoconnecta\\npairoflayerstoeachother.Inthedefaultneuralnetworklayerdescribedbyalinear\\ntransformationviaamatrixW,everyinputunitisconnectedtoeveryoutput\\nunit.Manyspecializednetworksinthechaptersaheadhavefewerconnections,so\\nthateachunitintheinputlayerisconnectedtoonlyasmallsubsetofunitsin\\ntheoutputlayer.Thesestrategiesforreducingthenumberofconnectionsreduce\\nthenumberofparametersandtheamountofcomputationrequiredtoevaluate\\nthenetwork,butareoftenhighlyproblem-dependent. Forexample,convolutional\\nnetworks,describedinchapter,usespecializedpatternsofsparseconnections 9\\nthatareveryeﬀectiveforcomputervisionproblems.Inthischapter,itisdiﬃcult\\ntogivemuchmorespeciﬁcadviceconcerningthearchitectureofagenericneural\\nnetwork.Subsequentchaptersdeveloptheparticulararchitecturalstrategiesthat\\nhavebeenfoundtoworkwellfordiﬀerentapplicationdomains.\\n2 0 2', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\n0 0 0 2 0 4 0 6 0 8 1 0 . . . . . .\\nN u m b e r o f p a r a m e t e r s × 1 089 19 29 39 49 59 69 7T e s t a c c u r a c y ( p e r c e n t ) 3,convolutional\\n3,fullyconnected\\n11,convolutional\\nFigure6.7:Deepermodelstendtoperformbetter.Thisisnotmerelybecausethemodelis\\nlarger.ThisexperimentfromGoodfellow2014d e t a l .()showsthatincreasingthenumber\\nofparametersinlayersofconvolutionalnetworkswithoutincreasingtheirdepthisnot\\nnearlyaseﬀectiveatincreasingtestsetperformance.Thelegendindicatesthedepthof\\nnetworkusedtomakeeachcurveandwhetherthecurverepresentsvariationinthesizeof\\ntheconvolutionalorthefullyconnectedlayers.Weobservethatshallowmodelsinthis\\ncontextoverﬁtataround20millionparameterswhiledeeponescanbeneﬁtfromhaving\\nover60million.Thissuggeststhatusingadeepmodelexpressesausefulpreferenceover\\nthespaceoffunctionsthemodelcanlearn.Speciﬁcally,itexpressesabeliefthatthe\\nfunctionshouldconsistofmanysimplerfunctionscomposedtogether.Thiscouldresult\\neitherinlearningarepresentationthatiscomposedinturnofsimplerrepresentations(e.g.,\\ncornersdeﬁnedintermsofedges)orinlearningaprogramwithsequentiallydependent\\nsteps(e.g.,ﬁrstlocateasetofobjects,thensegmentthemfromeachother,thenrecognize\\nthem).\\n2 0 3', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\n6. 5 Bac k - Prop a g a t i o n an d O t h er D i ﬀ eren t i at i on A l go-\\nri t h m s\\nWhenweuseafeedforwardneuralnetworktoacceptaninputxandproducean\\noutput ˆy,informationﬂowsforwardthroughthenetwork.Theinputsxprovide\\ntheinitialinformationthatthenpropagatesuptothehiddenunitsateachlayer\\nandﬁnallyproduces ˆy.Thisiscalledforwardpropagation.Duringtraining,\\nforwardpropagationcancontinueonwarduntilitproducesascalarcost J(θ).\\nTheback-propagationalgorithm( ,),oftensimplycalled Rumelhart e t a l .1986a\\nbackprop,allowstheinformationfromthecosttothenﬂowbackwardsthrough\\nthenetwork,inordertocomputethegradient.\\nComputingananalyticalexpressionforthegradientisstraightforward,but\\nnumericallyevaluatingsuchanexpressioncanbecomputationally expensive.The\\nback-propagationalgorithmdoessousingasimpleandinexpensiveprocedure.\\nThetermback-propagation isoften\\xa0misunders toodasmeaningthewhole\\nlearningalgorithmformulti-layerneuralnetworks.Actually,back-propagation\\nrefersonlytothemethodforcomputingthegradient,whileanotheralgorithm,\\nsuchasstochasticgradientdescent,isusedtoperformlearningusingthisgradient.\\nFurthermore,back-propagation isoftenmisunderstoodasbeingspeciﬁctomulti-\\nlayerneuralnetworks,butinprincipleitcancomputederivativesofanyfunction\\n(forsomefunctions,thecorrectresponseistoreportthatthederivativeofthe\\nfunctionisundeﬁned).Speciﬁcally,wewilldescribehowtocomputethegradient\\n∇ x f(xy ,)foranarbitraryfunction f,wherexisasetofvariableswhosederivatives\\naredesired,andyisanadditionalsetofvariablesthatareinputstothefunction\\nbutwhosederivativesarenotrequired.Inlearningalgorithms,thegradientwemost\\noftenrequireisthegradientofthecostfunctionwithrespecttotheparameters,\\n∇ θ J(θ).Manymachinelearningtasksinvolvecomputingotherderivatives,either\\naspartof\\xa0thelearning\\xa0process,\\xa0or\\xa0to analyzethelearned\\xa0model. The\\xa0back-\\npropagationalgorithmcanbeappliedtothesetasksaswell,andisnotrestricted\\ntocomputingthegradientofthecostfunctionwithrespecttotheparameters.The\\nideaofcomputingderivativesbypropagatinginformationthroughanetworkis\\nverygeneral,andcanbeusedtocomputevaluessuchastheJacobianofafunction\\nfwithmultipleoutputs.Werestrictourdescriptionheretothemostcommonly\\nusedcasewherehasasingleoutput. f\\n2 0 4', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\n6.5.1ComputationalGraphs\\nSofarwehavediscussedneuralnetworkswitharelativelyinformalgraphlanguage.\\nTodescribetheback-propagationalgorithmmoreprecisely,itishelpfultohavea\\nmoreprecise language. computationalgraph\\nManywaysofformalizingcomputationasgraphsarepossible.\\nHere,weuseeachnodeinthegraphtoindicateavariable.Thevariablemay\\nbeascalar,vector,matrix,tensor,orevenavariableofanothertype.\\nToformalizeourgraphs,wealsoneedtointroducetheideaofanoperation.\\nAnoperationisasimplefunctionofoneormorevariables.Ourgraphlanguage\\nisaccompanied byasetofallowableoperations.Functionsmorecomplicated\\nthantheoperationsinthissetmaybedescribedbycomposingmanyoperations\\ntogether.\\nWithoutlossofgenerality,\\xa0wedeﬁneanoperationtoreturnonlyasingle\\noutputvariable.Thisdoesnotlosegeneralitybecausetheoutputvariablecanhave\\nmultipleentries,suchasavector.Softwareimplementationsofback-propagation\\nusuallysupportoperationswithmultipleoutputs,butweavoidthiscaseinour\\ndescriptionbecauseitintroducesmanyextradetailsthatarenotimportantto\\nconceptualunderstanding.\\nIfavariable yiscomputedbyapplyinganoperationtoavariable x,then\\nwedrawadirectededgefrom xto y.\\xa0Wesometimesannotatetheoutputnode\\nwiththenameoftheoperationapplied,andothertimesomitthislabelwhenthe\\noperationisclearfromcontext.\\nExamplesofcomputational graphsareshowninﬁgure.6.8\\n6.5.2ChainRuleofCalculus\\nThechainruleofcalculus(nottobeconfusedwiththechainruleofprobability)is\\nusedtocomputethederivativesoffunctionsformedbycomposingotherfunctions\\nwhosederivativesareknown.Back-propagati onisanalgorithmthatcomputesthe\\nchainrule,withaspeciﬁcorderofoperationsthatishighlyeﬃcient.\\nLet xbearealnumber,andlet fand gbothbefunctionsmappingfromareal\\nnumbertoarealnumber.Supposethat y= g( x)and z= f( g( x)) = f( y).Then\\nthechainrulestatesthatd z\\nd x=d z\\nd yd y\\nd x. (6.44)\\nWecangeneralizethisbeyondthescalarcase.Supposethatx∈ Rm,y∈ Rn,\\n2 0 5', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nz z\\nxx yy\\n( a)×\\nx x ww\\n( b)u( 1 )u( 1 )\\nd o t\\nbbu( 2 )u( 2 )\\n+ˆ y ˆ y\\nσ\\n( c )XX WWU( 1 )U( 1 )\\nm a t m u l\\nb bU( 2 )U( 2 )\\n+HH\\nr e l u\\nx x ww\\n( d)ˆ yˆ y\\nd o t\\nλ λu( 1 )u( 1 )\\ns q ru( 2 )u( 2 )\\ns u mu( 3 )u( 3 )\\n×\\nFigure6.8:Examplesofcomputationalgraphs.Thegraphusingthe ( a ) ×operationto\\ncompute z= x y.Thegraphforthelogisticregressionprediction ( b ) ˆ y= σ\\ue000\\nx\\ue03ew+ b\\ue001\\n.\\nSomeoftheintermediateexpressionsdonothavenamesinthealgebraicexpression\\nbutneednamesinthegraph.Wesimplynamethe i-thsuchvariableu( ) i.The ( c )\\ncomputationalgraphfortheexpressionH=max{0 ,XW+b},whichcomputesadesign\\nmatrixofrectiﬁedlinearunitactivationsHgivenadesignmatrixcontainingaminibatch\\nofinputsX.Examplesa–cappliedatmostoneoperationtoeachvariable,butit ( d )\\nispossibletoapplymorethanoneoperation.Hereweshowacomputationgraphthat\\nappliesmorethanoneoperationtotheweightswofalinearregressionmodel.The\\nweightsareusedtomakeboththepredictionˆ yandtheweightdecaypenalty λ\\ue050\\niw2\\ni.\\n2 0 6', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\ngmapsfrom Rmto Rn,and fmapsfrom Rnto R.Ify= g(x) and z= f(y),then\\n∂ z\\n∂ x i=\\ue058\\nj∂ z\\n∂ y j∂ y j\\n∂ x i. (6.45)\\nInvectornotation,thismaybeequivalentlywrittenas\\n∇ x z=\\ue012∂y\\n∂x\\ue013\\ue03e\\n∇ y z , (6.46)\\nwhere∂ y\\n∂ xistheJacobianmatrixof. n m× g\\nFromthisweseethatthegradientofavariablexcanbeobtainedbymultiplying\\naJacobianmatrix∂ y\\n∂ xbyagradient∇ y z.Theback-propagation algorithmconsists\\nofperformingsuchaJacobian-gradient productforeachoperationinthegraph.\\nUsuallywedonotapplytheback-propagationalgorithmmerelytovectors,\\nbutrathertotensorsofarbitrarydimensionality.Conceptually,thisisexactlythe\\nsameasback-propagation withvectors.Theonlydiﬀerenceishowthenumbers\\narearrangedinagridtoformatensor.Wecouldimagineﬂatteningeachtensor\\nintoavectorbeforewerunback-propagation,computingavector-valuedgradient,\\nandthenreshapingthegradientbackintoatensor.Inthisrearrangedview,\\nback-propagationisstilljustmultiplyingJacobiansbygradients.\\nTodenotethegradientofavalue zwithrespecttoatensor X,wewrite ∇ X z,\\njustasif Xwereavector.Theindicesinto Xnowhavemultiplecoordinates—for\\nexample,a3-Dtensorisindexedbythreecoordinates.Wecanabstractthisaway\\nbyusingasinglevariable itorepresentthecompletetupleofindices.Forall\\npossibleindextuples i,(∇ X z) igives∂ z\\n∂ X i.Thisisexactlythesameashowforall\\npossibleintegerindices iintoavector,(∇ x z) igives∂ z\\n∂ x i.Usingthisnotation,we\\ncanwritethechainruleasitappliestotensors.Ifand ,then Y X= ( g) z f= () Y\\n∇ X z=\\ue058\\nj(∇ X Y j)∂ z\\n∂ Y j. (6.47)\\n6.5.3RecursivelyApplyingtheChainRuletoObtainBackprop\\nUsingthechainrule,itisstraightforwardtowritedownanalgebraicexpressionfor\\nthegradientofascalarwithrespecttoanynodeinthecomputational graphthat\\nproducedthatscalar.However,actuallyevaluatingthatexpressioninacomputer\\nintroducessomeextraconsiderations.\\nSpeciﬁcally,manysubexpressionsmayberepeatedseveraltimeswithinthe\\noverallexpressionforthegradient.Anyprocedurethatcomputesthegradient\\n2 0 7', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nwillneedtochoosewhethertostorethesesubexpressionsortorecomputethem\\nseveraltimes.Anexampleofhowtheserepeatedsubexpressionsariseisgivenin\\nﬁgure.Insomecases,computingthesamesubexpressiontwicewouldsimply 6.9\\nbewasteful.\\xa0Forcomplicatedgraphs,therecanbeexponentiallymanyofthese\\nwastedcomputations, makinganaiveimplementation ofthechainruleinfeasible.\\nInothercases,computingthesamesubexpressiontwicecouldbeavalidwayto\\nreducememoryconsumptionatthecostofhigherruntime.\\nWeﬁrstbeginbyaversionoftheback-propagationalgorithmthatspeciﬁesthe\\nactualgradientcomputationdirectly(algorithm alongwithalgorithm forthe 6.2 6.1\\nassociatedforwardcomputation), intheorderitwillactuallybedoneandaccording\\ntotherecursiveapplicationofchainrule.Onecouldeitherdirectlyperformthese\\ncomputations orviewthedescriptionofthealgorithmasasymbolicspeciﬁcation\\nofthecomputational graphforcomputingtheback-propagation. However,this\\nformulationdoesnotmakeexplicitthemanipulation andtheconstructionofthe\\nsymbolicgraphthatperformsthegradientcomputation.\\xa0Such aformulationis\\npresentedbelowinsection,withalgorithm ,wherewealsogeneralizeto 6.5.6 6.5\\nnodesthatcontainarbitrarytensors.\\nFirstconsideracomputational graphdescribinghowtocomputeasinglescalar\\nu( ) n(saythelossonatrainingexample).Thisscalaristhequantitywhose\\ngradientwewanttoobtain,withrespecttothe n iinputnodes u( 1 )to u( n i ).\\xa0In\\notherwordswewishtocompute∂ u() n\\n∂ u() iforall i∈{1 ,2 , . . . , n i}.Intheapplication\\nofback-propagationtocomputinggradientsforgradientdescentoverparameters,\\nu( ) nwillbethecostassociatedwithanexampleoraminibatch,while u( 1 )to u( n i )\\ncorrespondtotheparametersofthemodel.\\nWewillassumethatthenodesofthegraphhavebeenorderedinsuchaway\\nthatwecancomputetheiroutputoneaftertheother,startingat u( n i + 1 )and\\ngoingupto u( ) n.Asdeﬁnedinalgorithm ,eachnode6.1 u( ) iisassociatedwithan\\noperation f( ) iandiscomputedbyevaluatingthefunction\\nu( ) i= ( f A( ) i) (6.48)\\nwhere A( ) iisthesetofallnodesthatareparentsof u( ) i.\\nThatalgorithmspeciﬁestheforwardpropagationcomputation,whichwecould\\nputinagraph G.Inordertoperformback-propagation, wecanconstructa\\ncomputational graphthatdependsonGandaddstoitanextrasetofnodes.These\\nformasubgraph BwithonenodepernodeofG.Computation inBproceedsin\\nexactlythereverseoftheorderofcomputationinG,andeachnodeofBcomputes\\nthederivative∂ u() n\\n∂ u() iassociatedwiththeforwardgraphnode u( ) i.Thisisdone\\n2 0 8', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nAlgorithm6.1Aprocedurethatperformsthecomputations mapping n iinputs\\nu( 1 )to u( n i )toanoutput u( ) n.Thisdeﬁnesacomputational graphwhereeachnode\\ncomputesnumericalvalue u( ) ibyapplyingafunction f( ) itothesetofarguments\\nA( ) ithatcomprisesthevaluesofpreviousnodes u( ) j, j < i,with j P a∈ ( u( ) i).The\\ninputtothecomputational graphisthevectorx,andissetintotheﬁrst n inodes\\nu( 1 )to u( n i ).Theoutputofthecomputational graphisreadoﬀthelast(output)\\nnode u( ) n.\\nfor i , . . . , n = 1 ido\\nu( ) i← x i\\nendfor\\nfor i n= i+1 , . . . , ndo\\nA( ) i←{ u( ) j|∈ j P a u(( ) i)}\\nu( ) i← f( ) i( A( ) i)\\nendfor\\nreturn u( ) n\\nusingthechainrulewithrespecttoscalaroutput u( ) n:\\n∂ u( ) n\\n∂ u( ) j=\\ue058\\ni j P a u :∈ (() i )∂ u( ) n\\n∂ u( ) i∂ u( ) i\\n∂ u( ) j(6.49)\\nasspeciﬁedbyalgorithm .Thesubgraph6.2 Bcontainsexactlyoneedgeforeach\\nedgefromnode u( ) jtonode u( ) iofG.Theedgefrom u( ) jto u( ) iisassociatedwith\\nthecomputationof∂ u() i\\n∂ u() j.Inaddition,adotproductisperformedforeachnode,\\nbetweenthegradientalreadycomputedwithrespecttonodes u( ) ithatarechildren\\nof u( ) jandthevectorcontainingthepartialderivatives∂ u() i\\n∂ u() jforthesamechildren\\nnodes u( ) i.Tosummarize,theamountofcomputationrequiredforperforming\\ntheback-propagationscaleslinearlywiththenumberofedgesinG,wherethe\\ncomputationforeachedgecorrespondstocomputingapartialderivative(ofone\\nnodewithrespecttooneofitsparents)aswellasperformingonemultiplication\\nandoneaddition.Below,wegeneralizethisanalysistotensor-valuednodes,which\\nisjustawaytogroupmultiplescalarvaluesinthesamenodeandenablemore\\neﬃcientimplementations.\\nTheback-propagationalgorithmisdesignedtoreducethenumberofcommon\\nsubexpressionswithoutregardtomemory.Speciﬁcally,itperformsontheorder\\nofoneJacobianproductpernodeinthegraph.\\xa0Thiscanbeseenfromthefact\\nthatbackprop(algorithm )visitseachedgefromnode 6.2 u( ) jtonode u( ) iof\\nthegraphexactlyonceinordertoobtaintheassociatedpartialderivative∂ u() i\\n∂ u() j.\\n2 0 9', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nAlgorithm6.2Simpliﬁedversionoftheback-propagation algorithmforcomputing\\nthederivativesof u( ) nwithrespecttothevariablesinthegraph.Thisexampleis\\nintendedtofurtherunderstandingbyshowingasimpliﬁedcasewhereallvariables\\narescalars,andwewishtocomputethederivativeswithrespectto u( 1 ), . . . , u( n i ).\\nThissimpliﬁedversioncomputesthederivativesofallnodesinthegraph.\\xa0The\\ncomputational costofthisalgorithmisproportional tothenumberofedgesin\\nthegraph,assumingthatthepartialderivativeassociatedwitheachedgerequires\\naconstanttime.Thisisofthesameorderasthenumberofcomputations for\\ntheforwardpropagation. Each∂ u() i\\n∂ u() jisafunctionoftheparents u( ) jof u( ) i,thus\\nlinkingthenodesoftheforwardgraphtothoseaddedfortheback-propagation\\ngraph.\\nRunforwardpropagation(algorithm forthisexample)toobtaintheactiva- 6.1\\ntionsofthenetwork\\nInitialize grad_table,adatastructurethatwillstorethederivativesthathave\\nbeencomputed.Theentry g r a d t a b l e_ [ u( ) i]willstorethecomputedvalueof\\n∂ u() n\\n∂ u() i.\\ng r a d t a b l e_ [ u( ) n] 1←\\nfor do j n= −1downto1\\nThenextlinecomputes∂ u() n\\n∂ u() j=\\ue050\\ni j P a u :∈ (() i )∂ u() n\\n∂ u() i∂ u() i\\n∂ u() jusingstoredvalues:\\ng r a d t a b l e_ [ u( ) j] ←\\ue050\\ni j P a u :∈ (() i ) g r a d t a b l e_ [ u( ) i]∂ u() i\\n∂ u() j\\nendfor\\nreturn{ g r a d t a b l e_ [ u( ) i] = 1 | i , . . . , n i}\\nBack-propagationthusavoidstheexponentialexplosioninrepeatedsubexpressions.\\nHowever,otheralgorithmsmaybeabletoavoidmoresubexpressionsbyperforming\\nsimpliﬁcationsonthecomputational graph,ormaybeabletoconservememoryby\\nrecomputingratherthanstoringsomesubexpressions.Wewillrevisittheseideas\\nafterdescribingtheback-propagation algorithmitself.\\n6.5.4Back-PropagationComputationinFully-ConnectedMLP\\nToclarifytheabovedeﬁnitionoftheback-propagation computation,letusconsider\\nthespeciﬁcgraphassociatedwithafully-connected multi-layerMLP.\\nAlgorithmﬁrstshowstheforwardpropagation, whichmapsparametersto 6.3\\nthesupervisedloss L(ˆyy ,)associatedwithasingle(input,target) trainingexample\\n( )xy ,,with ˆytheoutputoftheneuralnetworkwhenisprovidedininput. x\\nAlgorithm\\xa0 then\\xa0shows\\xa0thecorresponding\\xa0computation\\xa0to be\\xa0donefor 6.4\\n2 1 0', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nz z\\nxxyy\\nw wfff\\nFigure6.9:Acomputationalgraphthatresultsinrepeatedsubexpressionswhencomputing\\nthegradient.Let w∈ Rbetheinputtothegraph.Weusethesamefunction f: R R→\\nastheoperationthatweapplyateverystepofachain: x= f( w), y= f( x), z= f( y).\\nTocompute∂ z\\n∂ w,weapplyequationandobtain: 6.44\\n∂ z\\n∂ w(6.50)\\n=∂ z\\n∂ y∂ y\\n∂ x∂ x\\n∂ w(6.51)\\n= f\\ue030() y f\\ue030() x f\\ue030() w (6.52)\\n= f\\ue030((())) f f w f\\ue030(()) f w f\\ue030() w (6.53)\\nEquationsuggestsanimplementationinwhichwecomputethevalueof 6.52 f( w)only\\nonceandstoreitinthevariable x.Thisistheapproachtakenbytheback-propagation\\nalgorithm.Analternativeapproachissuggestedbyequation,wherethesubexpression 6.53\\nf( w)appearsmorethanonce.Inthealternativeapproach, f( w)isrecomputedeachtime\\nitisneeded.Whenthememoryrequiredtostorethevalueoftheseexpressionsislow,the\\nback-propagationapproachofequationisclearlypreferablebecauseofitsreduced 6.52\\nruntime.However,equationisalsoavalidimplementationofthechainrule,andis 6.53\\nusefulwhenmemoryislimited.\\n2 1 1', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\napplyingtheback-propagation algorithmtothisgraph.\\nAlgorithms andaredemonstrationsthatarechosentobesimpleand 6.36.4\\nstraightforwardtounderstand.However,\\xa0theyarespecializedtoonespeciﬁc\\nproblem.\\nModernsoftwareimplementations arebasedonthegeneralizedformofback-\\npropagationdescribedinsectionbelow,whichcanaccommodateanycompu- 6.5.6\\ntationalgraphbyexplicitlymanipulating adatastructureforrepresentingsymbolic\\ncomputation.\\nAlgorithm6.3Forwardpropagationthroughatypicaldeepneuralnetworkand\\nthecomputationofthecostfunction.Theloss L(ˆyy ,)dependsontheoutput\\nˆyandonthetargety(seesectionforexamplesoflossfunctions).To 6.2.1.1\\nobtainthetotalcost J,thelossmaybeaddedtoaregularizer Ω( θ),where θ\\ncontainsalltheparameters(weightsandbiases).Algorithm showshowto 6.4\\ncomputegradientsof JwithrespecttoparametersWandb.Forsimplicity,this\\ndemonstrationusesonlyasingleinputexamplex.Practicalapplicationsshould\\nuseaminibatch.Seesectionforamorerealisticdemonstration. 6.5.7\\nRequire:Networkdepth, l\\nRequire:W( ) i, i , . . . , l , ∈{1 }theweightmatricesofthemodel\\nRequire:b( ) i, i , . . . , l , ∈{1 }thebiasparametersofthemodel\\nRequire:x,theinputtoprocess\\nRequire:y,thetargetoutput\\nh( 0 )= x\\nfordo k , . . . , l = 1\\na( ) k= b( ) k+W( ) kh( 1 ) k−\\nh( ) k= ( fa( ) k)\\nendfor\\nˆyh= ( ) l\\nJ L= (ˆyy ,)+Ω() λ θ\\n6.5.5Symbol-to-SymbolDerivatives\\nAlgebraicexpressionsandcomputational graphsbothoperateonsymbols,or\\nvariables\\xa0thatdo\\xa0not\\xa0havespeciﬁc\\xa0values.Thesealgebraic\\xa0and graph-based\\nrepresentationsarecalledsymbolicrepresentations.Whenweactuallyuseor\\ntrainaneuralnetwork,wemustassignspeciﬁcvaluestothesesymbols.We\\nreplaceasymbolicinputtothenetworkxwithaspeciﬁcnumericvalue,suchas\\n[123765 18] . , . ,− .\\ue03e.\\n2 1 2', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nAlgorithm6.4Backwardcomputationforthedeepneuralnetworkofalgo-\\nrithm,whichusesinadditiontotheinput 6.3 xatargety.Thiscomputation\\nyieldsthegradientsontheactivationsa( ) kforeachlayer k,startingfromthe\\noutputlayerandgoingbackwardstotheﬁrsthiddenlayer.Fromthesegradients,\\nwhichcanbeinterpretedasanindicationofhoweachlayer’soutputshouldchange\\ntoreduceerror,onecanobtainthegradientontheparametersofeachlayer.The\\ngradientsonweightsandbiasescanbeimmediately usedaspartofastochas-\\nticgradientupdate(performingtheupdaterightafterthegradientshavebeen\\ncomputed)orusedwithothergradient-basedoptimization methods.\\nAftertheforwardcomputation,computethegradientontheoutputlayer:\\ng←∇ ˆ y J= ∇ ˆ y L(ˆyy ,)\\nfor do k l , l , . . . , = −1 1\\nConvert\\xa0thegradienton\\xa0thelayer’s\\xa0output\\xa0into\\xa0a\\xa0gradient\\xa0into\\xa0thepre-\\nnonlinearityactivation(element-wisemultiplicationifiselement-wise): f\\ng←∇a() k J f = g\\ue00c\\ue030(a( ) k)\\nComputegradientsonweightsandbiases(includingtheregularizationterm,\\nwhereneeded):\\n∇b() k J λ = +g ∇b() kΩ() θ\\n∇W() k J= gh( 1 ) k−\\ue03e+ λ∇W() kΩ() θ\\nPropagatethegradientsw.r.t.thenextlower-levelhiddenlayer’sactivations:\\ng←∇h(1) k − J= W( ) k\\ue03eg\\nendfor\\n2 1 3', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nz z\\nxxyy\\nw wfffz z\\nxxyy\\nw wfff\\nd z\\nd yd z\\nd yf\\ue021\\nd y\\nd xd y\\nd xf\\ue021\\nd z\\nd xd z\\nd x×\\nd x\\nd wd x\\nd wf\\ue021\\nd z\\nd wd z\\nd w×\\nFigure6.10:Anexampleofthesymbol-to-symbolapproachtocomputingderivatives.In\\nthisapproach,theback-propagationalgorithmdoesnotneedtoeveraccessanyactual\\nspeciﬁcnumericvalues.Instead,itaddsnodestoacomputationalgraphdescribinghow\\ntocomputethesederivatives.Agenericgraphevaluationenginecanlatercomputethe\\nderivativesforanyspeciﬁcnumericvalues. ( L e f t )Inthisexample,webeginwithagraph\\nrepresenting z= f( f( f( w))).Weruntheback-propagationalgorithm,instructing ( R i g h t )\\nittoconstructthegraphfortheexpressioncorrespondingtod z\\nd w.Inthisexample,wedo\\nnotexplainhowtheback-propagationalgorithmworks.Thepurposeisonlytoillustrate\\nwhatthedesiredresultis:acomputationalgraphwithasymbolicdescriptionofthe\\nderivative.\\nSomeapproachestoback-propagationtakeacomputational graphandaset\\nofnumericalvaluesfortheinputstothegraph,thenreturnasetofnumerical\\nvaluesdescribingthegradientatthoseinputvalues.Wecallthisapproach“symbol-\\nto-number”diﬀerentiation. ThisistheapproachusedbylibrariessuchasTorch\\n( ,)andCaﬀe(,). Collobert e t a l .2011b Jia2013\\nAnotherapproachistotakeacomputational graphandaddadditionalnodes\\ntothegraphthatprovideasymbolicdescriptionofthedesiredderivatives.This\\nistheapproachtakenbyTheano( ,; ,) Bergstra e t a l .2010Bastien e t a l .2012\\nandTensorFlow( ,).Anexampleofhowthisapproachworks Abadi e t a l .2015\\nisillustratedinﬁgure.Theprimaryadvantageofthisapproachisthat 6.10\\nthederivativesaredescribedinthesamelanguageastheoriginalexpression.\\nBecausethederivativesarejustanothercomputational graph,itispossibletorun\\nback-propagationagain,diﬀerentiating thederivativesinordertoobtainhigher\\nderivatives.Computation ofhigher-orderderivativesisdescribedinsection.6.5.10\\nWewillusethelatterapproachanddescribetheback-propagationalgorithmin\\n2 1 4', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\ntermsofconstructingacomputational graphforthederivatives.Anysubsetofthe\\ngraphmaythenbeevaluatedusingspeciﬁcnumericalvaluesatalatertime.This\\nallowsustoavoidspecifyingexactlywheneachoperationshouldbecomputed.\\nInstead,agenericgraphevaluationenginecanevaluateeverynodeassoonasits\\nparents’valuesareavailable.\\nThedescriptionofthesymbol-to-symbolbasedapproachsubsumesthesymbol-\\nto-numberapproach.Thesymbol-to-numberapproachcanbeunderstoodas\\nperformingexactlythesamecomputations asaredoneinthegraphbuiltbythe\\nsymbol-to-symbolapproach.Thekeydiﬀerenceisthatthesymbol-to-number\\napproachdoesnotexposethegraph.\\n6.5.6GeneralBack-Propagation\\nTheback-propagationalgorithmisverysimple.Tocomputethegradientofsome\\nscalar zwithrespecttooneofitsancestorsxinthegraph,webeginbyobserving\\nthatthegradientwithrespectto zisgivenbyd z\\nd z=1.Wecanthencompute\\nthegradientwithrespecttoeachparentof zinthegraphbymultiplyingthe\\ncurrentgradientbytheJacobianoftheoperationthatproduced z.Wecontinue\\nmultiplyingbyJacobianstravelingbackwardsthroughthegraphinthiswayuntil\\nwereachx.Foranynodethatmaybereachedbygoingbackwardsfrom zthrough\\ntwoormorepaths,wesimplysumthegradientsarrivingfromdiﬀerentpathsat\\nthatnode.\\nMoreformally,eachnodeinthegraph Gcorrespondstoavariable.Toachieve\\nmaximumgenerality,wedescribethisvariableasbeingatensor V.\\xa0Tensorcan\\ningeneralhaveanynumberofdimensions.\\xa0Theysubsumescalars,vectors,and\\nmatrices.\\nWeassumethateachvariableisassociatedwiththefollowingsubroutines: V\\n• g e t o p e r a t i o n_ ( V):Thisreturnstheoperationthatcomputes V,repre-\\nsentedbytheedgescominginto Vinthecomputational graph.Forexample,\\ntheremaybeaPythonorC++classrepresentingthematrixmultiplication\\noperation,andtheget_operationfunction.Supposewehaveavariablethat\\niscreatedbymatrixmultiplication,C=AB.Then g e t o p e r a t i o n_ ( V)\\nreturnsapointertoaninstanceofthecorrespondingC++class.\\n• g e t c o n s u m e r s_ ( V ,G):Thisreturnsthelistofvariablesthatarechildrenof\\nVinthecomputational graph.G\\n• G g e t i n p u t s_ ( V ,):Thisreturnsthelistofvariablesthatareparentsof V\\ninthecomputational graph.G\\n2 1 5', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nEachoperationopisalsoassociatedwithabpropoperation.Thisbprop\\noperationcancomputeaJacobian-vectorproductasdescribedbyequation.6.47\\nThisishowtheback-propagationalgorithmisabletoachievegreatgenerality.\\nEachoperationisresponsibleforknowinghowtoback-propagate throughthe\\nedgesinthegraphthatitparticipatesin.Forexample,wemightuseamatrix\\nmultiplicationoperationtocreateavariableC=AB.Supposethatthegradient\\nofascalar zwithrespecttoCisgivenbyG.Thematrixmultiplication operation\\nisresponsiblefordeﬁningtwoback-propagation rules,oneforeachofitsinput\\narguments.Ifwecallthebpropmethodtorequestthegradientwithrespectto\\nAgiventhatthegradientontheoutputisG,thenthe b p r o pmethodofthe\\nmatrixmultiplicationoperationmuststatethatthegradientwithrespecttoA\\nisgivenbyGB\\ue03e.Likewise,ifwecallthe b p r o pmethodtorequestthegradient\\nwithrespecttoB,thenthematrixoperationisresponsibleforimplementing the\\nb p r o pmethodandspecifyingthatthedesiredgradientisgivenbyA\\ue03eG.The\\nback-propagationalgorithmitselfdoesnotneedtoknowanydiﬀerentiation rules.It\\nonlyneedstocalleachoperation’sbpropruleswiththerightarguments.Formally,\\no p b p r o p i n p u t s . ( , , X G)mustreturn\\n\\ue058\\ni(∇ X o p f i n p u t s .( ) i) G i , (6.54)\\nwhichisjustanimplementation ofthechainruleasexpressedinequation.6.47\\nHere, i n p u t sisalistofinputsthataresuppliedtotheoperation, op.fisthe\\nmathematical functionthattheoperationimplements, Xistheinputwhosegradient\\nwewishtocompute,andisthegradientontheoutputoftheoperation. G\\nTheop.bpropmethodshouldalwayspretendthatallofitsinputsaredistinct\\nfromeachother,eveniftheyarenot.Forexample,ifthemuloperatorispassed\\ntwocopiesof xtocompute x2,theop.bpropmethodshouldstillreturn xasthe\\nderivativewithrespecttobothinputs.Theback-propagation algorithmwilllater\\naddbothoftheseargumentstogethertoobtain 2 x,whichisthecorrecttotal\\nderivativeon. x\\nSoftwareimplementationsofback-propagation usuallyprovideboththeopera-\\ntionsandtheirbpropmethods,sothatusersofdeeplearningsoftwarelibrariesare\\nabletoback-propagatethroughgraphsbuiltusingcommonoperationslikematrix\\nmultiplication, exponents,logarithms,andsoon.Softwareengineerswhobuilda\\nnewimplementationofback-propagationoradvanceduserswhoneedtoaddtheir\\nownoperationtoanexistinglibrarymustusuallyderivetheop.bpropmethodfor\\nanynewoperationsmanually.\\nTheback-propagationalgorithmisformallydescribedinalgorithm .6.5\\n2 1 6', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nAlgorithm6.5Theoutermostskeletonoftheback-propagation algorithm.This\\nportiondoessimplesetupandcleanupwork.Mostoftheimportantworkhappens\\ninthe subroutineofalgorithm build_grad 6.6.\\nRequire: T,thetargetsetofvariableswhosegradientsmustbecomputed.\\nRequire:G,thecomputational graph\\nRequire: z,thevariabletobediﬀerentiated\\nLetG\\ue030beGprunedtocontainonlynodesthatareancestorsof zanddescendents\\nofnodesin. T\\nInitialize ,adatastructureassociatingtensorstotheirgradients grad_table\\ng r a d t a b l e_ [] 1 z←\\nfordo Vin T\\nb u i l d g r a d_ ( V , ,GG\\ue030, g r a d t a b l e_ )\\nendfor\\nReturn restrictedto grad_table T\\nInsection,weexplainedthatback-propagation wasdevelopedinorderto 6.5.2\\navoidcomputingthesamesubexpressioninthechainrulemultipletimes.Thenaive\\nalgorithmcouldhaveexponentialruntimeduetotheserepeatedsubexpressions.\\nNowthatwehavespeciﬁedtheback-propagationalgorithm,wecanunderstandits\\ncomputational cost.Ifweassumethateachoperationevaluationhasroughlythe\\nsamecost,thenwemayanalyzethecomputational costintermsofthenumber\\nofoperationsexecuted.Keepinmindherethatwerefertoanoperationasthe\\nfundamentalunitofourcomputational graph,whichmightactuallyconsistofvery\\nmanyarithmeticoperations(forexample,wemighthaveagraphthattreatsmatrix\\nmultiplicationasasingleoperation).Computingagradientinagraphwith nnodes\\nwillneverexecutemorethan O( n2)operationsorstoretheoutputofmorethan\\nO( n2) operations.Herewearecountingoperationsinthecomputational graph,not\\nindividualoperationsexecutedbytheunderlyinghardware,soitisimportantto\\nrememberthattheruntimeofeachoperationmaybehighlyvariable.Forexample,\\nmultiplyingtwomatricesthateachcontainmillionsofentriesmightcorrespondto\\nasingleoperationinthegraph.Wecanseethatcomputingthegradientrequiresas\\nmost O( n2) operationsbecausetheforwardpropagationstagewillatworstexecute\\nall nnodesintheoriginalgraph(dependingonwhichvalueswewanttocompute,\\nwemaynotneedtoexecutetheentiregraph).Theback-propagationalgorithm\\naddsoneJacobian-vectorproduct,whichshouldbeexpressedwith O(1)nodes,per\\nedgeintheoriginalgraph.Becausethecomputational graphisadirectedacyclic\\ngraphithasatmost O( n2)edges.Forthekindsofgraphsthatarecommonlyused\\ninpractice,thesituationisevenbetter.Mostneuralnetworkcostfunctionsare\\n2 1 7', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nAlgorithm6.6Theinnerloopsubroutine b u i l d g r a d_ ( V , ,GG\\ue030, g r a d t a b l e_ )of\\ntheback-propagationalgorithm,calledbytheback-propagationalgorithmdeﬁned\\ninalgorithm .6.5\\nRequire: V,thevariablewhosegradientshouldbeaddedtoand . Ggrad_table\\nRequire:G,thegraphtomodify.\\nRequire:G\\ue030,therestrictionoftonodesthatparticipateinthegradient. G\\nRequire:grad_table,adatastructuremappingnodestotheirgradients\\nif then Visingrad_table\\nReturn_ g r a d t a b l e[] V\\nendif\\ni←1\\nfor C V in_ g e t c o n s u m e r s( ,G\\ue030)do\\no p g e t o p e r a t i o n ←_ () C\\nD C ← b u i l d g r a d_ ( , ,GG\\ue030, g r a d t a b l e_ )\\nG( ) i← G o p b p r o p g e t i n p u t s . (_ ( C ,\\ue030) ) , , V D\\ni i←+1\\nendfor\\nG←\\ue050\\ni G( ) i\\ng r a d t a b l e_ [] = V G\\nInsertandtheoperationscreatingitinto G G\\nReturn G\\nroughlychain-structured,causingback-propagationtohave O( n)cost.Thisisfar\\nbetterthanthenaiveapproach,whichmightneedtoexecuteexponentiallymany\\nnodes.Thispotentiallyexponentialcostcanbeseenbyexpandingandrewriting\\ntherecursivechainrule(equation)non-recursively: 6.49\\n∂ u( ) n\\n∂ u( ) j=\\ue058\\npa t h ( u( π1), u( π2), . . . , u( π t)) ,\\nf r o m π1 = t o j π t = nt\\ue059\\nk = 2∂ u( π k )\\n∂ u( π k −1 ). (6.55)\\nSincethenumberofpathsfromnode jtonode ncangrowexponentiallyinthe\\nlengthofthesepaths,thenumberoftermsintheabovesum,whichisthenumber\\nofsuchpaths,cangrowexponentiallywiththedepthoftheforwardpropagation\\ngraph.Thislargecostwouldbeincurredbecausethesamecomputationfor\\n∂ u() i\\n∂ u() jwouldberedonemanytimes.\\xa0Toavoidsuchrecomputation, wecanthink\\nofback-propagation asatable-ﬁllingalgorithmthattakesadvantageofstoring\\nintermediateresults∂ u() n\\n∂ u() i.Eachnodeinthegraphhasacorrespondingslotina\\ntabletostorethegradientforthatnode.Byﬁllinginthesetableentriesinorder,\\n2 1 8', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nback-propagationavoidsrepeatingmanycommonsubexpressions.Thistable-ﬁlling\\nstrategyissometimescalled . dynamicprogramming\\n6.5.7Example:Back-PropagationforMLPTraining\\nAsanexample,wewalkthroughtheback-propagation algorithmasitisusedto\\ntrainamultilayerperceptron.\\nHerewedevelopaverysimplemultilayerperceptionwithasinglehidden\\nlayer.Totrainthismodel,wewilluseminibatchstochasticgradientdescent.\\nTheback-propagationalgorithmisusedtocomputethegradientofthecostona\\nsingleminibatch.Speciﬁcally,weuseaminibatchofexamplesfromthetraining\\nsetformattedasadesignmatrixXandavectorofassociatedclasslabelsy.\\nThenetworkcomputesalayerofhiddenfeaturesH=max{0 ,XW( 1 )}.To\\nsimplifythepresentationwedonotusebiasesinthismodel.Weassumethatour\\ngraphlanguageincludesareluoperationthatcancompute max{0 ,Z}element-\\nwise.Thepredictionsoftheunnormalized logprobabilities overclassesarethen\\ngivenbyHW( 2 ).Weassumethatourgraphlanguageincludesacross_entropy\\noperationthatcomputesthecross-entropybetweenthetargetsyandtheprobability\\ndistributiondeﬁnedbytheseunnormalized logprobabilities. Theresultingcross-\\nentropydeﬁnesthecost J M LE.Minimizingthiscross-entropyperformsmaximum\\nlikelihoodestimationoftheclassiﬁer.However,tomakethisexamplemorerealistic,\\nwealsoincludearegularizationterm.Thetotalcost\\nJ J= M LE+ λ\\uf8eb\\n\\uf8ed\\ue058\\ni , j\\ue010\\nW( 1 )\\ni , j\\ue0112\\n+\\ue058\\ni , j\\ue010\\nW( 2 )\\ni , j\\ue0112\\uf8f6\\n\\uf8f8 (6.56)\\nconsistsofthecross-entropyandaweightdecaytermwithcoeﬃcient λ.The\\ncomputational graphisillustratedinﬁgure.6.11\\nThecomputational graphforthegradientofthisexampleislargeenoughthat\\nitwouldbetedioustodrawortoread.Thisdemonstratesoneofthebeneﬁts\\noftheback-propagation algorithm,whichisthatitcanautomatically generate\\ngradientsthatwouldbestraightforwardbuttediousforasoftwareengineerto\\nderivemanually.\\nWecanroughlytraceoutthebehavioroftheback-propagation algorithm\\nbylookingattheforwardpropagationgraphinﬁgure.Totrain,wewish 6.11\\ntocomputeboth∇W(1) Jand ∇W(2) J.Therearetwodiﬀerentpathsleading\\nbackwardfrom Jtotheweights:onethroughthecross-entropycost,andone\\nthroughtheweightdecaycost.Theweightdecaycostisrelativelysimple;itwill\\nalwayscontribute 2 λW( ) itothegradientonW( ) i.\\n2 1 9', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nXXW( 1 )W( 1 )U( 1 )U( 1 )\\nm a t m u lHH\\nr e l u\\nU( 3 )U( 3 )\\ns q ru( 4 )u( 4 )\\ns u mλ λ u( 7 )u( 7 )W( 2 )W( 2 )U( 2 )U( 2 )\\nm a t m u ly yJ M L E J M L E\\nc r o s s _ e n t r o p y\\nU( 5 )U( 5 )\\ns q ru( 6 )u( 6 )\\ns u mu( 8 )u( 8 )J J\\n+\\n×\\n+\\nFigure6.11:Thecomputationalgraphusedtocomputethecostusedtotrainourexample\\nofasingle-layerMLPusingthecross-entropylossandweightdecay.\\nTheotherpaththroughthecross-entropycostisslightlymorecomplicated.\\nLetGbethegradientontheunnormalized logprobabilitiesU( 2 )providedby\\nthecross_entropyoperation.Theback-propagation algorithmnowneedsto\\nexploretwodiﬀerentbranches.Ontheshorterbranch,itaddsH\\ue03eGtothe\\ngradientonW( 2 ),usingtheback-propagation ruleforthesecondargumentto\\nthematrixmultiplication operation.Theotherbranchcorrespondstothelonger\\nchaindescendingfurtheralongthenetwork.First,theback-propagationalgorithm\\ncomputes ∇ H J=GW( 2 )\\ue03eusingtheback-propagationrulefortheﬁrstargument\\ntothematrixmultiplication operation.Next,thereluoperationusesitsback-\\npropagationruletozerooutcomponentsofthegradientcorrespondingtoentries\\nofU( 1 )thatwerelessthan.Lettheresultbecalled 0 G\\ue030.Thelaststepofthe\\nback-propagationalgorithmistousetheback-propagation ruleforthesecond\\nargumentoftheoperationtoadd matmul X\\ue03eG\\ue030tothegradientonW( 1 ).\\nAfterthesegradientshavebeencomputed,itistheresponsibilityofthegradient\\ndescentalgorithm,oranotheroptimization algorithm,tousethesegradientsto\\nupdatetheparameters.\\nFortheMLP,thecomputational costisdominatedbythecostofmatrix\\nmultiplication. Duringtheforwardpropagationstage,wemultiplybyeachweight\\n2 2 0', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nmatrix,resultingin O( w) multiply-adds,where wisthenumberofweights.During\\nthebackwardpropagationstage,wemultiplybythetransposeofeachweight\\nmatrix,whichhasthesamecomputational cost.Themainmemorycostofthe\\nalgorithmisthatweneedtostoretheinputtothenonlinearityofthehiddenlayer.\\nThisvalueisstoredfromthetimeitiscomputeduntilthebackwardpasshas\\nreturnedtothesamepoint.Thememorycostisthus O( m n h),where misthe\\nnumberofexamplesintheminibatchand n histhenumberofhiddenunits.\\n6.5.8Complications\\nOurdescriptionoftheback-propagation algorithmhereissimplerthantheimple-\\nmentationsactuallyusedinpractice.\\nAsnotedabove,wehaverestrictedthedeﬁnitionofanoperationtobea\\nfunctionthatreturnsasingletensor.Mostsoftwareimplementations needto\\nsupportoperationsthatcanreturnmorethanonetensor.Forexample,ifwewish\\ntocomputeboththemaximumvalueinatensorandtheindexofthatvalue,itis\\nbesttocomputebothinasinglepassthroughmemory,soitismosteﬃcientto\\nimplementthisprocedureasasingleoperationwithtwooutputs.\\nWe\\xa0havenot\\xa0described\\xa0how\\xa0tocontrolthememoryconsumption\\xa0ofback-\\npropagation. Back-propagati onofteninvolvessummationofmanytensorstogether.\\nInthenaiveapproach,eachofthesetensorswouldbecomputedseparately,then\\nallofthemwouldbeaddedinasecondstep.Thenaiveapproachhasanoverly\\nhighmemorybottleneckthatcanbeavoidedbymaintainingasinglebuﬀerand\\naddingeachvaluetothatbuﬀerasitiscomputed.\\nReal-worldimplementationsofback-propagation alsoneedtohandlevarious\\ndatatypes,suchas32-bitﬂoatingpoint,64-bitﬂoatingpoint,andintegervalues.\\nThepolicyforhandlingeachofthesetypestakesspecialcaretodesign.\\nSomeoperationshaveundeﬁnedgradients,anditisimportanttotrackthese\\ncasesanddeterminewhetherthegradientrequestedbytheuserisundeﬁned.\\nVariousothertechnicalitiesmakereal-worlddiﬀerentiation morecomplicated.\\nThesetechnicalitiesarenotinsurmountable,andthischapterhasdescribedthekey\\nintellectualtoolsneededtocomputederivatives,butitisimportanttobeaware\\nthatmanymoresubtletiesexist.\\n6.5.9DiﬀerentiationoutsidetheDeepLearningCommunity\\nThe\\xa0deeplearning\\xa0comm unityhas\\xa0beensomewhat\\xa0isolatedfrom\\xa0the\\xa0broader\\ncomputersciencecommunityandhaslargelydevelopeditsownculturalattitudes\\n2 2 1', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nconcerninghowtoperformdiﬀerentiation. Moregenerally,theﬁeldofautomatic\\ndiﬀerentiationisconcernedwithhowtocomputederivativesalgorithmically .\\nTheback-propagationalgorithmdescribedhereisonlyoneapproachtoautomatic\\ndiﬀerentiation.Itisaspecialcaseofabroaderclassoftechniquescalledreverse\\nmodeaccumulation.Otherapproachesevaluatethesubexpressionsofthechain\\nruleindiﬀerentorders.Ingeneral,\\xa0determining theorderofevaluationthat\\nresultsinthelowestcomputational costisadiﬃcultproblem.Findingtheoptimal\\nsequenceofoperationstocomputethegradientisNP-complete(,), Naumann2008\\ninthesensethatitmayrequiresimplifyingalgebraicexpressionsintotheirleast\\nexpensiveform.\\nForexample,supposewehavevariables p 1 , p 2 , . . . , p nrepresentingprobabilities\\nandvariables z 1 , z 2 , . . . , z nrepresentingunnormalized logprobabilities. Suppose\\nwedeﬁne\\nq i=exp( z i)\\ue050\\niexp( z i), (6.57)\\nwherewebuildthesoftmaxfunctionoutofexponentiation,summationanddivision\\noperations,\\xa0and\\xa0construct\\xa0a cross-entropyloss J=−\\ue050\\ni p ilog q i.Ahuman\\nmathematician canobservethatthederivativeof Jwithrespectto z itakesavery\\nsimpleform: q i− p i.Theback-propagation algorithmisnotcapableofsimplifying\\nthegradientthisway,andwillinsteadexplicitlypropagategradientsthroughallof\\nthelogarithmandexponentiationoperationsintheoriginalgraph.Somesoftware\\nlibrariessuchasTheano( ,; ,)areableto Bergstra e t a l .2010Bastien e t a l .2012\\nperformsomekindsofalgebraicsubstitutiontoimproveoverthegraphproposed\\nbythepureback-propagation algorithm.\\nWhentheforwardgraph Ghasasingleoutputnodeandeachpartialderivative\\n∂ u() i\\n∂ u() jcanbecomputedwithaconstantamountofcomputation,back-propagation\\nguaranteesthatthenumberofcomputations forthegradientcomputationisof\\nthesameorderasthenumberofcomputations fortheforwardcomputation: this\\ncanbeseeninalgorithm becauseeachlocalpartialderivative 6.2∂ u() i\\n∂ u() jneedsto\\nbecomputedonlyoncealongwithanassociatedmultiplication andadditionfor\\ntherecursivechain-ruleformulation(equation).Theoverallcomputationis 6.49\\ntherefore O(#edges).However,itcanpotentiallybereducedbysimplifyingthe\\ncomputational graphconstructedbyback-propagation,andthisisanNP-complete\\ntask.\\xa0ImplementationssuchasTheanoandTensorFlowuseheuristicsbasedon\\nmatchingknownsimpliﬁcationpatternsinordertoiterativelyattempttosimplify\\nthegraph.Wedeﬁnedback-propagation onlyforthecomputationofagradientofa\\nscalaroutputbutback-propagationcanbeextendedtocomputeaJacobian(either\\nof kdiﬀerentscalarnodesinthegraph,orofatensor-valuednodecontaining k\\nvalues).Anaiveimplementation maythenneed ktimesmorecomputation: for\\n2 2 2', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\neachscalarinternalnodeintheoriginalforwardgraph,thenaiveimplementation\\ncomputes kgradientsinsteadofasinglegradient.Whenthenumberofoutputsof\\nthegraphislargerthanthenumberofinputs,itissometimespreferabletouse\\nanotherformofautomaticdiﬀerentiationcalledforwardmodeaccumulation.\\nForwardmodecomputationhasbeenproposedforobtainingreal-timecomputation\\nofgradientsinrecurrentnetworks,forexample( ,).This WilliamsandZipser1989\\nalsoavoidstheneedtostorethevaluesandgradientsforthewholegraph,trading\\noﬀcomputational eﬃciencyformemory.Therelationshipbetweenforwardmode\\nandbackwardmodeisanalogoustotherelationshipbetweenleft-multiplyingversus\\nright-multiplyingasequenceofmatrices,suchas\\nABCD , (6.58)\\nwherethematricescanbethoughtofasJacobianmatrices.Forexample,ifD\\nisacolumnvectorwhileAhasmanyrows,thiscorrespondstoagraphwitha\\nsingleoutputandmanyinputs,andstartingthemultiplications fromtheend\\nandgoingbackwardsonlyrequiresmatrix-vector products.Thiscorrespondsto\\nthebackwardmode.Instead,startingtomultiplyfromtheleftwouldinvolvea\\nseriesofmatrix-matrix products,whichmakesthewholecomputationmuchmore\\nexpensive.However,ifAhasfewerrowsthanDhascolumns,itischeapertorun\\nthemultiplications left-to-right,correspondingtotheforwardmode.\\nInmanycommunitiesoutsideofmachinelearning,itismorecommontoim-\\nplementdiﬀerentiationsoftwarethatactsdirectlyontraditionalprogramming\\nlanguagecode,suchasPythonorCcode,andautomatically generatesprograms\\nthatdiﬀerentiatefunctionswrittenintheselanguages.Inthedeeplearningcom-\\nmunity,computational graphsareusuallyrepresentedbyexplicitdatastructures\\ncreatedbyspecializedlibraries.Thespecializedapproachhasthedrawbackof\\nrequiringthelibrarydevelopertodeﬁnethebpropmethodsforeveryoperation\\nandlimitingtheuserofthelibrarytoonlythoseoperationsthathavebeendeﬁned.\\nHowever,thespecializedapproachalsohasthebeneﬁtofallowingcustomized\\nback-propagationrulestobedevelopedforeachoperation,allowingthedeveloper\\ntoimprovespeedorstabilityinnon-obviouswaysthatanautomaticprocedure\\nwouldpresumablybeunabletoreplicate.\\nBack-propagationisthereforenottheonlywayortheoptimalwayofcomputing\\nthegradient,butitisaverypracticalmethodthatcontinuestoservethedeep\\nlearningcommunityverywell.Inthefuture,diﬀerentiation technologyfordeep\\nnetworksmayimproveasdeeplearningpractitionersbecomemoreawareofadvances\\ninthebroaderﬁeldofautomaticdiﬀerentiation.\\n2 2 3', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\n6.5.10Higher-OrderDerivatives\\nSomesoftwareframeworkssupporttheuseofhigher-orderderivatives.Amongthe\\ndeeplearningsoftwareframeworks,thisincludesatleastTheanoandTensorFlow.\\nTheselibrariesusethesamekindofdatastructuretodescribetheexpressionsfor\\nderivativesastheyusetodescribetheoriginalfunctionbeingdiﬀerentiated.This\\nmeansthatthesymbolicdiﬀerentiation machinerycanbeappliedtoderivatives.\\nInthecontextofdeeplearning,itisraretocomputeasinglesecondderivative\\nofascalarfunction.Instead,weareusuallyinterestedinpropertiesoftheHessian\\nmatrix.Ifwehaveafunction f: Rn→ R,thentheHessianmatrixisofsize n n×.\\nIntypicaldeeplearningapplications, nwillbethenumberofparametersinthe\\nmodel,whichcouldeasilynumberinthebillions.TheentireHessianmatrixis\\nthusinfeasibletoevenrepresent.\\nInsteadofexplicitlycomputingtheHessian,thetypicaldeeplearningapproach\\nistouseKrylovmethods.Krylovmethodsareasetofiterativetechniquesfor\\nperformingvariousoperationslikeapproximately invertingamatrixorﬁnding\\napproximationstoitseigenvectorsoreigenvalues,withoutusinganyoperation\\notherthanmatrix-vector products.\\nInordertouseKrylovmethodsontheHessian,weonlyneedtobeableto\\ncomputetheproductbetweentheHessianmatrixHandanarbitraryvectorv.A\\nstraightforwardtechnique( ,)fordoingsoistocompute Christianson1992\\nHv= ∇ x\\ue068\\n(∇ x f x())\\ue03ev\\ue069\\n. (6.59)\\nBothofthegradientcomputations inthisexpressionmaybecomputedautomati-\\ncallybytheappropriatesoftwarelibrary.Notethattheoutergradientexpression\\ntakesthegradientofafunctionoftheinnergradientexpression.\\nIfvisitselfavectorproducedbyacomputational graph,itisimportantto\\nspecifythattheautomaticdiﬀerentiationsoftwareshouldnotdiﬀerentiatethrough\\nthegraphthatproduced.v\\nWhilecomputingtheHessianisusuallynotadvisable,itispossibletodowith\\nHessianvectorproducts.OnesimplycomputesHe( ) iforall i= 1 , . . . , n ,where\\ne( ) iistheone-hotvectorwith e( ) i\\ni= 1andallotherentriesequalto0.\\n6. 6 Hi s t or i c a l Not es\\nFeedforwardnetworkscanbeseenaseﬃcientnonlinearfunctionapproximators\\nbasedonusinggradientdescenttominimizetheerrorinafunctionapproximation.\\n2 2 4', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nFromthispointofview,themodernfeedforwardnetworkistheculminationof\\ncenturiesofprogressonthegeneralfunctionapproximationtask.\\nThechainrulethatunderliestheback-propagation algorithmwasinvented\\ninthe17thcentury(,;,).Calculusandalgebrahave Leibniz1676L’Hôpital1696\\nlongbeenusedtosolveoptimization problemsinclosedform,butgradientdescent\\nwasnotintroducedasatechniqueforiterativelyapproximating thesolutionto\\noptimization problemsuntilthe19thcentury(Cauchy1847,).\\nBeginninginthe1940s,thesefunctionapproximation techniqueswereusedto\\nmotivatemachinelearningmodelssuchastheperceptron.However,theearliest\\nmodelswerebasedonlinearmodels.CriticsincludingMarvinMinskypointedout\\nseveraloftheﬂawsofthelinearmodelfamily,suchasitsinabilitytolearnthe\\nXORfunction,whichledtoabacklashagainsttheentireneuralnetworkapproach.\\nLearningnonlinearfunctionsrequiredthedevelopmentofamultilayerper-\\nceptronandameansofcomputingthegradientthroughsuchamodel.Eﬃcient\\napplicationsofthechainrulebasedondynamicprogramming begantoappear\\ninthe1960sand1970s,mostlyforcontrolapplications(,;Kelley1960Brysonand\\nDenham1961Dreyfus1962BrysonandHo1969Dreyfus1973 ,;,; ,;,)butalsofor\\nsensitivityanalysis(,). Linnainmaa1976Werbos1981()proposedapplyingthese\\ntechniquestotrainingartiﬁcialneuralnetworks.Theideawasﬁnallydeveloped\\ninpracticeafterbeingindependentlyrediscoveredindiﬀerentways(,;LeCun1985\\nParker1985Rumelhart 1986a ,; e t a l .,).ThebookParallelDistributedPro-\\ncessingpresentedtheresultsofsomeoftheﬁrstsuccessfulexperimentswith\\nback-propagationinachapter( ,)thatcontributedgreatly Rumelhart e t a l .1986b\\ntothepopularization ofback-propagation andinitiatedaveryactiveperiodof\\nresearchinmulti-layerneuralnetworks.\\xa0However,theideasputforwardbythe\\nauthorsofthatbookandinparticularbyRumelhartandHintongomuchbeyond\\nback-propagation.\\xa0Theyincludecrucialideasaboutthepossiblecomputational\\nimplementationofseveralcentralaspectsofcognitionandlearning,whichcame\\nunderthenameof“connectionism” becauseoftheimportancethisschoolofthought\\nplacesontheconnectionsbetweenneuronsasthelocusoflearningandmemory.\\nInparticular,theseideasincludethenotionofdistributedrepresentation(Hinton\\ne t a l .,).1986\\nFollowingthesuccessofback-propagatio n,neuralnetworkresearchgainedpop-\\nularityandreachedapeakintheearly1990s.Afterwards,othermachinelearning\\ntechniquesbecamemorepopularuntilthemoderndeeplearningrenaissancethat\\nbeganin2006.\\nThecoreideasbehindmodernfeedforwardnetworkshavenotchangedsub-\\nstantiallysincethe1980s.\\xa0Thesameback-propagationalgorithmandthesame\\n2 2 5', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\napproachestogradientdescentarestillinuse.Mostoftheimprovementinneural\\nnetworkperformancefrom1986to2015canbeattributedtotwofactors.First,\\nlargerdatasetshavereducedthedegreetowhichstatisticalgeneralization isa\\nchallengeforneuralnetworks.Second,neuralnetworkshavebecomemuchlarger,\\nduetomorepowerfulcomputers,andbettersoftwareinfrastructure.However,a\\nsmallnumberofalgorithmicchangeshaveimprovedtheperformance ofneural\\nnetworksnoticeably.\\nOneofthesealgorithmicchangeswasthereplacementofmeansquarederror\\nwiththecross-entropyfamilyoflossfunctions.Meansquarederrorwaspopularin\\nthe1980sand1990s,butwasgraduallyreplacedbycross-entropylossesandthe\\nprincipleofmaximumlikelihoodasideasspreadbetweenthestatisticscommunity\\nandthemachinelearningcommunity.Theuseofcross-entropylossesgreatly\\nimprovedtheperformanceofmodelswithsigmoidandsoftmaxoutputs,which\\nhadpreviouslysuﬀeredfromsaturationandslowlearningwhenusingthemean\\nsquarederrorloss.\\nTheothermajoralgorithmicchangethathasgreatlyimprovedtheperformance\\noffeedforwardnetworkswasthereplacementofsigmoidhiddenunitswithpiecewise\\nlinearhiddenunits,suchasrectiﬁedlinearunits.Rectiﬁcationusingthemax{0 , z}\\nfunctionwasintroducedinearlyneuralnetworkmodelsanddatesbackatleast\\nasfarastheCognitronandNeocognitron(Fukushima19751980,,).Theseearly\\nmodelsdid\\xa0notuserectiﬁed\\xa0linearunits,\\xa0but\\xa0insteadappliedrectiﬁcation\\xa0to\\nnonlinearfunctions.Despitetheearlypopularityofrectiﬁcation,rectiﬁcationwas\\nlargelyreplacedbysigmoidsinthe1980s,perhapsbecausesigmoidsperformbetter\\nwhenneuralnetworksareverysmall.Asoftheearly2000s,rectiﬁedlinearunits\\nwereavoidedduetoasomewhatsuperstitiousbeliefthatactivationfunctionswith\\nnon-diﬀerentiablepointsmustbeavoided.Thisbegantochangeinabout2009.\\nJarrett2009 e t a l .()observedthat“usingarectifyingnonlinearityisthesinglemost\\nimportantfactorinimprovingtheperformanceofarecognitionsystem”among\\nseveraldiﬀerentfactorsofneuralnetworkarchitecturedesign.\\nForsmalldatasets, ()observedthatusingrectifyingnon- Jarrett e t a l .2009\\nlinearitiesisevenmoreimportantthanlearningtheweightsofthehiddenlayers.\\nRandomweightsaresuﬃcienttopropagateusefulinformationthrougharectiﬁed\\nlinearnetwork,allowingtheclassiﬁerlayeratthetoptolearnhowtomapdiﬀerent\\nfeaturevectorstoclassidentities.\\nWhenmoredataisavailable,learningbeginstoextractenoughusefulknowledge\\ntoexceedtheperformanceofrandomlychosenparameters. () Glorot e t a l .2011a\\nshowedthatlearningisfareasierindeeprectiﬁedlinearnetworksthanindeep\\nnetworksthathavecurvatureortwo-sidedsaturationintheiractivationfunctions.\\n2 2 6', 'CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nRectiﬁedlinearunitsarealsoofhistoricalinterestbecausetheyshowthat\\nneurosciencehascontinuedtohave\\xa0aninﬂuenceonthe\\xa0developmentofdeep\\nlearningalgorithms. ()motivaterectiﬁedlinearunitsfrom Glorot e t a l .2011a\\nbiologicalconsiderations.Thehalf-rectifying nonlinearitywasintendedtocapture\\nthesepropertiesofbiologicalneurons:1)Forsomeinputs,biologicalneuronsare\\ncompletelyinactive.2)Forsomeinputs,abiologicalneuron’soutputisproportional\\ntoitsinput.3)Mostofthetime,biologicalneuronsoperateintheregimewhere\\ntheyareinactive(i.e.,theyshouldhavesparseactivations).\\nWhenthemodernresurgenceofdeeplearningbeganin2006,feedforward\\nnetworkscontinuedtohaveabadreputation.Fromabout2006-2012,itwaswidely\\nbelievedthatfeedforwardnetworkswouldnotperformwellunlesstheywereassisted\\nbyothermodels,suchasprobabilisticmodels.Today,itisnowknownthatwiththe\\nrightresourcesandengineeringpractices,feedforwardnetworksperformverywell.\\nToday,gradient-basedlearninginfeedforwardnetworksisusedasatooltodevelop\\nprobabilisticmodels,suchasthevariationalautoencoderandgenerativeadversarial\\nnetworks,describedinchapter.Ratherthanbeingviewedasanunreliable 20\\ntechnologythatmustbesupportedbyothertechniques,gradient-basedlearningin\\nfeedforwardnetworkshasbeenviewedsince2012asapowerfultechnologythat\\nmaybeappliedtomanyothermachinelearningtasks.In2006,thecommunity\\nusedunsupervisedlearningtosupportsupervisedlearning,andnow,ironically,it\\nismorecommontousesupervisedlearningtosupportunsupervisedlearning.\\nFeedforwardnetworkscontinuetohaveunfulﬁlledpotential.Inthefuture,we\\nexpecttheywillbeappliedtomanymoretasks,andthatadvancesinoptimization\\nalgorithmsandmodeldesignwillimprovetheirperformanceevenfurther.This\\nchapterhasprimarilydescribedtheneuralnetworkfamilyofmodels.Inthe\\nsubsequentchapters,weturntohowtousethesemodels—howtoregularizeand\\ntrainthem.\\n2 2 7']}, 'DeepLearning-chapter-5.pdf': {'text': ['C h a p t e r 5\\nMac h i n e L e ar n i n g B asics\\nDeeplearningisaspeciﬁckindofmachinelearning.Inordertounderstand\\ndeeplearningwell,onemusthaveasolidunderstandingofthebasicprinciplesof\\nmachinelearning.Thischapterprovidesabriefcourseinthemostimportantgeneral\\nprinciplesthatwillbeappliedthroughouttherestofthebook.Novicereadersor\\nthosewhowantawiderperspectiveareencouragedtoconsidermachinelearning\\ntextbookswithamorecomprehensivecoverageofthefundamentals,suchasMurphy\\n()or().Ifyouarealreadyfamiliarwithmachinelearningbasics, 2012Bishop2006\\nfeelfreetoskipaheadtosection.Thatsectioncoverssomeperspectives 5.11\\non\\xa0traditional machinelearning\\xa0techniques\\xa0thathavestrongly\\xa0inﬂuenced the\\ndevelopmentofdeeplearningalgorithms.\\nWebeginwithadeﬁnitionofwhatalearningalgorithmis,andpresentan\\nexample:thelinearregressionalgorithm.\\xa0W ethenproceedtodescribehowthe\\nchallengeofﬁttingthetrainingdatadiﬀersfromthechallengeofﬁndingpatterns\\nthatgeneralizetonewdata.Mostmachinelearningalgorithmshavesettings\\ncalledhyperparametersthatmustbedeterminedexternaltothelearningalgorithm\\nitself;wediscusshowtosettheseusingadditionaldata.Machinelearningis\\nessentiallyaformofappliedstatisticswithincreasedemphasisontheuseof\\ncomputerstostatisticallyestimatecomplicatedfunctionsandadecreasedemphasis\\nonprovingconﬁdenceintervalsaroundthesefunctions;wethereforepresentthe\\ntwocentralapproachestostatistics:frequentistestimatorsandBayesianinference.\\nMostmachinelearningalgorithmscanbedividedintothecategoriesofsupervised\\nlearningandunsupervisedlearning;wedescribethesecategoriesandgivesome\\nexamplesofsimplelearningalgorithmsfromeachcategory.\\xa0Mostdeeplearning\\nalgorithmsare\\xa0basedonan\\xa0optimization algorithmcalled\\xa0stochasticgradient\\ndescent.Wedescribehowtocombinevariousalgorithmcomponentssuchas\\n98', 'CHAPTER5.MACHINELEARNINGBASICS\\nanoptimization algorithm,acostfunction,amodel,andadatasettobuilda\\nmachinelearningalgorithm.Finally,insection,wedescribesomeofthe 5.11\\nfactorsthathavelimitedtheabilityoftraditionalmachinelearningtogeneralize.\\nThesechallengeshavemotivatedthedevelopmentofdeeplearningalgorithmsthat\\novercometheseobstacles.\\n5.1LearningAlgorithms\\nAmachinelearningalgorithmisanalgorithmthatisabletolearnfromdata.But\\nwhatdowemeanbylearning?Mitchell1997()providesthedeﬁnition“Acomputer\\nprogramissaidtolearnfromexperienceEwithrespecttosomeclassoftasksT\\nandperformancemeasureP,ifitsperformanceattasksinT,asmeasuredbyP,\\nimproveswithexperienceE.”Onecanimagineaverywidevarietyofexperiences\\nE,tasksT,andperformancemeasuresP,andwedonotmakeanyattemptinthis\\nbooktoprovideaformaldeﬁnitionofwhatmaybeusedforeachoftheseentities.\\nInstead,thefollowingsectionsprovideintuitivedescriptionsandexamplesofthe\\ndiﬀerentkindsoftasks,performance measuresandexperiencesthatcanbeused\\ntoconstructmachinelearningalgorithms.\\n5.1.1TheTask, T\\nMachinelearningallowsustotackletasksthataretoodiﬃculttosolvewith\\nﬁxedprogramswrittenanddesignedbyhumanbeings.Fromascientiﬁcand\\nphilosophicalpointofview,machinelearningisinterestingbecausedevelopingour\\nunderstandingofmachinelearningentailsdevelopingourunderstandingofthe\\nprinciplesthatunderlieintelligence.\\nInthisrelativelyformaldeﬁnitionoftheword“task,”theprocessoflearning\\nitselfisnotthetask.Learningisourmeansofattainingtheabilitytoperformthe\\ntask.Forexample,ifwewantarobottobeabletowalk,thenwalkingisthetask.\\nWecouldprogramtherobottolearntowalk,orwecouldattempttodirectlywrite\\naprogramthatspeciﬁeshowtowalkmanually.\\nMachinelearningtasksareusuallydescribedintermsofhowthemachine\\nlearningsystemshouldprocessanexample.Anexampleisacollectionoffeatures\\nthathavebeenquantitativelymeasuredfromsomeobjectoreventthatwewant\\nthemachinelearningsystemtoprocess.Wetypicallyrepresentanexampleasa\\nvectorx∈ Rnwhereeachentryx iofthevectorisanotherfeature.Forexample,\\nthefeaturesofanimageareusuallythevaluesofthepixelsintheimage.\\n9 9', 'CHAPTER5.MACHINELEARNINGBASICS\\nManykindsoftaskscanbesolvedwithmachinelearning.Someofthemost\\ncommonmachinelearningtasksincludethefollowing:\\n•Classiﬁcation:Inthistypeoftask,thecomputerprogramisaskedtospecify\\nwhichofkcategoriessomeinputbelongsto.Tosolvethistask,thelearning\\nalgorithmisusuallyaskedtoproduceafunctionf: Rn→{1,...,k}.When\\ny=f(x),themodelassignsaninputdescribedbyvectorxtoacategory\\nidentiﬁedbynumericcodey.Thereareothervariantsoftheclassiﬁcation\\ntask,forexample,wherefoutputsaprobabilitydistributionoverclasses.\\nAnexampleofaclassiﬁcationtaskisobjectrecognition,wheretheinput\\nisanimage(usuallydescribedasasetofpixelbrightnessvalues),andthe\\noutputisanumericcodeidentifyingtheobjectintheimage.Forexample,\\ntheWillowGaragePR2robotisabletoactasawaiterthatcanrecognize\\ndiﬀerentkindsofdrinksanddeliverthemtopeopleoncommand(Good-\\nfellow2010etal.,).Modernobjectrecognitionisbestaccomplishedwith\\ndeeplearning( ,; ,).Object Krizhevskyetal.2012IoﬀeandSzegedy2015\\nrecognitionisthesamebasictechnologythatallowscomputerstorecognize\\nfaces(Taigman 2014etal.,),whichcanbeusedtoautomatically tagpeople\\ninphotocollectionsandallowcomputerstointeractmorenaturallywith\\ntheirusers.\\n•Classiﬁcationwithmissinginputs:Classiﬁcationbecomesmorechal-\\nlengingifthecomputerprogramisnotguaranteedthateverymeasurement\\ninitsinputvectorwillalwaysbeprovided.Inordertosolvetheclassiﬁcation\\ntask,thelearningalgorithmonlyhastodeﬁneafunctionmapping single\\nfromavectorinputtoacategoricaloutput.Whensomeoftheinputsmay\\nbemissing,ratherthanprovidingasingleclassiﬁcationfunction,thelearning\\nalgorithmmustlearnaoffunctions.Eachfunctioncorrespondstoclassi- set\\nfyingxwithadiﬀerentsubsetofitsinputsmissing.Thiskindofsituation\\narisesfrequentlyinmedicaldiagnosis,becausemanykindsofmedicaltests\\nareexpensiveorinvasive.Onewaytoeﬃcientlydeﬁnesuchalargeset\\noffunctionsistolearnaprobabilitydistributionoveralloftherelevant\\nvariables,thensolvetheclassiﬁcationtaskbymarginalizing outthemissing\\nvariables.Withninputvariables,wecannowobtainall2ndiﬀerentclassiﬁ-\\ncationfunctionsneededforeachpossiblesetofmissinginputs,butweonly\\nneedtolearnasinglefunctiondescribingthejointprobabilitydistribution.\\nSeeGoodfellow2013betal.()foranexampleofadeepprobabilisticmodel\\nappliedtosuchataskinthisway.Manyoftheothertasksdescribedinthis\\nsectioncanalsobegeneralizedtoworkwithmissinginputs;classiﬁcation\\nwithmissinginputsisjustoneexampleofwhatmachinelearningcando.\\n1 0 0', 'CHAPTER5.MACHINELEARNINGBASICS\\n•Regression:Inthistypeoftask,thecomputerprogramisaskedtopredicta\\nnumericalvaluegivensomeinput.Tosolvethistask,thelearningalgorithm\\nisaskedtooutputafunctionf: Rn→ R.Thistypeoftaskissimilarto\\nclassiﬁcation,exceptthattheformatofoutputisdiﬀerent.Anexampleof\\naregressiontaskisthepredictionoftheexpectedclaimamountthatan\\ninsuredpersonwillmake(usedtosetinsurancepremiums),ortheprediction\\noffuturepricesofsecurities.Thesekindsofpredictionsarealsousedfor\\nalgorithmictrading.\\n•Transcription:Inthistypeoftask,themachinelearningsystemisasked\\ntoobservearelativelyunstructuredrepresentationofsomekindofdataand\\ntranscribeitintodiscrete,textualform.Forexample,inopticalcharacter\\nrecognition,thecomputerprogramisshownaphotographcontainingan\\nimageoftextandisaskedtoreturnthistextintheformofasequence\\nofcharacters(e.g.,inASCIIorUnicodeformat).GoogleStreetViewuses\\ndeeplearningtoprocessaddressnumbersinthisway( , Goodfellow etal.\\n2014d).Anotherexampleisspeechrecognition,wherethecomputerprogram\\nisprovidedanaudiowaveformandemitsasequenceofcharactersorword\\nIDcodesdescribingthewordsthatwerespokenintheaudiorecording.Deep\\nlearningisacrucialcomponentofmodernspeechrecognitionsystemsused\\natmajorcompaniesincludingMicrosoft,IBMandGoogle( ,Hintonetal.\\n2012b).\\n•Machinetranslation:Inamachinetranslationtask,theinputalready\\nconsistsofasequenceofsymbolsinsomelanguage,andthecomputerprogram\\nmustconvertthisintoasequenceofsymbolsinanotherlanguage.Thisis\\ncommonlyappliedtonaturallanguages,suchastranslatingfromEnglishto\\nFrench.Deeplearninghasrecentlybeguntohaveanimportantimpacton\\nthiskindoftask(Sutskever2014Bahdanau 2015 etal.,; etal.,).\\n•Structuredoutput:Structuredoutputtasksinvolveanytaskwherethe\\noutputisavector(orotherdatastructurecontainingmultiplevalues)with\\nimportantrelationshipsbetweenthediﬀerentelements.Thisisabroad\\ncategory,andsubsumesthetranscriptionandtranslationtasksdescribed\\nabove,butalsomanyothertasks.Oneexampleisparsing—mappinga\\nnaturallanguagesentenceintoatreethatdescribesitsgrammaticalstructure\\nandtaggingnodesofthetreesasbeingverbs,nouns,oradverbs,andsoon.\\nSee ()foranexampleofdeeplearningappliedtoaparsing Collobert2011\\ntask.Anotherexampleispixel-wisesegmentationofimages,\\xa0wherethe\\ncomputerprogramassignseverypixelinanimagetoaspeciﬁccategory.For\\n1 0 1', 'CHAPTER5.MACHINELEARNINGBASICS\\nexample,deeplearningcanbeusedtoannotatethelocationsofroadsin\\naerialphotographs(MnihandHinton2010,).Theoutputneednothaveits\\nformmirrorthestructureoftheinputascloselyasintheseannotation-style\\ntasks.Forexample,inimagecaptioning,thecomputerprogramobservesan\\nimageandoutputsanaturallanguagesentencedescribingtheimage(Kiros\\netal. etal. ,,;2014abMao,;2015Vinyals2015bDonahue2014 etal.,; etal.,;\\nKarpathyandLi2015Fang2015Xu2015 ,;etal.,;etal.,).Thesetasksare\\ncalledstructuredoutputtasksbecausetheprogrammustoutputseveral\\nvaluesthatarealltightlyinter-related.Forexample,thewordsproducedby\\nanimagecaptioningprogrammustformavalidsentence.\\n•Anomalydetection:Inthistypeoftask,thecomputerprogramsifts\\nthroughasetofeventsorobjects,andﬂagssomeofthemasbeingunusual\\noratypical.Anexampleofananomalydetectiontaskiscreditcardfraud\\ndetection.Bymodelingyourpurchasinghabits,acreditcardcompanycan\\ndetectmisuseofyourcards.Ifathiefstealsyourcreditcardorcreditcard\\ninformation,thethief’spurchaseswilloftencomefromadiﬀerentprobability\\ndistributionoverpurchasetypesthanyourown.Thecreditcardcompany\\ncanpreventfraudbyplacingaholdonanaccountassoonasthatcardhas\\nbeenusedforanuncharacteris ticpurchase.See ()fora Chandola etal.2009\\nsurveyofanomalydetectionmethods.\\n•Synthesisandsampling:Inthistypeoftask,themachinelearningal-\\ngorithmisaskedtogeneratenewexamplesthataresimilartothoseinthe\\ntrainingdata.\\xa0Synthesisandsamplingviamachinelearningcanbeuseful\\nformediaapplicationswhereitcanbeexpensiveorboringforanartistto\\ngeneratelargevolumesofcontentbyhand.Forexample,videogamescan\\nautomatically generatetexturesforlargeobjectsorlandscapes,ratherthan\\nrequiringanartisttomanuallylabeleachpixel(,).Insome Luoetal.2013\\ncases,wewantthesamplingorsynthesisproceduretogeneratesomespeciﬁc\\nkindofoutputgiventheinput.Forexample,inaspeechsynthesistask,we\\nprovideawrittensentenceandasktheprogramtoemitanaudiowaveform\\ncontainingaspokenversionofthatsentence.\\xa0Thisisakindofstructured\\noutputtask,butwiththeaddedqualiﬁcationthatthereisnosinglecorrect\\noutputforeachinput,andweexplicitlydesirealargeamountofvariationin\\ntheoutput,inorderfortheoutputtoseemmorenaturalandrealistic.\\n•Imputationofmissingvalues:Inthistypeoftask,themachinelearning\\nalgorithmisgivenanewexamplex∈ Rn,butwithsomeentriesx iofx\\nmissing.Thealgorithmmustprovideapredictionofthevaluesofthemissing\\nentries.\\n1 0 2', 'CHAPTER5.MACHINELEARNINGBASICS\\n•Denoising:Inthistypeoftask,themachinelearningalgorithmisgivenin\\ninputacorruptedexample˜x∈ Rnobtainedbyanunknowncorruptionprocess\\nfromacleanexamplex∈ Rn.Thelearnermustpredictthecleanexample\\nxfromitscorruptedversion˜x,ormoregenerallypredicttheconditional\\nprobabilitydistributionp(x|˜x).\\n•Densityestimationorprobabilitymassfunctionestimation:In\\nthedensityestimationproblem,themachinelearningalgorithmisasked\\ntolearnafunctionpmodel: Rn→ R,wherepmodel(x)canbeinterpreted\\nasaprobabilitydensityfunction(if xiscontinuous)oraprobabilitymass\\nfunction(if xisdiscrete)onthespacethattheexamplesweredrawnfrom.\\nTodosuchataskwell(wewillspecifyexactlywhatthatmeanswhenwe\\ndiscussperformancemeasuresP),thealgorithmneedstolearnthestructure\\nofthedataithasseen.Itmustknowwhereexamplesclustertightlyand\\nwheretheyareunlikelytooccur.Mostofthetasksdescribedaboverequire\\nthelearningalgorithmtoatleastimplicitlycapturethestructureofthe\\nprobabilitydistribution.Densityestimationallowsustoexplicitlycapture\\nthatdistribution.Inprinciple,wecanthenperformcomputations onthat\\ndistributioninordertosolvetheothertasksaswell.Forexample,ifwe\\nhaveperformeddensityestimationtoobtainaprobabilitydistributionp(x),\\nwecanusethatdistributiontosolvethemissingvalueimputationtask.If\\navaluex iismissingandalloftheothervalues,denotedx − i,aregiven,\\nthenweknowthedistributionoveritisgivenbyp(x i|x − i).Inpractice,\\ndensityestimationdoesnotalwaysallowustosolvealloftheserelatedtasks,\\nbecauseinmanycasestherequiredoperationsonp(x)arecomputationally\\nintractable.\\nOfcourse,manyothertasksandtypesoftasksarepossible.Thetypesoftasks\\nwelisthereareintendedonlytoprovideexamplesofwhatmachinelearningcan\\ndo,nottodeﬁnearigidtaxonomyoftasks.\\n5.1.2ThePerformanceMeasure, P\\nInordertoevaluatetheabilitiesofamachinelearningalgorithm,wemustdesign\\naquantitativemeasureofitsperformance.UsuallythisperformancemeasurePis\\nspeciﬁctothetaskbeingcarriedoutbythesystem. T\\nFortaskssuchasclassiﬁcation,classiﬁcationwithmissinginputs,andtran-\\nscription,weoftenmeasuretheaccuracyofthemodel.Accuracyisjustthe\\nproportionofexamplesforwhichthemodelproducesthecorrectoutput.Wecan\\n1 0 3', 'CHAPTER5.MACHINELEARNINGBASICS\\nalsoobtainequivalentinformationbymeasuringtheerrorrate,theproportion\\nofexamplesforwhichthemodelproducesanincorrectoutput.Weoftenreferto\\ntheerrorrateastheexpected0-1loss.The0-1lossonaparticularexampleis0\\nifitiscorrectlyclassiﬁedand1ifitisnot.Fortaskssuchasdensityestimation,\\nitdoesnotmakesensetomeasureaccuracy,errorrate,oranyotherkindof0-1\\nloss.Instead,wemustuseadiﬀerentperformancemetricthatgivesthemodel\\nacontinuous-valuedscoreforeachexample.Themostcommonapproachisto\\nreporttheaveragelog-probabilit ythemodelassignstosomeexamples.\\nUsuallyweareinterestedinhowwellthemachinelearningalgorithmperforms\\nondatathatithasnotseenbefore,sincethisdetermineshowwellitwillworkwhen\\ndeployedintherealworld.Wethereforeevaluatetheseperformancemeasuresusing\\natestsetofdatathatisseparatefromthedatausedfortrainingthemachine\\nlearningsystem.\\nThechoiceofperformancemeasuremayseemstraightforwardandobjective,\\nbutitisoftendiﬃculttochooseaperformancemeasurethatcorrespondswellto\\nthedesiredbehaviorofthesystem.\\nInsomecases,thisisbecauseitisdiﬃculttodecidewhatshouldbemeasured.\\nForexample,whenperformingatranscriptiontask,shouldwemeasuretheaccuracy\\nofthesystemattranscribingentiresequences,orshouldweuseamoreﬁne-grained\\nperformancemeasurethatgivespartialcreditforgettingsomeelementsofthe\\nsequencecorrect?Whenperformingaregressiontask,shouldwepenalizethe\\nsystemmoreifitfrequentlymakesmedium-sizedmistakesorifitrarelymakes\\nverylargemistakes?Thesekindsofdesignchoicesdependontheapplication.\\nInothercases,weknowwhatquantitywewouldideallyliketomeasure,but\\nmeasuringitisimpractical.Forexample,thisarisesfrequentlyinthecontextof\\ndensityestimation.Manyofthebestprobabilisticmodelsrepresentprobability\\ndistributionsonlyimplicitly.Computingtheactualprobabilityvalueassignedto\\naspeciﬁcpointinspaceinmanysuchmodelsisintractable.Inthesecases,one\\nmustdesignanalternativecriterionthatstillcorrespondstothedesignobjectives,\\nordesignagoodapproximationtothedesiredcriterion.\\n5.1.3TheExperience, E\\nMachinelearningalgorithmscanbebroadlycategorizedasunsupervisedor\\nsupervisedbywhatkindofexperiencetheyareallowedtohaveduringthe\\nlearningprocess.\\nMostofthelearningalgorithmsinthisbookcanbeunderstoodasbeingallowed\\ntoexperienceanentiredataset.Adatasetisacollectionofmanyexamples,as\\n1 0 4', 'CHAPTER5.MACHINELEARNINGBASICS\\ndeﬁnedinsection.Sometimeswewillalsocallexamples . 5.1.1 datapoints\\nOneoftheoldestdatasetsstudiedbystatisticiansandmachinelearningre-\\nsearchersistheIrisdataset(,).Itisacollectionofmeasurementsof Fisher1936\\ndiﬀerentpartsof150irisplants.Eachindividualplantcorrespondstooneexample.\\nThefeatureswithineachexamplearethemeasurementsofeachofthepartsofthe\\nplant:thesepallength,sepalwidth,petallengthandpetalwidth.Thedataset\\nalsorecordswhichspecieseachplantbelongedto.Threediﬀerentspeciesare\\nrepresentedinthedataset.\\nUnsupervisedlearningalgorithmsexperienceadatasetcontainingmany\\nfeatures,thenlearnusefulpropertiesofthestructureofthisdataset.Inthecontext\\nofdeeplearning,weusuallywanttolearntheentireprobabilitydistributionthat\\ngeneratedadataset,whetherexplicitlyasindensityestimationorimplicitlyfor\\ntaskslikesynthesisordenoising.Someotherunsupervisedlearningalgorithms\\nperformotherroles,likeclustering,whichconsistsofdividingthedatasetinto\\nclustersofsimilarexamples.\\nSupervisedlearningalgorithmsexperienceadatasetcontainingfeatures,\\nbuteachexampleisalsoassociatedwithalabelortarget.Forexample,theIris\\ndatasetisannotatedwiththespeciesofeachirisplant.Asupervisedlearning\\nalgorithmcanstudytheIrisdatasetandlearntoclassifyirisplantsintothree\\ndiﬀerentspeciesbasedontheirmeasurements.\\nRoughlyspeaking,unsupervisedlearninginvolvesobservingseveralexamples\\nofarandomvector x,andattemptingtoimplicitlyorexplicitlylearntheproba-\\nbilitydistributionp( x),orsomeinterestingpropertiesofthatdistribution,while\\nsupervisedlearninginvolvesobservingseveralexamplesofarandomvector xand\\nanassociatedvalueorvector y,andlearningtopredict yfrom x,usuallyby\\nestimatingp( y x|).Thetermsupervisedlearningoriginatesfromtheviewof\\nthetarget ybeingprovidedbyaninstructororteacherwhoshowsthemachine\\nlearningsystemwhattodo.Inunsupervisedlearning,thereisnoinstructoror\\nteacher,andthealgorithmmustlearntomakesenseofthedatawithoutthisguide.\\nUnsupervisedlearningandsupervisedlearningarenotformallydeﬁnedterms.\\nThelinesbetweenthemareoftenblurred.Manymachinelearningtechnologiescan\\nbeusedtoperformbothtasks.Forexample,thechainruleofprobabilitystates\\nthatforavector x∈ Rn,thejointdistributioncanbedecomposedas\\np() = xn\\ue059\\ni=1p(x i|x1,...,x i −1). (5.1)\\nThisdecompositionmeansthatwecansolvetheostensiblyunsupervisedproblemof\\nmodelingp( x) bysplittingitintonsupervisedlearningproblems.Alternatively,we\\n1 0 5', 'CHAPTER5.MACHINELEARNINGBASICS\\ncansolvethesupervisedlearningproblemoflearningp(y| x)byusingtraditional\\nunsupervised\\xa0learningtechnologiesto\\xa0learn\\xa0thejointdistributionp( x,y)and\\ninferring\\npy(| x) =p,y( x)\\ue050\\ny\\ue030p,y( x\\ue030). (5.2)\\nThoughunsupervisedlearningandsupervisedlearningarenotcompletelyformalor\\ndistinctconcepts,theydohelptoroughlycategorizesomeofthethingswedowith\\nmachinelearningalgorithms.Traditionally,peoplerefertoregression,classiﬁcation\\nandstructuredoutputproblemsassupervisedlearning.Densityestimationin\\nsupportofothertasksisusuallyconsideredunsupervisedlearning.\\nOthervariantsofthelearningparadigmarepossible.Forexample,insemi-\\nsupervisedlearning,someexamplesincludeasupervisiontargetbutothersdo\\nnot.Inmulti-instancelearning,anentirecollectionofexamplesislabeledas\\ncontainingornotcontaininganexampleofaclass,buttheindividualmembers\\nofthecollectionarenotlabeled.Forarecentexampleofmulti-instancelearning\\nwithdeepmodels,seeKotzias 2015etal.().\\nSomemachinelearningalgorithmsdonotjustexperienceaﬁxeddataset.For\\nexample,reinforcementlearningalgorithmsinteractwithanenvironment,so\\nthereisafeedbackloopbetweenthelearningsystemanditsexperiences.\\xa0Such\\nalgorithmsarebeyondthescopeofthisbook.Pleasesee () SuttonandBarto1998\\norBertsekasandTsitsiklis1996()forinformationaboutreinforcementlearning,\\nand ()forthedeeplearningapproachtoreinforcementlearning. Mnihetal.2013\\nMostmachinelearningalgorithmssimplyexperienceadataset.Adatasetcan\\nbedescribedinmanyways.Inallcases,adatasetisacollectionofexamples,\\nwhichareinturncollectionsoffeatures.\\nOnecommonwayofdescribingadatasetiswitha .Adesign designmatrix\\nmatrixisamatrixcontainingadiﬀerentexampleineachrow.Eachcolumnofthe\\nmatrixcorrespondstoadiﬀerentfeature.Forinstance,theIrisdatasetcontains\\n150exampleswithfourfeaturesforeachexample.Thismeanswecanrepresent\\nthedatasetwithadesignmatrixX∈ R1504 ×,whereX i ,1isthesepallengthof\\nplanti,X i ,2isthesepalwidthofplanti,etc.Wewilldescribemostofthelearning\\nalgorithmsinthisbookintermsofhowtheyoperateondesignmatrixdatasets.\\nOfcourse,todescribeadatasetasadesignmatrix,itmustbepossibleto\\ndescribeeachexampleasavector,andeachofthesevectorsmustbethesamesize.\\nThisisnotalwayspossible.Forexample,ifyouhaveacollectionofphotographs\\nwithdiﬀerentwidthsandheights,thendiﬀerentphotographswillcontaindiﬀerent\\nnumbersofpixels,sonotallofthephotographs maybedescribedwiththesame\\nlengthofvector.Sectionandchapterdescribehowtohandlediﬀerent 9.7 10\\n1 0 6', 'CHAPTER5.MACHINELEARNINGBASICS\\ntypesofsuchheterogeneous data.Incaseslikethese,ratherthandescribingthe\\ndatasetasamatrixwithmrows,wewilldescribeitasasetcontainingmelements:\\n{x(1),x(2),...,x() m}.Thisnotationdoesnotimplythatanytwoexamplevectors\\nx() iandx() jhavethesamesize.\\nInthecaseofsupervisedlearning,theexamplecontainsalabelortargetas\\nwellasacollectionoffeatures.Forexample,ifwewanttousealearningalgorithm\\ntoperformobjectrecognitionfromphotographs, weneedtospecifywhichobject\\nappearsineachofthephotos.Wemightdothiswithanumericcode,with0\\nsignifyingaperson,1signifyingacar,2signifyingacat,etc.Oftenwhenworking\\nwithadatasetcontainingadesignmatrixoffeatureobservationsX,wealso\\nprovideavectoroflabels,withyy iprovidingthelabelforexample.i\\nOfcourse,sometimesthelabelmaybemorethanjustasinglenumber.For\\nexample,ifwewanttotrainaspeechrecognitionsystemtotranscribeentire\\nsentences,thenthelabelforeachexamplesentenceisasequenceofwords.\\nJustasthereisnoformaldeﬁnitionofsupervisedandunsupervisedlearning,\\nthereisnorigidtaxonomyofdatasetsorexperiences.Thestructuresdescribedhere\\ncovermostcases,butitisalwayspossibletodesignnewonesfornewapplications.\\n5.1.4Example:LinearRegression\\nOurdeﬁnitionofamachinelearningalgorithmasanalgorithmthatiscapable\\nofimprovingacomputerprogram’sperformanceatsometaskviaexperienceis\\nsomewhatabstract.Tomakethismoreconcrete,wepresentanexampleofa\\nsimplemachinelearningalgorithm:linearregression.Wewillreturntothis\\nexamplerepeatedlyasweintroducemoremachinelearningconceptsthathelpto\\nunderstanditsbehavior.\\nAsthenameimplies,linearregressionsolvesaregressionproblem.\\xa0Inother\\nwords,thegoalistobuildasystemthatcantakeavectorx∈ Rnasinputand\\npredictthevalueofascalary∈ Rasitsoutput.Inthecaseoflinearregression,\\ntheoutputisalinearfunctionoftheinput.Letˆybethevaluethatourmodel\\npredictsshouldtakeon.Wedeﬁnetheoutputtobe y\\nˆy= w\\ue03ex (5.3)\\nwherew∈ Rnisavectorof .parameters\\nParametersarevaluesthatcontrolthebehaviorofthesystem.Inthiscase,w iis\\nthecoeﬃcientthatwemultiplybyfeaturex ibeforesummingupthecontributions\\nfromallthefeatures.Wecanthinkofwasasetofweightsthatdeterminehow\\neachfeatureaﬀectstheprediction.\\xa0If afeaturex ireceivesapositiveweightw i,\\n1 0 7', 'CHAPTER5.MACHINELEARNINGBASICS\\nthenincreasingthevalueofthatfeatureincreasesthevalueofourprediction ˆy.\\nIfafeaturereceivesanegativeweight,thenincreasingthevalueofthatfeature\\ndecreasesthevalueofourprediction.Ifafeature’sweightislargeinmagnitude,\\nthenithasalargeeﬀectontheprediction.Ifafeature’sweightiszero,ithasno\\neﬀectontheprediction.\\nWethushaveadeﬁnitionofourtaskT:\\xa0topredictyfromxbyoutputting\\nˆy= w\\ue03ex.Nextweneedadeﬁnitionofourperformancemeasure,.P\\nSupposethatwehaveadesignmatrixofmexampleinputsthatwewillnot\\nusefortraining,onlyforevaluatinghowwellthemodelperforms.Wealsohave\\navectorofregressiontargetsprovidingthecorrectvalueofyforeachofthese\\nexamples.Becausethisdatasetwillonlybeusedforevaluation,wecallitthetest\\nset.WerefertothedesignmatrixofinputsasX()testandthevectorofregression\\ntargetsasy()test.\\nOnewayofmeasuringtheperformanceofthemodelistocomputethemean\\nsquarederrorofthemodelonthetestset.Ifˆy()testgivesthepredictionsofthe\\nmodelonthetestset,thenthemeansquarederrorisgivenby\\nMSEtest=1\\nm\\ue058\\ni(ˆy()test−y()test)2\\ni. (5.4)\\nIntuitively,onecanseethatthiserrormeasuredecreasesto0when ˆy()test=y()test.\\nWecanalsoseethat\\nMSEtest=1\\nm||ˆy()test−y()test||2\\n2, (5.5)\\nsotheerrorincreaseswhenevertheEuclideandistancebetweenthepredictions\\nandthetargetsincreases.\\nTomakeamachinelearningalgorithm,weneedtodesignanalgorithmthat\\nwillimprovetheweightswinawaythatreducesMSEtestwhenthealgorithm\\nisallowedtogainexperiencebyobservingatrainingset(X()train,y()train).One\\nintuitivewayofdoingthis(whichwewilljustifylater,insection)isjustto 5.5.1\\nminimizethemeansquarederroronthetrainingset,MSEtrain.\\nTominimizeMSEtrain,wecansimplysolveforwhereitsgradientis: 0\\n∇ wMSEtrain= 0 (5.6)\\n⇒∇ w1\\nm||ˆy()train−y()train||2\\n2= 0 (5.7)\\n⇒1\\nm∇ w||X()trainwy−()train||2\\n2= 0 (5.8)\\n1 0 8', 'CHAPTER5.MACHINELEARNINGBASICS\\n− − 1 0 . 0 5 0 0 0 5 1 0 . . . .\\nx1− 3− 2− 10123yL i n ea r r eg r es s i o n ex a m p l e\\n0 5 1 0 1 5 . . .\\nw10 2 0 .0 2 5 .0 3 0 .0 3 5 .0 4 0 .0 4 5 .0 5 0 .0 5 5 .MSE(train)O p t i m i za t i o n o f w\\nFigure5.1:Alinearregressionproblem,withatrainingsetconsistingoftendatapoints,\\neachcontainingonefeature.Becausethereisonlyonefeature,theweightvectorw\\ncontainsonlyasingleparametertolearn,w 1. ( L e f t )Observethatlinearregressionlearns\\ntosetw 1suchthattheliney=w 1xcomesascloseaspossibletopassingthroughallthe\\ntrainingpoints.Theplottedpointindicatesthevalueof ( R i g h t ) w 1foundbythenormal\\nequations,whichwecanseeminimizesthemeansquarederroronthetrainingset.\\n⇒∇ w\\ue010\\nX()trainwy−()train\\ue011\\ue03e\\ue010\\nX()trainwy−()train\\ue011\\n= 0(5.9)\\n⇒∇ w\\ue010\\nw\\ue03eX()train \\ue03eX()trainww−2\\ue03eX()train \\ue03ey()train+y()train \\ue03ey()train\\ue011\\n= 0\\n(5.10)\\n⇒2X()train \\ue03eX()trainwX−2()train \\ue03ey()train= 0(5.11)\\n⇒w=\\ue010\\nX()train \\ue03eX()train\\ue011−1\\nX()train \\ue03ey()train(5.12)\\nThesystemofequationswhosesolutionisgivenbyequationisknownas 5.12\\nthenormalequations.Evaluatingequationconstitutesasimplelearning 5.12\\nalgorithm.Foranexampleofthelinearregressionlearningalgorithminaction,\\nseeﬁgure.5.1\\nItisworthnotingthatthetermlinearregressionisoftenusedtoreferto\\naslightlymoresophisticatedmodelwithoneadditionalparameter—an intercept\\nterm.Inthismodelb\\nˆy= w\\ue03ex+b (5.13)\\nsothemappingfromparameterstopredictionsisstillalinearfunctionbutthe\\nmappingfromfeaturestopredictionsisnowanaﬃnefunction.Thisextensionto\\naﬃnefunctionsmeansthattheplotofthemodel’spredictionsstilllookslikea\\nline,butitneednotpassthroughtheorigin.Insteadofaddingthebiasparameter\\n1 0 9', 'CHAPTER5.MACHINELEARNINGBASICS\\nb,onecancontinuetousethemodelwithonlyweightsbutaugmentxwithan\\nextraentrythatisalwayssetto.Theweightcorrespondingtotheextraentry 1 1\\nplaystheroleofthebiasparameter.Wewillfrequentlyusetheterm“linear”when\\nreferringtoaﬃnefunctionsthroughoutthisbook.\\nTheintercepttermbisoftencalledthebiasparameteroftheaﬃnetransfor-\\nmation.Thisterminologyderivesfromthepointofviewthattheoutputofthe\\ntransformationisbiasedtowardbeingbintheabsenceofanyinput.Thisterm\\nisdiﬀerentfromtheideaofastatisticalbias,inwhichastatisticalestimation\\nalgorithm’sexpectedestimateofaquantityisnotequaltothetruequantity.\\nLinearregressionisofcourseanextremelysimpleandlimitedlearningalgorithm,\\nbutitprovidesanexampleofhowalearningalgorithmcanwork.Inthesubsequent\\nsectionswewilldescribesomeofthebasicprinciplesunderlyinglearningalgorithm\\ndesignanddemonstratehowtheseprinciplescanbeusedtobuildmorecomplicated\\nlearningalgorithms.\\n5.2Capacity,OverﬁttingandUnderﬁtting\\nThecentralchallengeinmachinelearningisthatwemustperformwellonnew,\\npreviouslyunseeninputs—notjustthoseonwhichourmodelwastrained.\\xa0The\\nabilitytoperformwellonpreviouslyunobservedinputsiscalledgeneralization.\\nTypically,whentrainingamachinelearningmodel,wehaveaccesstoatraining\\nset,wecancomputesomeerrormeasureonthetrainingsetcalledthetraining\\nerror,andwereducethistrainingerror.Sofar,whatwehavedescribedissimply\\nanoptimization problem.Whatseparatesmachinelearningfromoptimization is\\nthatwewantthegeneralizationerror,alsocalledthetesterror,tobelowas\\nwell.\\xa0Thegeneralization errorisdeﬁnedastheexpectedvalueoftheerrorona\\nnewinput.Heretheexpectationistakenacrossdiﬀerentpossibleinputs,drawn\\nfromthedistributionofinputsweexpectthesystemtoencounterinpractice.\\nWetypicallyestimatethegeneralization errorofamachinelearningmodelby\\nmeasuringitsperformanceonatestsetofexamplesthatwerecollectedseparately\\nfromthetrainingset.\\nInourlinearregressionexample,wetrainedthemodelbyminimizingthe\\ntrainingerror,\\n1\\nm()train||X()trainwy−()train||2\\n2, (5.14)\\nbutweactuallycareaboutthetesterror,1\\nm()test||X()testwy−()test||2\\n2.\\nHowcanweaﬀectperformanceonthetestsetwhenwegettoobserveonlythe\\n1 1 0', 'CHAPTER5.MACHINELEARNINGBASICS\\ntrainingset?Theﬁeldofstatisticallearningtheoryprovidessomeanswers.If\\nthetrainingandthetestsetarecollectedarbitrarily,thereisindeedlittlewecan\\ndo.Ifweareallowedtomakesomeassumptionsabouthowthetrainingandtest\\nsetarecollected,thenwecanmakesomeprogress.\\nThetrainandtestdataaregeneratedbyaprobabilitydistributionoverdatasets\\ncalledthedatageneratingprocess.Wetypicallymakeasetofassumptions\\nknowncollectivelyasthei.i.d.\\xa0assumptions.\\xa0Theseassumptionsarethatthe\\nexamplesineachdatasetareindependentfromeachother,andthatthetrain\\nsetandtestsetareidenticallydistributed,drawnfromthesameprobability\\ndistributionaseachother.\\xa0Thisassumptionallowsustodescribethedatagen-\\neratingprocesswithaprobabilitydistributionoverasingleexample.Thesame\\ndistributionisthenusedtogenerateeverytrainexampleandeverytestexample.\\nWecallthatsharedunderlyingdistributionthedatageneratingdistribution,\\ndenotedpdata.Thisprobabilisticframeworkandthei.i.d.assumptionsallowusto\\nmathematically studytherelationshipbetweentrainingerrorandtesterror.\\nOneimmediateconnectionwecanobservebetweenthetrainingandtesterror\\nisthattheexpectedtrainingerrorofarandomlyselectedmodelisequaltothe\\nexpectedtesterrorofthatmodel.Supposewehaveaprobabilitydistribution\\np(x,y)andwesamplefromitrepeatedlytogeneratethetrainsetandthetest\\nset.Forsomeﬁxedvaluew,theexpectedtrainingseterrorisexactlythesameas\\ntheexpectedtestseterror,becausebothexpectationsareformedusingthesame\\ndatasetsamplingprocess.Theonlydiﬀerencebetweenthetwoconditionsisthe\\nnameweassigntothedatasetwesample.\\nOfcourse,\\xa0when\\xa0weuseamachinelearning\\xa0algorithm,\\xa0w edonotﬁxthe\\nparametersaheadoftime,thensamplebothdatasets.Wesamplethetrainingset,\\nthenuseittochoosetheparameterstoreducetrainingseterror,thensamplethe\\ntestset.Underthisprocess,theexpectedtesterrorisgreaterthanorequalto\\ntheexpectedvalueoftrainingerror.Thefactorsdetermininghowwellamachine\\nlearningalgorithmwillperformareitsabilityto:\\n1.\\xa0Makethetrainingerrorsmall.\\n2.\\xa0Makethegapbetweentrainingandtesterrorsmall.\\nThesetwofactorscorrespondtothetwocentralchallengesinmachinelearning:\\nunderﬁttingandoverﬁtting.Underﬁttingoccurswhenthemodelisnotableto\\nobtainasuﬃcientlylowerrorvalueonthetrainingset.Overﬁttingoccurswhen\\nthegapbetweenthetrainingerrorandtesterroristoolarge.\\nWecancontrolwhetheramodelismorelikelytooverﬁtorunderﬁtbyaltering\\nitscapacity.Informally,amodel’scapacityisitsabilitytoﬁtawidevarietyof\\n1 1 1', 'CHAPTER5.MACHINELEARNINGBASICS\\nfunctions.Modelswithlowcapacitymaystruggletoﬁtthetrainingset.Models\\nwithhighcapacitycanoverﬁtbymemorizingpropertiesofthetrainingsetthatdo\\nnotservethemwellonthetestset.\\nOnewaytocontrolthecapacityofalearningalgorithmisbychoosingits\\nhypothesisspace,thesetoffunctionsthatthelearningalgorithmisallowedto\\nselectasbeingthesolution.Forexample,thelinearregressionalgorithmhasthe\\nsetofalllinearfunctionsofitsinputasitshypothesisspace.Wecangeneralize\\nlinearregressiontoincludepolynomials,ratherthanjustlinearfunctions,inits\\nhypothesisspace.Doingsoincreasesthemodel’scapacity.\\nApolynomialofdegreeonegivesusthelinearregressionmodelwithwhichwe\\narealreadyfamiliar,withprediction\\nˆybwx. = + (5.15)\\nByintroducingx2asanotherfeatureprovidedtothelinearregressionmodel,we\\ncanlearnamodelthatisquadraticasafunctionof:x\\nˆybw = +1xw+2x2. (5.16)\\nThoughthismodelimplementsaquadraticfunctionofits,theoutputis input\\nstillalinearfunctionoftheparameters,sowecanstillusethenormalequations\\ntotrainthemodelinclosedform.Wecancontinuetoaddmorepowersofxas\\nadditionalfeatures,forexampletoobtainapolynomialofdegree9:\\nˆyb= +9\\ue058\\ni=1w ixi. (5.17)\\nMachinelearningalgorithmswillgenerallyperformbestwhentheircapacity\\nisappropriateforthetruecomplexityofthetasktheyneedtoperformandthe\\namountoftrainingdatatheyareprovidedwith.Modelswithinsuﬃcientcapacity\\nareunabletosolvecomplextasks.Modelswithhighcapacitycansolvecomplex\\ntasks,butwhentheircapacityishigherthanneededtosolvethepresenttaskthey\\nmayoverﬁt.\\nFigureshowsthisprincipleinaction.Wecomparealinear,quadratic 5.2\\nanddegree-9predictorattemptingtoﬁtaproblemwherethetrueunderlying\\nfunctionisquadratic.\\xa0Thelinearfunctionisunabletocapturethecurvaturein\\nthetrueunderlyingproblem,soitunderﬁts.Thedegree-9predictoriscapableof\\nrepresentingthecorrectfunction,butitisalsocapableofrepresentinginﬁnitely\\nmanyotherfunctionsthatpassexactlythroughthetrainingpoints,becausewe\\n1 1 2', 'CHAPTER5.MACHINELEARNINGBASICS\\nhavemoreparametersthantrainingexamples.Wehavelittlechanceofchoosing\\nasolutionthatgeneralizeswellwhensomanywildlydiﬀerentsolutionsexist.In\\nthisexample,thequadraticmodelisperfectlymatchedtothetruestructureof\\nthetasksoitgeneralizeswelltonewdata.\\n\\ue078\\ue030\\ue079\\ue055\\ue06e \\ue064 \\ue065 \\ue072 \\ue066 \\ue069 \\ue074 \\ue074 \\ue069 \\ue06e \\ue067\\n\\ue078\\ue030\\ue079\\ue041\\ue070 \\ue070 \\ue072 \\ue06f \\ue070 \\ue072 \\ue069 \\ue061 \\ue074 \\ue065 \\ue020 \\ue063 \\ue061 \\ue070 \\ue061 \\ue063 \\ue069 \\ue074 \\ue079\\n\\ue078\\ue030\\ue079\\ue04f \\ue076 \\ue065 \\ue072 \\ue066 \\ue069 \\ue074 \\ue074 \\ue069 \\ue06e \\ue067\\nFigure5.2:Weﬁtthreemodelstothisexampletrainingset.Thetrainingdatawas\\ngeneratedsynthetically,byrandomlysamplingxvaluesandchoosingydeterministically\\nbyevaluatingaquadraticfunction.\\xa0 ( L e f t )Alinearfunctionﬁttothedatasuﬀersfrom\\nunderﬁtting—itcannotcapturethecurvaturethatispresentinthedata. A ( C e n t e r )\\nquadraticfunctionﬁttothedatageneralizeswelltounseenpoints.Itdoesnotsuﬀerfrom\\nasigniﬁcantamountofoverﬁttingorunderﬁtting.Apolynomialofdegree9ﬁtto ( R i g h t )\\nthedatasuﬀersfromoverﬁtting.HereweusedtheMoore-Penrosepseudoinversetosolve\\ntheunderdeterminednormalequations.Thesolutionpassesthroughallofthetraining\\npointsexactly,butwehavenotbeenluckyenoughforittoextractthecorrectstructure.\\nItnowhasadeepvalleyinbetweentwotrainingpointsthatdoesnotappearinthetrue\\nunderlyingfunction.Italsoincreasessharplyontheleftsideofthedata,whilethetrue\\nfunctiondecreasesinthisarea.\\nSofarwehavedescribedonlyonewayofchangingamodel’scapacity:by\\nchangingthenumberofinputfeaturesithas,andsimultaneouslyaddingnew\\nparametersassociatedwiththosefeatures.Thereareinfactmanywaysofchanging\\namodel’scapacity.Capacityisnotdeterminedonlybythechoiceofmodel.The\\nmodelspeciﬁeswhichfamilyoffunctionsthelearningalgorithmcanchoosefrom\\nwhenvaryingtheparametersinordertoreduceatrainingobjective.Thisiscalled\\ntherepresentationalcapacityofthemodel.Inmanycases,ﬁndingthebest\\nfunctionwithinthisfamilyisaverydiﬃcultoptimization problem.Inpractice,\\nthelearningalgorithmdoesnotactuallyﬁndthebestfunction,butmerelyone\\nthatsigniﬁcantlyreducesthetrainingerror.Theseadditionallimitations,suchas\\n1 1 3', 'CHAPTER5.MACHINELEARNINGBASICS\\ntheimperfectionoftheoptimization algorithm,meanthatthelearningalgorithm’s\\neﬀectivecapacitymaybelessthantherepresentationalcapacityofthemodel\\nfamily.\\nOurmodernideasaboutimprovingthegeneralization ofmachinelearning\\nmodelsarereﬁnementsofthoughtdatingbacktophilosophersatleastasearly\\nasPtolemy.Manyearlyscholarsinvokeaprincipleofparsimonythatisnow\\nmostwidelyknownasOccam’srazor(c.1287-1347).Thisprinciplestatesthat\\namongcompetinghypothesesthatexplainknownobservationsequallywell,one\\nshouldchoosethe“simplest”one.Thisideawasformalizedandmademoreprecise\\ninthe20thcenturybythefoundersofstatisticallearningtheory(Vapnikand\\nChervonenkis1971Vapnik1982Blumer1989Vapnik1995 ,;,; etal.,;,).\\nStatisticallearningtheoryprovidesvariousmeansofquantifyingmodelcapacity.\\nAmongthese,themostwell-knownistheVapnik-Chervonenkisdimension,or\\nVCdimension.TheVCdimensionmeasuresthecapacityofabinaryclassiﬁer.The\\nVCdimensionisdeﬁnedasbeingthelargestpossiblevalueofmforwhichthere\\nexistsatrainingsetofmdiﬀerentxpointsthattheclassiﬁercanlabelarbitrarily.\\nQuantifyingthecapacityofthemodelallowsstatisticallearningtheoryto\\nmakequantitativepredictions.Themostimportantresultsinstatisticallearning\\ntheoryshowthatthediscrepancybetweentrainingerrorandgeneralization error\\nisboundedfromabovebyaquantitythatgrowsasthemodelcapacitygrowsbut\\nshrinksasthenumberoftrainingexamplesincreases(VapnikandChervonenkis,\\n1971Vapnik1982Blumer 1989Vapnik1995 ;,; etal.,;,).Theseboundsprovide\\nintellectualjustiﬁcationthatmachinelearningalgorithmscanwork,buttheyare\\nrarelyusedinpracticewhenworkingwithdeeplearningalgorithms.Thisisin\\npartbecausetheboundsareoftenquitelooseandinpartbecauseitcanbequite\\ndiﬃculttodeterminethecapacityofdeeplearningalgorithms.\\xa0Theproblemof\\ndeterminingthecapacityofadeeplearningmodelisespeciallydiﬃcultbecausethe\\neﬀectivecapacityislimitedbythecapabilitiesoftheoptimization algorithm,and\\nwehavelittletheoreticalunderstandingoftheverygeneralnon-convexoptimization\\nproblemsinvolvedindeeplearning.\\nWemustrememberthatwhilesimplerfunctionsaremorelikelytogeneralize\\n(tohaveasmallgapbetweentrainingandtesterror)wemuststillchoosea\\nsuﬃcientlycomplexhypothesistoachievelowtrainingerror.Typically,training\\nerrordecreasesuntilitasymptotestotheminimumpossibleerrorvalueasmodel\\ncapacityincreases(assumingtheerrormeasurehasaminimumvalue).Typically,\\ngeneralization errorhasaU-shapedcurveasafunctionofmodelcapacity.Thisis\\nillustratedinﬁgure.5.3\\nToreachthemostextremecaseofarbitrarilyhighcapacity,weintroduce\\n1 1 4', 'CHAPTER5.MACHINELEARNINGBASICS\\n0 O pti m a l C a pa c i t y\\nC a pa c i t yE r r o rU nde r ﬁtti ng z o ne O v e r ﬁtti ng z o ne\\nG e ne r a l i z a t i o n g a pT r a i n i n g e r r o r\\nG e n e r a l i z a t i o n e r r o r\\nFigure5.3:Typicalrelationshipbetweencapacityanderror.Trainingandtesterror\\nbehavediﬀerently.Attheleftendofthegraph,trainingerrorandgeneralizationerror\\narebothhigh.Thisistheunderﬁttingregime.Asweincreasecapacity,trainingerror\\ndecreases,butthegapbetweentrainingandgeneralizationerrorincreases.Eventually,\\nthesizeofthisgapoutweighsthedecreaseintrainingerror,andweentertheoverﬁtting\\nregime,wherecapacityistoolarge,abovetheoptimalcapacity.\\ntheconceptofnon-parametricmodels.Sofar,wehaveseenonlyparametric\\nmodels,suchaslinearregression.Parametricmodelslearnafunctiondescribed\\nbyaparametervectorwhosesizeisﬁniteandﬁxedbeforeanydataisobserved.\\nNon-parametric modelshavenosuchlimitation.\\nSometimes,non-parametric modelsarejusttheoreticalabstractions(suchas\\nanalgorithmthatsearchesoverallpossibleprobabilitydistributions)thatcannot\\nbeimplemented inpractice.However,wecanalsodesignpracticalnon-parametric\\nmodelsbymakingtheircomplexityafunctionofthetrainingsetsize.Oneexample\\nofsuchanalgorithmisnearestneighborregression.Unlikelinearregression,\\nwhichhasaﬁxed-lengthvectorofweights,thenearestneighborregressionmodel\\nsimplystorestheXandyfromthetrainingset.\\xa0Whenaskedtoclassifyatest\\npointx,themodellooksupthenearestentryinthetrainingsetandreturnsthe\\nassociatedregressiontarget.Inotherwords,ˆy=y iwherei=argmin||X i ,:−||x2\\n2.\\nThealgorithmcanalsobegeneralizedtodistancemetricsotherthantheL2norm,\\nsuchaslearneddistancemetrics( ,).Ifthealgorithmis Goldbergeretal.2005\\nallowedtobreaktiesbyaveragingthey ivaluesforallX i ,:thataretiedfornearest,\\nthenthisalgorithmisabletoachievetheminimumpossibletrainingerror(which\\nmightbegreaterthanzero,iftwoidenticalinputsareassociatedwithdiﬀerent\\noutputs)onanyregressiondataset.\\nFinally,wecanalsocreateanon-parametric learningalgorithmbywrappinga\\n1 1 5', 'CHAPTER5.MACHINELEARNINGBASICS\\nparametriclearningalgorithminsideanotheralgorithmthatincreasesthenumber\\nofparametersasneeded.Forexample,wecouldimagineanouterloopoflearning\\nthatchangesthedegreeofthepolynomiallearnedbylinearregressionontopofa\\npolynomialexpansionoftheinput.\\nTheidealmodelisanoraclethatsimplyknowsthetrueprobabilitydistribution\\nthatgeneratesthedata.\\xa0Evensuchamodelwillstillincursomeerroronmany\\nproblems,becausetheremaystillbesomenoiseinthedistribution.Inthecase\\nofsupervisedlearning,themappingfromxtoymaybeinherentlystochastic,\\norymaybeadeterministicfunctionthatinvolvesothervariablesbesidesthose\\nincludedinx.Theerrorincurredbyanoraclemakingpredictionsfromthetrue\\ndistributioniscalledthe p,y(x)Bayeserror.\\nTrainingandgeneralization errorvaryasthesizeofthetrainingsetvaries.\\nExpectedgeneralization errorcanneverincreaseasthenumberoftrainingexamples\\nincreases.Fornon-parametric models,moredatayieldsbettergeneralization until\\nthebestpossibleerrorisachieved.Anyﬁxedparametricmodelwithlessthan\\noptimalcapacitywillasymptotetoanerrorvaluethatexceedstheBayeserror.See\\nﬁgureforanillustration.Notethatitispossibleforthemodeltohaveoptimal 5.4\\ncapacityandyetstillhavealargegapbetweentrainingandgeneralization error.\\nInthissituation,wemaybeabletoreducethisgapbygatheringmoretraining\\nexamples.\\n5.2.1TheNoFreeLunchTheorem\\nLearningtheoryclaimsthatamachinelearningalgorithmcangeneralizewellfrom\\naﬁnitetrainingsetofexamples.Thisseemstocontradictsomebasicprinciplesof\\nlogic.Inductivereasoning,orinferringgeneralrulesfromalimitedsetofexamples,\\nisnotlogicallyvalid.\\xa0Tologicallyinferaruledescribingeverymemberofaset,\\nonemusthaveinformationabouteverymemberofthatset.\\nInpart,machinelearningavoidsthisproblembyoﬀeringonlyprobabilisticrules,\\nratherthantheentirelycertainrulesusedinpurelylogicalreasoning.\\xa0Machine\\nlearningpromisestoﬁndrulesthatareprobably most correctaboutmembersof\\nthesettheyconcern.\\nUnfortunately,eventhisdoesnotresolvetheentireproblem.Thenofree\\nlunchtheoremformachinelearning(Wolpert1996,)statesthat,averagedover\\nallpossibledatageneratingdistributions,everyclassiﬁcationalgorithmhasthe\\nsameerrorratewhenclassifyingpreviouslyunobservedpoints.Inotherwords,\\ninsomesense,nomachinelearningalgorithmisuniversallyanybetterthanany\\nother.Themostsophisticatedalgorithmwecanconceiveofhasthesameaverage\\n1 1 6', 'CHAPTER5.MACHINELEARNINGBASICS\\n\\ue031 \\ue030\\ue030\\ue031 \\ue030\\ue031\\ue031 \\ue030\\ue032\\ue031 \\ue030\\ue033\\ue031 \\ue030\\ue034\\ue031 \\ue030\\ue035\\n\\ue04e\\ue075\\ue06d \\ue062 \\ue065\\ue072\\ue020 \\ue06f \\ue066 \\ue020 \\ue074 \\ue072\\ue061 \\ue069 \\ue06e \\ue069 \\ue06e \\ue067 \\ue020 \\ue065\\ue078 \\ue061 \\ue06d \\ue070 \\ue06c \\ue065 \\ue073\\ue030 \\ue02e \\ue030\\ue030 \\ue02e \\ue035\\ue031 \\ue02e \\ue030\\ue031 \\ue02e \\ue035\\ue032 \\ue02e \\ue030\\ue032 \\ue02e \\ue035\\ue033 \\ue02e \\ue030\\ue033 \\ue02e \\ue035\\ue045 \\ue072\\ue072\\ue06f \\ue072\\ue020 \\ue028 \\ue04d \\ue053 \\ue045 \\ue029\\ue042 \\ue061 \\ue079 \\ue065 \\ue073\\ue020 \\ue065 \\ue072 \\ue072 \\ue06f \\ue072\\n\\ue054 \\ue072 \\ue061 \\ue069 \\ue06e \\ue020 \\ue028 \\ue071 \\ue075 \\ue061 \\ue064 \\ue072 \\ue061 \\ue074 \\ue069 \\ue063 \\ue029\\n\\ue054 \\ue065 \\ue073\\ue074 \\ue020 \\ue028 \\ue071 \\ue075 \\ue061 \\ue064 \\ue072 \\ue061 \\ue074 \\ue069 \\ue063 \\ue029\\n\\ue054 \\ue065 \\ue073\\ue074 \\ue020 \\ue028 \\ue06f \\ue070 \\ue074 \\ue069 \\ue06d \\ue061 \\ue06c \\ue020 \\ue063 \\ue061 \\ue070 \\ue061 \\ue063 \\ue069 \\ue074 \\ue079 \\ue029\\n\\ue054 \\ue072 \\ue061 \\ue069 \\ue06e \\ue020 \\ue028 \\ue06f \\ue070 \\ue074 \\ue069 \\ue06d \\ue061 \\ue06c \\ue020 \\ue063 \\ue061 \\ue070 \\ue061 \\ue063 \\ue069 \\ue074 \\ue079 \\ue029\\n\\ue031 \\ue030\\ue030\\ue031 \\ue030\\ue031\\ue031 \\ue030\\ue032\\ue031 \\ue030\\ue033\\ue031 \\ue030\\ue034\\ue031 \\ue030\\ue035\\n\\ue04e\\ue075\\ue06d \\ue062 \\ue065\\ue072\\ue020 \\ue06f \\ue066 \\ue020 \\ue074 \\ue072\\ue061 \\ue069 \\ue06e \\ue069 \\ue06e \\ue067 \\ue020 \\ue065\\ue078 \\ue061 \\ue06d \\ue070 \\ue06c \\ue065 \\ue073\\ue030\\ue035\\ue031 \\ue030\\ue031 \\ue035\\ue032 \\ue030\\ue04f \\ue070 \\ue074 \\ue069 \\ue06d \\ue061 \\ue06c \\ue020 \\ue063\\ue061 \\ue070 \\ue061 \\ue063\\ue069\\ue074\\ue079 \\ue020 \\ue028 \\ue070 \\ue06f \\ue06c \\ue079 \\ue06e \\ue06f \\ue06d \\ue069 \\ue061 \\ue06c \\ue020 \\ue064 \\ue065 \\ue067 \\ue072\\ue065 \\ue065 \\ue029\\nFigure5.4:Theeﬀectofthetrainingdatasetsizeonthetrainandtesterror,aswellas\\nontheoptimalmodelcapacity.Weconstructedasyntheticregressionproblembasedon\\naddingamoderateamountofnoisetoadegree-5polynomial,generatedasingletestset,\\nandthengeneratedseveraldiﬀerentsizesoftrainingset.Foreachsize,wegenerated40\\ndiﬀerenttrainingsetsinordertoploterrorbarsshowing95percentconﬁdenceintervals.\\n( T o p )TheMSEonthetrainingandtestsetfortwodiﬀerentmodels:aquadraticmodel,\\nandamodelwithdegreechosentominimizethetesterror.Bothareﬁtinclosedform.For\\nthequadraticmodel,thetrainingerrorincreasesasthesizeofthetrainingsetincreases.\\nThisisbecauselargerdatasetsarehardertoﬁt.Simultaneously,thetesterrordecreases,\\nbecausefewerincorrecthypothesesareconsistentwiththetrainingdata.Thequadratic\\nmodeldoesnothaveenoughcapacitytosolvethetask,soitstesterrorasymptotesto\\nahighvalue.ThetesterroratoptimalcapacityasymptotestotheBayeserror.The\\ntrainingerrorcanfallbelowtheBayeserror,duetotheabilityofthetrainingalgorithm\\ntomemorizespeciﬁcinstancesofthetrainingset.Asthetrainingsizeincreasestoinﬁnity,\\nthetrainingerrorofanyﬁxed-capacitymodel(here,thequadraticmodel)mustrisetoat\\nleasttheBayeserror.\\xa0Asthetrainingsetsizeincreases,theoptimalcapacity ( Bottom )\\n(shownhereasthedegreeoftheoptimalpolynomialregressor)increases.\\xa0Theoptimal\\ncapacityplateausafterreachingsuﬃcientcomplexitytosolvethetask.\\n1 1 7', 'CHAPTER5.MACHINELEARNINGBASICS\\nperformance(overallpossibletasks)asmerelypredictingthateverypointbelongs\\ntothesameclass.\\nFortunately,theseresultsholdonlywhenweaverageoverpossibledata all\\ngeneratingdistributions.Ifwemakeassumptionsaboutthekindsofprobability\\ndistributionsweencounterinreal-worldapplications,thenwecandesignlearning\\nalgorithmsthatperformwellonthesedistributions.\\nThismeansthatthegoalofmachinelearningresearchisnottoseekauniversal\\nlearningalgorithmortheabsolutebestlearningalgorithm.Instead,ourgoalisto\\nunderstandwhatkindsofdistributionsarerelevanttothe“realworld”thatanAI\\nagentexperiences,andwhatkindsofmachinelearningalgorithmsperformwellon\\ndatadrawnfromthekindsofdatageneratingdistributionswecareabout.\\n5.2.2Regularization\\nThenofreelunchtheoremimpliesthatwemustdesignourmachinelearning\\nalgorithmstoperformwellonaspeciﬁctask.Wedosobybuildingasetof\\npreferencesintothelearningalgorithm.Whenthesepreferencesarealignedwith\\nthelearningproblemsweaskthealgorithmtosolve,itperformsbetter.\\nSofar,theonlymethodofmodifyingalearningalgorithmthatwehavediscussed\\nconcretelyistoincreaseordecreasethemodel’srepresentationalcapacitybyadding\\norremovingfunctionsfromthehypothesisspaceofsolutionsthelearningalgorithm\\nisabletochoose.Wegavethespeciﬁcexampleofincreasingordecreasingthe\\ndegreeofapolynomialforaregressionproblem.Theviewwehavedescribedso\\nfarisoversimpliﬁed.\\nThebehaviorofouralgorithmisstronglyaﬀectednotjustbyhowlargewe\\nmakethesetoffunctionsallowedinitshypothesisspace,butbythespeciﬁcidentity\\nofthosefunctions.Thelearningalgorithmwehavestudiedsofar,linearregression,\\nhasahypothesisspaceconsistingofthesetoflinearfunctionsofitsinput.These\\nlinearfunctionscanbeveryusefulforproblemswheretherelationshipbetween\\ninputsandoutputstrulyisclosetolinear.Theyarelessusefulforproblems\\nthatbehaveinaverynonlinearfashion.Forexample,linearregressionwould\\nnotperformverywellifwetriedtouseittopredict sin(x)fromx.Wecanthus\\ncontroltheperformanceofouralgorithmsbychoosingwhatkindoffunctionswe\\nallowthemtodrawsolutionsfrom,aswellasbycontrollingtheamountofthese\\nfunctions.\\nWecanalsogivealearningalgorithmapreferenceforonesolutioninits\\nhypothesisspacetoanother.Thismeansthatbothfunctionsareeligible,butone\\nispreferred.Theunpreferredsolutionwillbechosenonlyifitﬁtsthetraining\\n1 1 8', 'CHAPTER5.MACHINELEARNINGBASICS\\ndatasigniﬁcantlybetterthanthepreferredsolution.\\nForexample,wecanmodifythetrainingcriterionforlinearregressiontoinclude\\nweightdecay.Toperformlinearregressionwithweightdecay,weminimizeasum\\ncomprisingboththemeansquarederroronthetrainingandacriterionJ(w)that\\nexpressesapreferencefortheweightstohavesmallersquaredL2norm.Speciﬁcally,\\nJ() = wMSEtrain+λw\\ue03ew, (5.18)\\nwhereλisavaluechosenaheadoftimethatcontrolsthestrengthofourpreference\\nforsmallerweights.Whenλ= 0,weimposenopreference,andlargerλforcesthe\\nweightstobecomesmaller.\\xa0MinimizingJ(w)resultsinachoiceofweightsthat\\nmakeatradeoﬀbetweenﬁttingthetrainingdataandbeingsmall.Thisgivesus\\nsolutionsthathaveasmallerslope,orputweightonfewerofthefeatures.Asan\\nexampleofhowwecancontrolamodel’stendencytooverﬁtorunderﬁtviaweight\\ndecay,wecantrainahigh-degreepolynomialregressionmodelwithdiﬀerentvalues\\nof.Seeﬁgurefortheresults. λ 5.5\\n\\ue078\\ue030\\ue079\\ue055 \\ue06e \\ue064 \\ue065 \\ue072 \\ue066 \\ue069 \\ue074 \\ue074 \\ue069 \\ue06e \\ue067\\n\\ue028 \\ue045 \\ue078 \\ue063 \\ue065 \\ue073\\ue073\\ue069\\ue076 \\ue065 \\ue020 \\ue0b8 \\ue029\\n\\ue078\\ue030\\ue079\\ue041 \\ue070 \\ue070 \\ue072 \\ue06f \\ue070 \\ue072 \\ue069 \\ue061 \\ue074 \\ue065 \\ue020 \\ue077 \\ue065 \\ue069 \\ue067 \\ue068 \\ue074 \\ue020 \\ue064 \\ue065 \\ue063 \\ue061 \\ue079\\n\\ue028 \\ue04d \\ue065 \\ue064 \\ue069 \\ue075 \\ue06d \\ue020 \\ue0b8 \\ue029\\n\\ue078\\ue030\\ue079\\ue04f \\ue076 \\ue065 \\ue072 \\ue066 \\ue069 \\ue074 \\ue074 \\ue069 \\ue06e \\ue067\\n\\ue028 \\ue030 \\ue029 \\ue0b8 \\ue021\\nFigure5.5:Weﬁtahigh-degreepolynomialregressionmodeltoourexampletrainingset\\nfromﬁgure.Thetruefunctionisquadratic,buthereweuseonlymodelswithdegree9. 5.2\\nWevarytheamountofweightdecaytopreventthesehigh-degreemodelsfromoverﬁtting.\\n( L e f t )Withverylargeλ,wecanforcethemodeltolearnafunctionwithnoslopeat\\nall.Thisunderﬁtsbecauseitcanonlyrepresentaconstantfunction.Witha ( C e n t e r )\\nmediumvalueof,thelearningalgorithmrecoversacurvewiththerightgeneralshape. λ\\nEventhoughthemodeliscapableofrepresentingfunctionswithmuchmorecomplicated\\nshape,weightdecayhasencouragedittouseasimplerfunctiondescribedbysmaller\\ncoeﬃcients.Withweightdecayapproachingzero(i.e.,usingtheMoore-Penrose ( R i g h t )\\npseudoinversetosolvetheunderdeterminedproblemwithminimalregularization),the\\ndegree-9polynomialoverﬁtssigniﬁcantly,aswesawinﬁgure.5.2\\n1 1 9', 'CHAPTER5.MACHINELEARNINGBASICS\\nMoregenerally,wecanregularizeamodelthatlearnsafunctionf(x;θ)by\\naddingapenaltycalledaregularizertothecostfunction.Inthecaseofweight\\ndecay,theregularizerisΩ(w) =w\\ue03ew.Inchapter,wewillseethatmanyother 7\\nregularizersarepossible.\\nExpressingpreferencesforonefunctionoveranotherisamoregeneralway\\nofcontrollingamodel’scapacitythanincludingorexcludingmembersfromthe\\nhypothesisspace.Wecanthinkofexcludingafunctionfromahypothesisspaceas\\nexpressinganinﬁnitelystrongpreferenceagainstthatfunction.\\nInourweightdecayexample,weexpressedourpreferenceforlinearfunctions\\ndeﬁnedwithsmallerweightsexplicitly,\\xa0viaanextraterminthecriterionwe\\nminimize.Thereare\\xa0many\\xa0otherwaysof\\xa0expressing preferencesfor\\xa0diﬀerent\\nsolutions,bothimplicitlyandexplicitly.Together,thesediﬀerentapproaches\\nareknownasregularization.\\xa0Regularizationisanymodiﬁcationwemaketoa\\nlearningalgorithmthatisintendedtoreduceitsgeneralizationerrorbutnotits\\ntrainingerror.Regularizationisoneofthecentralconcernsoftheﬁeldofmachine\\nlearning,rivaledinitsimportanceonlybyoptimization.\\nThenofreelunchtheoremhasmadeitclearthatthereisnobestmachine\\nlearningalgorithm,and,inparticular,nobestformofregularization. Instead\\nwemustchooseaformofregularizationthatiswell-suitedtotheparticulartask\\nwewanttosolve.Thephilosophyofdeeplearningingeneralandthisbookin\\nparticularisthataverywiderangeoftasks(suchasalloftheintellectualtasks\\nthatpeoplecando)mayallbesolvedeﬀectivelyusingverygeneral-purposeforms\\nofregularization.\\n5.3HyperparametersandValidationSets\\nMostmachinelearningalgorithmshaveseveralsettingsthatwecanusetocontrol\\nthebehaviorofthelearningalgorithm.Thesesettingsarecalledhyperparame-\\nters.Thevaluesofhyperparameters arenotadaptedbythelearningalgorithm\\nitself(thoughwecan\\xa0designa\\xa0nestedlearning\\xa0procedure\\xa0where one\\xa0learning\\nalgorithmlearnsthebesthyperparametersforanotherlearningalgorithm).\\nInthepolynomialregressionexamplewesawinﬁgure,thereisasingle 5.2\\nhyperparameter:thedegreeofthepolynomial,whichactsasacapacityhyper-\\nparameter.Theλvalueusedtocontrolthestrengthofweightdecayisanother\\nexampleofahyperparameter.\\nSometimesasettingischosentobeahyperparameter thatthelearningal-\\ngorithmdoesnotlearnbecauseitisdiﬃculttooptimize.Morefrequently,the\\n1 2 0', 'CHAPTER5.MACHINELEARNINGBASICS\\nsettingmustbeahyperparameter becauseitisnotappropriatetolearnthat\\nhyperparameteronthetrainingset.Thisappliestoallhyperparameters that\\ncontrolmodelcapacity.Iflearnedonthetrainingset,suchhyperparameters would\\nalwayschoosethemaximumpossiblemodelcapacity,resultinginoverﬁtting(refer\\ntoﬁgure).Forexample,wecanalwaysﬁtthetrainingsetbetterwithahigher 5.3\\ndegreepolynomialandaweightdecaysettingofλ= 0thanwecouldwithalower\\ndegreepolynomialandapositiveweightdecaysetting.\\nTosolvethisproblem,weneedavalidationsetofexamplesthatthetraining\\nalgorithmdoesnotobserve.\\nEarlierwediscussedhowaheld-outtestset,composedofexamplescomingfrom\\nthesamedistributionasthetrainingset,canbeusedtoestimatethegeneralization\\nerrorofalearner,afterthelearningprocesshascompleted.Itisimportantthatthe\\ntestexamplesarenotusedinanywaytomakechoicesaboutthemodel,including\\nitshyperparameters .\\xa0Forthisreason,noexamplefromthetestsetcanbeused\\ninthevalidationset.Therefore,wealwaysconstructthevalidationsetfromthe\\ntrainingdata.Speciﬁcally,wesplitthetrainingdataintotwodisjointsubsets.One\\nofthesesubsetsisusedtolearntheparameters.Theothersubsetisourvalidation\\nset,usedtoestimatethegeneralization errorduringoraftertraining,allowing\\nforthehyperparameterstobeupdatedaccordingly.Thesubsetofdatausedto\\nlearntheparametersisstilltypicallycalledthetrainingset,eventhoughthis\\nmaybeconfusedwiththelargerpoolofdatausedfortheentiretrainingprocess.\\nThesubsetofdatausedtoguidetheselectionofhyperparameters iscalledthe\\nvalidationset.Typically,oneusesabout80%ofthetrainingdatafortrainingand\\n20%forvalidation.Sincethevalidationsetisusedto“train”thehyperparameters ,\\nthevalidationseterrorwillunderestimatethegeneralization error,thoughtypically\\nbyasmalleramountthanthetrainingerror.Afterallhyperparameter optimization\\niscomplete,thegeneralization errormaybeestimatedusingthetestset.\\nInpractice,\\xa0when thesametestsethasbeenusedrepeatedlytoevaluate\\nperformanceofdiﬀerentalgorithmsovermanyyears,andespeciallyifweconsider\\nalltheattemptsfromthescientiﬁccommunityatbeatingthereportedstate-of-\\nthe-artperformanceonthattestset,weenduphavingoptimisticevaluationswith\\nthetestsetaswell.Benchmarkscanthusbecomestaleandthendonotreﬂectthe\\ntrueﬁeldperformance ofatrainedsystem.Thankfully,thecommunitytendsto\\nmoveontonew(andusuallymoreambitiousandlarger)benchmarkdatasets.\\n1 2 1', 'CHAPTER5.MACHINELEARNINGBASICS\\n5.3.1Cross-Validation\\nDividingthedatasetintoaﬁxedtrainingsetandaﬁxedtestsetcanbeproblematic\\nifitresultsinthetestsetbeingsmall.Asmalltestsetimpliesstatisticaluncertainty\\naroundtheestimatedaveragetesterror,makingitdiﬃculttoclaimthatalgorithm\\nAworksbetterthanalgorithmonthegiventask. B\\nWhenthedatasethashundredsofthousandsofexamplesormore,thisisnota\\nseriousissue.Whenthedatasetistoosmall,arealternativeproceduresenableone\\ntousealloftheexamplesintheestimationofthemeantesterror,atthepriceof\\nincreasedcomputational cost.Theseproceduresarebasedontheideaofrepeating\\nthetrainingandtestingcomputationondiﬀerentrandomlychosensubsetsorsplits\\noftheoriginaldataset.Themostcommonoftheseisthek-foldcross-validation\\nprocedure,showninalgorithm ,inwhichapartitionofthedatasetisformedby 5.1\\nsplittingitintoknon-overlappingsubsets.Thetesterrormaythenbeestimated\\nbytakingtheaveragetesterroracrossktrials.Ontriali,thei-thsubsetofthe\\ndataisusedasthetestsetandtherestofthedataisusedasthetrainingset.One\\nproblemisthatthereexistnounbiasedestimatorsofthevarianceofsuchaverage\\nerrorestimators(BengioandGrandvalet2004,),butapproximationsaretypically\\nused.\\n5.4Estimators,BiasandVariance\\nTheﬁeldofstatisticsgivesusmanytoolsthatcanbeusedtoachievethemachine\\nlearninggoalofsolvingatasknotonlyonthetrainingsetbutalsotogeneralize.\\nFoundationalconceptssuchasparameterestimation,biasandvarianceareuseful\\ntoformallycharacterizenotionsofgeneralization, underﬁttingandoverﬁtting.\\n5.4.1PointEstimation\\nPointestimationistheattempttoprovidethesingle“best”predictionofsome\\nquantityofinterest.Ingeneralthequantityofinterestcanbeasingleparameter\\noravectorofparametersinsomeparametricmodel,suchastheweightsinour\\nlinearregressionexampleinsection,butitcanalsobeawholefunction. 5.1.4\\nInordertodistinguishestimatesofparametersfromtheirtruevalue,our\\nconventionwillbetodenoteapointestimateofaparameterbyθ ˆθ.\\nLet{x(1),...,x() m}beasetofmindependentandidenticallydistributed\\n1 2 2', 'CHAPTER5.MACHINELEARNINGBASICS\\nAlgorithm5.1Thek-foldcross-validationalgorithm.Itcanbeusedtoestimate\\ngeneralization errorofalearningalgorithmAwhenthegivendataset Distoo\\nsmallforasimpletrain/testortrain/validsplittoyieldaccurateestimationof\\ngeneralization error,becausethemeanofalossLonasmalltestsetmayhavetoo\\nhighvariance.Thedataset Dcontainsaselementstheabstractexamplesz() i(for\\nthei-thexample),whichcouldstandforan(input,target) pairz() i= (x() i,y() i)\\ninthecaseofsupervisedlearning,orforjustaninputz() i=x() iinthecase\\nofunsupervisedlearning.\\xa0The algorithmreturnsthevectoroferrorseforeach\\nexamplein D,whosemeanistheestimatedgeneralization error.\\xa0Theerrorson\\nindividualexamplescanbeusedtocomputeaconﬁdenceintervalaroundthemean\\n(equation).\\xa0Whiletheseconﬁdenceintervalsarenotwell-justiﬁedafterthe 5.47\\nuseofcross-validation,itisstillcommonpracticetousethemtodeclarethat\\nalgorithmAisbetterthanalgorithmBonlyiftheconﬁdenceintervaloftheerror\\nofalgorithmAliesbelowanddoesnotintersecttheconﬁdenceintervalofalgorithm\\nB.\\nDeﬁneKFoldXV(): D,A,L,k\\nRequire: D,thegivendataset,withelementsz() i\\nRequire:A,thelearningalgorithm,seenasafunctionthattakesadatasetas\\ninputandoutputsalearnedfunction\\nRequire:L,thelossfunction,seenasafunctionfromalearnedfunctionfand\\nanexamplez() i∈ ∈ Dtoascalar R\\nRequire:k,thenumberoffolds\\nSplitintomutuallyexclusivesubsets Dk D i,whoseunionis. D\\nfordoikfromto1\\nf i= (A D D\\\\ i)\\nforz() jin D ido\\ne j= (Lf i,z() j)\\nendfor\\nendfor\\nReturne\\n1 2 3', 'CHAPTER5.MACHINELEARNINGBASICS\\n(i.i.d.)datapoints.A orisanyfunctionofthedata: pointestimatorstatistic\\nˆθ m= (gx(1),...,x() m). (5.19)\\nThedeﬁnitiondoesnotrequirethatgreturnavaluethatisclosetothetrue\\nθoreventhattherangeofgisthesameasthesetofallowablevaluesofθ.\\nThisdeﬁnitionofapointestimatorisverygeneralandallowsthedesignerofan\\nestimatorgreatﬂexibility.Whilealmostanyfunctionthusqualiﬁesasanestimator,\\nagoodestimatorisafunctionwhoseoutputisclosetothetrueunderlyingθthat\\ngeneratedthetrainingdata.\\nFornow,wetakethefrequentistperspectiveonstatistics.Thatis,weassume\\nthatthetrueparametervalueθisﬁxedbutunknown,whilethepointestimate\\nˆθisafunctionofthedata.Sincethedataisdrawnfromarandomprocess,any\\nfunctionofthedataisrandom.Therefore ˆθisarandomvariable.\\nPointestimationcanalsorefertotheestimationoftherelationshipbetween\\ninputandtargetvariables.Werefertothesetypesofpointestimatesasfunction\\nestimators.\\nFunctionEstimationAswementionedabove,sometimesweareinterestedin\\nperformingfunctionestimation(orfunctionapproximation).Herewearetryingto\\npredictavariableygivenaninputvectorx.Weassumethatthereisafunction\\nf(x)thatdescribestheapproximate relationshipbetweenyandx.Forexample,\\nwemayassumethaty=f(x)+\\ue00f,where\\ue00fstandsforthepartofythatisnot\\npredictablefromx.\\xa0Infunctionestimation,weareinterestedinapproximating\\nfwithamodelorestimate ˆf.Functionestimationisreallyjustthesameas\\nestimatingaparameterθ;thefunctionestimator ˆfissimplyapointestimatorin\\nfunctionspace.Thelinearregressionexample(discussedaboveinsection)and5.1.4\\nthepolynomialregressionexample(discussedinsection)arebothexamplesof 5.2\\nscenariosthatmaybeinterpretedeitherasestimatingaparameterworestimating\\nafunction ˆf y mappingfromtox.\\nWenowreviewthemostcommonlystudiedpropertiesofpointestimatorsand\\ndiscusswhattheytellusabouttheseestimators.\\n5.4.2Bias\\nThebiasofanestimatorisdeﬁnedas:\\nbias(ˆθ m) = ( Eˆθ m)−θ (5.20)\\n1 2 4', 'CHAPTER5.MACHINELEARNINGBASICS\\nwheretheexpectationisoverthedata(seenassamplesfromarandomvariable)\\nandθisthetrueunderlyingvalueofθusedtodeﬁnethedatageneratingdistri-\\nbution.Anestimator ˆθ missaidtobeunbiasedifbias(ˆθ m) = 0,whichimplies\\nthat E(ˆθ m)=θ.Anestimator ˆθ missaidtobeasymptoticallyunbiasedif\\nlim m → ∞bias(ˆθ m) = 0,whichimpliesthatlim m → ∞ E(ˆθ m) = θ.\\nExample:BernoulliDistributionConsiderasetofsamples {x(1),...,x() m}\\nthatareindependentlyandidenticallydistributedaccordingtoaBernoullidistri-\\nbutionwithmean:θ\\nPx(() i;) = θθx() i(1 )−θ(1 − x() i). (5.21)\\nAcommonestimatorfortheθparameterofthisdistributionisthemeanofthe\\ntrainingsamples:\\nˆθ m=1\\nmm\\ue058\\ni=1x() i. (5.22)\\nTodeterminewhetherthisestimatorisbiased,wecansubstituteequation5.22\\nintoequation:5.20\\nbias(ˆθ m) = [ Eˆθ m]−θ (5.23)\\n= E\\ue022\\n1\\nmm\\ue058\\ni=1x() i\\ue023\\n−θ (5.24)\\n=1\\nmm\\ue058\\ni=1E\\ue068\\nx() i\\ue069\\n−θ (5.25)\\n=1\\nmm\\ue058\\ni=11\\ue058\\nx() i=0\\ue010\\nx() iθx() i(1 )−θ(1 − x() i)\\ue011\\n−θ(5.26)\\n=1\\nmm\\ue058\\ni=1()θ−θ (5.27)\\n= = 0θθ− (5.28)\\nSince bias(ˆθ) = 0,wesaythatourestimator ˆθisunbiased.\\nExample:GaussianDistributionEstimatoroftheMeanNow,consider\\nasetofsamples {x(1),...,x() m}thatareindependentlyandidenticallydistributed\\naccordingtoaGaussiandistributionp(x() i) =N(x() i;µ,σ2),wherei∈{1,...,m}.\\n1 2 5', 'CHAPTER5.MACHINELEARNINGBASICS\\nRecallthattheGaussianprobabilitydensityfunctionisgivenby\\npx(() i;µ,σ2) =1√\\n2πσ2exp\\ue020\\n−1\\n2(x() i−µ)2\\nσ2\\ue021\\n.(5.29)\\nAcommonestimatoroftheGaussianmeanparameterisknownasthesample\\nmean:\\nˆµ m=1\\nmm\\ue058\\ni=1x() i(5.30)\\nTodeterminethebiasofthesamplemean,weareagaininterestedincalculating\\nitsexpectation:\\nbias(ˆµ m) = [ˆ Eµ m]−µ (5.31)\\n= E\\ue022\\n1\\nmm\\ue058\\ni=1x() i\\ue023\\n−µ (5.32)\\n=\\ue020\\n1\\nmm\\ue058\\ni=1E\\ue068\\nx() i\\ue069\\ue021\\n−µ (5.33)\\n=\\ue020\\n1\\nmm\\ue058\\ni=1µ\\ue021\\n−µ (5.34)\\n= = 0µµ− (5.35)\\nThusweﬁndthatthesamplemeanisanunbiasedestimatorofGaussianmean\\nparameter.\\nExample:EstimatorsoftheVarianceofaGaussianDistributionAsan\\nexample,wecomparetwodiﬀerentestimatorsofthevarianceparameterσ2ofa\\nGaussiandistribution.Weareinterestedinknowingifeitherestimatorisbiased.\\nTheﬁrstestimatorofσ2weconsiderisknownasthesamplevariance:\\nˆσ2\\nm=1\\nmm\\ue058\\ni=1\\ue010\\nx() i−ˆµ m\\ue0112\\n, (5.36)\\nwhere ˆµ misthesamplemean,deﬁnedabove.Moreformally,weareinterestedin\\ncomputing\\nbias(ˆσ2\\nm) = [ˆ Eσ2\\nm]−σ2(5.37)\\n1 2 6', 'CHAPTER5.MACHINELEARNINGBASICS\\nWebeginbyevaluatingtheterm E[ˆσ2\\nm]:\\nE[ˆσ2\\nm] = E\\ue022\\n1\\nmm\\ue058\\ni=1\\ue010\\nx() i−ˆµ m\\ue0112\\ue023\\n(5.38)\\n=m−1\\nmσ2(5.39)\\nReturningtoequation,weconcludethatthebiasof 5.37 ˆσ2\\nmis−σ2/m.Therefore,\\nthesamplevarianceisabiasedestimator.\\nTheunbiasedsamplevarianceestimator\\n˜σ2\\nm=1\\nm−1m\\ue058\\ni=1\\ue010\\nx() i−ˆµ m\\ue0112\\n(5.40)\\nprovidesanalternativeapproach.Asthenamesuggeststhisestimatorisunbiased.\\nThatis,weﬁndthat E[˜σ2\\nm] = σ2:\\nE[˜σ2\\nm] = E\\ue022\\n1\\nm−1m\\ue058\\ni=1\\ue010\\nx() i−ˆµ m\\ue0112\\ue023\\n(5.41)\\n=m\\nm−1E[ˆσ2\\nm] (5.42)\\n=m\\nm−1\\ue012m−1\\nmσ2\\ue013\\n(5.43)\\n= σ2. (5.44)\\nWehavetwoestimators:oneisbiasedandtheotherisnot.Whileunbiased\\nestimatorsareclearlydesirable,theyarenotalwaysthe“best”estimators.Aswe\\nwillseeweoftenusebiasedestimatorsthatpossessotherimportantproperties.\\n5.4.3VarianceandStandardError\\nAnotherpropertyoftheestimatorthatwemightwanttoconsiderishowmuch\\nweexpectittovaryasafunctionofthedatasample.Justaswecomputedthe\\nexpectationoftheestimatortodetermineitsbias,wecancomputeitsvariance.\\nThevarianceofanestimatorissimplythevariance\\nVar(ˆθ) (5.45)\\nwheretherandomvariableisthetrainingset.Alternately,thesquarerootofthe\\nvarianceiscalledthe ,denotedstandarderror SE(ˆθ).\\n1 2 7', 'CHAPTER5.MACHINELEARNINGBASICS\\nThevarianceorthestandarderrorofanestimatorprovidesameasureofhow\\nwewouldexpecttheestimatewecomputefromdatatovaryasweindependently\\nresamplethedatasetfromtheunderlyingdatageneratingprocess.Justaswe\\nmightlikeanestimatortoexhibitlowbiaswewouldalsolikeittohaverelatively\\nlowvariance.\\nWhenwecomputeanystatisticusingaﬁnitenumberofsamples,ourestimate\\nofthetrueunderlyingparameterisuncertain,inthesensethatwecouldhave\\nobtainedothersamplesfromthesamedistributionandtheirstatisticswouldhave\\nbeendiﬀerent.Theexpecteddegreeofvariationinanyestimatorisasourceof\\nerrorthatwewanttoquantify.\\nThestandarderrorofthemeanisgivenby\\nSE(ˆµ m) =\\ue076\\ue075\\ue075\\ue074Var\\ue022\\n1\\nmm\\ue058\\ni=1x() i\\ue023\\n=σ√m, (5.46)\\nwhereσ2isthetruevarianceofthesamplesxi.Thestandarderrorisoften\\nestimatedbyusinganestimateofσ.Unfortunately,neitherthesquarerootof\\nthesamplevariancenorthesquarerootoftheunbiasedestimatorofthevariance\\nprovideanunbiasedestimateofthestandarddeviation.Bothapproachestend\\ntounderestimatethetruestandarddeviation,butarestillusedinpractice.The\\nsquarerootoftheunbiasedestimatorofthevarianceislessofanunderestimate.\\nForlarge,theapproximation isquitereasonable. m\\nThestandarderrorofthemeanisveryusefulinmachinelearningexperiments.\\nWeoftenestimatethegeneralization errorbycomputingthesamplemeanofthe\\nerroronthetestset.Thenumberofexamplesinthetestsetdeterminesthe\\naccuracyofthisestimate.Takingadvantageofthecentrallimittheorem,which\\ntellsusthatthemeanwillbeapproximatelydistributedwithanormaldistribution,\\nwecanusethestandarderrortocomputetheprobabilitythatthetrueexpectation\\nfallsinanychoseninterval.Forexample,the95%conﬁdenceintervalcenteredon\\nthemean ˆµ mis\\n(ˆµ m−196SE( ˆ.µ m)ˆ,µ m+196SE( ˆ.µ m)), (5.47)\\nunderthenormaldistributionwithmean ˆµ mandvariance SE(ˆµ m)2.Inmachine\\nlearningexperiments,itiscommontosaythatalgorithmAisbetterthanalgorithm\\nBiftheupperboundofthe95%conﬁdenceintervalfortheerrorofalgorithmAis\\nlessthanthelowerboundofthe95%conﬁdenceintervalfortheerrorofalgorithm\\nB.\\n1 2 8', 'CHAPTER5.MACHINELEARNINGBASICS\\nExample:\\xa0BernoulliDistributionWeonceagainconsiderasetofsamples\\n{x(1),...,x() m}drawnindependentlyandidenticallyfromaBernoullidistribution\\n(recallP(x() i;θ) =θx() i(1−θ)(1 − x() i)).Thistimeweareinterestedincomputing\\nthevarianceoftheestimator ˆθ m=1\\nm\\ue050m\\ni=1x() i.\\nVar\\ue010\\nˆθ m\\ue011\\n= Var\\ue020\\n1\\nmm\\ue058\\ni=1x() i\\ue021\\n(5.48)\\n=1\\nm2m\\ue058\\ni=1Var\\ue010\\nx() i\\ue011\\n(5.49)\\n=1\\nm2m\\ue058\\ni=1θθ (1−) (5.50)\\n=1\\nm2mθθ (1−) (5.51)\\n=1\\nmθθ (1−) (5.52)\\nThevarianceoftheestimatordecreasesasafunctionofm,thenumberofexamples\\ninthedataset.Thisisacommonpropertyofpopularestimatorsthatwewill\\nreturntowhenwediscussconsistency(seesection).5.4.5\\n5.4.4TradingoﬀBiasandVariancetoMinimizeMeanSquared\\nError\\nBiasandvariancemeasuretwodiﬀerentsourcesoferrorinanestimator.Bias\\nmeasurestheexpecteddeviationfromthetruevalueofthefunctionorparameter.\\nVarianceontheotherhand,providesameasureofthedeviationfromtheexpected\\nestimatorvaluethatanyparticularsamplingofthedataislikelytocause.\\nWhathappenswhenwearegivenachoicebetweentwoestimators,onewith\\nmorebiasandonewithmorevariance?Howdowechoosebetweenthem?For\\nexample,imaginethatweareinterestedinapproximating thefunctionshownin\\nﬁgureandweareonlyoﬀeredthechoicebetweenamodelwithlargebiasand 5.2\\nonethatsuﬀersfromlargevariance.Howdowechoosebetweenthem?\\nThemostcommonwaytonegotiatethistrade-oﬀistousecross-validation.\\nEmpirically,cross-validationishighlysuccessfulonmanyreal-worldtasks.Alter-\\nnatively,wecanalsocomparethemeansquarederror(MSE)oftheestimates:\\nMSE = [( Eˆθ m−θ)2] (5.53)\\n= Bias(ˆθ m)2+Var(ˆθ m) (5.54)\\n1 2 9', 'CHAPTER5.MACHINELEARNINGBASICS\\nTheMSEmeasurestheoverallexpecteddeviation—in asquarederrorsense—\\nbetweentheestimatorandthetruevalueoftheparameterθ.Asisclearfrom\\nequation,evaluatingtheMSEincorporatesboththebiasandthevariance. 5.54\\nDesirableestimatorsarethosewithsmallMSEandtheseareestimatorsthat\\nmanagetokeepboththeirbiasandvariancesomewhatincheck.\\nC apac i t yB i as Ge ne r al i z at i on\\ne r r orV ar i anc e\\nO pt i m al\\nc apac i t yO v e r ﬁt t i ng\\xa0z o n e U nde r ﬁt t i ng\\xa0z o n e\\nFigure5.6:Ascapacityincreases(x-axis),bias(dotted)tendstodecreaseandvariance\\n(dashed)tendstoincrease,yieldinganotherU-shapedcurveforgeneralizationerror(bold\\ncurve).Ifwevarycapacityalongoneaxis,thereisanoptimalcapacity,withunderﬁtting\\nwhenthecapacityisbelowthisoptimumandoverﬁttingwhenitisabove.Thisrelationship\\nissimilartotherelationshipbetweencapacity,underﬁtting,andoverﬁtting,discussedin\\nsectionandﬁgure. 5.2 5.3\\nTherelationshipbetweenbiasandvarianceistightlylinkedtothemachine\\nlearningconceptsofcapacity,underﬁttingandoverﬁtting.Inthecasewheregen-\\neralizationerrorismeasuredbytheMSE(wherebiasandvariancearemeaningful\\ncomponentsofgeneralization error),increasingcapacitytendstoincreasevariance\\nanddecreasebias.Thisisillustratedinﬁgure,whereweseeagaintheU-shaped 5.6\\ncurveofgeneralization errorasafunctionofcapacity.\\n5.4.5Consistency\\nSofarwehavediscussedthepropertiesofvariousestimatorsforatrainingsetof\\nﬁxedsize.Usually,wearealsoconcernedwiththebehaviorofanestimatorasthe\\namountoftrainingdatagrows.Inparticular,weusuallywishthat,asthenumber\\nofdatapointsminourdatasetincreases,ourpointestimatesconvergetothetrue\\n1 3 0', 'CHAPTER5.MACHINELEARNINGBASICS\\nvalueofthecorrespondingparameters.Moreformally,wewouldlikethat\\nplimm → ∞ˆθ m= θ. (5.55)\\nThesymbolplimindicatesconvergenceinprobability,meaningthatforany\\ue00f>0,\\nP(|ˆθ m−|θ>\\ue00f)→0asm→∞.Theconditiondescribedbyequationis5.55\\nknownasconsistency.Itissometimesreferredtoasweakconsistency,with\\nstrongconsistencyreferringtothealmostsureconvergenceofˆθtoθ.Almost\\nsureconvergenceofasequenceofrandomvariables x(1), x(2),...toavaluex\\noccurswhenp(lim m → ∞ x() m= ) = 1x.\\nConsistencyensuresthatthebiasinducedbytheestimatordiminishesasthe\\nnumberofdataexamplesgrows.However,thereverseisnottrue—asymptotic\\nunbiasednessdoesnotimplyconsistency.\\xa0Forexample,considerestimatingthe\\nmeanparameterµofanormaldistributionN(x;µ,σ2),withadatasetconsisting\\nofmsamples:{x(1),...,x() m}.Wecouldusetheﬁrstsamplex(1)ofthedataset\\nasanunbiasedestimator:ˆθ=x(1).Inthatcase, E(ˆθ m)=θsotheestimator\\nisunbiasednomatterhowmanydatapointsareseen.This,ofcourse,implies\\nthattheestimateisasymptoticallyunbiased.However,thisisnotaconsistent\\nestimatorasitisthecasethat not ˆθ m→ →∞θmas.\\n5.5MaximumLikelihoodEstimation\\nPreviously,wehaveseensomedeﬁnitionsofcommonestimatorsandanalyzed\\ntheirproperties.Butwheredidtheseestimatorscomefrom?Ratherthanguessing\\nthatsomefunctionmightmakeagoodestimatorandthenanalyzingitsbiasand\\nvariance,wewouldliketohavesomeprinciplefromwhichwecanderivespeciﬁc\\nfunctionsthataregoodestimatorsfordiﬀerentmodels.\\nThemostcommonsuchprincipleisthemaximumlikelihoodprinciple.\\nConsiderasetofmexamples X={x(1),...,x() m}drawnindependentlyfrom\\nthetruebutunknowndatageneratingdistributionpdata() x.\\nLetpmodel( x;θ)beaparametricfamilyofprobabilitydistributionsoverthe\\nsamespaceindexedbyθ.Inotherwords,pmodel(x;θ)mapsanyconﬁgurationx\\ntoarealnumberestimatingthetrueprobabilitypdata()x.\\nThemaximumlikelihoodestimatorforisthendeﬁnedas θ\\nθML= argmax\\nθpmodel(;) Xθ (5.56)\\n= argmax\\nθm\\ue059\\ni=1pmodel(x() i;)θ (5.57)\\n1 3 1', 'CHAPTER5.MACHINELEARNINGBASICS\\nThisproductovermanyprobabilitiescanbeinconvenientforavarietyofreasons.\\nForexample,itispronetonumericalunderﬂow.Toobtainamoreconvenient\\nbutequivalentoptimization problem,weobservethattakingthelogarithmofthe\\nlikelihooddoesnotchangeitsargmaxbutdoesconvenientlytransformaproduct\\nintoasum:\\nθML= argmax\\nθm\\ue058\\ni=1logpmodel(x() i;)θ. (5.58)\\nBecausetheargmaxdoesnotchangewhenwerescalethecostfunction,wecan\\ndividebymtoobtainaversionofthecriterionthatisexpressedasanexpectation\\nwithrespecttotheempiricaldistributionˆpdatadeﬁnedbythetrainingdata:\\nθML= argmax\\nθE x ∼ˆ pdatalogpmodel(;)xθ. (5.59)\\nOnewaytointerpretmaximumlikelihoodestimationistoviewitasminimizing\\nthedissimilaritybetweentheempiricaldistributionˆpdatadeﬁnedbythetraining\\nsetandthemodeldistribution,withthedegreeofdissimilaritybetweenthetwo\\nmeasuredbytheKLdivergence.TheKLdivergenceisgivenby\\nDKL(ˆpdata\\ue06bpmodel) = E x ∼ˆ pdata[log ˆpdata()logx−pmodel()]x.(5.60)\\nThetermontheleftisafunctiononlyofthedatageneratingprocess,notthe\\nmodel.ThismeanswhenwetrainthemodeltominimizetheKLdivergence,we\\nneedonlyminimize\\n− E x ∼ˆ pdata[logpmodel()]x (5.61)\\nwhichisofcoursethesameasthemaximization inequation.5.59\\nMinimizingthisKLdivergencecorrespondsexactlytominimizingthecross-\\nentropybetweenthedistributions.Manyauthorsusetheterm“cross-entropy”to\\nidentifyspeciﬁcallythenegativelog-likelihoodofaBernoulliorsoftmaxdistribution,\\nbutthatisamisnomer.Anylossconsistingofanegativelog-likelihoodisacross-\\nentropybetweentheempiricaldistributiondeﬁnedbythetrainingsetandthe\\nprobabilitydistributiondeﬁnedbymodel.Forexample,meansquarederroristhe\\ncross-entropybetweentheempiricaldistributionandaGaussianmodel.\\nWecanthusseemaximumlikelihoodasanattempttomakethemodeldis-\\ntributionmatchtheempiricaldistributionˆpdata.Ideally,wewouldliketomatch\\nthetruedatageneratingdistributionpdata,butwehavenodirectaccesstothis\\ndistribution.\\nWhiletheoptimalθisthesameregardlessofwhetherwearemaximizingthe\\nlikelihoodorminimizingtheKLdivergence,thevaluesoftheobjectivefunctions\\n1 3 2', 'CHAPTER5.MACHINELEARNINGBASICS\\narediﬀerent.Insoftware,weoftenphrasebothasminimizingacostfunction.\\nMaximumlikelihoodthusbecomesminimization ofthenegativelog-likelihood\\n(NLL),orequivalently,minimization ofthecrossentropy.Theperspectiveof\\nmaximumlikelihoodasminimumKLdivergencebecomeshelpfulinthiscase\\nbecausetheKLdivergencehasaknownminimumvalueofzero.Thenegative\\nlog-likelihoodcanactuallybecomenegativewhenisreal-valued.x\\n5.5.1ConditionalLog-LikelihoodandMeanSquaredError\\nThemaximumlikelihoodestimatorcanreadilybegeneralizedtothecasewhere\\nourgoalistoestimateaconditionalprobabilityP( y x|;θ)inordertopredict y\\ngiven x.Thisisactuallythemostcommonsituationbecauseitformsthebasisfor\\nmostsupervisedlearning.IfXrepresentsallourinputsandYallourobserved\\ntargets,thentheconditionalmaximumlikelihoodestimatoris\\nθML= argmax\\nθP. ( ;)YX|θ (5.62)\\nIftheexamplesareassumedtobei.i.d.,thenthiscanbedecomposedinto\\nθML= argmax\\nθm\\ue058\\ni=1log(Py() i|x() i;)θ. (5.63)\\nExample:LinearRegressionasMaximumLikelihoodLinearregression,\\nintroducedearlierinsection,maybejustiﬁedasamaximumlikelihood 5.1.4\\nprocedure.Previously,wemotivatedlinearregressionasanalgorithmthatlearns\\ntotakeaninputxandproduceanoutputvalue ˆy.Themappingfromxtoˆyis\\nchosentominimizemeansquarederror,acriterionthatweintroducedmoreorless\\narbitrarily.Wenowrevisitlinearregressionfromthepointofviewofmaximum\\nlikelihoodestimation.Insteadofproducingasingleprediction ˆy,wenowthink\\nofthemodelasproducingaconditionaldistributionp(y|x).Wecanimagine\\nthatwithaninﬁnitelylargetrainingset,wemightseeseveraltrainingexamples\\nwiththesameinputvaluexbutdiﬀerentvaluesofy.\\xa0Thegoalofthelearning\\nalgorithmisnowtoﬁtthedistributionp(y|x)toallofthosediﬀerentyvalues\\nthatareallcompatiblewithx.Toderivethesamelinearregressionalgorithm\\nweobtainedbefore,wedeﬁnep(y|x) =N(y;ˆy(x;w),σ2).Thefunction ˆy(x;w)\\ngivesthepredictionofthemeanoftheGaussian.Inthisexample,weassumethat\\nthevarianceisﬁxedtosomeconstantσ2chosenbytheuser.Wewillseethatthis\\nchoiceofthefunctionalformofp(y|x)causesthemaximumlikelihoodestimation\\nproceduretoyieldthesamelearningalgorithmaswedevelopedbefore.Sincethe\\n1 3 3', 'CHAPTER5.MACHINELEARNINGBASICS\\nexamplesareassumedtobei.i.d.,theconditionallog-likelihood(equation)is5.63\\ngivenby\\nm\\ue058\\ni=1log(py() i|x() i;)θ (5.64)\\n= log −mσ−m\\n2log(2)π−m\\ue058\\ni=1\\ue00d\\ue00dˆy() i−y() i\\ue00d\\ue00d2\\n2σ2,(5.65)\\nwhere ˆy() iistheoutputofthelinearregressiononthei-thinputx() iandmisthe\\nnumberofthetrainingexamples.Comparingthelog-likelihoodwiththemean\\nsquarederror,\\nMSEtrain=1\\nmm\\ue058\\ni=1||ˆy() i−y() i||2, (5.66)\\nweimmediately seethatmaximizingthelog-likelihoodwithrespecttowyields\\nthesameestimateoftheparameterswasdoesminimizingthemeansquarederror.\\nThetwocriteriahavediﬀerentvaluesbutthesamelocationoftheoptimum.This\\njustiﬁestheuseoftheMSEasamaximumlikelihoodestimationprocedure.Aswe\\nwillsee,themaximumlikelihoodestimatorhasseveraldesirableproperties.\\n5.5.2PropertiesofMaximumLikelihood\\nThemainappealofthemaximumlikelihoodestimatoristhatitcanbeshownto\\nbethebestestimatorasymptotically,asthenumberofexamplesm→∞,interms\\nofitsrateofconvergenceasincreases.m\\nUnderappropriate\\xa0conditions,\\xa0the maximumlikelihood\\xa0estimatorhas\\xa0the\\npropertyofconsistency(seesectionabove),meaningthatasthenumber 5.4.5\\noftrainingexamplesapproachesinﬁnity,themaximumlikelihoodestimateofa\\nparameterconvergestothetruevalueoftheparameter.Theseconditionsare:\\n•Thetruedistributionpdatamustliewithinthemodelfamilypmodel(·;θ).\\nOtherwise,noestimatorcanrecoverpdata.\\n•Thetruedistributionpdatamustcorrespondtoexactlyonevalueofθ.Other-\\nwise,maximumlikelihoodcanrecoverthecorrectpdata,butwillnotbeable\\ntodeterminewhichvalueofwasusedbythedatageneratingprocessing. θ\\nThereareotherinductiveprinciplesbesidesthemaximumlikelihoodestima-\\ntor,manyofwhichsharethepropertyofbeingconsistentestimators.\\xa0However,\\n1 3 4', 'CHAPTER5.MACHINELEARNINGBASICS\\nconsistentestimatorscandiﬀerintheirstatisticeﬃciency,meaningthatone\\nconsistentestimatormayobtainlowergeneralization errorforaﬁxednumberof\\nsamplesm,orequivalently,mayrequirefewerexamplestoobtainaﬁxedlevelof\\ngeneralization error.\\nStatisticaleﬃciencyistypicallystudiedintheparametriccase(likeinlinear\\nregression)whereourgoalistoestimatethevalueofaparameter(andassuming\\nitispossibletoidentifythetrueparameter),notthevalueofafunction.Awayto\\nmeasurehowclosewearetothetrueparameterisbytheexpectedmeansquared\\nerror,computingthesquareddiﬀerencebetweentheestimatedandtrueparameter\\nvalues,wheretheexpectationisovermtrainingsamplesfromthedatagenerating\\ndistribution.Thatparametricmeansquarederrordecreasesasmincreases,and\\nformlarge,theCramér-Raolowerbound(,;,)showsthatno Rao1945Cramér1946\\nconsistentestimatorhasalowermeansquarederrorthanthemaximumlikelihood\\nestimator.\\nForthesereasons(consistencyandeﬃciency),maximumlikelihoodisoften\\nconsideredthepreferredestimatortouseformachinelearning.Whenthenumber\\nofexamplesissmallenoughtoyieldoverﬁttingbehavior,regularizationstrategies\\nsuchasweightdecaymaybeusedtoobtainabiasedversionofmaximumlikelihood\\nthathaslessvariancewhentrainingdataislimited.\\n5.6BayesianStatistics\\nSofarwehavediscussedfrequentiststatisticsandapproachesbasedonestimat-\\ningasinglevalueofθ,thenmakingallpredictionsthereafterbasedonthatone\\nestimate.Anotherapproachistoconsiderallpossiblevaluesofθwhenmakinga\\nprediction.ThelatteristhedomainofBayesianstatistics.\\nAsdiscussed\\xa0insection\\xa0,\\xa0the\\xa0frequen tist\\xa0perspective\\xa0isthat\\xa0thetrue 5.4.1\\nparametervalueθisﬁxedbutunknown,whilethepointestimate ˆθisarandom\\nvariableonaccountofitbeingafunctionofthedataset(whichisseenasrandom).\\nTheBayesianperspectiveonstatisticsisquitediﬀerent.\\xa0The Bayesianuses\\nprobabilitytoreﬂectdegreesofcertaintyofstatesofknowledge.Thedatasetis\\ndirectlyobservedandsoisnotrandom.Ontheotherhand,thetrueparameterθ\\nisunknownoruncertainandthusisrepresentedasarandomvariable.\\nBeforeobservingthedata,werepresentourknowledgeofθusingtheprior\\nprobabilitydistribution,p(θ)(sometimesreferredtoassimply“theprior”).\\nGenerally,themachinelearningpractitionerselectsapriordistributionthatis\\nquitebroad(i.e.withhighentropy)toreﬂectahighdegreeofuncertaintyinthe\\n1 3 5', 'CHAPTER5.MACHINELEARNINGBASICS\\nvalueofθbeforeobservinganydata.Forexample,onemightassume that apriori\\nθliesinsomeﬁniterangeorvolume,withauniformdistribution.\\xa0Manypriors\\ninsteadreﬂectapreferencefor“simpler”\\xa0solutions(suchassmallermagnitude\\ncoeﬃcients,orafunctionthatisclosertobeingconstant).\\nNowconsiderthatwehaveasetofdatasamples {x(1),...,x() m}.Wecan\\nrecovertheeﬀectofdataonourbeliefaboutθbycombiningthedatalikelihood\\npx((1),...,x() m|θ)withthepriorviaBayes’rule:\\npx(θ|(1),...,x() m) =px((1),...,x() m|θθ)(p)\\npx((1),...,x() m)(5.67)\\nInthescenarioswhereBayesianestimationistypicallyused,thepriorbeginsasa\\nrelativelyuniformorGaussiandistributionwithhighentropy,andtheobservation\\nofthedatausuallycausestheposteriortoloseentropyandconcentratearounda\\nfewhighlylikelyvaluesoftheparameters.\\nRelativetomaximumlikelihoodestimation,Bayesianestimationoﬀerstwo\\nimportantdiﬀerences.First,unlikethemaximumlikelihoodapproachthatmakes\\npredictionsusingapointestimateofθ,theBayesianapproachistomakepredictions\\nusingafulldistributionoverθ.Forexample,afterobservingmexamples,the\\npredicteddistributionoverthenextdatasample,x(+1) m,isgivenby\\npx((+1) m|x(1),...,x() m) =\\ue05a\\npx((+1) m| |θθ)(px(1),...,x() m)d.θ(5.68)\\nHereeachvalueofθwithpositiveprobabilitydensitycontributestotheprediction\\nofthenextexample,withthecontributionweightedbytheposteriordensityitself.\\nAfterhavingobserved{x(1),...,x() m},ifwearestillquiteuncertainaboutthe\\nvalueofθ,thenthisuncertaintyisincorporated directlyintoanypredictionswe\\nmightmake.\\nInsection,wediscussedhowthefrequentistapproachaddressestheuncer- 5.4\\ntaintyinagivenpointestimateofθbyevaluatingitsvariance.Thevarianceof\\ntheestimatorisanassessmentofhowtheestimatemightchangewithalternative\\nsamplingsoftheobserveddata.TheBayesiananswertothequestionofhowtodeal\\nwiththeuncertaintyintheestimatoristosimplyintegrateoverit,whichtendsto\\nprotectwellagainstoverﬁtting.\\xa0Thisintegralisofcoursejustanapplicationof\\nthelawsofprobability,makingtheBayesianapproachsimpletojustify,whilethe\\nfrequentistmachineryforconstructinganestimatorisbasedontheratheradhoc\\ndecisiontosummarizeallknowledgecontainedinthedatasetwithasinglepoint\\nestimate.\\nThesecondimportantdiﬀerencebetweentheBayesianapproachtoestimation\\nandthemaximumlikelihoodapproachisduetothecontributionoftheBayesian\\n1 3 6', 'CHAPTER5.MACHINELEARNINGBASICS\\npriordistribution.Thepriorhasaninﬂuencebyshiftingprobabilitymassdensity\\ntowardsregionsoftheparameterspacethatarepreferred .Inpractice, apriori\\ntheprioroftenexpressesapreferenceformodelsthataresimplerormoresmooth.\\nCriticsoftheBayesianapproachidentifythepriorasasourceofsubjectivehuman\\njudgmentimpactingthepredictions.\\nBayesianmethodstypicallygeneralizemuchbetterwhenlimitedtrainingdata\\nisavailable,buttypicallysuﬀerfromhighcomputational costwhenthenumberof\\ntrainingexamplesislarge.\\nExample:BayesianLinearRegressionHereweconsidertheBayesianesti-\\nmationapproachtolearningthelinearregressionparameters.Inlinearregression,\\nwelearnalinearmappingfromaninputvectorx∈ Rntopredictthevalueofa\\nscalar.Thepredictionisparametrized bythevector y∈ R w∈ Rn:\\nˆy= w\\ue03ex. (5.69)\\nGivenasetofmtrainingsamples (X()train,y()train),wecanexpresstheprediction\\nofovertheentiretrainingsetas: y\\nˆy()train= X()trainw. (5.70)\\nExpressedasaGaussianconditionaldistributionony()train,wehave\\np(y()train|X()train,wy ) = (N()train;X()trainwI,) (5.71)\\n∝exp\\ue012\\n−1\\n2(y()train−X()trainw)\\ue03e(y()train−X()trainw)\\ue013\\n,\\n(5.72)\\nwherewefollowthestandardMSEformulationinassumingthattheGaussian\\nvarianceonyisone.Inwhatfollows,toreducethenotationalburden,wereferto\\n(X()train,y()train) ( ) assimplyXy,.\\nTodeterminetheposteriordistributionoverthemodelparametervectorw,we\\nﬁrstneedtospecifyapriordistribution.Thepriorshouldreﬂectournaivebelief\\naboutthevalueoftheseparameters.Whileitissometimesdiﬃcultorunnatural\\ntoexpressourpriorbeliefsintermsoftheparametersofthemodel,inpracticewe\\ntypicallyassumeafairlybroaddistributionexpressingahighdegreeofuncertainty\\naboutθ.\\xa0Forreal-valuedparametersitiscommontouseaGaussianasaprior\\ndistribution:\\np() = (;w Nwµ0, Λ0) exp∝\\ue012\\n−1\\n2(wµ−0)\\ue03eΛ−1\\n0(wµ−0)\\ue013\\n,(5.73)\\n1 3 7', 'CHAPTER5.MACHINELEARNINGBASICS\\nwhereµ0and Λ0arethepriordistributionmeanvectorandcovariancematrix\\nrespectively.1\\nWiththepriorthusspeciﬁed,wecannowproceedindeterminingtheposterior\\ndistributionoverthemodelparameters.\\np,p,p (wX|y) ∝(yX|w)()w (5.74)\\n∝exp\\ue012\\n−1\\n2( )yXw−\\ue03e( )yXw−\\ue013\\nexp\\ue012\\n−1\\n2(wµ−0)\\ue03eΛ−1\\n0(wµ−0)\\ue013\\n(5.75)\\n∝exp\\ue012\\n−1\\n2\\ue010\\n−2y\\ue03eXww+\\ue03eX\\ue03eXww+\\ue03eΛ−1\\n0wµ−2\\ue03e\\n0 Λ−1\\n0w\\ue011\\ue013\\n.\\n(5.76)\\nWenowdeﬁne Λ m=\\ue000\\nX\\ue03eX+ Λ−1\\n0\\ue001 −1andµ m= Λ m\\ue000\\nX\\ue03ey+ Λ−1\\n0µ0\\ue001\\n.Using\\nthesenewvariables,weﬁndthattheposteriormayberewrittenasaGaussian\\ndistribution:\\np, (wX|y) exp∝\\ue012\\n−1\\n2(wµ− m)\\ue03eΛ−1\\nm(wµ− m)+1\\n2µ\\ue03e\\nm Λ−1\\nmµ m\\ue013\\n(5.77)\\n∝exp\\ue012\\n−1\\n2(wµ− m)\\ue03eΛ−1\\nm(wµ− m)\\ue013\\n. (5.78)\\nAlltermsthatdonotincludetheparametervectorwhavebeenomitted;they\\nareimpliedbythefactthatthedistributionmustbenormalizedtointegrateto.1\\nEquationshowshowtonormalizeamultivariateGaussiandistribution. 3.23\\nExaminingthisposteriordistributionallowsustogainsomeintuitionforthe\\neﬀectofBayesianinference.Inmostsituations,wesetµ0to 0.Ifweset Λ0=1\\nαI,\\nthenµ mgivesthesameestimateofwasdoesfrequentistlinearregressionwitha\\nweightdecaypenaltyofαw\\ue03ew.OnediﬀerenceisthattheBayesianestimateis\\nundeﬁnedifαissettozero—-wearenotallowedtobegintheBayesianlearning\\nprocesswithaninﬁnitelywideprioronw.Themoreimportantdiﬀerenceisthat\\ntheBayesianestimateprovidesacovariancematrix,showinghowlikelyallthe\\ndiﬀerentvaluesofare,ratherthanprovidingonlytheestimate w µ m.\\n5.6.1Maximum (MAP)Estimation A P o s t e ri o ri\\nWhilethemostprincipledapproachistomakepredictionsusingthefullBayesian\\nposteriordistributionovertheparameterθ,itisstilloftendesirabletohavea\\n1Un l e s s t h e re i s a re a s o n t o a s s u m e a p a rtic u l a r c o v a ria n c e s t ru c t u re , we t y p i c a l l y a s s u m e a\\nd i a g o n a l c o v a ria n c e m a t rix Λ0= diag( λ0) .\\n1 3 8', 'CHAPTER5.MACHINELEARNINGBASICS\\nsinglepointestimate.\\xa0Onecommonreasonfordesiringapointestimateisthat\\nmostoperationsinvolvingtheBayesianposteriorformostinterestingmodelsare\\nintractable,andapointestimateoﬀersatractableapproximation.Ratherthan\\nsimplyreturningtothemaximumlikelihoodestimate,wecanstillgainsomeof\\nthebeneﬁtoftheBayesianapproachbyallowingthepriortoinﬂuencethechoice\\nofthepointestimate.Onerationalwaytodothisistochoosethemaximum\\naposteriori(MAP)pointestimate.TheMAPestimatechoosesthepointof\\nmaximalposteriorprobability(ormaximalprobabilitydensityinthemorecommon\\ncaseofcontinuous):θ\\nθMAP= argmax\\nθp( ) = argmaxθx|\\nθlog( )+log() pxθ|pθ.(5.79)\\nWerecognize,aboveontherighthandside,logp(xθ|),i.e.thestandardlog-\\nlikelihoodterm,and,correspondingtothepriordistribution. log()pθ\\nAsanexample,consideralinearregressionmodelwithaGaussianprioron\\ntheweightsw.IfthispriorisgivenbyN(w; 0,1\\nλI2),thenthelog-priortermin\\nequationisproportional tothefamiliar 5.79 λw\\ue03ewweightdecaypenalty,plusa\\ntermthatdoesnotdependonwanddoesnotaﬀectthelearningprocess.MAP\\nBayesianinferencewithaGaussianpriorontheweightsthuscorrespondstoweight\\ndecay.\\nAswithfullBayesianinference,MAPBayesianinferencehastheadvantageof\\nleveraginginformationthatisbroughtbythepriorandcannotbefoundinthe\\ntrainingdata.Thisadditionalinformationhelpstoreducethevarianceinthe\\nMAPpointestimate(incomparisontotheMLestimate).However,itdoessoat\\nthepriceofincreasedbias.\\nManyregularizedestimationstrategies,suchasmaximumlikelihoodlearning\\nregularizedwithweightdecay,canbeinterpretedasmakingtheMAPapproxima-\\ntiontoBayesianinference.Thisviewapplieswhentheregularizationconsistsof\\naddinganextratermtotheobjectivefunctionthatcorrespondstologp(θ).Not\\nallregularizationpenaltiescorrespondtoMAPBayesianinference.Forexample,\\nsomeregularizertermsmaynotbethelogarithmofaprobabilitydistribution.\\nOtherregularizationtermsdependonthedata,whichofcourseapriorprobability\\ndistributionisnotallowedtodo.\\nMAPBayesianinferenceprovidesastraightforwardwaytodesigncomplicated\\nyetinterpretableregularizationterms.Forexample,amorecomplicatedpenalty\\ntermcanbederivedbyusingamixtureofGaussians,ratherthanasingleGaussian\\ndistribution,astheprior(NowlanandHinton1992,).\\n1 3 9', 'CHAPTER5.MACHINELEARNINGBASICS\\n5.7SupervisedLearningAlgorithms\\nRecallfromsectionthatsupervisedlearningalgorithmsare,roughlyspeaking, 5.1.3\\nlearningalgorithmsthatlearntoassociatesomeinputwithsomeoutput,givena\\ntrainingsetofexamplesofinputsxandoutputsy.\\xa0Inmanycasestheoutputs\\nymaybediﬃculttocollectautomatically andmustbeprovidedbyahuman\\n“supervisor,”butthetermstillappliesevenwhenthetrainingsettargetswere\\ncollectedautomatically .\\n5.7.1ProbabilisticSupervisedLearning\\nMost\\xa0supervised\\xa0learning\\xa0algorithms\\xa0inthis\\xa0book\\xa0are\\xa0based\\xa0on estimating\\xa0a\\nprobabilitydistributionp(y|x).Wecandothissimplybyusingmaximum\\nlikelihoodestimationtoﬁndthebestparametervectorθforaparametricfamily\\nofdistributions .py(|xθ;)\\nWehavealreadyseenthatlinearregressioncorrespondstothefamily\\npyy (| Nxθ;) = (;θ\\ue03exI,). (5.80)\\nWecangeneralizelinearregressiontotheclassiﬁcationscenariobydeﬁninga\\ndiﬀerentfamilyofprobabilitydistributions.Ifwehavetwoclasses,class0and\\nclass1,thenweneedonlyspecifytheprobabilityofoneoftheseclasses.The\\nprobabilityofclass1determinestheprobabilityofclass0,becausethesetwovalues\\nmustaddupto1.\\nThenormaldistributionoverreal-valuednumbersthatweusedforlinear\\nregressionisparametrized intermsofamean.Anyvaluewesupplyforthismean\\nisvalid.Adistributionoverabinaryvariableisslightlymorecomplicated,because\\nitsmeanmustalwaysbebetween0and1.Onewaytosolvethisproblemistouse\\nthelogisticsigmoidfunctiontosquashtheoutputofthelinearfunctionintothe\\ninterval(0,1)andinterpretthatvalueasaprobability:\\npy σ (= 1 ;) = |xθ (θ\\ue03ex). (5.81)\\nThisapproachisknownaslogisticregression(asomewhatstrangenamesince\\nweusethemodelforclassiﬁcationratherthanregression).\\nInthecaseoflinearregression,wewereabletoﬁndtheoptimalweightsby\\nsolvingthenormalequations.Logisticregressionissomewhatmorediﬃcult.There\\nisnoclosed-formsolutionforitsoptimalweights.Instead,wemustsearchfor\\nthembymaximizingthelog-likelihood.Wecandothisbyminimizingthenegative\\nlog-likelihood(NLL)usinggradientdescent.\\n1 4 0', 'CHAPTER5.MACHINELEARNINGBASICS\\nThissamestrategycanbeappliedtoessentiallyanysupervisedlearningproblem,\\nbywritingdownaparametricfamilyofconditionalprobabilitydistributionsover\\ntherightkindofinputandoutputvariables.\\n5.7.2SupportVectorMachines\\nOneofthemostinﬂuentialapproachestosupervisedlearningisthesupportvector\\nmachine(,; Boseretal.1992CortesandVapnik1995,).Thismodelissimilarto\\nlogisticregressioninthatitisdrivenbyalinearfunctionw\\ue03ex+b.Unlikelogistic\\nregression,thesupportvectormachinedoesnotprovideprobabilities, butonly\\noutputsaclassidentity.TheSVMpredictsthatthepositiveclassispresentwhen\\nw\\ue03ex+bispositive.Likewise,itpredictsthatthenegativeclassispresentwhen\\nw\\ue03ex+bisnegative.\\nOnekeyinnovationassociatedwithsupportvectormachinesisthekernel\\ntrick.Thekerneltrickconsistsofobservingthatmanymachinelearningalgorithms\\ncanbewrittenexclusivelyintermsofdotproductsbetweenexamples.Forexample,\\nitcanbeshownthatthelinearfunctionusedbythesupportvectormachinecan\\nbere-writtenas\\nw\\ue03ex+= +bbm\\ue058\\ni=1α ix\\ue03ex() i(5.82)\\nwherex() iisatrainingexampleandαisavectorofcoeﬃcients.Rewritingthe\\nlearningalgorithmthiswayallowsustoreplacexbytheoutputofagivenfeature\\nfunctionφ(x) andthedotproductwithafunctionk(xx,() i) =φ(x)·φ(x() i) called\\nakernel.The ·operatorrepresentsaninnerproductanalogoustoφ(x)\\ue03eφ(x() i).\\nForsomefeaturespaces,wemaynotuseliterallythevectorinnerproduct.In\\nsomeinﬁnitedimensionalspaces,weneedtouseotherkindsofinnerproducts,for\\nexample,innerproductsbasedonintegrationratherthansummation.Acomplete\\ndevelopmentofthesekindsofinnerproductsisbeyondthescopeofthisbook.\\nAfterreplacingdotproductswithkernelevaluations,wecanmakepredictions\\nusingthefunction\\nfb () = x +\\ue058\\niα ik,(xx() i). (5.83)\\nThisfunctionisnonlinearwithrespecttox,buttherelationshipbetweenφ(x)\\nandf(x)islinear.Also,therelationshipbetweenαandf(x)islinear.The\\nkernel-basedfunctionisexactlyequivalenttopreprocessingthedatabyapplying\\nφ()xtoallinputs,thenlearningalinearmodelinthenewtransformedspace.\\nThekerneltrickispowerfulfortworeasons.First,itallowsustolearnmodels\\nthatarenonlinearasafunctionofxusingconvexoptimization techniquesthatare\\n1 4 1', 'CHAPTER5.MACHINELEARNINGBASICS\\nguaranteedtoconvergeeﬃciently.Thisispossiblebecauseweconsiderφﬁxedand\\noptimizeonlyα,i.e.,theoptimization algorithmcanviewthedecisionfunction\\nasbeinglinearinadiﬀerentspace.Second,thekernelfunctionkoftenadmits\\nanimplementationthatissigniﬁcantlymorecomputational eﬃcientthannaively\\nconstructingtwovectorsandexplicitlytakingtheirdotproduct. φ()x\\nInsomecases,φ(x)canevenbeinﬁnitedimensional,whichwouldresultin\\naninﬁnitecomputational costforthenaive,explicitapproach.Inmanycases,\\nk(xx,\\ue030)isanonlinear,tractablefunctionofxevenwhenφ(x)isintractable.As\\nanexampleofaninﬁnite-dimens ionalfeaturespacewithatractablekernel,we\\nconstructafeaturemappingφ(x)overthenon-negativeintegersx.Supposethat\\nthismappingreturnsavectorcontainingxonesfollowedbyinﬁnitelymanyzeros.\\nWecanwriteakernelfunctionk(x,x() i) =min(x,x() i)thatisexactlyequivalent\\ntothecorrespondinginﬁnite-dimens ionaldotproduct.\\nThemostcommonlyusedkernelistheGaussiankernel\\nk, ,σ (uvuv ) = (N −;02I) (5.84)\\nwhere N(x;µ, Σ)isthestandardnormaldensity.Thiskernelisalsoknownas\\ntheradialbasisfunction(RBF)kernel,becauseitsvaluedecreasesalonglines\\ninvspaceradiatingoutwardfromu.TheGaussiankernelcorrespondstoadot\\nproductinaninﬁnite-dimens ionalspace,butthederivationofthisspaceisless\\nstraightforwardthaninourexampleofthekernelovertheintegers. min\\nWecanthinkoftheGaussiankernelasperformingakindoftemplatematch-\\ning.Atrainingexamplexassociatedwithtraininglabelybecomesatemplate\\nforclassy.Whenatestpointx\\ue030isnearxaccordingtoEuclideandistance,the\\nGaussiankernelhasalargeresponse,indicatingthatx\\ue030isverysimilartothex\\ntemplate.Themodelthenputsalargeweightontheassociatedtraininglabely.\\nOverall,thepredictionwillcombinemanysuchtraininglabelsweightedbythe\\nsimilarityofthecorrespondingtrainingexamples.\\nSupportvectormachinesarenottheonlyalgorithmthatcanbeenhanced\\nusingthekerneltrick.Manyotherlinearmodelscanbeenhancedinthisway.The\\ncategoryofalgorithmsthatemploythekerneltrickisknownaskernelmachines\\norkernelmethods( ,; WilliamsandRasmussen1996Schölkopf1999etal.,).\\nAmajordrawbacktokernelmachinesisthatthecostofevaluatingthedecision\\nfunctionislinearinthenumberoftrainingexamples,becausethei-thexample\\ncontributesatermα ik(xx,() i)tothedecisionfunction.Supportvectormachines\\nareabletomitigatethisbylearninganαvectorthatcontainsmostlyzeros.\\nClassifyinganewexamplethenrequiresevaluatingthekernelfunctiononlyfor\\nthetrainingexamplesthathavenon-zeroα i.Thesetrainingexamplesareknown\\n1 4 2', 'CHAPTER5.MACHINELEARNINGBASICS\\nassupportvectors.\\nKernelmachinesalsosuﬀerfromahighcomputational costoftrainingwhen\\nthedatasetislarge.Wewillrevisitthisideainsection.Kernelmachineswith 5.9\\ngenerickernelsstruggletogeneralizewell.Wewillexplainwhyinsection.The5.11\\nmodernincarnationofdeeplearningwasdesignedtoovercometheselimitationsof\\nkernelmachines.ThecurrentdeeplearningrenaissancebeganwhenHintonetal.\\n()demonstratedthataneuralnetworkcouldoutperformtheRBFkernelSVM 2006\\nontheMNISTbenchmark.\\n5.7.3OtherSimpleSupervisedLearningAlgorithms\\nWehavealreadybrieﬂyencounteredanothernon-probabilis ticsupervisedlearning\\nalgorithm,nearestneighborregression.Moregenerally,k-nearestneighborsis\\nafamilyoftechniquesthatcanbeusedforclassiﬁcationorregression.Asa\\nnon-parametric learningalgorithm,k-nearestneighborsisnotrestrictedtoaﬁxed\\nnumberofparameters.Weusuallythinkofthek-nearestneighborsalgorithm\\nasnothavinganyparameters,butratherimplementingasimplefunctionofthe\\ntrainingdata.Infact,thereisnotevenreallyatrainingstageorlearningprocess.\\nInstead,attesttime,whenwewanttoproduceanoutputyforanewtestinputx,\\nweﬁndthek-nearestneighborstoxinthetrainingdataX.Wethenreturnthe\\naverageofthecorrespondingyvaluesinthetrainingset.Thisworksforessentially\\nanykindofsupervisedlearningwherewecandeﬁneanaverageoveryvalues.In\\nthecaseofclassiﬁcation,wecanaverageoverone-hotcodevectorscwithc y= 1\\nandc i= 0forallothervaluesofi.Wecantheninterprettheaverageoverthese\\none-hotcodesasgivingaprobabilitydistributionoverclasses.Asanon-parametric\\nlearningalgorithm,k-nearestneighborcanachieveveryhighcapacity.Forexample,\\nsupposewehaveamulticlassclassiﬁcationtaskandmeasureperformancewith0-1\\nloss.Inthissetting,-nearestneighborconvergestodoubletheBayeserrorasthe 1\\nnumberoftrainingexamplesapproachesinﬁnity.TheerrorinexcessoftheBayes\\nerrorresultsfromchoosingasingleneighborbybreakingtiesbetweenequally\\ndistantneighborsrandomly.Whenthereisinﬁnitetrainingdata,alltestpointsx\\nwillhaveinﬁnitelymanytrainingsetneighborsatdistancezero.Ifweallowthe\\nalgorithmtousealloftheseneighborstovote,ratherthanrandomlychoosingone\\nofthem,theprocedureconvergestotheBayeserrorrate.\\xa0Thehighcapacityof\\nk-nearestneighborsallowsittoobtainhighaccuracygivenalargetrainingset.\\nHowever,itdoessoathighcomputational cost,anditmaygeneralizeverybadly\\ngivenasmall,ﬁnitetrainingset.Oneweaknessofk-nearestneighborsisthatit\\ncannotlearnthatonefeatureismorediscriminativethananother.Forexample,\\nimaginewehavearegressiontaskwithx∈ R100drawnfromanisotropicGaussian\\n1 4 3', 'CHAPTER5.MACHINELEARNINGBASICS\\ndistribution,butonlyasinglevariablex1isrelevanttotheoutput.Suppose\\nfurtherthatthisfeaturesimplyencodestheoutputdirectly,i.e.thaty=x1inall\\ncases.Nearestneighborregressionwillnotbeabletodetectthissimplepattern.\\nThenearestneighborofmostpointsxwillbedeterminedbythelargenumberof\\nfeaturesx2throughx100,notbythelonefeaturex1.\\xa0Thustheoutputonsmall\\ntrainingsetswillessentiallyberandom.\\n1 4 4', 'CHAPTER5.MACHINELEARNINGBASICS\\n0\\n101\\n1110 1\\n011\\n1111 1110110\\n10010\\n001110 11111101001 00\\n010 01111\\n111\\n11\\nFigure5.7:Diagramsdescribinghowadecisiontreeworks. ( T o p )Eachnodeofthetree\\nchoosestosendtheinputexampletothechildnodeontheleft(0)ororthechildnodeon\\ntheright(1).Internalnodesaredrawnascirclesandleafnodesassquares.Eachnodeis\\ndisplayedwithabinarystringidentiﬁercorrespondingtoitspositioninthetree,obtained\\nbyappendingabittoitsparentidentiﬁer(0=chooseleftortop,1=chooserightorbottom).\\n( Bottom )Thetreedividesspaceintoregions.The2Dplaneshowshowadecisiontree\\nmightdivide R2.Thenodesofthetreeareplottedinthisplane,witheachinternalnode\\ndrawnalongthedividinglineitusestocategorizeexamples,andleafnodesdrawninthe\\ncenteroftheregionofexamplestheyreceive.Theresultisapiecewise-constantfunction,\\nwithonepieceperleaf.Eachleafrequiresatleastonetrainingexampletodeﬁne,soitis\\nnotpossibleforthedecisiontreetolearnafunctionthathasmorelocalmaximathanthe\\nnumberoftrainingexamples.\\n1 4 5', 'CHAPTER5.MACHINELEARNINGBASICS\\nAnothertypeoflearningalgorithmthatalsobreakstheinputspaceintoregions\\nandhasseparateparametersforeachregionisthedecisiontree( , Breimanetal.\\n1984)anditsmanyvariants.Asshowninﬁgure,eachnodeofthedecision 5.7\\ntreeisassociatedwitharegionintheinputspace,andinternalnodesbreakthat\\nregionintoonesub-regionforeachchildofthenode(typicallyusinganaxis-aligned\\ncut).\\xa0Spaceisthussub-dividedintonon-overlappingregions,withaone-to-one\\ncorrespondencebetweenleafnodesandinputregions.Eachleafnodeusuallymaps\\neverypointinitsinputregiontothesameoutput.Decisiontreesareusually\\ntrainedwithspecializedalgorithmsthatarebeyondthescopeofthisbook.The\\nlearningalgorithmcanbeconsiderednon-parametric ifitisallowedtolearnatree\\nofarbitrarysize,thoughdecisiontreesareusuallyregularizedwithsizeconstraints\\nthatturnthemintoparametricmodelsinpractice.Decisiontreesastheyare\\ntypicallyused,withaxis-alignedsplitsandconstantoutputswithineachnode,\\nstruggletosolvesomeproblemsthatareeasyevenforlogisticregression.For\\nexample,ifwehaveatwo-classproblemandthepositiveclassoccurswherever\\nx2>x1,thedecisionboundaryisnotaxis-aligned.Thedecisiontreewillthus\\nneedtoapproximatethedecisionboundarywithmanynodes,implementingastep\\nfunctionthatconstantlywalksbackandforthacrossthetruedecisionfunction\\nwithaxis-alignedsteps.\\nAswehaveseen,nearestneighborpredictorsanddecisiontreeshavemany\\nlimitations.Nonetheless,theyareusefullearningalgorithmswhencomputational\\nresourcesareconstrained.Wecanalsobuildintuitionformoresophisticated\\nlearningalgorithmsbythinkingaboutthesimilaritiesanddiﬀerencesbetween\\nsophisticatedalgorithmsand-NNordecisiontreebaselines. k\\nSee (),\\xa0(),\\xa0 ()orothermachine Murphy2012Bishop2006Hastieetal.2001\\nlearningtextbooksformorematerialontraditionalsupervisedlearningalgorithms.\\n5.8UnsupervisedLearningAlgorithms\\nRecallfromsectionthatunsupervisedalgorithmsarethosethatexperience 5.1.3\\nonly“features”butnotasupervisionsignal.Thedistinctionbetweensupervised\\nandunsupervisedalgorithmsisnotformallyandrigidlydeﬁnedbecausethereisno\\nobjectivetestfordistinguishingwhetheravalueisafeatureoratargetprovidedby\\nasupervisor.Informally,unsupervisedlearningreferstomostattemptstoextract\\ninformationfromadistributionthatdonotrequirehumanlabortoannotate\\nexamples.Thetermisusuallyassociatedwithdensityestimation,learningto\\ndrawsamplesfromadistribution,learningtodenoisedatafromsomedistribution,\\nﬁndingamanifoldthatthedataliesnear,orclusteringthedataintogroupsof\\n1 4 6', 'CHAPTER5.MACHINELEARNINGBASICS\\nrelatedexamples.\\nAclassicunsupervisedlearningtaskistoﬁndthe“best”representationofthe\\ndata.By‘best’wecanmeandiﬀerentthings,butgenerallyspeakingwearelooking\\nforarepresentationthatpreservesasmuchinformationaboutxaspossiblewhile\\nobeyingsomepenaltyorconstraintaimedatkeepingtherepresentation or simpler\\nmoreaccessiblethanitself.x\\nTherearemultiplewaysofdeﬁningarepresentation.Threeofthe simpler\\xa0\\nmostcommonincludelowerdimensionalrepresentations,sparserepresentations\\nandindependentrepresentations.Low-dimensionalrepresentationsattemptto\\ncompressasmuchinformationaboutxaspossibleinasmallerrepresentation.\\nSparserepresentations(,; ,; Barlow1989OlshausenandField1996Hintonand\\nGhahramani1997,)embedthedatasetintoarepresentationwhoseentriesare\\nmostlyzeroesformostinputs.Theuseofsparserepresentationstypicallyrequires\\nincreasingthedimensionalityoftherepresentation,sothattherepresentation\\nbecomingmostlyzeroesdoesnotdiscardtoomuchinformation. Thisresultsinan\\noverallstructureoftherepresentationthattendstodistributedataalongtheaxes\\noftherepresentationspace.Independentrepresentationsattempttodisentangle\\nthesourcesofvariationunderlyingthedatadistributionsuchthatthedimensions\\noftherepresentationarestatisticallyindependent.\\nOf\\xa0coursethese\\xa0three\\xa0criteriaare\\xa0certainly\\xa0notmutuallyexclusive.Low-\\ndimensionalrepresentationsoftenyieldelementsthathavefewerorweakerde-\\npendenciesthantheoriginalhigh-dimensionaldata.Thisisbecauseonewayto\\nreducethesizeofarepresentationistoﬁndandremoveredundancies.Identifying\\nandremovingmoreredundancyallowsthedimensionalityreductionalgorithmto\\nachievemorecompressionwhilediscardinglessinformation.\\nThenotionofrepresentationisoneofthecentralthemesofdeeplearningand\\nthereforeoneofthecentralthemesinthisbook.Inthissection,wedevelopsome\\nsimpleexamplesofrepresentationlearningalgorithms.Together,theseexample\\nalgorithmsshowhowtooperationalizeallthreeofthecriteriaabove.Mostofthe\\nremainingchaptersintroduceadditionalrepresentationlearningalgorithmsthat\\ndevelopthesecriteriaindiﬀerentwaysorintroduceothercriteria.\\n5.8.1PrincipalComponentsAnalysis\\nInsection,wesawthattheprincipalcomponentsanalysisalgorithmprovides 2.12\\nameansofcompressingdata.WecanalsoviewPCAasanunsupervisedlearning\\nalgorithmthatlearnsarepresentationofdata.Thisrepresentationisbasedon\\ntwoofthecriteriaforasimplerepresentationdescribedabove.PCAlearnsa\\n1 4 7', 'CHAPTER5.MACHINELEARNINGBASICS\\n− − 2 0 1 0 0 1 0 2 0\\nx 1− 2 0− 1 001 02 0x 2\\n− − 2 0 1 0 0 1 0 2 0\\nz 1− 2 0− 1 001 02 0z 2\\nFigure5.8:PCAlearnsalinearprojectionthatalignsthedirectionofgreatestvariance\\nwiththeaxesofthenewspace. ( L e f t )Theoriginaldataconsistsofsamplesofx.Inthis\\nspace,thevariancemightoccuralongdirectionsthatarenotaxis-aligned.\\xa0 ( R i g h t )The\\ntransformeddataz=x\\ue03eWnowvariesmostalongtheaxisz 1.Thedirectionofsecond\\nmostvarianceisnowalongz 2.\\nrepresentationthathaslowerdimensionalitythantheoriginalinput.Italsolearns\\narepresentationwhoseelementshavenolinearcorrelationwitheachother.This\\nisaﬁrststeptowardthecriterionoflearningrepresentationswhoseelementsare\\nstatisticallyindependent.Toachievefullindependence,arepresentationlearning\\nalgorithmmustalsoremovethenonlinearrelationshipsbetweenvariables.\\nPCAlearnsanorthogonal,lineartransformationofthedatathatprojectsan\\ninputxtoarepresentationzasshowninﬁgure.Insection,wesawthat 5.8 2.12\\nwecouldlearnaone-dimensional representationthatbestreconstructstheoriginal\\ndata(inthesenseofmeansquarederror)andthatthisrepresentationactually\\ncorrespondstotheﬁrstprincipalcomponentofthedata.ThuswecanusePCA\\nasasimpleandeﬀectivedimensionalityreductionmethodthatpreservesasmuch\\noftheinformationinthedataaspossible(again,asmeasuredbyleast-squares\\nreconstructionerror).Inthefollowing,wewillstudyhowthePCArepresentation\\ndecorrelatestheoriginaldatarepresentation.X\\nLetusconsiderthemn×-dimensionaldesignmatrixX.Wewillassumethat\\nthedatahasameanofzero, E[x] = 0.Ifthisisnotthecase,thedatacaneasily\\nbecenteredbysubtractingthemeanfromallexamplesinapreprocessingstep.\\nTheunbiasedsamplecovariancematrixassociatedwithisgivenby:X\\nVar[] =x1\\nm−1X\\ue03eX. (5.85)\\n1 4 8', 'CHAPTER5.MACHINELEARNINGBASICS\\nPCAﬁndsarepresentation(throughlineartransformation)z=x\\ue03eWwhere\\nVar[]zisdiagonal.\\nInsection,wesawthattheprincipalcomponentsofadesignmatrix 2.12 X\\naregivenbytheeigenvectorsofX\\ue03eX.Fromthisview,\\nX\\ue03eXWW = Λ\\ue03e. (5.86)\\nInthissection,weexploitanalternativederivationoftheprincipalcomponents.The\\nprincipalcomponentsmayalsobeobtainedviathesingularvaluedecomposition.\\nSpeciﬁcally,theyaretherightsingularvectorsofX.Toseethis,letWbethe\\nrightsingularvectorsinthedecompositionX=UW Σ\\ue03e.\\xa0Wethenrecoverthe\\noriginaleigenvectorequationwithastheeigenvectorbasis: W\\nX\\ue03eX=\\ue010\\nUW Σ\\ue03e\\ue011\\ue03e\\nUW Σ\\ue03e= W Σ2W\\ue03e.(5.87)\\nTheSVDishelpfultoshowthatPCAresultsinadiagonal Var[z].Usingthe\\nSVDof,wecanexpressthevarianceofas: X X\\nVar[] =x1\\nm−1X\\ue03eX (5.88)\\n=1\\nm−1(UW Σ\\ue03e)\\ue03eUW Σ\\ue03e(5.89)\\n=1\\nm−1W Σ\\ue03eU\\ue03eUW Σ\\ue03e(5.90)\\n=1\\nm−1W Σ2W\\ue03e, (5.91)\\nwhereweusethefactthatU\\ue03eU=IbecausetheUmatrixofthesingularvalue\\ndecompositionisdeﬁnedtobeorthogonal.Thisshowsthatifwetakez=x\\ue03eW,\\nwecanensurethatthecovarianceofisdiagonalasrequired: z\\nVar[] =z1\\nm−1Z\\ue03eZ (5.92)\\n=1\\nm−1W\\ue03eX\\ue03eXW (5.93)\\n=1\\nm−1W\\ue03eW Σ2W\\ue03eW (5.94)\\n=1\\nm−1Σ2, (5.95)\\nwherethistimeweusethefactthatW\\ue03eW=I,againfromthedeﬁnitionofthe\\nSVD.\\n1 4 9', 'CHAPTER5.MACHINELEARNINGBASICS\\nTheaboveanalysisshowsthatwhenweprojectthedataxtoz,viathelinear\\ntransformationW,theresultingrepresentationhasadiagonalcovariancematrix\\n(asgivenby Σ2)whichimmediatelyimpliesthattheindividualelementsofzare\\nmutuallyuncorrelated.\\nThisabilityofPCAtotransformdataintoarepresentationwheretheelements\\naremutuallyuncorrelated isaveryimportantpropertyofPCA.Itisasimple\\nexampleofarepresentationthatattemptstodisentangletheunknownfactorsof\\nvariationunderlyingthedata.\\xa0InthecaseofPCA,thisdisentanglingtakesthe\\nformofﬁndingarotationoftheinputspace(describedbyW)thatalignsthe\\nprincipalaxesofvariancewiththebasisofthenewrepresentationspaceassociated\\nwith.z\\nWhilecorrelationisanimportantcategoryofdependencybetweenelementsof\\nthedata,wearealsointerestedinlearningrepresentationsthatdisentanglemore\\ncomplicatedformsoffeaturedependencies.Forthis,wewillneedmorethanwhat\\ncanbedonewithasimplelineartransformation.\\n5.8.2-meansClustering k\\nAnotherexampleofasimplerepresentationlearningalgorithmisk-meansclustering.\\nThek-meansclusteringalgorithmdividesthetrainingsetintokdiﬀerentclusters\\nofexamplesthatareneareachother.Wecanthusthinkofthealgorithmas\\nprovidingak-dimensionalone-hotcodevectorhrepresentinganinputx.Ifx\\nbelongstoclusteri,thenh i= 1andallotherentriesoftherepresentationhare\\nzero.\\nTheone-hotcodeprovidedbyk-meansclusteringisanexampleofasparse\\nrepresentation,becausethemajorityofitsentriesarezeroforeveryinput.Later,\\nwewilldevelopotheralgorithmsthatlearnmoreﬂexiblesparserepresentations,\\nwheremorethanoneentrycanbenon-zeroforeachinputx.One-hotcodes\\nareanextremeexampleofsparserepresentationsthatlosemanyofthebeneﬁts\\nofadistributedrepresentation.Theone-hotcodestillconferssomestatistical\\nadvantages(itnaturallyconveystheideathatallexamplesinthesameclusterare\\nsimilartoeachother)anditconfersthecomputational advantagethattheentire\\nrepresentationmaybecapturedbyasingleinteger.\\nThek-meansalgorithmworksbyinitializingkdiﬀerentcentroids{µ(1),...,µ() k}\\ntodiﬀerentvalues,thenalternatingbetweentwodiﬀerentstepsuntilconvergence.\\nInonestep,eachtrainingexampleisassignedtoclusteri,whereiistheindexof\\nthenearestcentroidµ() i.Intheotherstep,eachcentroidµ() iisupdatedtothe\\nmeanofalltrainingexamplesx() jassignedtocluster.i\\n1 5 0', 'CHAPTER5.MACHINELEARNINGBASICS\\nOnediﬃcultypertainingtoclusteringisthattheclusteringproblemisinherently\\nill-posed,inthesensethatthereisnosinglecriterionthatmeasureshowwella\\nclusteringofthedatacorrespondstotherealworld.Wecanmeasurepropertiesof\\ntheclusteringsuchastheaverageEuclideandistancefromaclustercentroidtothe\\nmembersofthecluster.Thisallowsustotellhowwellweareabletoreconstruct\\nthetrainingdatafromtheclusterassignments.Wedonotknowhowwellthe\\nclusterassignmentscorrespondtopropertiesoftherealworld.Moreover,there\\nmaybemanydiﬀerentclusteringsthatallcorrespondwelltosomepropertyof\\ntherealworld.Wemayhopetoﬁndaclusteringthatrelatestoonefeaturebut\\nobtainadiﬀerent,equallyvalidclusteringthatisnotrelevanttoourtask.For\\nexample,supposethatweruntwoclusteringalgorithmsonadatasetconsistingof\\nimagesofredtrucks,imagesofredcars,imagesofgraytrucks,andimagesofgray\\ncars.Ifweaskeachclusteringalgorithmtoﬁndtwoclusters,onealgorithmmay\\nﬁndaclusterofcarsandaclusteroftrucks,whileanothermayﬁndaclusterof\\nredvehiclesandaclusterofgrayvehicles.Supposewealsorunathirdclustering\\nalgorithm,whichisallowedtodeterminethenumberofclusters.Thismayassign\\ntheexamplestofourclusters,redcars,redtrucks,graycars,andgraytrucks.This\\nnewclusteringnowatleastcapturesinformationaboutbothattributes,butithas\\nlostinformationaboutsimilarity.Redcarsareinadiﬀerentclusterfromgray\\ncars,justastheyareinadiﬀerentclusterfromgraytrucks.\\xa0Theoutputofthe\\nclusteringalgorithmdoesnottellusthatredcarsaremoresimilartograycars\\nthantheyaretograytrucks.Theyarediﬀerentfromboththings,andthatisall\\nweknow.\\nTheseissuesillustratesomeofthereasonsthatwemaypreferadistributed\\nrepresentationtoaone-hotrepresentation.Adistributedrepresentationcouldhave\\ntwoattributesforeachvehicle—onerepresentingitscolorandonerepresenting\\nwhetheritisacaroratruck.Itisstillnotentirelyclearwhattheoptimal\\ndistributedrepresentationis(howcanthelearningalgorithmknowwhetherthe\\ntwoattributesweareinterestedinarecolorandcar-versus-truckratherthan\\nmanufacturerandage?)buthavingmanyattributesreducestheburdenonthe\\nalgorithmtoguesswhichsingleattributewecareabout,andallowsustomeasure\\nsimilaritybetweenobjectsinaﬁne-grainedwaybycomparingmanyattributes\\ninsteadofjusttestingwhetheroneattributematches.\\n5.9StochasticGradientDescent\\nNearlyallofdeeplearningispoweredbyoneveryimportantalgorithm:stochastic\\ngradientdescentorSGD.Stochasticgradientdescentisanextensionofthe\\n1 5 1', 'CHAPTER5.MACHINELEARNINGBASICS\\ngradientdescentalgorithmintroducedinsection.4.3\\nArecurringprobleminmachinelearningisthatlargetrainingsetsarenecessary\\nforgoodgeneralization, butlargetrainingsetsarealsomorecomputationally\\nexpensive.\\nThecostfunctionusedbyamachinelearningalgorithmoftendecomposesasa\\nsumovertrainingexamplesofsomeper-examplelossfunction.Forexample,the\\nnegativeconditionallog-likelihoodofthetrainingdatacanbewrittenas\\nJ() = θ E x ,y ∼ˆ pdataL,y,(xθ) =1\\nmm\\ue058\\ni=1L(x() i,y() i,θ)(5.96)\\nwhereistheper-exampleloss L L,y,py. (xθ) = log− (|xθ;)\\nFortheseadditivecostfunctions,gradientdescentrequirescomputing\\n∇ θJ() =θ1\\nmm\\ue058\\ni=1∇ θL(x() i,y() i,.θ) (5.97)\\nThecomputational costofthisoperationisO(m).Asthetrainingsetsizegrowsto\\nbillionsofexamples,thetimetotakeasinglegradientstepbecomesprohibitively\\nlong.\\nTheinsightofstochasticgradientdescentisthatthegradientisanexpectation.\\nTheexpectationmaybeapproximately estimatedusingasmallsetofsamples.\\nSpeciﬁcally,oneachstepofthealgorithm,wecansampleaminibatchofexamples\\nB={x(1),...,x( m\\ue030)}drawnuniformlyfromthetrainingset.Theminibatchsize\\nm\\ue030istypicallychosentobearelativelysmallnumberofexamples,rangingfrom\\n1toafewhundred.Crucially,m\\ue030isusuallyheldﬁxedasthetrainingsetsizem\\ngrows.Wemayﬁtatrainingsetwithbillionsofexamplesusingupdatescomputed\\nononlyahundredexamples.\\nTheestimateofthegradientisformedas\\ng=1\\nm\\ue030∇ θm\\ue030\\ue058\\ni=1L(x() i,y() i,.θ) (5.98)\\nusingexamplesfromtheminibatch.Thestochasticgradientdescentalgorithm B\\nthenfollowstheestimatedgradientdownhill:\\nθθg ← −\\ue00f, (5.99)\\nwhereisthelearningrate. \\ue00f\\n1 5 2', 'CHAPTER5.MACHINELEARNINGBASICS\\nGradientdescentingeneralhasoftenbeenregardedassloworunreliable.In\\nthepast,theapplicationofgradientdescenttonon-convexoptimization problems\\nwasregardedasfoolhardyorunprincipled. Today,weknowthatthemachine\\nlearningmodelsdescribedinpartworkverywellwhentrainedwithgradient II\\ndescent.Theoptimization algorithmmaynotbeguaranteedtoarriveatevena\\nlocalminimuminareasonableamountoftime,butitoftenﬁndsaverylowvalue\\nofthecostfunctionquicklyenoughtobeuseful.\\nStochasticgradientdescenthasmanyimportantusesoutsidethecontextof\\ndeeplearning.Itisthemainwaytotrainlargelinearmodelsonverylarge\\ndatasets.Foraﬁxedmodelsize,thecostperSGDupdatedoesnotdependonthe\\ntrainingsetsizem.Inpractice,weoftenusealargermodelasthetrainingsetsize\\nincreases,butwearenotforcedtodoso.Thenumberofupdatesrequiredtoreach\\nconvergenceusuallyincreaseswithtrainingsetsize.\\xa0However,asmapproaches\\ninﬁnity,themodelwilleventuallyconvergetoitsbestpossibletesterrorbefore\\nSGDhassampledeveryexampleinthetrainingset.Increasingmfurtherwillnot\\nextendtheamountoftrainingtimeneededtoreachthemodel’sbestpossibletest\\nerror.Fromthispointofview,onecanarguethattheasymptoticcostoftraining\\namodelwithSGDisasafunctionof. O(1) m\\nPriortotheadventofdeeplearning,themainwaytolearnnonlinearmodels\\nwastousethekerneltrickincombinationwithalinearmodel.Manykernellearning\\nalgorithmsrequireconstructinganmm×matrixG i , j=k(x() i,x() j).Constructing\\nthismatrixhascomputational costO(m2),whichisclearlyundesirablefordatasets\\nwith\\xa0billions of\\xa0examples. In\\xa0academia, starting\\xa0in2006,deep\\xa0learning was\\ninitiallyinterestingbecauseitwasabletogeneralizetonewexamplesbetter\\nthancompetingalgorithmswhentrainedonmedium-sizeddatasetswithtensof\\nthousandsofexamples.Soonafter,deeplearninggarneredadditionalinterestin\\nindustry,becauseitprovidedascalablewayoftrainingnonlinearmodelsonlarge\\ndatasets.\\nStochasticgradientdescentandmanyenhancements toitaredescribedfurther\\ninchapter.8\\n5.10BuildingaMachineLearningAlgorithm\\nNearlyalldeeplearningalgorithmscanbedescribedasparticularinstancesof\\nafairlysimplerecipe:combineaspeciﬁcationofadataset,acostfunction,an\\noptimization procedureandamodel.\\nForexample,thelinearregressionalgorithmcombinesadatasetconsistingof\\n1 5 3', 'CHAPTER5.MACHINELEARNINGBASICS\\nXyand,thecostfunction\\nJ,b(w) = − E x ,y ∼ˆ pdatalogpmodel( )y|x, (5.100)\\nthemodelspeciﬁcationpmodel(y|x) =N(y;x\\ue03ew+b,1),and,inmostcases,the\\noptimization algorithmdeﬁnedbysolvingforwherethegradientofthecostiszero\\nusingthenormalequations.\\nByrealizingthatwecanreplaceanyofthesecomponentsmostlyindependently\\nfromtheothers,wecanobtainaverywidevarietyofalgorithms.\\nThecostfunctiontypicallyincludesatleastonetermthatcausesthelearning\\nprocesstoperformstatisticalestimation.Themostcommoncostfunctionisthe\\nnegativelog-likelihood,sothatminimizingthecostfunctioncausesmaximum\\nlikelihoodestimation.\\nThecostfunctionmayalsoincludeadditionalterms,suchasregularization\\nterms.Forexample,wecanaddweightdecaytothelinearregressioncostfunction\\ntoobtain\\nJ,bλ (w) = ||||w2\\n2− E x ,y ∼ˆ pdatalogpmodel( )y|x.(5.101)\\nThisstillallowsclosed-formoptimization.\\nIfwechangethemodeltobenonlinear,thenmostcostfunctionscannolonger\\nbeoptimizedinclosedform.Thisrequiresustochooseaniterativenumerical\\noptimization procedure,suchasgradientdescent.\\nTherecipeforconstructingalearningalgorithmbycombiningmodels,costs,and\\noptimization algorithmssupportsbothsupervisedandunsupervisedlearning.The\\nlinearregressionexampleshowshowtosupportsupervisedlearning.Unsupervised\\nlearningcanbesupportedbydeﬁningadatasetthatcontainsonlyXandproviding\\nanappropriateunsupervisedcostandmodel.Forexample,wecanobtaintheﬁrst\\nPCAvectorbyspecifyingthatourlossfunctionis\\nJ() = w E x ∼ˆ pdata||− ||xr(;)xw2\\n2 (5.102)\\nwhileourmodelisdeﬁnedtohavewwithnormoneandreconstructionfunction\\nr() = xw\\ue03exw.\\nInsomecases,thecostfunctionmaybeafunctionthatwecannotactually\\nevaluate,forcomputational reasons.Inthesecases,wecanstillapproximately\\nminimizeitusingiterativenumericaloptimization solongaswehavesomewayof\\napproximatingitsgradients.\\nMostmachinelearningalgorithmsmakeuseofthisrecipe,thoughitmaynot\\nimmediatelybeobvious.Ifamachinelearningalgorithmseemsespeciallyuniqueor\\n1 5 4', 'CHAPTER5.MACHINELEARNINGBASICS\\nhand-designed,itcanusuallybeunderstoodasusingaspecial-caseoptimizer.Some\\nmodelssuchasdecisiontreesork-meansrequirespecial-caseoptimizersbecause\\ntheircostfunctionshaveﬂatregionsthatmaketheminappropriate forminimization\\nbygradient-basedoptimizers.Recognizingthatmostmachinelearningalgorithms\\ncanbedescribedusingthisrecipehelpstoseethediﬀerentalgorithmsaspartofa\\ntaxonomyofmethodsfordoingrelatedtasksthatworkforsimilarreasons,rather\\nthanasalonglistofalgorithmsthateachhaveseparatejustiﬁcations.\\n5.11ChallengesMotivatingDeepLearning\\nThesimplemachinelearningalgorithmsdescribedinthischapterworkverywellon\\nawidevarietyofimportantproblems.However,theyhavenotsucceededinsolving\\nthecentralproblemsinAI,suchasrecognizingspeechorrecognizingobjects.\\nThedevelopmentofdeeplearningwasmotivatedinpartbythefailureof\\ntraditionalalgorithmstogeneralizewellonsuchAItasks.\\nThissectionisabouthowthechallengeofgeneralizingtonewexamplesbecomes\\nexponentiallymorediﬃcultwhenworkingwithhigh-dimensionaldata,andhow\\nthemechanismsusedtoachievegeneralization intraditionalmachinelearning\\nareinsuﬃcienttolearncomplicatedfunctionsinhigh-dimensionalspaces.Such\\nspacesalsooftenimposehighcomputational costs.Deeplearningwasdesignedto\\novercometheseandotherobstacles.\\n5.11.1TheCurseofDimensionality\\nManymachinelearningproblemsbecomeexceedinglydiﬃcultwhenthenumber\\nofdimensionsinthedataishigh.Thisphenomenon isknownasthecurseof\\ndimensionality.Ofparticularconcernisthatthenumberofpossibledistinct\\nconﬁgurations ofasetofvariablesincreasesexponentiallyasthenumberofvariables\\nincreases.\\n1 5 5', 'CHAPTER5.MACHINELEARNINGBASICS\\nFigure5.9:Asthenumberofrelevantdimensionsofthedataincreases(fromleftto\\nright),thenumberofconﬁgurationsofinterestmaygrowexponentially. ( L e f t )Inthis\\none-dimensionalexample,wehaveonevariableforwhichweonlycaretodistinguish10\\nregionsofinterest.Withenoughexamplesfallingwithineachoftheseregions(eachregion\\ncorrespondstoacellintheillustration),learningalgorithmscaneasilygeneralizecorrectly.\\nAstraightforwardwaytogeneralizeistoestimatethevalueofthetargetfunctionwithin\\neachregion(andpossiblyinterpolatebetweenneighboringregions).With2 ( C e n t e r )\\ndimensionsitismorediﬃculttodistinguish10diﬀerentvaluesofeachvariable.\\xa0Weneed\\ntokeeptrackofupto10×10=100regions,andweneedatleastthatmanyexamplesto\\ncoverallthoseregions.With3dimensionsthisgrowsto ( R i g h t ) 103= 1000regionsandat\\nleastthatmanyexamples.Forddimensionsandvvaluestobedistinguishedalongeach\\naxis,weseemtoneedO(vd)regionsandexamples.\\xa0Thisisaninstanceofthecurseof\\ndimensionality.FiguregraciouslyprovidedbyNicolasChapados.\\nThecurseofdimensionalityarisesinmanyplacesincomputerscience,and\\nespeciallysoinmachinelearning.\\nOnechallengeposedbythecurseofdimensionalityisastatisticalchallenge.\\nAsillustratedinﬁgure,astatisticalchallengearisesbecausethenumberof 5.9\\npossibleconﬁgurations ofxismuchlargerthanthenumberoftrainingexamples.\\nTounderstandtheissue,letusconsiderthattheinputspaceisorganizedintoa\\ngrid,likeintheﬁgure.Wecandescribelow-dimensional spacewithalownumber\\nofgridcellsthataremostlyoccupiedbythedata.Whengeneralizingtoanewdata\\npoint,wecanusuallytellwhattodosimplybyinspectingthetrainingexamples\\nthatlieinthesamecellasthenewinput.Forexample,ifestimatingtheprobability\\ndensityatsomepointx,wecanjustreturnthenumberoftrainingexamplesin\\nthesameunitvolumecellasx,dividedbythetotalnumberoftrainingexamples.\\nIfwewishtoclassifyanexample,wecanreturnthemostcommonclassoftraining\\nexamplesinthesamecell.\\xa0Ifwearedoingregressionwecanaveragethetarget\\nvaluesobservedovertheexamplesinthatcell.Butwhataboutthecellsforwhich\\nwehaveseennoexample?Becauseinhigh-dimensionalspacesthenumberof\\nconﬁgurations ishuge,muchlargerthanournumberofexamples,atypicalgridcell\\nhasnotrainingexampleassociatedwithit.Howcouldwepossiblysaysomething\\n1 5 6', 'CHAPTER5.MACHINELEARNINGBASICS\\nmeaningfulaboutthesenewconﬁgurations? Manytraditionalmachinelearning\\nalgorithmssimplyassumethattheoutputatanewpointshouldbeapproximately\\nthesameastheoutputatthenearesttrainingpoint.\\n5.11.2LocalConstancyandSmoothnessRegularization\\nInordertogeneralizewell,machinelearningalgorithmsneedtobeguidedbyprior\\nbeliefsaboutwhatkindoffunctiontheyshouldlearn.Previously,wehaveseen\\nthesepriorsincorporatedasexplicitbeliefsintheformofprobabilitydistributions\\noverparametersofthemodel.Moreinformally,wemayalsodiscusspriorbeliefsas\\ndirectlyinﬂuencingtheitselfandonlyindirectlyactingontheparameters function\\nviatheireﬀectonthefunction.Additionally,weinformallydiscusspriorbeliefsas\\nbeingexpressedimplicitly,bychoosingalgorithmsthatarebiasedtowardchoosing\\nsomeclassoffunctionsoveranother,eventhoughthesebiasesmaynotbeexpressed\\n(orevenpossibletoexpress)intermsofaprobabilitydistributionrepresentingour\\ndegreeofbeliefinvariousfunctions.\\nAmongthemostwidelyusedoftheseimplicit“priors”\\xa0isthesmoothness\\npriororlocalconstancyprior.Thispriorstatesthatthefunctionwelearn\\nshouldnotchangeverymuchwithinasmallregion.\\nManysimpleralgorithmsrelyexclusivelyonthispriortogeneralizewell,and\\nasaresulttheyfailtoscaletothestatisticalchallengesinvolvedinsolvingAI-\\nleveltasks.Throughoutthisbook,wewilldescribehowdeeplearningintroduces\\nadditional(explicit\\xa0andimplicit)priorsinorder\\xa0toreducethegeneralization\\nerroronsophisticatedtasks.Here,weexplainwhythesmoothnessprioraloneis\\ninsuﬃcientforthesetasks.\\nTherearemanydiﬀerentwaystoimplicitlyorexplicitlyexpressapriorbelief\\nthatthelearnedfunctionshouldbesmoothorlocallyconstant.Allofthesediﬀerent\\nmethodsaredesignedtoencouragethelearningprocesstolearnafunctionf∗that\\nsatisﬁesthecondition\\nf∗() x≈f∗(+)x\\ue00f (5.103)\\nformostconﬁgurationsxandsmallchange\\ue00f.Inotherwords,ifweknowagood\\nanswerforaninputx(forexample,ifxisalabeledtrainingexample)thenthat\\nanswerisprobablygoodintheneighborhoodofx.Ifwehaveseveralgoodanswers\\ninsomeneighborhoodwewouldcombinethem(bysomeformofaveragingor\\ninterpolation)toproduceananswerthatagreeswithasmanyofthemasmuchas\\npossible.\\nAnextremeexampleofthelocalconstancyapproachisthek-nearestneighbors\\nfamilyoflearningalgorithms.Thesepredictorsareliterallyconstantovereach\\n1 5 7', 'CHAPTER5.MACHINELEARNINGBASICS\\nregioncontainingallthepointsxthathavethesamesetofknearestneighborsin\\nthetrainingset.Fork= 1,thenumberofdistinguishableregionscannotbemore\\nthanthenumberoftrainingexamples.\\nWhilethek-nearestneighborsalgorithmcopiestheoutputfromnearbytraining\\nexamples,mostkernelmachinesinterpolatebetweentrainingsetoutputsassociated\\nwithnearbytrainingexamples.Animportantclassofkernelsisthefamilyoflocal\\nkernelswherek(uv,)islargewhenu=vanddecreasesasuandvgrowfarther\\napartfromeachother.Alocalkernelcanbethoughtofasasimilarityfunction\\nthatperformstemplatematching,bymeasuringhowcloselyatestexamplex\\nresembleseachtrainingexamplex() i.\\xa0Muchofthemodernmotivationfordeep\\nlearningisderivedfromstudyingthelimitationsoflocaltemplatematchingand\\nhowdeepmodelsareabletosucceedincaseswherelocaltemplatematchingfails\\n( ,). Bengioetal.2006b\\nDecisiontreesalsosuﬀerfromthelimitationsofexclusivelysmoothness-based\\nlearningbecausetheybreaktheinputspaceintoasmanyregionsasthereare\\nleavesanduseaseparateparameter(orsometimesmanyparametersforextensions\\nofdecisiontrees)ineachregion.Ifthetargetfunctionrequiresatreewithat\\nleastnleavestoberepresentedaccurately,thenatleastntrainingexamplesare\\nrequiredtoﬁtthetree.Amultipleofnisneededtoachievesomelevelofstatistical\\nconﬁdenceinthepredictedoutput.\\nIngeneral,todistinguishO(k)regionsininputspace,allofthesemethods\\nrequireO(k) examples.TypicallythereareO(k) parameters,withO(1) parameters\\nassociatedwitheachoftheO(k)regions.Thecaseofanearestneighborscenario,\\nwhereeachtrainingexamplecanbeusedtodeﬁneatmostoneregion,isillustrated\\ninﬁgure.5.10\\nIsthereawaytorepresentacomplexfunctionthathasmanymoreregions\\ntobedistinguishedthanthenumberoftrainingexamples?Clearly,assuming\\nonlysmoothnessoftheunderlyingfunctionwillnotallowalearnertodothat.\\nFor\\xa0example,\\xa0imagine that\\xa0thetargetfunctionis\\xa0akind\\xa0ofcheckerboard.A\\ncheckerboardcontainsmanyvariationsbutthereisasimplestructuretothem.\\nImaginewhathappenswhenthenumberoftrainingexamplesissubstantially\\nsmallerthanthenumberofblackandwhitesquaresonthecheckerboard.Based\\nononlylocalgeneralization andthesmoothnessorlocalconstancyprior,wewould\\nbeguaranteedtocorrectlyguessthecolorofanewpointifitlieswithinthesame\\ncheckerboardsquareasatrainingexample.Thereisnoguaranteethatthelearner\\ncouldcorrectlyextendthecheckerboardpatterntopointslyinginsquaresthatdo\\nnotcontaintrainingexamples.Withthisprioralone,theonlyinformationthatan\\nexampletellsusisthecolorofitssquare,andtheonlywaytogetthecolorsofthe\\n1 5 8', 'CHAPTER5.MACHINELEARNINGBASICS\\nFigure5.10:\\xa0Illustrationofhowthenearestneighboralgorithmbreaksuptheinputspace\\nintoregions.\\xa0Anexample(representedherebyacircle)withineachregiondeﬁnesthe\\nregionboundary(representedherebythelines).Theyvalueassociatedwitheachexample\\ndeﬁneswhattheoutputshouldbeforallpointswithinthecorrespondingregion.\\xa0The\\nregionsdeﬁnedbynearestneighbormatchingformageometricpatterncalledaVoronoi\\ndiagram.Thenumberofthesecontiguousregionscannotgrowfasterthanthenumber\\noftrainingexamples.Whilethisﬁgureillustratesthebehaviorofthenearestneighbor\\nalgorithmspeciﬁcally,othermachinelearningalgorithmsthatrelyexclusivelyonthe\\nlocalsmoothnesspriorforgeneralizationexhibitsimilarbehaviors:eachtrainingexample\\nonlyinformsthelearnerabouthowtogeneralizeinsomeneighborhoodimmediately\\nsurroundingthatexample.\\n1 5 9', 'CHAPTER5.MACHINELEARNINGBASICS\\nentirecheckerboardrightistocovereachofitscellswithatleastoneexample.\\nThesmoothnessassumptionandtheassociatednon-parametric learningalgo-\\nrithmsworkextremelywellsolongasthereareenoughexamplesforthelearning\\nalgorithmtoobservehighpointsonmostpeaksandlowpointsonmostvalleys\\nofthetrueunderlyingfunctiontobelearned.Thisisgenerallytruewhenthe\\nfunctiontobelearnedissmoothenoughandvariesinfewenoughdimensions.\\nInhighdimensions,evenaverysmoothfunctioncanchangesmoothlybutina\\ndiﬀerentwayalongeachdimension.Ifthefunctionadditionallybehavesdiﬀerently\\nindiﬀerentregions,itcanbecomeextremelycomplicatedtodescribewithasetof\\ntrainingexamples.Ifthefunctioniscomplicated(wewanttodistinguishahuge\\nnumberofregionscomparedtothenumberofexamples),isthereanyhopeto\\ngeneralizewell?\\nTheanswertobothofthesequestions—whetheritispossibletorepresent\\nacomplicatedfunctioneﬃciently,andwhetheritispossiblefortheestimated\\nfunctiontogeneralizewelltonewinputs—isyes.Thekeyinsightisthatavery\\nlargenumberofregions,e.g.,O(2k),canbedeﬁnedwithO(k)examples,solong\\nasweintroducesomedependenciesbetweentheregionsviaadditionalassumptions\\nabouttheunderlyingdatageneratingdistribution.Inthisway,wecanactually\\ngeneralizenon-locally( ,; ,).Many BengioandMonperrus2005Bengioetal.2006c\\ndiﬀerentdeeplearningalgorithmsprovideimplicitorexplicitassumptionsthatare\\nreasonableforabroadrangeofAItasksinordertocapturetheseadvantages.\\nOtherapproachestomachinelearningoftenmakestronger,task-speciﬁcas-\\nsumptions.Forexample,wecouldeasilysolvethecheckerboardtaskbyproviding\\ntheassumptionthatthetargetfunctionisperiodic.Usuallywedonotincludesuch\\nstrong,task-speciﬁcassumptionsintoneuralnetworkssothattheycangeneralize\\ntoamuchwidervarietyofstructures.AItaskshavestructurethatismuchtoo\\ncomplextobelimitedtosimple,manuallyspeciﬁedpropertiessuchasperiodicity,\\nsowewantlearningalgorithmsthatembodymoregeneral-purpos eassumptions.\\nThecoreideaindeeplearningisthatweassumethatthedatawasgeneratedby\\nthecompositionoffactorsorfeatures,potentiallyatmultiplelevelsinahierarchy.\\nManyothersimilarlygenericassumptionscanfurtherimprovedeeplearningal-\\ngorithms.\\xa0Theseapparentlymildassumptionsallowanexponentialgaininthe\\nrelationshipbetweenthenumberofexamplesandthenumberofregionsthatcan\\nbedistinguished.Theseexponentialgainsaredescribedmorepreciselyinsections\\n6.4.115.415.5,and.Theexponentialadvantagesconferredbytheuseofdeep,\\ndistributedrepresentationscountertheexponentialchallengesposedbythecurse\\nofdimensionality.\\n1 6 0', 'CHAPTER5.MACHINELEARNINGBASICS\\n5.11.3ManifoldLearning\\nAnimportantconceptunderlyingmanyideasinmachinelearningisthatofa\\nmanifold.\\nAmanifoldisaconnected\\xa0region. Mathematically ,\\xa0it\\xa0isasetofpoints,\\nassociatedwithaneighborhoodaroundeachpoint.Fromanygivenpoint,the\\nmanifoldlocallyappearstobeaEuclideanspace.Ineverydaylife,weexperience\\nthesurfaceoftheworldasa2-Dplane,butitisinfactasphericalmanifoldin\\n3-Dspace.\\nThedeﬁnitionofaneighborhoodsurroundingeachpointimpliestheexistence\\noftransformationsthatcanbeappliedtomoveonthemanifoldfromoneposition\\ntoaneighboringone.Intheexampleoftheworld’ssurfaceasamanifold,onecan\\nwalknorth,south,east,orwest.\\nAlthoughthereisaformalmathematical meaningtotheterm“manifold,”in\\nmachinelearningittendstobeusedmorelooselytodesignateaconnectedset\\nofpointsthatcanbeapproximatedwellbyconsideringonlyasmallnumberof\\ndegreesoffreedom,ordimensions,embeddedinahigher-dimens ionalspace.Each\\ndimensioncorrespondstoalocaldirectionofvariation.Seeﬁgureforan5.11\\nexampleoftrainingdatalyingnearaone-dimensional manifoldembeddedintwo-\\ndimensionalspace.Inthecontextofmachinelearning,weallowthedimensionality\\nofthemanifoldtovaryfromonepointtoanother.\\xa0This oftenhappenswhena\\nmanifoldintersectsitself.Forexample,aﬁgureeightisamanifoldthathasasingle\\ndimensioninmostplacesbuttwodimensionsattheintersectionatthecenter.\\n0 5 1 0 1 5 2 0 2 5 3 0 3 5 4 0 . . . . . . . .− 1 0 .− 0 5 .0 0 .0 5 .1 0 .1 5 .2 0 .2 5 .\\nFigure5.11:Datasampledfromadistributioninatwo-dimensionalspacethatisactually\\nconcentratednearaone-dimensionalmanifold,likeatwistedstring.Thesolidlineindicates\\ntheunderlyingmanifoldthatthelearnershouldinfer.\\n1 6 1', 'CHAPTER5.MACHINELEARNINGBASICS\\nManymachinelearningproblemsseemhopelessifweexpectthemachine\\nlearningalgorithmtolearnfunctionswithinterestingvariationsacrossallof Rn.\\nManifoldlearningalgorithmssurmountthisobstaclebyassumingthatmost\\nof Rnconsistsofinvalidinputs,\\xa0andthatinterestinginputsoccuronlyalong\\nacollectionofmanifoldscontainingasmallsubsetofpoints,withinteresting\\nvariationsintheoutputofthelearnedfunctionoccurringonlyalongdirections\\nthatlieonthemanifold,orwithinterestingvariationshappeningonlywhenwe\\nmovefromonemanifoldtoanother.Manifoldlearningwasintroducedinthecase\\nofcontinuous-valueddataandtheunsupervisedlearningsetting,althoughthis\\nprobabilityconcentrationideacanbegeneralizedtobothdiscretedataandthe\\nsupervisedlearningsetting:thekeyassumptionremainsthatprobabilitymassis\\nhighlyconcentrated.\\nTheassumptionthatthedataliesalongalow-dimensional manifoldmaynot\\nalwaysbecorrectoruseful.WearguethatinthecontextofAItasks,suchas\\nthosethatinvolveprocessingimages,sounds,ortext,themanifoldassumptionis\\natleastapproximatelycorrect.Theevidenceinfavorofthisassumptionconsists\\noftwocategoriesofobservations.\\nTheﬁrstobservationinfavorofthemanifoldhypothesisisthattheproba-\\nbilitydistributionoverimages,textstrings,andsoundsthatoccurinreallifeis\\nhighlyconcentrated.Uniformnoiseessentiallyneverresemblesstructuredinputs\\nfromthesedomains.\\xa0Figureshowshow,instead,uniformlysampledpoints 5.12\\nlooklikethepatternsofstaticthatappearonanalogtelevisionsetswhennosignal\\nisavailable.Similarly,ifyougenerateadocumentbypickinglettersuniformlyat\\nrandom,whatistheprobabilitythatyouwillgetameaningfulEnglish-language\\ntext?Almostzero,again,becausemostofthelongsequencesoflettersdonot\\ncorrespondtoanaturallanguagesequence:thedistributionofnaturallanguage\\nsequencesoccupiesaverysmallvolumeinthetotalspaceofsequencesofletters.\\n1 6 2', 'CHAPTER5.MACHINELEARNINGBASICS\\nFigure5.12:Samplingimagesuniformlyatrandom(byrandomlypickingeachpixel\\naccordingtoauniformdistribution)givesrisetonoisyimages.Althoughthereisanon-\\nzeroprobabilitytogenerateanimageofafaceoranyotherobjectfrequentlyencountered\\ninAIapplications,weneveractuallyobservethishappeninginpractice.Thissuggests\\nthattheimagesencounteredinAIapplicationsoccupyanegligibleproportionofthe\\nvolumeofimagespace.\\nOfcourse,concentratedprobabilitydistributionsarenotsuﬃcienttoshow\\nthatthedataliesonareasonablysmallnumberofmanifolds.Wemustalso\\nestablishthattheexamplesweencounterareconnectedtoeachotherbyother\\n1 6 3', 'CHAPTER5.MACHINELEARNINGBASICS\\nexamples,witheachexamplesurroundedbyotherhighlysimilarexamplesthat\\nmaybereachedbyapplyingtransformationstotraversethemanifold.Thesecond\\nargumentinfavorofthemanifoldhypothesisisthatwecanalsoimaginesuch\\nneighborhoodsandtransformations,atleastinformally.Inthecaseofimages,we\\ncancertainlythinkofmanypossibletransformationsthatallowustotraceouta\\nmanifoldinimagespace:wecangraduallydimorbrightenthelights,gradually\\nmoveorrotateobjectsintheimage,graduallyalterthecolorsonthesurfacesof\\nobjects,etc.Itremainslikelythattherearemultiplemanifoldsinvolvedinmost\\napplications.Forexample,themanifoldofimagesofhumanfacesmaynotbe\\nconnectedtothemanifoldofimagesofcatfaces.\\nThesethoughtexperimentssupportingthemanifoldhypothesesconveysomein-\\ntuitivereasonssupportingit.Morerigorousexperiments\\xa0(Cayton2005Narayanan,;\\nandMitter2010Schölkopf1998RoweisandSaul2000Tenenbaum ,; etal.,; ,; etal.,\\n2000Brand2003BelkinandNiyogi2003DonohoandGrimes2003Weinberger ;,; ,; ,;\\nandSaul2004,)clearlysupportthehypothesisforalargeclassofdatasetsof\\ninterestinAI.\\nWhenthedataliesonalow-dimensional manifold,itcanbemostnatural\\nformachinelearningalgorithmstorepresentthedataintermsofcoordinateson\\nthemanifold,ratherthanintermsofcoordinatesin Rn.Ineverydaylife,wecan\\nthinkofroadsas1-Dmanifoldsembeddedin3-Dspace.Wegivedirectionsto\\nspeciﬁcaddressesintermsofaddressnumbersalongthese1-Droads,notinterms\\nofcoordinatesin3-Dspace.Extractingthesemanifoldcoordinatesischallenging,\\nbutholdsthepromisetoimprovemanymachinelearningalgorithms.Thisgeneral\\nprincipleisappliedinmanycontexts.Figureshowsthemanifoldstructureof 5.13\\nadatasetconsistingoffaces.Bytheendofthisbook,wewillhavedevelopedthe\\nmethodsnecessarytolearnsuchamanifoldstructure.Inﬁgure,wewillsee 20.6\\nhowamachinelearningalgorithmcansuccessfullyaccomplishthisgoal.\\nThisconcludespart,whichhasprovidedthebasicconceptsinmathematics I\\nandmachinelearningwhichareemployedthroughouttheremainingpartsofthe\\nbook.Youarenowpreparedtoembarkuponyourstudyofdeeplearning.\\n1 6 4', 'CHAPTER5.MACHINELEARNINGBASICS\\nFigure5.13:TrainingexamplesfromtheQMULMultiviewFaceDataset( ,) Gong e t a l .2000\\nforwhichthesubjectswereaskedtomoveinsuchawayastocoverthetwo-dimensional\\nmanifoldcorrespondingtotwoanglesofrotation.Wewouldlikelearningalgorithmstobe\\nabletodiscoveranddisentanglesuchmanifoldcoordinates.Figureillustratessucha 20.6\\nfeat.\\n1 6 5']}, 'DeepLearning-chapter-4.pdf': {'text': ['C h a p t e r 4\\nNumericalComputation\\nMachinelearningalgorithmsusuallyrequireahighamountofnumericalcompu-\\ntation.Thistypicallyreferstoalgorithmsthatsolvemathematical problemsby\\nmethodsthatupdateestimatesofthesolutionviaaniterativeprocess,ratherthan\\nanalyticallyderivingaformulaprovidingasymbolicexpressionforthecorrectso-\\nlution.Commonoperationsincludeoptimization (ﬁndingthevalueofanargument\\nthatminimizesormaximizesafunction)andsolvingsystemsoflinearequations.\\nEvenjustevaluatingamathematical functiononadigitalcomputercanbediﬃcult\\nwhenthefunctioninvolvesrealnumbers,whichcannotberepresentedprecisely\\nusingaﬁniteamountofmemory.\\n4. 1 O v erﬂ o w an d Un d erﬂ o w\\nThefundamentaldiﬃcultyinperformingcontinuousmathonadigitalcomputer\\nisthatweneedtorepresentinﬁnitelymanyrealnumberswithaﬁnitenumber\\nofbitpatterns.Thismeansthatforalmostallrealnumbers,\\xa0weincursome\\napproximationerrorwhenwerepresentthenumberinthecomputer.Inmany\\ncases,thisisjustroundingerror.Roundingerrorisproblematic, especiallywhen\\nitcompoundsacrossmanyoperations,andcancausealgorithmsthatworkin\\ntheorytofailinpracticeiftheyarenotdesignedtominimizetheaccumulationof\\nroundingerror.\\nOneformofroundingerrorthatisparticularlydevastatingis under ﬂo w.\\nUnderﬂowoccurswhennumbersnearzeroareroundedtozero.Manyfunctions\\nbehavequalitativelydiﬀerentlywhentheirargumentiszeroratherthanasmall\\npositivenumber.Forexample,weusuallywanttoavoiddivisionbyzero(some\\n80', 'CHAPTER4.NUMERICALCOMPUTATION\\nsoftwareenvironmentswillraiseexceptionswhenthisoccurs,otherswillreturna\\nresultwithaplaceholdernot-a-numbervalue)ortakingthelogarithmofzero(this\\nisusuallytreatedas−∞,whichthenbecomesnot-a-numberifitisusedformany\\nfurtherarithmeticoperations).\\nAnotherhighlydamagingformofnumericalerroris o v e r ﬂo w.Overﬂowoccurs\\nwhennumberswithlargemagnitudeareapproximatedas∞or−∞.Further\\narithmeticwillusuallychangetheseinﬁnitevaluesintonot-a-numbervalues.\\nOneexampleofafunctionthatmustbestabilizedagainstunderﬂowand\\noverﬂowisthesoftmaxfunction.Thesoftmaxfunctionisoftenusedtopredictthe\\nprobabilities associatedwithamultinoullidistribution.Thesoftmaxfunctionis\\ndeﬁnedtobe\\nsoftmax() x i=exp( x i)\\ue050n\\nj = 1exp( x j). (4.1)\\nConsiderwhathappenswhenallofthe x iareequaltosomeconstant c.Analytically,\\nwecanseethatalloftheoutputsshouldbeequalto1\\nn.Numerically,thismay\\nnotoccurwhen chaslargemagnitude.If cisverynegative,thenexp( c)will\\nunderﬂow.Thismeansthedenominator ofthesoftmaxwillbecome0,sotheﬁnal\\nresultisundeﬁned.When cisverylargeandpositive,exp( c)willoverﬂow,again\\nresultingintheexpressionasawholebeingundeﬁned.Bothofthesediﬃculties\\ncanberesolvedbyinsteadevaluating softmax( z)where z= x−max i x i.Simple\\nalgebrashowsthatthevalueofthesoftmaxfunctionisnotchangedanalyticallyby\\naddingorsubtractingascalarfromtheinputvector.Subtracting max i x iresults\\ninthelargestargumenttoexpbeing0,whichrulesoutthepossibilityofoverﬂow.\\nLikewise,atleastoneterminthedenominator hasavalueof1,whichrulesout\\nthepossibilityofunderﬂowinthedenominator leadingtoadivisionbyzero.\\nThereisstillonesmallproblem.Underﬂowinthenumeratorcanstillcause\\ntheexpressionasawholetoevaluatetozero.Thismeansthatifweimplement\\nlogsoftmax( x)byﬁrstrunningthesoftmaxsubroutinethenpassingtheresultto\\nthelogfunction,wecoulderroneouslyobtain −∞.Instead,wemustimplement\\naseparatefunctionthatcalculates logsoftmaxinanumericallystableway.The\\nlogsoftmaxfunctioncanbestabilizedusingthesametrickasweusedtostabilize\\nthefunction. softmax\\nForthemostpart,wedonotexplicitlydetailallofthenumericalconsiderations\\ninvolvedinimplementing thevariousalgorithmsdescribedinthisbook.Developers\\noflow-levellibrariesshouldkeepnumericalissuesinmindwhenimplementing\\ndeeplearningalgorithms.Mostreadersofthisbookcansimplyrelyonlow-\\nlevellibrariesthatprovidestableimplementations .Insomecases,itispossible\\ntoimplementanewalgorithmandhavethenewimplementation automatically\\n8 1', 'CHAPTER4.NUMERICALCOMPUTATION\\nstabilized.Theano( ,; ,)isanexample Bergstra e t a l .2010Bastien e t a l .2012\\nofasoftwarepackagethatautomatically detectsandstabilizesmanycommon\\nnumericallyunstableexpressionsthatariseinthecontextofdeeplearning.\\n4. 2 P o or C on d i t i o n i n g\\nConditioning referstohowrapidlyafunctionchangeswithrespecttosmallchanges\\ninitsinputs.Functionsthatchangerapidlywhentheirinputsareperturbedslightly\\ncanbeproblematicforscientiﬁccomputationbecauseroundingerrorsintheinputs\\ncanresultinlargechangesintheoutput.\\nConsiderthefunction f( x)= A− 1x.When A∈ Rn n ×hasaneigenvalue\\ndecomposition,its c o ndi t i o n num beris\\nmax\\ni , j\\ue00c\\ue00c\\ue00c\\ue00cλ i\\nλ j\\ue00c\\ue00c\\ue00c\\ue00c. (4.2)\\nThisistheratioofthemagnitudeofthelargestandsmallesteigenvalue.When\\nthisnumberislarge,matrixinversionisparticularlysensitivetoerrorintheinput.\\nThissensitivityisanintrinsicpropertyofthematrixitself,nottheresult\\nofroundingerrorduringmatrixinversion.Poorlyconditionedmatricesamplify\\npre-existingerrorswhenwemultiplybythetruematrixinverse.Inpractice,the\\nerrorwillbecompoundedfurtherbynumericalerrorsintheinversionprocessitself.\\n4. 3 Gradi en t - Bas e d O p t i m i z a t i o n\\nMostdeeplearningalgorithmsinvolveoptimization ofsomesort.\\xa0Optimization\\nreferstothetaskofeitherminimizingormaximizingsomefunction f( x) byaltering\\nx.\\xa0Weusuallyphrasemostoptimization problemsintermsofminimizing f( x).\\nMaximization maybeaccomplishedviaaminimization algorithmbyminimizing\\n− f() x.\\nThefunctionwewanttominimizeormaximizeiscalledthe o b j e c t i v e f unc -\\nt i o nor c r i t e r i o n.Whenweareminimizingit,\\xa0wemayalsocallitthe c o st\\nf unc t i o n, l o ss f unc t i o n,or e r r o r f unc t i o n.\\xa0Inthisbook,weusetheseterms\\ninterchangeably,thoughsomemachinelearningpublicationsassignspecialmeaning\\ntosomeoftheseterms.\\nWeoftendenotethevaluethatminimizesormaximizesafunctionwitha\\nsuperscript.Forexample,wemightsay ∗ x∗= argmin() f x.\\n8 2', 'CHAPTER4.NUMERICALCOMPUTATION\\n− − − − 20. 15. 10. 05 00 05 10 15 20 ......\\nx−20.−15.−10.−05.00.05.10.15.20.\\nGlobalminimumat= 0.x\\nSincef\\ue030() = 0,gradient x\\ndescent haltshere.\\nFor 0,wehave x< f\\ue030() 0,x<\\nsowecandecreasebyf\\nmoving rightward.For 0,wehave x> f\\ue030() 0,x>\\nsowecandecreasebyf\\nmoving leftward.\\nf x() =1\\n2x2\\nf\\ue030() = x x\\nFigure4.1:Anillustrationofhowthegradientdescentalgorithmusesthederivativesofa\\nfunctioncanbeusedtofollowthefunctiondownhilltoaminimum.\\nWeassumethereaderisalreadyfamiliarwithcalculus,butprovideabrief\\nreviewofhowcalculusconceptsrelatetooptimization here.\\nSupposewehaveafunction y= f( x),whereboth xand yarerealnumbers.\\nThe der i v at i v eofthisfunctionisdenotedas f\\ue030( x)orasd y\\nd x.Thederivative f\\ue030( x)\\ngivestheslopeof f( x)atthepoint x.Inotherwords,itspeciﬁeshowtoscale\\nasmallchangeintheinputinordertoobtainthecorrespondingchangeinthe\\noutput: f x \\ue00f f x \\ue00f f (+) ≈()+\\ue030() x.\\nThederivativeisthereforeusefulforminimizingafunctionbecauseittells\\nushowtochange xinordertomakeasmallimprovementin y.Forexample,\\nweknowthat f( x \\ue00f−sign( f\\ue030( x)))islessthan f( x)forsmallenough \\ue00f.Wecan\\nthusreduce f( x)bymoving xinsmallstepswithoppositesignofthederivative.\\nThistechniqueiscalled g r adi e n t desc e n t(Cauchy1847,).Seeﬁgureforan4.1\\nexampleofthistechnique.\\nWhen f\\ue030( x) = 0,thederivativeprovidesnoinformationaboutwhichdirection\\ntomove.Pointswhere f\\ue030( x)=0areknownas c r i t i c al p o i nt sor st at i o na r y\\np o i n t s.A l o c al m i ni m umisapointwhere f( x)islowerthanatallneighboring\\npoints,soitisnolongerpossibletodecrease f( x)bymakinginﬁnitesimalsteps.\\nA l o c al m ax i m u misapointwhere f( x)ishigherthanatallneighboringpoints,\\n8 3', 'CHAPTER4.NUMERICALCOMPUTATION\\nMinimum Maximum Saddlepoint\\nFigure4.2:Examplesofeachofthethreetypesofcriticalpointsin1-D.Acriticalpointis\\napointwithzeroslope.Suchapointcaneitherbealocalminimum,whichislowerthan\\ntheneighboringpoints,alocalmaximum,whichishigherthantheneighboringpoints,or\\nasaddlepoint,whichhasneighborsthatarebothhigherandlowerthanthepointitself.\\nsoitisnotpossibletoincrease f( x)bymakinginﬁnitesimalsteps.Somecritical\\npointsareneithermaximanorminima.Theseareknownas saddle p o i nt s.See\\nﬁgureforexamplesofeachtypeofcriticalpoint. 4.2\\nApointthatobtainstheabsolutelowestvalueof f( x)isa g l o bal m i ni m um.\\nItispossiblefortheretobeonlyoneglobalminimumormultipleglobalminimaof\\nthefunction.Itisalsopossiblefortheretobelocalminimathatarenotglobally\\noptimal.Inthecontextofdeeplearning,weoptimizefunctionsthatmayhave\\nmanylocalminimathatarenotoptimal,andmanysaddlepointssurroundedby\\nveryﬂatregions.Allofthismakesoptimization verydiﬃcult,especiallywhenthe\\ninputtothefunctionismultidimensional.Wethereforeusuallysettleforﬁndinga\\nvalueof fthatisverylow,butnotnecessarilyminimalinanyformalsense.See\\nﬁgureforanexample.4.3\\nWeoftenminimizefunctionsthathavemultipleinputs: f: Rn→ R.Forthe\\nconceptof“minimization”\\xa0to makesense,theremuststillbeonlyone(scalar)\\noutput.\\nForfunctionswithmultipleinputs,wemustmakeuseoftheconceptof par t i al\\nder i v at i v e s.Thepartialderivative∂\\n∂ x if( x)measureshow fchangesasonlythe\\nvariable x iincreasesatpoint x.The g r adi e n tgeneralizesthenotionofderivative\\ntothecasewherethederivativeiswithrespecttoavector:thegradientof fisthe\\nvectorcontainingallofthepartialderivatives,denoted ∇ x f( x).Element iofthe\\ngradientisthepartialderivativeof fwithrespectto x i.Inmultipledimensions,\\n8 4', 'CHAPTER4.NUMERICALCOMPUTATION\\nxf x()\\nIdeally,wewouldlike\\ntoarriveattheglobal\\nminimum, butthis\\nmight notbepossible.Thislocalminimum\\nperformsnearlyaswellas\\ntheglobalone,\\nsoitisanacceptable\\nhaltingpoint.\\nThislocalminimumperforms\\npoorlyandshouldbeavoided.\\nFigure4.3:Optimizationalgorithmsmayfailtoﬁndaglobalminimumwhenthereare\\nmultiplelocalminimaorplateauspresent.Inthecontextofdeeplearning,wegenerally\\nacceptsuchsolutionseventhoughtheyarenottrulyminimal,solongastheycorrespond\\ntosigniﬁcantlylowvaluesofthecostfunction.\\ncriticalpointsarepointswhereeveryelementofthegradientisequaltozero.\\nThe di r e c t i o n a l der i v at i v eindirection(aunitvector)istheslopeofthe u\\nfunction findirection u.Inotherwords,thedirectionalderivativeisthederivative\\nofthefunction f( x+ α u)withrespectto α,evaluatedat α= 0.Usingthechain\\nrule,wecanseethat∂\\n∂ αf α (+ x u)evaluatesto u\\ue03e∇ x f α () xwhen = 0.\\nTominimize f,wewouldliketoﬁndthedirectioninwhich fdecreasesthe\\nfastest.Wecandothisusingthedirectionalderivative:\\nmin\\nu u ,\\ue03e u = 1u\\ue03e∇ x f() x (4.3)\\n=min\\nu u ,\\ue03e u = 1|||| u 2||∇ x f() x|| 2cos θ (4.4)\\nwhere θistheanglebetween uandthegradient.Substitutingin|||| u 2= 1and\\nignoringfactorsthatdonotdependon u,thissimpliﬁestomin ucos θ.Thisis\\nminimizedwhen upointsintheoppositedirectionasthegradient.Inother\\nwords,thegradientpointsdirectlyuphill,andthenegativegradientpointsdirectly\\ndownhill.Wecandecrease fbymovinginthedirectionofthenegativegradient.\\nThisisknownasthe or . m e t ho d o f st e e p e st desc e nt g r adi e nt desc e nt\\nSteepestdescentproposesanewpoint\\nx\\ue030= x−∇ \\ue00f x f() x (4.5)\\n8 5', 'CHAPTER4.NUMERICALCOMPUTATION\\nwhere \\ue00fisthe l e ar ni ng r at e,apositivescalardeterminingthesizeofthestep.\\nWecanchoose \\ue00finseveraldiﬀerentways.Apopularapproachistoset \\ue00ftoasmall\\nconstant.Sometimes,wecansolveforthestepsizethatmakesthedirectional\\nderivativevanish.Anotherapproachistoevaluate f \\ue00f ( x−∇ x f()) xforseveral\\nvaluesof \\ue00fandchoosetheonethatresultsinthesmallestobjectivefunctionvalue.\\nThislaststrategyiscalleda l i ne se ar c h.\\nSteepestdescentconvergeswheneveryelementofthegradientiszero(or,in\\npractice,veryclosetozero).Insomecases,wemaybeabletoavoidrunningthis\\niterativealgorithm,andjustjumpdirectlytothecriticalpointbysolvingthe\\nequation ∇ x f() = 0 xfor. x\\nAlthoughgradientdescentislimitedtooptimization incontinuousspaces,the\\ngeneralconceptofrepeatedlymakingasmallmove(thatisapproximately thebest\\nsmallmove)towardsbetterconﬁgurations canbegeneralizedtodiscretespaces.\\nAscendinganobjectivefunctionofdiscreteparametersiscalled hi l l c l i m bi ng\\n( ,). RusselandNorvig2003\\n4 . 3 . 1 B ey o n d t h e G ra d i en t : Ja co b i a n a n d Hessi a n Ma t ri ces\\nSometimesweneedtoﬁndallofthepartialderivativesofafunctionwhoseinput\\nandoutputarebothvectors.Thematrixcontainingallsuchpartialderivativesis\\nknownasa J ac o bi an m at r i x.Speciﬁcally,ifwehaveafunction f: Rm→ Rn,\\nthentheJacobianmatrix J∈ Rn m ×ofisdeﬁnedsuchthat f J i , j=∂\\n∂ x jf() x i.\\nWearealsosometimesinterestedinaderivativeofaderivative.Thisisknown\\nasa se c o nd der i v at i v e.Forexample,forafunction f: Rn→ R,thederivative\\nwithrespectto x iofthederivativeof fwithrespectto x jisdenotedas∂2\\n∂ x i ∂ x jf.\\nInasingledimension,wecandenoted2\\nd x2 fby f\\ue030 \\ue030( x).Thesecondderivativetells\\nushowtheﬁrstderivativewillchangeaswevarytheinput.Thisisimportant\\nbecauseittellsuswhetheragradientstepwillcauseasmuchofanimprovement\\naswewouldexpectbasedonthegradientalone.Wecanthinkofthesecond\\nderivativeasmeasuring c ur v at ur e.Supposewehaveaquadraticfunction(many\\nfunctionsthatariseinpracticearenotquadraticbutcanbeapproximated well\\nasquadratic,atleastlocally).Ifsuchafunctionhasasecondderivativeofzero,\\nthenthereisnocurvature.Itisaperfectlyﬂatline,anditsvaluecanbepredicted\\nusingonlythegradient.Ifthegradientis,thenwecanmakeastepofsize 1 \\ue00f\\nalongthenegativegradient,andthecostfunctionwilldecreaseby \\ue00f.Ifthesecond\\nderivativeisnegative,thefunctioncurvesdownward,sothecostfunctionwill\\nactuallydecreasebymorethan \\ue00f.Finally,ifthesecondderivativeispositive,the\\nfunctioncurvesupward,sothecostfunctioncandecreasebylessthan \\ue00f.See\\n8 6', 'CHAPTER4.NUMERICALCOMPUTATION\\nxf x()N e g a t i v e c u r v a t u r e\\nxf x()N o c u r v a t u r e\\nxf x()P o s i t i v e c u r v a t u r e\\nFigure4.4:Thesecondderivativedeterminesthecurvatureofafunction.Hereweshow\\nquadraticfunctionswithvariouscurvature.Thedashedlineindicatesthevalueofthecost\\nfunctionwewouldexpectbasedonthegradientinformationaloneaswemakeagradient\\nstepdownhill.Inthecaseofnegativecurvature,thecostfunctionactuallydecreasesfaster\\nthanthegradientpredicts.Inthecaseofnocurvature,thegradientpredictsthedecrease\\ncorrectly.Inthecaseofpositivecurvature,thefunctiondecreasesslowerthanexpected\\nandeventuallybeginstoincrease,sostepsthataretoolargecanactuallyincreasethe\\nfunctioninadvertently.\\nﬁguretoseehowdiﬀerentformsofcurvatureaﬀecttherelationshipbetween 4.4\\nthevalueofthecostfunctionpredictedbythegradientandthetruevalue.\\nWhenourfunctionhasmultipleinputdimensions,therearemanysecond\\nderivatives.Thesederivativescanbecollectedtogetherintoamatrixcalledthe\\nHessian m at r i x.TheHessianmatrix isdeﬁnedsuchthat H x()( f)\\nH x()( f) i , j=∂2\\n∂ x i ∂ x jf .() x (4.6)\\nEquivalently,theHessianistheJacobianofthegradient.\\nAnywherethatthesecondpartialderivativesarecontinuous,thediﬀerential\\noperatorsarecommutative,i.e.theirordercanbeswapped:\\n∂2\\n∂ x i ∂ x jf() = x∂2\\n∂ x j ∂ x if .() x (4.7)\\nThisimpliesthat H i , j= H j , i,sotheHessianmatrixissymmetricatsuchpoints.\\nMostofthefunctionsweencounterinthecontextofdeeplearninghaveasymmetric\\nHessianalmosteverywhere.\\xa0Because theHessianmatrixisrealandsymmetric,\\nwecandecomposeitintoasetofrealeigenvaluesandanorthogonalbasisof\\n8 7', 'CHAPTER4.NUMERICALCOMPUTATION\\neigenvectors.Thesecondderivativeinaspeciﬁcdirectionrepresentedbyaunit\\nvector disgivenby d\\ue03eH d.When disaneigenvectorof H,thesecondderivative\\ninthatdirectionisgivenbythecorrespondingeigenvalue.Forotherdirectionsof\\nd,thedirectionalsecondderivativeisaweightedaverageofalloftheeigenvalues,\\nwithweightsbetween0and1,andeigenvectorsthathavesmalleranglewith d\\nreceivingmoreweight.Themaximumeigenvaluedeterminesthemaximumsecond\\nderivativeandtheminimumeigenvaluedeterminestheminimumsecondderivative.\\nThe(directional)secondderivativetellsushowwellwecanexpectagradient\\ndescentsteptoperform.Wecanmakeasecond-orderTaylorseriesapproximation\\ntothefunction aroundthecurrentpoint f() x x( 0 ):\\nf f () x≈( x( 0 ))+( x x−( 0 ))\\ue03eg+1\\n2( x x−( 0 ))\\ue03eH x x (−( 0 )) .(4.8)\\nwhere gisthegradientand HistheHessianat x( 0 ).\\xa0Ifweusealearningrate\\nof \\ue00f,thenthenewpoint xwillbegivenby x( 0 )− \\ue00f g.Substitutingthisintoour\\napproximation,weobtain\\nf( x( 0 )− ≈ \\ue00f g) f( x( 0 ))− \\ue00f g\\ue03eg+1\\n2\\ue00f2g\\ue03eH g . (4.9)\\nTherearethree\\xa0termshere:theoriginalvalue\\xa0ofthefunction,\\xa0the expected\\nimprovementduetotheslopeofthefunction,andthecorrectionwemustapply\\ntoaccountforthecurvatureofthefunction.Whenthislasttermistoolarge,the\\ngradientdescentstepcanactuallymoveuphill.When g\\ue03eH giszeroornegative,\\ntheTaylorseriesapproximationpredictsthatincreasing \\ue00fforeverwilldecrease f\\nforever.Inpractice,theTaylorseriesisunlikelytoremainaccurateforlarge \\ue00f,so\\nonemustresorttomoreheuristicchoicesof \\ue00finthiscase.When g\\ue03eH gispositive,\\nsolvingfortheoptimalstepsizethatdecreasestheTaylorseriesapproximation of\\nthefunctionthemostyields\\n\\ue00f∗=g\\ue03eg\\ng\\ue03eH g. (4.10)\\nIntheworstcase,when galignswiththeeigenvectorof Hcorrespondingtothe\\nmaximaleigenvalue λ m a x,thenthisoptimalstepsizeisgivenby1\\nλmax.Tothe\\nextentthatthefunctionweminimizecanbeapproximatedwellbyaquadratic\\nfunction,theeigenvaluesoftheHessianthusdeterminethescaleofthelearning\\nrate.\\nThesecondderivativecanbeusedtodeterminewhetheracriticalpointis\\nalocalmaximum,alocalminimum,orsaddlepoint.Recallthatonacritical\\npoint, f\\ue030( x) = 0.Whenthesecondderivative f\\ue030 \\ue030( x) >0,theﬁrstderivative f\\ue030( x)\\nincreasesaswemovetotherightanddecreasesaswemovetotheleft.Thismeans\\n8 8', 'CHAPTER4.NUMERICALCOMPUTATION\\nf\\ue030( x \\ue00f−) <0and f\\ue030( x+ \\ue00f) >0forsmallenough \\ue00f.Inotherwords,aswemove\\nright,theslopebeginstopointuphilltotheright,andaswemoveleft,theslope\\nbeginstopointuphilltotheleft.\\xa0Thus,when f\\ue030( x)=0and f\\ue030 \\ue030( x) >0,wecan\\nconcludethat xisalocalminimum.Similarly,when f\\ue030( x) = 0and f\\ue030 \\ue030( x) <0,we\\ncanconcludethat xisalocalmaximum.Thisisknownasthe se c o nd der i v at i v e\\nt e st.Unfortunately,when f\\ue030 \\ue030( x) = 0,thetestisinconclusive.Inthiscase xmay\\nbeasaddlepoint,orapartofaﬂatregion.\\nInmultipledimensions,weneedtoexamineallofthesecondderivativesofthe\\nfunction.UsingtheeigendecompositionoftheHessianmatrix,wecangeneralize\\nthesecondderivativetesttomultipledimensions.Atacriticalpoint,where\\n∇ x f( x) = 0,wecanexaminetheeigenvaluesoftheHessiantodeterminewhether\\nthecriticalpointisalocalmaximum,localminimum,orsaddlepoint.Whenthe\\nHessianispositivedeﬁnite(allitseigenvaluesarepositive),thepointisalocal\\nminimum.Thiscanbeseenbyobservingthatthedirectionalsecondderivative\\ninanydirectionmustbepositive,andmakingreferencetotheunivariatesecond\\nderivativetest.Likewise,whentheHessianisnegativedeﬁnite(allitseigenvalues\\narenegative),thepointisalocalmaximum.Inmultipledimensions,itisactually\\npossibletoﬁndpositiveevidenceofsaddlepointsinsomecases.\\xa0Whenatleast\\noneeigenvalueispositiveandatleastoneeigenvalueisnegative,weknowthat\\nxisalocalmaximumononecrosssectionof fbutalocalminimumonanother\\ncrosssection.Seeﬁgureforanexample.Finally,themultidimensionalsecond 4.5\\nderivativetestcanbeinconclusive,justliketheunivariateversion.Thetestis\\ninconclusivewheneverallofthenon-zeroeigenvalueshavethesamesign,butat\\nleastoneeigenvalueiszero.Thisisbecausetheunivariatesecondderivativetestis\\ninconclusiveinthecrosssectioncorrespondingtothezeroeigenvalue.\\nInmultipledimensions,thereisadiﬀerentsecondderivativeforeachdirection\\natasinglepoint.TheconditionnumberoftheHessianatthispointmeasures\\nhowmuchthesecondderivativesdiﬀerfromeachother.WhentheHessianhasa\\npoorconditionnumber,gradientdescentperformspoorly.Thisisbecauseinone\\ndirection,thederivativeincreasesrapidly,whileinanotherdirection,itincreases\\nslowly.Gradientdescentisunawareofthischangeinthederivativesoitdoesnot\\nknowthatitneedstoexplorepreferentially inthedirectionwherethederivative\\nremainsnegativeforlonger.Italsomakesitdiﬃculttochooseagoodstepsize.\\nThestepsizemustbesmallenoughtoavoidovershootingtheminimumandgoing\\nuphillindirectionswithstrongpositivecurvature.Thisusuallymeansthatthe\\nstepsizeistoosmalltomakesigniﬁcantprogressinotherdirectionswithless\\ncurvature.Seeﬁgureforanexample.4.6\\nThisissuecanberesolvedbyusinginformationfromtheHessianmatrixtoguide\\n8 9', 'CHAPTER4.NUMERICALCOMPUTATION\\n\\ue078\\ue031\\x00\\ue031 \\ue035\\ue030\\ue031 \\ue035\\ue078 \\ue032\\x00\\ue031 \\ue035\\ue030\\ue031 \\ue035\\ue066\\ue078\\ue028\\ue031\\ue03b \\ue078\\ue032\\ue029\\n\\x00\\ue035 \\ue030 \\ue030\\ue030\\ue035 \\ue030 \\ue030\\nFigure4.5:Asaddlepointcontainingbothpositiveandnegativecurvature.Thefunction\\ninthisexampleis f( x)= x2\\n1− x2\\n2.Alongtheaxiscorrespondingto x 1,thefunction\\ncurvesupward.ThisaxisisaneigenvectoroftheHessianandhasapositiveeigenvalue.\\nAlongtheaxiscorrespondingto x 2,thefunctioncurvesdownward.Thisdirectionisan\\neigenvectoroftheHessianwithnegativeeigenvalue.Thename“saddlepoint”derivesfrom\\nthesaddle-likeshapeofthisfunction.Thisisthequintessentialexampleofafunction\\nwithasaddlepoint.Inmorethanonedimension,itisnotnecessarytohaveaneigenvalue\\nof0inordertogetasaddlepoint:itisonlynecessarytohavebothpositiveandnegative\\neigenvalues.Wecanthinkofasaddlepointwithbothsignsofeigenvaluesasbeingalocal\\nmaximumwithinonecrosssectionandalocalminimumwithinanothercrosssection.\\n9 0', 'CHAPTER4.NUMERICALCOMPUTATION\\n− − − 3 0 2 0 1 0 0 1 0 2 0\\nx 1− 3 0− 2 0− 1 001 02 0x 2\\nFigure4.6:Gradientdescentfailstoexploitthecurvatureinformationcontainedinthe\\nHessianmatrix.Hereweusegradientdescenttominimizeaquadraticfunction f( x) whose\\nHessianmatrixhasconditionnumber5.Thismeansthatthedirectionofmostcurvature\\nhasﬁvetimesmorecurvaturethanthedirectionofleastcurvature.Inthiscase,themost\\ncurvatureisinthedirection[1 ,1]\\ue03eandtheleastcurvatureisinthedirection[1 ,−1]\\ue03e.The\\nredlinesindicatethepathfollowedbygradientdescent.Thisveryelongatedquadratic\\nfunctionresemblesalongcanyon.Gradientdescentwastestimerepeatedlydescending\\ncanyonwalls,becausetheyarethesteepestfeature.Becausethestepsizeissomewhat\\ntoolarge,ithasatendencytoovershootthebottomofthefunctionandthusneedsto\\ndescendtheoppositecanyonwallonthenextiteration.Thelargepositiveeigenvalue\\noftheHessiancorrespondingtotheeigenvectorpointedinthisdirectionindicatesthat\\nthisdirectionalderivativeisrapidlyincreasing,soanoptimizationalgorithmbasedon\\ntheHessiancouldpredictthatthesteepestdirectionisnotactuallyapromisingsearch\\ndirectioninthiscontext.\\n9 1', 'CHAPTER4.NUMERICALCOMPUTATION\\nthesearch.Thesimplestmethodfordoingsoisknownas Newt o n’ s m e t ho d.\\nNewton’smethodisbasedonusingasecond-orderTaylorseriesexpansionto\\napproximatenearsomepoint f() x x( 0 ):\\nf f () x≈( x( 0 ))+( x x−( 0 ))\\ue03e∇ x f( x( 0 ))+1\\n2( x x−( 0 ))\\ue03eH x()( f( 0 ))( x x−( 0 )) .(4.11)\\nIfwethensolveforthecriticalpointofthisfunction,weobtain:\\nx∗= x( 0 )− H x()( f( 0 ))− 1∇ x f( x( 0 )) . (4.12)\\nWhen fisapositivedeﬁnitequadraticfunction,Newton’smethodconsistsof\\napplyingequationoncetojumptotheminimumofthefunctiondirectly. 4.12\\nWhen fisnottrulyquadraticbutcanbelocallyapproximatedasapositive\\ndeﬁnitequadratic,Newton’smethodconsistsofapplyingequationmultiple4.12\\ntimes.\\xa0Iterativelyupdatingtheapproximation andjumpingtotheminimumof\\ntheapproximation canreachthecriticalpointmuchfasterthangradientdescent\\nwould.Thisisausefulpropertynearalocalminimum,butitcanbeaharmful\\npropertynearasaddlepoint.Asdiscussedinsection,Newton’smethodis 8.2.3\\nonlyappropriatewhenthenearbycriticalpointisaminimum(alltheeigenvalues\\noftheHessianarepositive),whereasgradientdescentisnotattractedtosaddle\\npointsunlessthegradientpointstowardthem.\\nOptimization algorithmsthatuseonlythegradient,suchasgradientdescent,\\narecalled ﬁr st - o r d e r o pt i m i z a t i o n al g o r i t hms.Optimization algorithmsthat\\nalsousetheHessianmatrix,suchasNewton’smethod,arecalled se c o nd-or d e r\\no pt i m i z a t i o n al g o r i t hms(NocedalandWright2006,).\\nThe\\xa0optimization algorithms\\xa0employedin\\xa0mostcontextsin\\xa0this\\xa0book\\xa0are\\napplicabletoawidevarietyoffunctions,butcomewithalmostnoguarantees.\\nDeeplearningalgorithmstendtolackguaranteesbecausethefamilyoffunctions\\nusedindeeplearningisquitecomplicated.Inmanyotherﬁelds,thedominant\\napproachtooptimization istodesignoptimization algorithmsforalimitedfamily\\noffunctions.\\nInthecontextofdeeplearning,wesometimesgainsomeguaranteesbyrestrict-\\ningourselvestofunctionsthatareeither L i psc hi t z c o n t i n uousorhaveLipschitz\\ncontinuousderivatives.ALipschitzcontinuousfunctionisafunction fwhoserate\\nofchangeisboundedbya L i psc hi t z c o nst antL:\\n∀∀| − |≤L||−|| x , y , f() x f() y x y 2 . (4.13)\\nThispropertyisusefulbecauseitallowsustoquantifyourassumptionthata\\nsmallchangeintheinputmadebyanalgorithmsuchasgradientdescentwillhave\\n9 2', 'CHAPTER4.NUMERICALCOMPUTATION\\nasmallchangeintheoutput.Lipschitzcontinuityisalsoafairlyweakconstraint,\\nandmanyoptimizationproblemsindeeplearningcanbemadeLipschitzcontinuous\\nwithrelativelyminormodiﬁcations.\\nPerhapsthemostsuccessfulﬁeldofspecializedoptimization is c o n v e x o p-\\nt i m i z at i o n.Convexoptimization algorithmsareabletoprovidemanymore\\nguaranteesbymakingstrongerrestrictions.Convexoptimization algorithmsare\\napplicableonlytoconvexfunctions—functionsforwhichtheHessianispositive\\nsemideﬁniteeverywhere.Suchfunctionsarewell-behavedbecausetheylacksaddle\\npointsandalloftheirlocalminimaarenecessarilyglobalminima.However,most\\nproblemsindeeplearningarediﬃculttoexpressintermsofconvexoptimization.\\nConvexoptimization isusedonlyasasubroutineofsomedeeplearningalgorithms.\\nIdeasfromtheanalysisofconvexoptimization algorithmscanbeusefulforproving\\ntheconvergenceofdeeplearningalgorithms.However,ingeneral,theimportance\\nofconvexoptimization isgreatlydiminishedinthecontextofdeeplearning.For\\nmoreinformationaboutconvexoptimization, seeBoydandVandenberghe2004()\\norRockafellar1997().\\n4. 4 C on s t ra i n ed O p t i m i z a t i o n\\nSometimeswewishnotonlytomaximizeorminimizeafunction f( x)overall\\npossible\\xa0values\\xa0of x.Insteadwemay\\xa0wishto\\xa0ﬁnd\\xa0themaximal\\xa0or\\xa0minimal\\nvalue\\xa0of f( x)for\\xa0valuesof xinsome\\xa0set S.Thisis\\xa0known\\xa0as c o nst r ai n e d\\no pt i m i z a t i o n.Points xthatliewithintheset Sarecalled f e asi bl epointsin\\nconstrainedoptimization terminology.\\nWeoftenwishtoﬁndasolutionthatissmallinsomesense.Acommon\\napproachinsuchsituationsistoimposeanormconstraint,suchas. ||||≤ x 1\\nOnesimpleapproachtoconstrainedoptimization issimplytomodifygradient\\ndescenttakingtheconstraintintoaccount.Ifweuseasmallconstantstepsize \\ue00f,\\nwecanmakegradientdescentsteps,thenprojecttheresultbackinto S.Ifweuse\\nalinesearch,wecansearchonlyoverstepsizes \\ue00fthatyieldnew xpointsthatare\\nfeasible,orwecanprojecteachpointonthelinebackintotheconstraintregion.\\nWhenpossible,thismethodcanbemademoreeﬃcientbyprojectingthegradient\\nintothetangentspaceofthefeasibleregionbeforetakingthesteporbeginning\\nthelinesearch(,).Rosen1960\\nAmoresophisticatedapproachistodesignadiﬀerent,unconstrainedopti-\\nmizationproblemwhosesolutioncanbeconvertedintoasolutiontotheoriginal,\\nconstrainedoptimization problem.Forexample,ifwewanttominimize f( x)for\\n9 3', 'CHAPTER4.NUMERICALCOMPUTATION\\nx∈ R2with xconstrainedtohaveexactlyunit L2norm,wecaninsteadminimize\\ng( θ) = f([cossin θ , θ]\\ue03e)withrespectto θ,thenreturn[cossin θ , θ]asthesolution\\ntotheoriginalproblem.Thisapproachrequirescreativity;thetransformation\\nbetweenoptimization problemsmustbedesignedspeciﬁcallyforeachcasewe\\nencounter.\\nThe K ar ush– K u h n – T uc k e r(KKT)approach1providesaverygeneralso-\\nlutiontoconstrainedoptimization. WiththeKKTapproach,weintroducea\\nnewfunctioncalledthe g e ner al i z e d L agr angi a nor g e ner al i z e d L agr ange\\nf unc t i o n.\\nTodeﬁnetheLagrangian,weﬁrstneedtodescribe Sintermsofequations\\nandinequalities.\\xa0W ewantadescriptionof Sintermsof mfunctions g( ) iand n\\nfunctions h( ) jsothat S={|∀ x i , g( ) i( x) = 0and∀ j , h( ) j( x)≤0}.Theequations\\ninvolving g( ) iarecalledthe e q ual i t y c o nst r ai n t sandtheinequalitiesinvolving\\nh( ) jarecalled . i neq ual i t y c o nst r ai n t s\\nWeintroducenewvariables λ iand α jforeachconstraint,thesearecalledthe\\nKKTmultipliers.ThegeneralizedLagrangianisthendeﬁnedas\\nL , , f ( x λ α) = ()+ x\\ue058\\niλ i g( ) i()+ x\\ue058\\njα j h( ) j() x .(4.14)\\nWecannowsolveaconstrainedminimization problemusingunconstrained\\noptimization ofthegeneralizedLagrangian.Observethat,solongasatleastone\\nfeasiblepointexistsandisnotpermittedtohavevalue,then f() x ∞\\nmin\\nxmax\\nλmax\\nα α , ≥ 0L , , . ( x λ α) (4.15)\\nhasthesameoptimalobjectivefunctionvalueandsetofoptimalpointsas x\\nmin\\nx ∈ Sf .() x (4.16)\\nThisfollowsbecauseanytimetheconstraintsaresatisﬁed,\\nmax\\nλmax\\nα α , ≥ 0L , , f , ( x λ α) = () x (4.17)\\nwhileanytimeaconstraintisviolated,\\nmax\\nλmax\\nα α , ≥ 0L , , . ( x λ α) = ∞ (4.18)\\n1Th e K K T a p p ro a c h g e n e ra l i z e s t h e m e t h o d o f La gra n ge m u lt ip lie r s wh i c h a l l o ws e q u a l i t y\\nc o n s t ra i n t s b u t n o t i n e q u a l i t y c o n s t ra i n t s .\\n9 4', 'CHAPTER4.NUMERICALCOMPUTATION\\nThesepropertiesguaranteethatnoinfeasiblepointcanbeoptimal,andthatthe\\noptimumwithinthefeasiblepointsisunchanged.\\nToperformconstrainedmaximization, wecanconstructthegeneralizedLa-\\ngrangefunctionof,whichleadstothisoptimization problem: − f() x\\nmin\\nxmax\\nλmax\\nα α , ≥ 0− f()+ x\\ue058\\niλ i g( ) i()+ x\\ue058\\njα j h( ) j() x .(4.19)\\nWemayalsoconvertthistoaproblemwithmaximization intheouterloop:\\nmax\\nxmin\\nλmin\\nα α , ≥ 0f()+ x\\ue058\\niλ i g( ) i() x−\\ue058\\njα j h( ) j() x .(4.20)\\nThesignofthetermfortheequalityconstraintsdoesnotmatter;wemaydeﬁneit\\nwithadditionorsubtractionaswewish,becausetheoptimization isfreetochoose\\nanysignforeach λ i.\\nTheinequalityconstraintsareparticularlyinteresting.Wesaythataconstraint\\nh( ) i( x)is ac t i v eif h( ) i( x∗) = 0.Ifaconstraintisnotactive,thenthesolutionto\\ntheproblemfoundusingthatconstraintwouldremainatleastalocalsolutionif\\nthatconstraintwereremoved.Itispossiblethataninactiveconstraintexcludes\\nothersolutions.Forexample,aconvexproblemwithanentireregionofglobally\\noptimalpoints(awide,ﬂat,regionofequalcost)couldhaveasubsetofthis\\nregioneliminatedbyconstraints,oranon-convexproblemcouldhavebetterlocal\\nstationarypointsexcludedbyaconstraintthatisinactiveatconvergence.However,\\nthepointfoundatconvergenceremainsastationarypointwhetherornotthe\\ninactiveconstraintsareincluded.Becauseaninactive h( ) ihasnegativevalue,then\\nthesolutiontomin xmax λmax α α , ≥ 0 L( x λ α , ,)willhave α i=0.Wecanthus\\nobservethatatthesolution, α h\\ue00c( x)= 0.Inotherwords,forall i,weknow\\nthatatleastoneoftheconstraints α i≥0and h( ) i( x)≤0mustbeactiveatthe\\nsolution.Togainsomeintuitionforthisidea,wecansaythateitherthesolution\\nisontheboundaryimposedbytheinequalityandwemustuseitsKKTmultiplier\\ntoinﬂuencethesolutionto x,ortheinequalityhasnoinﬂuenceonthesolution\\nandwerepresentthisbyzeroingoutitsKKTmultiplier.\\nAsimplesetofpropertiesdescribetheoptimalpointsofconstrainedopti-\\nmizationproblems.ThesepropertiesarecalledtheKarush-Kuhn-Tucker(KKT)\\nconditions(,;Karush1939KuhnandTucker1951,).Theyarenecessaryconditions,\\nbutnotalwayssuﬃcientconditions,forapointtobeoptimal.Theconditionsare:\\n•ThegradientofthegeneralizedLagrangianiszero.\\n•AllconstraintsonbothandtheKKTmultipliersaresatisﬁed. x\\n9 5', 'CHAPTER4.NUMERICALCOMPUTATION\\n•Theinequalityconstraintsexhibit“complementary slackness”: α h\\ue00c( x) = 0.\\nFormoreinformationabouttheKKTapproach,seeNocedalandWright2006().\\n4. 5 E x am p l e: L i n ear L eas t S q u are s\\nSupposewewanttoﬁndthevalueofthatminimizes x\\nf() = x1\\n2||−|| A x b2\\n2 . (4.21)\\nTherearespecializedlinearalgebraalgorithmsthatcansolvethisproblemeﬃciently.\\nHowever,wecanalsoexplorehowtosolveitusinggradient-basedoptimization as\\nasimpleexampleofhowthesetechniqueswork.\\nFirst,weneedtoobtainthegradient:\\n∇ x f() = x A\\ue03e( ) = A x b− A\\ue03eA x A−\\ue03eb . (4.22)\\nWecanthenfollowthisgradientdownhill,takingsmallsteps.Seealgorithm4.1\\nfordetails.\\nAl g o r i t hm 4 . 1Analgorithmtominimize f( x) =1\\n2||−|| A x b2\\n2withrespectto x\\nusinggradientdescent,startingfromanarbitraryvalueof. x\\nSetthestepsize()andtolerance()tosmall,positivenumbers. \\ue00f δ\\nwhi l e|| A\\ue03eA x A−\\ue03eb|| 2 > δ do\\nx x← − \\ue00f\\ue000\\nA\\ue03eA x A−\\ue03eb\\ue001\\ne nd whi l e\\nOnecanalsosolvethisproblemusingNewton’smethod.Inthiscase,because\\nthetruefunctionisquadratic,thequadraticapproximation employedbyNewton’s\\nmethodisexact,andthealgorithmconvergestotheglobalminimuminasingle\\nstep.\\nNowsuppose\\xa0we\\xa0wishto\\xa0minimizethesame\\xa0function,butsubjectto\\xa0the\\nconstraint x\\ue03ex≤1.Todoso,weintroducetheLagrangian\\nL , λ f λ ( x) = ()+ x\\ue010\\nx\\ue03ex−1\\ue011\\n. (4.23)\\nWecannowsolvetheproblem\\nmin\\nxmax\\nλ , λ ≥ 0L , λ . ( x) (4.24)\\n9 6', 'CHAPTER4.NUMERICALCOMPUTATION\\nThesmallest-normsolutiontotheunconstrainedleastsquaresproblemmaybe\\nfoundusingtheMoore-Penrosepseudoinverse: x= A+b.Ifthispointisfeasible,\\nthenitisthesolutiontotheconstrainedproblem.Otherwise,wemustﬁnda\\nsolutionwheretheconstraintisactive.Bydiﬀerentiating theLagrangianwith\\nrespectto,weobtaintheequation x\\nA\\ue03eA x A−\\ue03eb x+2 λ= 0 . (4.25)\\nThistellsusthatthesolutionwilltaketheform\\nx A= (\\ue03eA I+2 λ)− 1A\\ue03eb . (4.26)\\nThemagnitudeof λmustbechosensuchthattheresultobeystheconstraint.We\\ncanﬁndthisvaluebyperforminggradientascenton.Todoso,observe λ\\n∂\\n∂ λL , λ( x) = x\\ue03ex−1 . (4.27)\\nWhenthenormof xexceeds1,thisderivativeispositive,sotofollowthederivative\\nuphillandincreasetheLagrangianwithrespectto λ,weincrease λ.Becausethe\\ncoeﬃcientonthe x\\ue03expenaltyhasincreased,solvingthelinearequationfor xwill\\nnowyieldasolutionwithsmallernorm.Theprocessofsolvingthelinearequation\\nandadjusting λcontinuesuntil xhasthecorrectnormandthederivativeon λis\\n0.\\nThisconcludesthemathematical preliminaries thatweusetodevelopmachine\\nlearningalgorithms.Wearenowreadytobuildandanalyzesomefull-ﬂedged\\nlearningsystems.\\n9 7']}, 'DeepLearning-chapter1.pdf': {'text': ['C h a p t e r 1\\nI n t ro d u ct i on\\nInventorshavelongdreamedofcreatingmachinesthatthink.Thisdesiredates\\nbacktoatleastthetimeofancientGreece.ThemythicalﬁguresPygmalion,\\nDaedalus,andHephaestusmayallbeinterpretedaslegendaryinventors,and\\nGalatea,Talos,andPandoramayallberegardedasartiﬁciallife( , OvidandMartin\\n2004Sparkes1996Tandy1997 ;,;,).\\nWhenprogrammable computerswereﬁrstconceived,peoplewonderedwhether\\nsuchmachinesmightbecomeintelligent,overahundredyearsbeforeonewas\\nbuilt(Lovelace1842,).Today, ar t i ﬁc i al i n t e l l i g e nc e(AI)isathrivingﬁeldwith\\nmanypracticalapplicationsandactiveresearchtopics.Welooktointelligent\\nsoftwaretoautomateroutinelabor,understandspeechorimages,makediagnoses\\ninmedicineandsupportbasicscientiﬁcresearch.\\nIntheearlydaysofartiﬁcialintelligence,theﬁeldrapidlytackledandsolved\\nproblemsthatareintellectually diﬃcultforhumanbeingsbutrelativelystraight-\\nforwardforcomputers—problemsthatcanbedescribedbyalistofformal,math-\\nematicalrules.\\xa0Thetruechallengetoartiﬁcialintelligenceprovedtobesolving\\nthetasksthatareeasyforpeopletoperformbuthardforpeopletodescribe\\nformally—probl emsthatwesolveintuitively,thatfeelautomatic,likerecognizing\\nspokenwordsorfacesinimages.\\nThisbookisaboutasolutiontothesemoreintuitiveproblems.Thissolutionis\\ntoallowcomputerstolearnfromexperienceandunderstandtheworldintermsofa\\nhierarchyofconcepts,witheachconceptdeﬁnedintermsofitsrelationtosimpler\\nconcepts.Bygatheringknowledgefromexperience,thisapproachavoidstheneed\\nforhumanoperatorstoformallyspecifyalloftheknowledgethatthecomputer\\nneeds.Thehierarchyofconceptsallowsthecomputertolearncomplicatedconcepts\\nbybuildingthemoutofsimplerones.Ifwedrawagraphshowinghowthese\\n1', 'CHAPTER1.INTRODUCTION\\nconceptsarebuiltontopofeachother,thegraphisdeep,withmanylayers.For\\nthisreason,wecallthisapproachtoAI . deep l e ar ni ng\\nManyoftheearlysuccessesofAItookplaceinrelativelysterileandformal\\nenvironmentsanddidnotrequirecomputerstohavemuchknowledgeabout\\ntheworld.Forexample,IBM’sDeepBluechess-playingsystemdefeatedworld\\nchampionGarryKasparovin1997(,).Chessisofcourseaverysimple Hsu2002\\nworld,containingonlysixty-fourlocationsandthirty-twopiecesthatcanmove\\ninonlyrigidlycircumscribedways.Devisingasuccessfulchessstrategyis\\xa0a\\ntremendousaccomplishment,\\xa0butthechallengeisnotduetothediﬃcultyof\\ndescribingthesetofchesspiecesandallowablemovestothecomputer.Chess\\ncanbecompletelydescribedbyaverybrieflistofcompletelyformalrules,easily\\nprovidedaheadoftimebytheprogrammer.\\nIronically,abstractandformaltasksthatareamongthemostdiﬃcultmental\\nundertakings forahumanbeingareamongtheeasiestforacomputer.Computers\\nhavelongbeenabletodefeateventhebesthumanchessplayer,butareonly\\nrecentlymatchingsomeoftheabilitiesofaveragehumanbeingstorecognizeobjects\\norspeech.Aperson’severydayliferequiresanimmenseamountofknowledge\\nabouttheworld.Muchofthisknowledgeissubjectiveandintuitive,andtherefore\\ndiﬃculttoarticulateinaformalway.Computersneedtocapturethissame\\nknowledgeinordertobehaveinanintelligentway.Oneofthekeychallengesin\\nartiﬁcialintelligenceishowtogetthisinformalknowledgeintoacomputer.\\nSeveralartiﬁcialintelligenceprojectshavesoughttohard-codeknowledgeabout\\ntheworldinformallanguages.Acomputercanreasonaboutstatementsinthese\\nformallanguagesautomatically usinglogicalinferencerules.Thisisknownasthe\\nk no wl e dge baseapproachtoartiﬁcialintelligence.Noneoftheseprojectshasled\\ntoamajorsuccess.OneofthemostfamoussuchprojectsisCyc( , LenatandGuha\\n1989).Cycisaninferenceengineandadatabaseofstatementsinalanguage\\ncalledCycL.Thesestatementsareenteredbyastaﬀofhumansupervisors.Itisan\\nunwieldyprocess.Peoplestruggletodeviseformalruleswithenoughcomplexity\\ntoaccuratelydescribetheworld.Forexample,Cycfailedtounderstandastory\\naboutapersonnamedFredshavinginthemorning(,).Itsinference Linde1992\\nenginedetectedaninconsistencyinthestory:\\xa0itknewthatpeopledonothave\\nelectricalparts,butbecauseFredwasholdinganelectricrazor,itbelievedthe\\nentity“FredWhileShaving”containedelectricalparts.Itthereforeaskedwhether\\nFredwasstillapersonwhilehewasshaving.\\nThediﬃcultiesfacedbysystemsrelyingonhard-codedknowledgesuggest\\nthatAIsystemsneedtheabilitytoacquiretheirownknowledge,byextracting\\npatternsfromrawdata.Thiscapabilityisknownas m ac hi ne l e ar ni ng.The\\n2', 'CHAPTER1.INTRODUCTION\\nintroductionofmachinelearningallowedcomputerstotackleproblemsinvolving\\nknowledgeoftherealworldandmakedecisionsthatappearsubjective.Asimple\\nmachinelearningalgorithmcalled l o g i st i c r e g r e ssi o ncandeterminewhetherto\\nrecommendcesareandelivery(Mor-Yosef1990 e t a l .,).Asimplemachinelearning\\nalgorithmcalled nai v e B a y e scanseparatelegitimatee-mailfromspame-mail.\\nTheperformanceofthesesimplemachinelearningalgorithmsdependsheavily\\nonthe r e pr e se n t at i o nofthedatatheyaregiven.Forexample,whenlogistic\\nregressionisusedtorecommendcesareandelivery,theAIsystemdoesnotexamine\\nthepatientdirectly.Instead,thedoctortellsthesystemseveralpiecesofrelevant\\ninformation, suchasthepresenceorabsenceofauterinescar.Eachpieceof\\ninformationincludedintherepresentationofthepatientisknownasa f e at ur e.\\nLogisticregressionlearnshoweachofthesefeaturesofthepatientcorrelateswith\\nvariousoutcomes.However,itcannotinﬂuencethewaythatthefeaturesare\\ndeﬁnedinanyway.\\xa0IflogisticregressionwasgivenanMRIscanofthepatient,\\nratherthanthedoctor’sformalizedreport,itwouldnotbeabletomakeuseful\\npredictions.IndividualpixelsinanMRIscanhavenegligiblecorrelationwithany\\ncomplications thatmightoccurduringdelivery.\\nThisdependenceonrepresentationsisageneralphenomenon thatappears\\nthroughoutcomputerscienceandevendailylife.Incomputerscience,opera-\\ntionssuchassearchingacollectionofdatacanproceedexponentiallyfasterif\\nthecollectionisstructuredandindexedintelligently.Peoplecaneasilyperform\\narithmeticonArabicnumerals,butﬁndarithmeticonRomannumeralsmuch\\nmoretime-consuming. Itisnotsurprisingthatthechoiceofrepresentationhasan\\nenormouseﬀectontheperformanceofmachinelearningalgorithms.Forasimple\\nvisualexample,seeﬁgure.1.1\\nManyartiﬁcialintelligencetaskscanbesolvedbydesigningtherightsetof\\nfeaturestoextractforthattask,thenprovidingthesefeaturestoasimplemachine\\nlearningalgorithm.Forexample,ausefulfeatureforspeakeridentiﬁcationfrom\\nsoundisanestimateofthesizeofspeaker’svocaltract.Itthereforegivesastrong\\nclueastowhetherthespeakerisaman,woman,orchild.\\nHowever,formanytasks,itisdiﬃculttoknowwhatfeaturesshouldbeextracted.\\nForexample,supposethatwewouldliketowriteaprogramtodetectcarsin\\nphotographs. Weknowthatcarshavewheels,sowemightliketousethepresence\\nofawheelasafeature.Unfortunately,itisdiﬃculttodescribeexactlywhata\\nwheellookslikeintermsofpixelvalues.Awheelhasasimplegeometricshapebut\\nitsimagemaybecomplicatedbyshadowsfallingonthewheel,thesunglaringoﬀ\\nthemetalpartsofthewheel,thefenderofthecaroranobjectintheforeground\\nobscuringpartofthewheel,andsoon.\\n3', 'CHAPTER1.INTRODUCTION\\n\\ue078\\ue079\\ue043 \\ue061 \\ue072 \\ue074 \\ue065 \\ue073\\ue069\\ue061\\ue06e\\ue020 \\ue063 \\ue06f \\ue06f \\ue072 \\ue064 \\ue069 \\ue06e \\ue061 \\ue074 \\ue065 \\ue073\\n\\ue072\\ue0b5\\ue050 \\ue06f \\ue06c \\ue061 \\ue072 \\ue020 \\ue063 \\ue06f \\ue06f \\ue072 \\ue064 \\ue069 \\ue06e \\ue061 \\ue074 \\ue065 \\ue073\\nFigure1.1:Exampleofdiﬀerentrepresentations:supposewewanttoseparatetwo\\ncategoriesofdatabydrawingalinebetweentheminascatterplot.Intheplotontheleft,\\nwerepresentsomedatausingCartesiancoordinates,andthetaskisimpossible.Intheplot\\nontheright,werepresentthedatawithpolarcoordinatesandthetaskbecomessimpleto\\nsolvewithaverticalline.FigureproducedincollaborationwithDavidWarde-Farley.\\nOnesolutiontothisproblemistousemachinelearningtodiscovernotonly\\nthemappingfromrepresentationtooutputbutalsotherepresentationitself.\\nThisapproachisknownas r e pr e se n t at i o n l e ar ni ng.\\xa0Learnedrepresentations\\noftenresultinmuchbetterperformancethancanbeobtainedwithhand-designed\\nrepresentations.TheyalsoallowAIsystemstorapidlyadapttonewtasks,with\\nminimalhumanintervention.Arepresentationlearningalgorithmcandiscovera\\ngoodsetoffeaturesforasimpletaskinminutes,oracomplextaskinhoursto\\nmonths.Manuallydesigningfeaturesforacomplextaskrequiresagreatdealof\\nhumantimeandeﬀort;itcantakedecadesforanentirecommunityofresearchers.\\nThequintessentialexampleofarepresentationlearningalgorithmisthe au-\\nt o e nc o der.Anautoencoderisthecombinationofan e nc o derfunctionthat\\nconvertstheinputdataintoadiﬀerentrepresentation,anda dec o derfunction\\nthatconvertsthenewrepresentationbackintotheoriginalformat.Autoencoders\\naretrainedtopreserveasmuchinformationaspossiblewhenaninputisrun\\nthroughtheencoderandthenthedecoder,butarealsotrainedtomakethenew\\nrepresentationhavevariousniceproperties.Diﬀerentkindsofautoencodersaimto\\nachievediﬀerentkindsofproperties.\\nWhendesigningfeaturesoralgorithmsforlearningfeatures,ourgoalisusually\\ntoseparatethe f ac t o r s o f v ar i at i o nthatexplaintheobserveddata.Inthis\\ncontext,weusetheword“factors”simplytorefertoseparatesourcesofinﬂuence;\\nthefactorsareusuallynotcombinedbymultiplication. Suchfactorsareoftennot\\n4', 'CHAPTER1.INTRODUCTION\\nquantitiesthataredirectlyobserved.Instead,theymayexisteitherasunobserved\\nobjectsorunobservedforcesinthephysicalworldthataﬀectobservablequantities.\\nTheymayalsoexistasconstructsinthehumanmindthatprovideusefulsimplifying\\nexplanationsorinferredcausesoftheobserveddata.Theycanbethoughtofas\\nconceptsorabstractionsthathelpusmakesenseoftherichvariabilityinthedata.\\nWhenanalyzingaspeechrecording,thefactorsofvariationincludethespeaker’s\\nage,theirsex,theiraccentandthewordsthattheyarespeaking.Whenanalyzing\\nanimageofacar,thefactorsofvariationincludethepositionofthecar,itscolor,\\nandtheangleandbrightnessofthesun.\\nAmajorsourceofdiﬃcultyinmanyreal-worldartiﬁcialintelligenceapplications\\nisthatmanyofthefactorsofvariationinﬂuenceeverysinglepieceofdataweare\\nabletoobserve.Theindividualpixelsinanimageofaredcarmightbeveryclose\\ntoblackatnight.Theshapeofthecar’ssilhouettedependsontheviewingangle.\\nMostapplicationsrequireusto thefactorsofvariationanddiscardthe d i s e nt a ng l e\\nonesthatwedonotcareabout.\\nOfcourse,itcanbeverydiﬃculttoextractsuchhigh-level,abstractfeatures\\nfromrawdata.Manyofthesefactorsofvariation,suchasaspeaker’saccent,\\ncanbeidentiﬁedonlyusingsophisticated,nearlyhuman-levelunderstandingof\\nthedata.Whenitisnearlyasdiﬃculttoobtainarepresentationastosolvethe\\noriginalproblem,representationlearningdoesnot,atﬁrstglance,seemtohelpus.\\nD e e p l e ar ni ngsolvesthiscentralprobleminrepresentationlearningbyintro-\\nducingrepresentationsthatareexpressedintermsofother,simplerrepresentations.\\nDeeplearningallowsthecomputertobuildcomplexconceptsoutofsimplercon-\\ncepts.Figureshowshowadeeplearningsystemcanrepresenttheconceptof 1.2\\nanimageofapersonbycombiningsimplerconcepts,suchascornersandcontours,\\nwhichareinturndeﬁnedintermsofedges.\\nThequintessentialexampleofadeeplearningmodelisthefeedforwarddeep\\nnetworkor m ul t i l a y e r p e r c e pt r o n(MLP).Amultilayerperceptronisjusta\\nmathematical functionmappingsomesetofinputvaluestooutputvalues.The\\nfunctionisformedbycomposingmanysimplerfunctions.Wecanthinkofeach\\napplicationofadiﬀerentmathematical functionasprovidinganewrepresentation\\noftheinput.\\nTheideaoflearningtherightrepresentationforthedataprovidesoneperspec-\\ntiveondeeplearning.Anotherperspectiveondeeplearningisthatdepthallowsthe\\ncomputertolearnamulti-stepcomputerprogram.Eachlayeroftherepresentation\\ncanbethoughtofasthestateofthecomputer’smemoryafterexecutinganother\\nsetofinstructionsinparallel.Networkswithgreaterdepthcanexecutemore\\ninstructionsinsequence.Sequentialinstructionsoﬀergreatpowerbecauselater\\n5', 'CHAPTER1.INTRODUCTION\\nVisible\\xa0layer\\n(input\\xa0pixels)1st\\xa0hidden\\xa0layer\\n(edges)2nd\\xa0hidden\\xa0layer\\n(corners\\xa0and\\ncontours)3rd\\xa0hidden\\xa0layer\\n(object\\xa0parts)CARPERSONANIMALOutput\\n(object\\xa0identity)\\nFigure1.2:Illustrationofadeeplearningmodel.Itisdiﬃcultforacomputertounderstand\\nthemeaningofrawsensoryinputdata,suchasthisimagerepresentedasacollection\\nofpixelvalues.Thefunctionmappingfromasetofpixelstoanobjectidentityisvery\\ncomplicated.Learningorevaluatingthismappingseemsinsurmountableiftackleddirectly.\\nDeeplearningresolvesthisdiﬃcultybybreakingthedesiredcomplicatedmappingintoa\\nseriesofnestedsimplemappings,eachdescribedbyadiﬀerentlayerofthemodel.The\\ninputispresentedatthevisiblelayer,sonamedbecauseitcontainsthevariablesthat\\nweareabletoobserve.Thenaseriesofhiddenlayersextractsincreasinglyabstract\\nfeaturesfromtheimage.Theselayersarecalled“hidden”becausetheirvaluesarenotgiven\\ninthedata;insteadthemodelmustdeterminewhichconceptsareusefulforexplaining\\ntherelationshipsintheobserveddata.Theimagesherearevisualizationsofthekind\\noffeaturerepresentedbyeachhiddenunit.Giventhepixels,theﬁrstlayercaneasily\\nidentifyedges,bycomparingthebrightnessofneighboringpixels.Giventheﬁrsthidden\\nlayer’sdescriptionoftheedges,thesecondhiddenlayercaneasilysearchforcornersand\\nextendedcontours,whicharerecognizableascollectionsofedges.Giventhesecondhidden\\nlayer’sdescriptionoftheimageintermsofcornersandcontours,thethirdhiddenlayer\\ncandetectentirepartsofspeciﬁcobjects,byﬁndingspeciﬁccollectionsofcontoursand\\ncorners.Finally,thisdescriptionoftheimageintermsoftheobjectpartsitcontainscan\\nbeusedtorecognizetheobjectspresentintheimage.Imagesreproducedwithpermission\\nfromZeilerandFergus2014().\\n6', 'CHAPTER1.INTRODUCTION\\nx 1 x 1σ\\nw 1 w 1×\\nx 2 x 2 w 2 w 2×+El e me n t\\nS e t\\n+\\n×\\nσ\\nxx wwEl e me n t\\nS e t\\nL ogi s t i c\\nR e gr e s s i onL ogi s t i c\\nR e gr e s s i on\\nFigure1.3:Illustrationofcomputationalgraphsmappinganinputtoanoutputwhere\\neachnodeperformsanoperation.Depthisthelengthofthelongestpathfrominputto\\noutputbutdependsonthedeﬁnitionofwhatconstitutesapossiblecomputationalstep.\\nThecomputationdepictedinthesegraphsistheoutputofalogisticregressionmodel,\\nσ ( wTx ),whereσisthelogisticsigmoidfunction.Ifweuseaddition,multiplicationand\\nlogisticsigmoidsastheelementsofourcomputerlanguage,thenthismodelhasdepth\\nthree.Ifweviewlogisticregressionasanelementitself,thenthismodelhasdepthone.\\ninstructionscanreferbacktotheresultsofearlierinstructions.Accordingtothis\\nviewofdeeplearning,notalloftheinformationinalayer’sactivationsnecessarily\\nencodesfactorsofvariationthatexplaintheinput.Therepresentationalsostores\\nstateinformationthathelpstoexecuteaprogramthatcanmakesenseoftheinput.\\nThisstateinformationcouldbeanalogoustoacounterorpointerinatraditional\\ncomputerprogram.Ithasnothingtodowiththecontentoftheinputspeciﬁcally,\\nbutithelpsthemodeltoorganizeitsprocessing.\\nTherearetwomainwaysofmeasuringthedepthofamodel.Theﬁrstviewis\\nbasedonthenumberofsequentialinstructionsthatmustbeexecutedtoevaluate\\nthearchitecture.Wecanthinkofthisasthelengthofthelongestpaththrough\\naﬂowchartthatdescribeshowtocomputeeachofthemodel’soutputsgiven\\nitsinputs.Justastwoequivalentcomputerprogramswillhavediﬀerentlengths\\ndependingonwhichlanguagetheprogramiswrittenin,thesamefunctionmay\\nbedrawnasaﬂowchartwithdiﬀerentdepthsdependingonwhichfunctionswe\\nallowtobeusedasindividualstepsintheﬂowchart.Figureillustrateshowthis 1.3\\nchoiceoflanguagecangivetwodiﬀerentmeasurementsforthesamearchitecture.\\nAnotherapproach,usedbydeepprobabilisticmodels,regardsthedepthofa\\nmodelasbeingnotthedepthofthecomputational graphbutthedepthofthe\\ngraphdescribinghowconceptsarerelatedtoeachother.Inthiscase,thedepth\\n7', 'CHAPTER1.INTRODUCTION\\noftheﬂowchartofthecomputations neededtocomputetherepresentationof\\neachconceptmaybemuchdeeperthanthegraphoftheconceptsthemselves.\\nThisisbecausethesystem’sunderstandingofthesimplerconceptscanbereﬁned\\ngiveninformationaboutthemorecomplexconcepts.Forexample,anAIsystem\\nobservinganimageofafacewithoneeyeinshadowmayinitiallyonlyseeoneeye.\\nAfterdetectingthatafaceispresent,itcantheninferthatasecondeyeisprobably\\npresentaswell.\\xa0Inthiscase,thegraphofconceptsonlyincludestwolayers—a\\nlayerforeyesandalayerforfaces—butthegraphofcomputations includes 2n\\nlayersifwereﬁneourestimateofeachconceptgiventheothertimes. n\\nBecauseitisnotalwaysclearwhichofthesetwoviews—thedepthofthe\\ncomputational graph,orthedepthoftheprobabilisticmodelinggraph—ismost\\nrelevant,andbecausediﬀerentpeoplechoosediﬀerentsetsofsmallestelements\\nfromwhichtoconstructtheirgraphs,thereisnosinglecorrectvalueforthe\\ndepthofanarchitecture,justasthereisnosinglecorrectvalueforthelengthof\\nacomputerprogram.\\xa0Nor isthereaconsensusabouthowmuchdepthamodel\\nrequirestoqualifyas“deep.”However,deeplearningcansafelyberegardedasthe\\nstudyofmodelsthateitherinvolveagreateramountofcompositionoflearned\\nfunctionsorlearnedconceptsthantraditionalmachinelearningdoes.\\nTosummarize,deeplearning,thesubjectofthisbook,isanapproachtoAI.\\nSpeciﬁcally,itisatypeofmachinelearning,atechniquethatallowscomputer\\nsystemstoimprovewithexperienceanddata.\\xa0Accordingtotheauthorsofthis\\nbook,machinelearningistheonlyviableapproachtobuildingAIsystemsthat\\ncanoperateincomplicated,real-worldenvironments.Deeplearningisaparticular\\nkindofmachinelearningthatachievesgreatpowerandﬂexibilitybylearningto\\nrepresenttheworldasanestedhierarchyofconcepts,witheachconceptdeﬁnedin\\nrelationtosimplerconcepts,andmoreabstractrepresentationscomputedinterms\\noflessabstractones.Figureillustratestherelationshipbetweenthesediﬀerent 1.4\\nAIdisciplines.Figuregivesahigh-levelschematicofhoweachworks. 1.5\\n1. 1 Wh o S h ou l d R ead T h i s Bo ok ?\\nThisbookcanbeusefulforavarietyofreaders,butwewroteitwithtwomain\\ntargetaudiencesinmind.Oneofthesetargetaudiencesisuniversitystudents\\n(undergraduate orgraduate)learningaboutmachinelearning,includingthosewho\\narebeginningacareerindeeplearningandartiﬁcialintelligenceresearch.The\\nothertargetaudienceissoftwareengineerswhodonothaveamachinelearning\\norstatisticsbackground, butwanttorapidlyacquireoneandbeginusingdeep\\nlearningintheirproductorplatform.Deeplearninghasalreadyprovenusefulin\\n8', 'CHAPTER1.INTRODUCTION\\nAIMachine\\xa0learningRepresentation\\xa0learningDeep\\xa0learning\\nExample:\\nKnowledge\\nbasesExample:\\nLogistic\\nregressionExample:\\nShallow\\nautoencoders Example:\\nMLPs\\nFigure1.4:AVenndiagramshowinghowdeeplearningisakindofrepresentationlearning,\\nwhichisinturnakindofmachinelearning,whichisusedformanybutnotallapproaches\\ntoAI.EachsectionoftheVenndiagramincludesanexampleofanAItechnology.\\n9', 'CHAPTER1.INTRODUCTION\\nInputHand-\\ndesigned\\xa0\\nprogramOutput\\nInputHand-\\ndesigned\\xa0\\nfeaturesMapping\\xa0from\\xa0\\nfeaturesOutput\\nInputFeaturesMapping\\xa0from\\xa0\\nfeaturesOutput\\nInputSimple\\xa0\\nfeaturesMapping\\xa0from\\xa0\\nfeaturesOutput\\nAdditional\\xa0\\nlayers\\xa0of\\xa0more\\xa0\\nabstract\\xa0\\nfeatures\\nRule-based\\nsystemsClassic\\nmachine\\nlearning Representation\\nlearningDeep\\nlearning\\nFigure1.5:\\xa0FlowchartsshowinghowthediﬀerentpartsofanAIsystemrelatetoeach\\notherwithindiﬀerentAIdisciplines.Shadedboxesindicatecomponentsthatareableto\\nlearnfromdata.\\n1 0', 'CHAPTER1.INTRODUCTION\\nmanysoftwaredisciplinesincludingcomputervision,speechandaudioprocessing,\\nnaturallanguageprocessing,robotics,bioinformatics andchemistry,videogames,\\nsearchengines,onlineadvertisingandﬁnance.\\nThisbookhasbeenorganizedintothreepartsinordertobestaccommodatea\\nvarietyofreaders.Partintroducesbasicmathematical toolsandmachinelearning I\\nconcepts.Partdescribesthemostestablisheddeeplearningalgorithmsthatare II\\nessentiallysolvedtechnologies.Partdescribesmorespeculativeideasthatare III\\nwidelybelievedtobeimportantforfutureresearchindeeplearning.\\nReadersshouldfeelfreetoskippartsthatarenotrelevantgiventheirinterests\\norbackground. Readersfamiliarwithlinearalgebra,probability,andfundamental\\nmachinelearningconceptscanskippart,forexample,whilereaderswhojustwant I\\ntoimplementaworkingsystemneednotreadbeyondpart.Tohelpchoosewhich II\\nchapterstoread,ﬁgureprovidesaﬂowchartshowingthehigh-levelorganization 1.6\\nofthebook.\\nWedoassumethatallreaderscomefromacomputersciencebackground. We\\nassumefamiliaritywithprogramming, abasicunderstandingofcomputational\\nperformanceissues,complexitytheory,introductory levelcalculusandsomeofthe\\nterminologyofgraphtheory.\\n1. 2 Hi s t or i c a l T ren d s i n D eep L earni n g\\nItiseasiesttounderstanddeeplearningwithsomehistoricalcontext.Ratherthan\\nprovidingadetailedhistoryofdeeplearning,weidentifyafewkeytrends:\\n•Deeplearninghashadalongandrichhistory,buthasgonebymanynames\\nreﬂectingdiﬀerentphilosophicalviewpoints,andhaswaxedandwanedin\\npopularity.\\n•Deeplearninghasbecomemoreusefulastheamountofavailabletraining\\ndatahasincreased.\\n•Deeplearningmodelshavegrowninsizeovertimeascomputerinfrastructure\\n(bothhardwareandsoftware)fordeeplearninghasimproved.\\n•Deeplearninghassolvedincreasinglycomplicatedapplicationswithincreasing\\naccuracyovertime.\\n1 1', 'CHAPTER1.INTRODUCTION\\n1.\\xa0Introduction\\nPart\\xa0I:\\xa0Applied\\xa0Math\\xa0and\\xa0Machine\\xa0Learning\\xa0Basics\\n2.\\xa0Linear\\xa0Algebra3.\\xa0Probability\\xa0and\\xa0\\nInformation\\xa0Theory\\n4.\\xa0Numerical\\xa0\\nComputation5.\\xa0Machine\\xa0Learning\\xa0\\nBasics\\nPart\\xa0II:\\xa0Deep\\xa0Networks:\\xa0Modern\\xa0Practices\\n6.\\xa0Deep\\xa0Feedforward\\xa0\\nNetworks\\n7.\\xa0Regularization8.\\xa0Optimization 9.\\xa0\\xa0CNNs10.\\xa0\\xa0RNNs\\n11.\\xa0Practical\\xa0\\nMethodology12.\\xa0Applications\\nPart\\xa0III:\\xa0Deep\\xa0Learning\\xa0Research\\n13.\\xa0Linear\\xa0Factor\\xa0\\nModels14.\\xa0Autoencoders15.\\xa0Representation\\xa0\\nLearning\\n16.\\xa0Structured\\xa0\\nProbabilistic\\xa0Models17.\\xa0Monte\\xa0Carlo\\xa0\\nMethods\\n18.\\xa0Partition\\xa0\\nFunction19.\\xa0Inference\\n20.\\xa0Deep\\xa0Generative\\xa0\\nModels\\nFigure1.6:Thehigh-levelorganizationofthebook.Anarrowfromonechaptertoanother\\nindicatesthattheformerchapterisprerequisitematerialforunderstandingthelatter.\\n1 2', 'CHAPTER1.INTRODUCTION\\n1 . 2 . 1 T h e Ma n y Na m es a n d Ch a n g i n g F o rt u n es o f Neu ra l Net -\\nw o rks\\nWeexpectthatmanyreadersofthisbookhaveheardofdeeplearningasan\\nexcitingnewtechnology,andaresurprisedtoseeamentionof“history”inabook\\naboutanemergingﬁeld.Infact,deeplearningdatesbacktothe1940s.Deep\\nlearningonly a p p e a r stobenew,becauseitwasrelativelyunpopularforseveral\\nyearsprecedingitscurrentpopularity,andbecauseithasgonethroughmany\\ndiﬀerentnames,andhasonlyrecentlybecomecalled“deeplearning.”Theﬁeld\\nhasbeenrebrandedmanytimes,reﬂectingtheinﬂuenceofdiﬀerentresearchers\\nanddiﬀerentperspectives.\\nAcomprehensivehistoryofdeeplearningisbeyondthescopeofthistextbook.\\nHowever,somebasiccontextisusefulforunderstandingdeeplearning.Broadly\\nspeaking,therehavebeenthreewavesofdevelopmentofdeeplearning:deep\\nlearning\\xa0known\\xa0as c y b e r net i c sin\\xa0the\\xa01940s–1960s,\\xa0deep\\xa0learning\\xa0knownas\\nc o nnec t i o n i s minthe1980s–1990s,andthecurrentresurgenceunderthename\\ndeeplearningbeginningin2006.Thisisquantitativelyillustratedinﬁgure.1.7\\nSomeoftheearliestlearningalgorithmswerecognizetodaywereintended\\ntobecomputational modelsofbiologicallearning,i.e.modelsofhowlearning\\nhappensorcouldhappeninthebrain.\\xa0Asaresult,oneofthenamesthatdeep\\nlearninghasgonebyis ar t i ﬁc i al neur al net w o r k s(ANNs).Thecorresponding\\nperspectiveondeeplearningmodelsisthattheyareengineeredsystemsinspired\\nbythebiologicalbrain(whetherthehumanbrainorthebrainofanotheranimal).\\nWhilethekindsofneuralnetworksusedformachinelearninghavesometimes\\nbeenusedtounderstandbrainfunction( ,),theyare HintonandShallice1991\\ngenerallynotdesignedtoberealisticmodelsofbiologicalfunction.Theneural\\nperspectiveondeeplearningismotivatedbytwomainideas.Oneideaisthat\\nthebrainprovidesaproofbyexamplethatintelligentbehaviorispossible,anda\\nconceptuallystraightforwardpathtobuildingintelligenceistoreverseengineerthe\\ncomputational principlesbehindthebrainandduplicateitsfunctionality.Another\\nperspectiveisthatitwouldbedeeplyinterestingtounderstandthebrainandthe\\nprinciplesthatunderliehumanintelligence,somachinelearningmodelsthatshed\\nlightonthesebasicscientiﬁcquestionsareusefulapartfromtheirabilitytosolve\\nengineeringapplications.\\nThemodernterm“deeplearning”goesbeyondtheneuroscientiﬁcperspective\\nonthecurrentbreedofmachinelearningmodels.Itappealstoamoregeneral\\nprincipleoflearning m u l t i p l e l e v e l s o f c o m p o s i t i o n,whichcanbeappliedinmachine\\nlearningframeworksthatarenotnecessarilyneurallyinspired.\\n1 3', 'CHAPTER1.INTRODUCTION\\n1940 1950 1960 1970 1980 1990 2000\\nYear0.0000000.0000500.0001000.0001500.0002000.000250FrequencyofWordorPhrase\\nc y b e r n e t i c s\\n( c o n n e c t i o n i s m + n e u r a l n e t w o r k s )\\nFigure1.7:Theﬁgureshowstwoofthethreehistoricalwavesofartiﬁcialneuralnets\\nresearch,asmeasuredbythefrequencyofthephrases“cybernetics”and“connectionism”or\\n“neuralnetworks”accordingtoGoogleBooks(thethirdwaveistoorecenttoappear).The\\nﬁrstwavestartedwithcyberneticsinthe1940s–1960s, withthedevelopmentoftheories\\nofbiologicallearning( ,;,)andimplementationsof McCullochandPitts1943Hebb1949\\ntheﬁrstmodelssuchastheperceptron(Rosenblatt1958,)allowingthetrainingofasingle\\nneuron.Thesecondwavestartedwiththeconnectionistapproachofthe1980–1995period,\\nwithback-propagation( ,)totrainaneuralnetworkwithoneortwo Rumelhart e t a l .1986a\\nhiddenlayers.Thecurrentandthirdwave,deeplearning,startedaround2006(Hinton\\ne t a l . e t a l . e t a l . ,;2006Bengio,;2007Ranzato,),andisjustnowappearinginbook 2007a\\nformasof2016.Theothertwowavessimilarlyappearedinbookformmuchlaterthan\\nthecorrespondingscientiﬁcactivityoccurred.\\n1 4', 'CHAPTER1.INTRODUCTION\\nTheearliestpredecessorsofmoderndeeplearningweresimplelinearmodels\\nmotivatedfromaneuroscientiﬁcperspective.Thesemodelsweredesignedto\\ntakeasetofninputvalues x 1,...,x nandassociatethemwithanoutput y.\\nThesemodelswouldlearnasetofweightsw 1,...,w nandcomputetheiroutput\\nf ( x w, ) =x 1w 1 + · · · +x nw n.Thisﬁrstwaveofneuralnetworksresearchwas\\nknownascybernetics,asillustratedinﬁgure.1.7\\nTheMcCulloch-PittsNeuron( ,)wasanearlymodel McCullochandPitts1943\\nofbrainfunction.Thislinearmodelcouldrecognizetwodiﬀerentcategoriesof\\ninputsbytestingwhether f ( x w, )ispositiveornegative.Ofcourse,forthemodel\\ntocorrespondtothedesireddeﬁnitionofthecategories,theweightsneededtobe\\nsetcorrectly.Theseweightscouldbesetbythehumanoperator.\\xa0Inthe1950s,\\ntheperceptron(Rosenblatt19581962,,)becametheﬁrstmodelthatcouldlearn\\ntheweightsdeﬁningthecategoriesgivenexamplesofinputsfromeachcategory.\\nThe adapt i v e l i near e l e m e n t(ADALINE),whichdatesfromaboutthesame\\ntime,simplyreturnedthevalueoff ( x )itselftopredictarealnumber(Widrow\\nandHoﬀ1960,),andcouldalsolearntopredictthesenumbersfromdata.\\nThesesimplelearningalgorithmsgreatlyaﬀectedthemodernlandscapeofma-\\nchinelearning.ThetrainingalgorithmusedtoadapttheweightsoftheADALINE\\nwasaspecialcaseofanalgorithmcalled st o c hast i c g r adi e n t desc e n t.Slightly\\nmodiﬁedversionsofthestochasticgradientdescentalgorithmremainthedominant\\ntrainingalgorithmsfordeeplearningmodelstoday.\\nModelsbasedonthef ( x w, )usedbytheperceptronandADALINEarecalled\\nl i near m o del s.Thesemodelsremainsomeofthemostwidelyusedmachine\\nlearningmodels,thoughinmanycasestheyare t r a i ne dindiﬀerentwaysthanthe\\noriginalmodelsweretrained.\\nLinearmodelshavemanylimitations.Mostfamously,theycannotlearnthe\\nXORfunction,where f ( [ 0, 1], w ) = 1and f ( [ 1, 0], w ) = 1butf ( [ 1, 1], w ) = 0\\nandf ( [ 0, 0], w ) = 0.Criticswhoobservedtheseﬂawsinlinearmodelscaused\\nabacklashagainstbiologicallyinspiredlearningingeneral(MinskyandPapert,\\n1969).Thiswastheﬁrstmajordipinthepopularityofneuralnetworks.\\nToday,neuroscienceisregardedasanimportantsourceofinspirationfordeep\\nlearningresearchers,butitisnolongerthepredominant guidefortheﬁeld.\\nThemainreasonforthediminishedrole\\xa0ofneuroscienceindeeplearning\\nresearchtodayisthatwesimplydonothaveenoughinformationaboutthebrain\\ntouseitasaguide.Toobtainadeepunderstandingoftheactualalgorithmsused\\nbythebrain,wewouldneedtobeabletomonitortheactivityof(atthevery\\nleast)thousandsofinterconnectedneuronssimultaneously.Becausewearenot\\nabletodothis,wearefarfromunderstandingevensomeofthemostsimpleand\\n1 5', 'CHAPTER1.INTRODUCTION\\nwell-studiedpartsofthebrain( ,). OlshausenandField2005\\nNeurosciencehasgivenusareasontohopethatasingledeeplearningalgorithm\\ncansolvemanydiﬀerenttasks.Neuroscientistshavefoundthatferretscanlearnto\\n“see”withtheauditoryprocessingregionoftheirbrainiftheirbrainsarerewired\\ntosendvisualsignalstothatarea(VonMelchner 2000 e t a l .,).Thissuggeststhat\\nmuchofthemammalianbrainmightuseasinglealgorithmtosolvemostofthe\\ndiﬀerenttasksthatthebrainsolves.Beforethishypothesis,machinelearning\\nresearchwasmorefragmented,withdiﬀerentcommunitiesofresearchersstudying\\nnaturallanguageprocessing,vision,motionplanningandspeechrecognition.Today,\\ntheseapplicationcommunitiesarestillseparate,butitiscommonfordeeplearning\\nresearchgroupstostudymanyorevenalloftheseapplicationareassimultaneously.\\nWeareabletodrawsomeroughguidelinesfromneuroscience.Thebasicideaof\\nhavingmanycomputational unitsthatbecomeintelligentonlyviatheirinteractions\\nwitheachotherisinspiredbythebrain.TheNeocognitron(Fukushima1980,)\\nintroducedapowerfulmodelarchitectureforprocessingimagesthatwasinspired\\nbythestructureofthemammalianvisualsystemandlaterbecamethebasis\\nforthemodernconvolutionalnetwork( ,),aswewillseein LeCun e t a l .1998b\\nsection.Mostneuralnetworkstodayarebasedonamodelneuroncalled 9.10\\nthe r e c t i ﬁed l i near uni t.TheoriginalCognitron(Fukushima1975,)introduced\\namorecomplicatedversionthatwashighlyinspiredbyourknowledgeofbrain\\nfunction.Thesimpliﬁedmodernversionwasdevelopedincorporatingideasfrom\\nmanyviewpoints,with ()and ()citing NairandHinton2010Glorot e t a l .2011a\\nneuroscienceasaninﬂuence,and ()citingmoreengineering- Jarrett e t a l .2009\\norientedinﬂuences.Whileneuroscienceisanimportantsourceofinspiration,it\\nneednotbetakenasarigidguide.Weknowthatactualneuronscomputevery\\ndiﬀerentfunctionsthanmodernrectiﬁedlinearunits,butgreaterneuralrealism\\nhasnotyetledtoanimprovementinmachinelearningperformance.Also,while\\nneurosciencehassuccessfullyinspiredseveralneuralnetwork a r c h i t e c t u r e s,we\\ndonotyetknowenoughaboutbiologicallearningforneurosciencetooﬀermuch\\nguidanceforthe l e a r ning a l g o r i t h m sweusetotrainthesearchitectures.\\nMediaaccountsoftenemphasizethesimilarityofdeeplearningtothebrain.\\nWhileitistruethatdeeplearningresearchersaremorelikelytocitethebrainasan\\ninﬂuencethanresearchersworkinginothermachinelearningﬁeldssuchaskernel\\nmachinesorBayesianstatistics,oneshouldnotviewdeeplearningasanattempt\\ntosimulatethebrain.Moderndeeplearningdrawsinspirationfrommanyﬁelds,\\nespeciallyappliedmathfundamentalslikelinearalgebra,probability,information\\ntheory,andnumericaloptimization. Whilesomedeeplearningresearcherscite\\nneuroscienceasanimportantsourceofinspiration,othersarenotconcernedwith\\n1 6', 'CHAPTER1.INTRODUCTION\\nneuroscienceatall.\\nItis\\xa0worth\\xa0notingthat\\xa0theeﬀorttounderstandhowthe\\xa0brainworkson\\nan\\xa0algorithmic\\xa0lev el\\xa0is\\xa0alive\\xa0andwell.This\\xa0endeavor\\xa0is\\xa0primarily\\xa0knownas\\n“computational neuroscience”andisaseparateﬁeldofstudyfromdeeplearning.\\nItiscommonforresearcherstomovebackandforthbetweenbothﬁelds.The\\nﬁeldofdeeplearningisprimarilyconcernedwithhowtobuildcomputersystems\\nthatareabletosuccessfullysolvetasksrequiringintelligence,whiletheﬁeldof\\ncomputational neuroscienceisprimarilyconcernedwithbuildingmoreaccurate\\nmodelsofhowthebrainactuallyworks.\\nInthe1980s,thesecondwaveofneuralnetworkresearchemergedingreat\\npartviaamovementcalled c o nnec t i o n i s mor par al l e l di st r i but e d pr o c e ss-\\ni ng( ,; ,).\\xa0Connectionism arosein Rumelhart e t a l .1986cMcClelland e t a l .1995\\nthecontextofcognitivescience.Cognitivescienceisaninterdisciplinaryapproach\\ntounderstandingthemind,combiningmultiplediﬀerentlevelsofanalysis.During\\ntheearly1980s,mostcognitivescientistsstudiedmodelsofsymbolicreasoning.\\nDespitetheirpopularity,symbolicmodelswerediﬃculttoexplainintermsof\\nhowthebraincouldactuallyimplementthemusingneurons.Theconnectionists\\nbegantostudymodelsofcognitionthatcouldactuallybegroundedinneural\\nimplementations(TouretzkyandMinton1985,),revivingmanyideasdatingback\\ntotheworkofpsychologistDonaldHebbinthe1940s(,).Hebb1949\\nThecentralideainconnectionism isthatalargenumberofsimplecomputational\\nunitscanachieveintelligentbehaviorwhennetworkedtogether.Thisinsight\\nappliesequallytoneuronsinbiologicalnervoussystemsandtohiddenunitsin\\ncomputational models.\\nSeveralkeyconceptsaroseduringtheconnectionism movementofthe1980s\\nthatremaincentraltotoday’sdeeplearning.\\nOneoftheseconceptsisthatof di st r i but e d r e pr e se n t at i o n(Hinton e t a l .,\\n1986).Thisistheideathateachinputtoasystemshouldberepresentedby\\nmanyfeatures,andeachfeatureshouldbeinvolvedintherepresentationofmany\\npossibleinputs.Forexample,supposewehaveavisionsystemthatcanrecognize\\ncars,trucks,andbirdsandtheseobjectscaneachbered,green,orblue.Oneway\\nofrepresentingtheseinputswouldbetohaveaseparateneuronorhiddenunit\\nthatactivatesforeachoftheninepossiblecombinations:redtruck,redcar,red\\nbird,greentruck,andsoon.Thisrequiresninediﬀerentneurons,andeachneuron\\nmustindependentlylearntheconceptofcolorandobjectidentity.Onewayto\\nimproveonthissituationistouseadistributedrepresentation,withthreeneurons\\ndescribingthecolorandthreeneuronsdescribingtheobjectidentity.Thisrequires\\nonlysixneuronstotalinsteadofnine,andtheneurondescribingrednessisableto\\n1 7', 'CHAPTER1.INTRODUCTION\\nlearnaboutrednessfromimagesofcars,trucksandbirds,notonlyfromimages\\nofonespeciﬁccategoryofobjects.\\xa0Theconceptofdistributedrepresentationis\\ncentraltothisbook,andwillbedescribedingreaterdetailinchapter.15\\nAnothermajoraccomplishmentoftheconnectionistmovementwasthesuc-\\ncessfuluseofback-propagation totraindeepneuralnetworkswithinternalrepre-\\nsentationsandthepopularization oftheback-propagation algorithm(Rumelhart\\ne t a l .,;,).Thisalgorithmhaswaxedandwanedinpopularity 1986aLeCun1987\\nbutasofthiswritingiscurrentlythedominantapproachtotrainingdeepmodels.\\nDuringthe1990s,researchersmadeimportantadvancesinmodelingsequences\\nwithneuralnetworks.()and ()identiﬁedsomeof Hochreiter1991Bengio e t a l .1994\\nthefundamentalmathematical diﬃcultiesinmodelinglongsequences,describedin\\nsection.10.7HochreiterandSchmidhuber1997()introducedthelongshort-term\\nmemoryorLSTMnetworktoresolvesomeofthesediﬃculties.Today,theLSTM\\niswidelyusedformanysequencemodelingtasks,includingmanynaturallanguage\\nprocessingtasksatGoogle.\\nThesecondwaveofneuralnetworksresearchlasteduntilthemid-1990s.Ven-\\nturesbasedonneuralnetworksandotherAItechnologiesbegantomakeunrealisti-\\ncallyambitiousclaimswhileseekinginvestments.WhenAIresearchdidnotfulﬁll\\ntheseunreasonableexpectations,investorsweredisappointed.Simultaneously,\\notherﬁeldsofmachinelearningmadeadvances.Kernelmachines(,Boser e t a l .\\n1992CortesandVapnik1995Schölkopf1999 Jor- ; ,; e t a l .,)andgraphicalmodels(\\ndan1998,)bothachievedgoodresultsonmanyimportanttasks.Thesetwofactors\\nledtoadeclineinthepopularityofneuralnetworksthatlasteduntil2007.\\nDuringthistime,neuralnetworkscontinuedtoobtainimpressiveperformance\\nonsometasks( ,; ,).TheCanadianInstitute LeCun e t a l .1998bBengio e t a l .2001\\nforAdvancedResearch(CIFAR)helpedtokeepneuralnetworksresearchalive\\nviaitsNeuralComputation andAdaptivePerception(NCAP)researchinitiative.\\nThisprogramunitedmachinelearningresearchgroupsledbyGeoﬀreyHinton\\natUniversityofToronto,YoshuaBengioatUniversityofMontreal,andYann\\nLeCunatNewYorkUniversity.TheCIFARNCAPresearchinitiativehada\\nmulti-disciplinarynaturethatalsoincludedneuroscientistsandexpertsinhuman\\nandcomputervision.\\nAtthispointintime,deepnetworksweregenerallybelievedtobeverydiﬃcult\\ntotrain.\\xa0Wenowknowthatalgorithmsthathaveexistedsincethe1980swork\\nquitewell,butthiswasnotapparentcirca2006.Theissueisperhapssimplythat\\nthesealgorithmsweretoocomputationally costlytoallowmuchexperimentation\\nwiththehardwareavailableatthetime.\\nThethirdwaveofneuralnetworksresearchbeganwithabreakthrough in\\n1 8', 'CHAPTER1.INTRODUCTION\\n2006.GeoﬀreyHintonshowedthatakindofneuralnetworkcalledadeepbelief\\nnetworkcouldbeeﬃcientlytrainedusingastrategycalledgreedylayer-wisepre-\\ntraining( ,),whichwillbedescribedinmoredetailinsection. Hinton e t a l .2006 15.1\\nTheotherCIFAR-aﬃliatedresearchgroupsquicklyshowedthatthesamestrategy\\ncouldbeusedtotrainmanyotherkindsofdeepnetworks( ,; Bengio e t a l .2007\\nRanzato 2007a e t a l .,)andsystematicallyhelpedtoimprovegeneralization on\\ntestexamples.Thiswaveofneuralnetworksresearchpopularizedtheuseofthe\\nterm“deeplearning”toemphasizethatresearcherswerenowabletotraindeeper\\nneuralnetworksthanhadbeenpossiblebefore,andtofocusattentiononthe\\ntheoreticalimportanceofdepth( ,; , BengioandLeCun2007DelalleauandBengio\\n2011Pascanu2014aMontufar2014 ; e t a l .,; e t a l .,).Atthistime,deepneural\\nnetworksoutperformedcompetingAIsystemsbasedonothermachinelearning\\ntechnologiesaswellashand-designedfunctionality.Thisthirdwaveofpopularity\\nofneuralnetworkscontinuestothetimeofthiswriting,thoughthefocusofdeep\\nlearningresearchhaschangeddramatically withinthetimeofthiswave.The\\nthirdwavebeganwithafocusonnewunsupervisedlearningtechniquesandthe\\nabilityofdeepmodelstogeneralizewellfromsmalldatasets,buttodaythereis\\nmoreinterestinmucholdersupervisedlearningalgorithmsandtheabilityofdeep\\nmodelstoleveragelargelabeleddatasets.\\n1 . 2 . 2 In creasin g D a t a s et S i zes\\nOnemaywonderwhydeeplearninghasonlyrecentlybecomerecognizedasa\\ncrucialtechnologythoughtheﬁrstexperimentswithartiﬁcialneuralnetworkswere\\nconductedinthe1950s.Deeplearninghasbeensuccessfullyusedincommercial\\napplicationssincethe1990s,butwasoftenregardedasbeingmoreofanartthan\\natechnologyandsomethingthatonlyanexpertcoulduse,untilrecently.Itistrue\\nthatsomeskillisrequiredtogetgoodperformancefromadeeplearningalgorithm.\\nFortunately,theamountofskillrequiredreducesastheamountoftrainingdata\\nincreases.Thelearningalgorithmsreachinghumanperformanceoncomplextasks\\ntodayarenearlyidenticaltothelearningalgorithmsthatstruggledtosolvetoy\\nproblemsinthe1980s,thoughthemodelswetrainwiththesealgorithmshave\\nundergonechangesthatsimplifythetrainingofverydeeparchitectures.Themost\\nimportantnewdevelopmentisthattodaywecanprovidethesealgorithmswith\\ntheresourcestheyneedtosucceed.Figureshowshowthesizeofbenchmark 1.8\\ndatasetshasincreasedremarkablyovertime.Thistrendisdrivenbytheincreasing\\ndigitizationofsociety.Asmoreandmoreofouractivitiestakeplaceoncomputers,\\nmoreandmoreofwhatwedoisrecorded.Asourcomputersareincreasingly\\nnetworkedtogether,itbecomeseasiertocentralizetheserecordsandcuratethem\\n1 9', 'CHAPTER1.INTRODUCTION\\nintoadatasetappropriateformachinelearningapplications.Theageof“Big\\nData”hasmademachinelearningmucheasierbecausethekeyburdenofstatistical\\nestimation—generalizingwelltonewdataafterobservingonlyasmallamount\\nofdata—hasbeenconsiderablylightened.Asof2016,aroughruleofthumb\\nisthatasuperviseddeeplearningalgorithmwillgenerallyachieveacceptable\\nperformancewitharound5,000labeledexamplespercategory,andwillmatchor\\nexceedhumanperformancewhentrainedwithadatasetcontainingatleast10\\nmillionlabeledexamples.Workingsuccessfullywithdatasetssmallerthanthisis\\nanimportantresearcharea,focusinginparticularonhowwecantakeadvantage\\noflargequantitiesofunlabeledexamples,withunsupervisedorsemi-supervised\\nlearning.\\n1 . 2 . 3 In creasin g Mo d el S i zes\\nAnotherkeyreasonthatneuralnetworksarewildlysuccessfultodayafterenjoying\\ncomparativelylittlesuccesssincethe1980sisthatwehavethecomputational\\nresourcestorunmuchlargermodelstoday.Oneofthemaininsightsofconnection-\\nismisthatanimalsbecomeintelligentwhenmanyoftheirneuronsworktogether.\\nAnindividualneuronorsmallcollectionofneuronsisnotparticularlyuseful.\\nBiologicalneuronsarenotespeciallydenselyconnected.Asseeninﬁgure,1.10\\nourmachinelearningmodelshavehadanumberofconnectionsperneuronthat\\nwaswithinanorderofmagnitudeofevenmammalianbrainsfordecades.\\nIntermsofthetotalnumberofneurons,neuralnetworkshavebeenastonishingly\\nsmalluntilquiterecently,asshowninﬁgure.Sincetheintroductionofhidden 1.11\\nunits,artiﬁcialneuralnetworkshavedoubledinsizeroughlyevery2.4years.This\\ngrowthisdrivenbyfastercomputerswithlargermemoryandbytheavailability\\noflargerdatasets.Largernetworksareabletoachievehigheraccuracyonmore\\ncomplextasks.Thistrendlookssettocontinuefordecades.Unlessnewtechnologies\\nallowfasterscaling,artiﬁcialneuralnetworkswillnothavethesamenumberof\\nneuronsasthehumanbrainuntilatleastthe2050s.Biologicalneuronsmay\\nrepresentmorecomplicatedfunctionsthancurrentartiﬁcialneurons,sobiological\\nneuralnetworksmaybeevenlargerthanthisplotportrays.\\nInretrospect,itisnotparticularlysurprisingthatneuralnetworkswithfewer\\nneuronsthanaleechwereunabletosolvesophisticatedartiﬁcialintelligenceprob-\\nlems.Eventoday’snetworks,whichweconsiderquitelargefromacomputational\\nsystemspointofview,aresmallerthanthenervoussystemofevenrelatively\\nprimitivevertebrateanimalslikefrogs.\\nTheincreaseinmodelsizeovertime,duetotheavailabilityoffasterCPUs,\\n2 0', 'CHAPTER1.INTRODUCTION\\n1900 1950 198520002015\\nYear100101102103104105106107108109Datasetsize(numberexamples)\\nIrisMNISTPublicSVHN\\nImageNet\\nCIFAR-10ImageNet10k\\nILSVRC  2014Sports-1M\\nRotatedTvs.C Tvs.Gvs.FCriminalsCanadianHansard\\nWMT\\nFigure1.8:Datasetsizeshaveincreasedgreatlyovertime.Intheearly1900s,statisticians\\nstudieddatasetsusinghundredsorthousandsofmanuallycompiledmeasurements(,Garson\\n1900Gosset1908Anderson1935Fisher1936 ;,;,;,).Inthe1950sthrough1980s,thepioneers\\nofbiologicallyinspiredmachinelearningoftenworkedwithsmall,syntheticdatasets,such\\naslow-resolutionbitmapsofletters,thatweredesignedtoincurlowcomputationalcostand\\ndemonstratethatneuralnetworkswereabletolearnspeciﬁckindsoffunctions(Widrow\\nandHoﬀ1960Rumelhart1986b ,; e t a l .,).Inthe1980sand1990s,machinelearning\\nbecamemorestatisticalinnatureandbegantoleveragelargerdatasetscontainingtens\\nofthousandsofexamplessuchastheMNISTdataset(showninﬁgure)ofscans 1.9\\nofhandwrittennumbers( ,).Intheﬁrstdecadeofthe2000s,more LeCun e t a l .1998b\\nsophisticateddatasetsofthissamesize,suchastheCIFAR-10dataset(Krizhevskyand\\nHinton2009,)continuedtobeproduced.Towardtheendofthatdecadeandthroughout\\ntheﬁrsthalfofthe2010s,signiﬁcantlylargerdatasets,containinghundredsofthousands\\ntotensofmillionsofexamples,completelychangedwhatwaspossiblewithdeeplearning.\\nThesedatasetsincludedthepublicStreetViewHouseNumbersdataset( , Netzer e t a l .\\n2011),variousversionsoftheImageNetdataset( ,,; Deng e t a l .20092010aRussakovsky\\ne t a l . e t a l . ,),andtheSports-1Mdataset( 2014a Karpathy,).Atthetopofthe 2014\\ngraph,weseethatdatasetsoftranslatedsentences,suchasIBM’sdatasetconstructed\\nfromtheCanadianHansard( ,)andtheWMT2014EnglishtoFrench Brown e t a l .1990\\ndataset(Schwenk2014,)aretypicallyfaraheadofotherdatasetsizes.\\n2 1', 'CHAPTER1.INTRODUCTION\\nFigure1.9:ExampleinputsfromtheMNISTdataset.The“NIST”standsforNational\\nInstituteofStandardsandTechnology,theagencythatoriginallycollectedthisdata.\\nThe“M”standsfor“modiﬁed,”sincethedatahasbeenpreprocessedforeasierusewith\\nmachinelearningalgorithms.TheMNISTdatasetconsistsofscansofhandwrittendigits\\nandassociatedlabelsdescribingwhichdigit0–9iscontainedineachimage.Thissimple\\nclassiﬁcationproblemisoneofthesimplestandmostwidelyusedtestsindeeplearning\\nresearch.Itremainspopulardespitebeingquiteeasyformoderntechniquestosolve.\\nGeoﬀreyHintonhasdescribeditas“the d r o s o p h i l aofmachinelearning,”meaningthat\\nitallowsmachinelearningresearcherstostudytheiralgorithmsincontrolledlaboratory\\nconditions,muchasbiologistsoftenstudyfruitﬂies.\\n2 2', 'CHAPTER1.INTRODUCTION\\ntheadventofgeneralpurposeGPUs(describedinsection),fasternetwork 12.1.2\\nconnectivityandbettersoftwareinfrastructurefordistributedcomputing,isoneof\\nthemostimportanttrendsinthehistoryofdeeplearning.Thistrendisgenerally\\nexpectedtocontinuewellintothefuture.\\n1 . 2 . 4 In creasin g A ccu ra cy , Co m p l e xi t y a n d Rea l - W o rl d Im p a ct\\nSincethe1980s,deeplearninghasconsistentlyimprovedinitsabilitytoprovide\\naccuraterecognitionorprediction.Moreover,deeplearninghasconsistentlybeen\\nappliedwithsuccesstobroaderandbroadersetsofapplications.\\nTheearliestdeepmodelswereusedtorecognizeindividualobjectsintightly\\ncropped,extremelysmallimages( ,).Sincethentherehas Rumelhart e t a l .1986a\\nbeenagradualincreaseinthesizeofimagesneuralnetworkscouldprocess.Modern\\nobjectrecognitionnetworksprocessrichhigh-resolutionphotographs anddonot\\nhavearequirementthatthephotobecroppedneartheobjecttoberecognized\\n( ,).Similarly,theearliestnetworkscouldonlyrecognize Krizhevsky e t a l .2012\\ntwokindsofobjects(orinsomecases,theabsenceorpresenceofasinglekindof\\nobject),whilethesemodernnetworkstypicallyrecognizeatleast1,000diﬀerent\\ncategoriesofobjects.\\xa0ThelargestcontestinobjectrecognitionistheImageNet\\nLargeScaleVisualRecognitionChallenge(ILSVRC)heldeachyear.Adramatic\\nmomentinthemeteoricriseofdeeplearningcamewhenaconvolutionalnetwork\\nwonthischallengefortheﬁrsttimeandbyawidemargin,bringingdownthe\\nstate-of-the-art top-5errorratefrom26.1%to15.3%( ,), Krizhevsky e t a l .2012\\nmeaningthattheconvolutionalnetworkproducesarankedlistofpossiblecategories\\nforeachimageandthecorrectcategoryappearedintheﬁrstﬁveentriesofthis\\nlistforallbut15.3%ofthetestexamples.Sincethen,thesecompetitionsare\\nconsistentlywonbydeepconvolutionalnets,andasofthiswriting,advancesin\\ndeeplearninghavebroughtthelatesttop-5errorrateinthiscontestdownto3.6%,\\nasshowninﬁgure.1.12\\nDeeplearninghasalsohadadramaticimpactonspeechrecognition.After\\nimprovingthroughoutthe1990s,theerrorratesforspeechrecognitionstagnated\\nstartinginabout2000.Theintroductionofdeeplearning(,; Dahl e t a l .2010Deng\\ne t a l . e t a l . e t a l . ,;2010bSeide,;2011Hinton,)tospeechrecognitionresulted 2012a\\ninasuddendropoferrorrates,withsomeerrorratescutinhalf.Wewillexplore\\nthishistoryinmoredetailinsection.12.3\\nDeepnetworkshavealsohadspectacularsuccessesforpedestriandetectionand\\nimagesegmentation( ,; Sermanet e t a l .2013Farabet2013Couprie e t a l .,; e t a l .,\\n2013)andyieldedsuperhumanperformanceintraﬃcsignclassiﬁcation(Ciresan\\n2 3', 'CHAPTER1.INTRODUCTION\\n1 9 5 0 1 9 8 5 2 0 0 0 2 0 1 5\\nY e a r1 011 021 031 04C o nne c t i o ns p e r ne ur o n\\n12\\n34\\n567\\n89\\n1 0\\nF r ui t ﬂyMo useC a tH um a n\\nFigure1.10:Initially,thenumberofconnectionsbetweenneuronsinartiﬁcialneural\\nnetworkswaslimitedbyhardwarecapabilities.Today,thenumberofconnectionsbetween\\nneuronsismostlyadesignconsideration.Someartiﬁcialneuralnetworkshavenearlyas\\nmanyconnectionsperneuronasacat,anditisquitecommonforotherneuralnetworks\\ntohaveasmanyconnectionsperneuronassmallermammalslikemice.Eventhehuman\\nbraindoesnothaveanexorbitantamountofconnectionsperneuron.Biologicalneural\\nnetworksizesfrom (). Wikipedia2015\\n1.Adaptivelinearelement( ,) WidrowandHoﬀ1960\\n2.Neocognitron(Fukushima1980,)\\n3.GPU-acceleratedconvolutionalnetwork( ,) Chellapilla e t al.2006\\n4.DeepBoltzmannmachine(SalakhutdinovandHinton2009a,)\\n5.Unsupervisedconvolutionalnetwork( ,) Jarrett e t al.2009\\n6.GPU-acceleratedmultilayerperceptron( ,) Ciresan e t al.2010\\n7.Distributedautoencoder(,) Le e t al.2012\\n8.Multi-GPUconvolutionalnetwork( ,) Krizhevsky e t al.2012\\n9.COTSHPCunsupervisedconvolutionalnetwork( ,) Coates e t al.2013\\n10.GoogLeNet( ,) Szegedy e t al.2014a\\n2 4', 'CHAPTER1.INTRODUCTION\\ne t a l .,).2012\\nAtthesametimethatthescaleandaccuracyofdeepnetworkshasincreased,\\nsohasthecomplexityofthetasksthattheycansolve. () Goodfellow e t a l .2014d\\nshowedthatneuralnetworkscouldlearntooutputanentiresequenceofcharacters\\ntranscribedfromanimage,ratherthanjustidentifyingasingleobject.Previously,\\nitwaswidelybelievedthatthiskindoflearningrequiredlabelingoftheindividual\\nelementsofthesequence( ,).Recurrentneuralnetworks, GülçehreandBengio2013\\nsuchastheLSTMsequencemodelmentionedabove,arenowusedtomodel\\nrelationshipsbetween s e q u e nc e s s e q u e nc e s andother ratherthanjustﬁxedinputs.\\nThissequence-to-sequencelearningseemstobeonthecuspofrevolutionizing\\nanotherapplication:machinetranslation(Sutskever2014Bahdanau e t a l .,; e t a l .,\\n2015).\\nThistrendofincreasingcomplexityhasbeenpushedtoitslogicalconclusion\\nwiththeintroductionofneuralTuringmachines(Graves2014a e t a l .,)thatlearn\\ntoreadfrommemorycellsandwritearbitrarycontenttomemorycells.Such\\nneuralnetworkscanlearnsimpleprogramsfromexamplesofdesiredbehavior.For\\nexample,theycanlearntosortlistsofnumbersgivenexamplesofscrambledand\\nsortedsequences.Thisself-programming technologyisinitsinfancy,butinthe\\nfuturecouldinprinciplebeappliedtonearlyanytask.\\nAnothercrowningachievementofdeeplearningisitsextensiontothedomainof\\nr e i nf o r c e m e n t l e ar ni ng.Inthecontextofreinforcementlearning,anautonomous\\nagentmustlearntoperformataskbytrialanderror,withoutanyguidancefrom\\nthehumanoperator.DeepMinddemonstratedthatareinforcementlearningsystem\\nbasedondeeplearningiscapableoflearningtoplayAtarivideogames,reaching\\nhuman-levelperformanceonmanytasks(,).Deeplearninghas Mnih e t a l .2015\\nalsosigniﬁcantlyimprovedtheperformanceofreinforcementlearningforrobotics\\n(,). Finn e t a l .2015\\nManyoftheseapplicationsofdeeplearningarehighlyproﬁtable.Deeplearning\\nisnowused\\xa0bymanytoptechnologycompanies\\xa0includi ngGoogle,\\xa0Microsoft,\\nFacebook,IBM,Baidu,Apple,Adobe,Netﬂix,NVIDIAandNEC.\\nAdvancesindeeplearninghavealsodependedheavilyonadvancesinsoftware\\ninfrastructure.SoftwarelibrariessuchasTheano( ,; Bergstra e t a l .2010Bastien\\ne t a l . e t a l . ,),PyLearn2( 2012 Goodfellow,),Torch( ,), 2013c Collobert e t a l .2011b\\nDistBelief(,),Caﬀe(,),MXNet(,),and Dean e t a l .2012 Jia2013 Chen e t a l .2015\\nTensorFlow(,)haveallsupportedimportantresearchprojectsor Abadi e t a l .2015\\ncommercialproducts.\\nDeeplearninghasalsomadecontributionsbacktoothersciences.Modern\\nconvolutionalnetworksforobjectrecognitionprovideamodelofvisualprocessing\\n2 5', 'CHAPTER1.INTRODUCTION\\nthatneuroscientistscanstudy(,).Deeplearningalsoprovidesuseful DiCarlo2013\\ntoolsforprocessingmassiveamountsofdataandmakingusefulpredictionsin\\nscientiﬁcﬁelds.Ithasbeensuccessfullyusedtopredicthowmoleculeswillinteract\\ninordertohelppharmaceutical companiesdesignnewdrugs(,), Dahl e t a l .2014\\ntosearchforsubatomicparticles(,),andtoautomatically parse Baldi e t a l .2014\\nmicroscopeimagesusedtoconstructa3-Dmapofthehumanbrain(Knowles-\\nBarley2014 e t a l .,).Weexpectdeeplearningtoappearinmoreandmorescientiﬁc\\nﬁeldsinthefuture.\\nInsummary,deeplearningisanapproachtomachinelearningthathasdrawn\\nheavilyonourknowledgeofthehumanbrain,statisticsandappliedmathasit\\ndevelopedoverthepastseveraldecades.Inrecentyears,ithasseentremendous\\ngrowthinitspopularityandusefulness,dueinlargeparttomorepowerfulcom-\\nputers,largerdatasetsandtechniquestotraindeepernetworks.Theyearsahead\\narefullofchallengesandopportunitiestoimprovedeeplearningevenfurtherand\\nbringittonewfrontiers.\\n2 6', 'CHAPTER1.INTRODUCTION\\n1950 198520002015 2056\\nYear10− 210− 1100101102103104105106107108109101 0101 1Numberofneurons(logarithmicscale)\\n123\\n456\\n78\\n91011\\n121314\\n151617\\n181920\\nSpongeRoundwormLeechAntBeeFrogOctopusHuman\\nFigure1.11:Sincetheintroductionofhiddenunits,artiﬁcialneuralnetworkshavedoubled\\ninsizeroughlyevery2.4years.Biologicalneuralnetworksizesfrom (). Wikipedia2015\\n1.Perceptron(,,) Rosenblatt19581962\\n2.Adaptivelinearelement( ,) WidrowandHoﬀ1960\\n3.Neocognitron(Fukushima1980,)\\n4.Earlyback-propagationnetwork( ,) Rumelhart e t al.1986b\\n5.Recurrentneuralnetworkforspeechrecognition(RobinsonandFallside1991,)\\n6.Multilayerperceptronforspeechrecognition( ,) Bengio e t al.1991\\n7.Meanﬁeldsigmoidbeliefnetwork(,) Saul e t al.1996\\n8.LeNet-5( ,) LeCun e t al.1998b\\n9.Echostatenetwork( ,) JaegerandHaas2004\\n10.Deepbeliefnetwork( ,) Hinton e t al.2006\\n11.GPU-acceleratedconvolutionalnetwork( ,) Chellapilla e t al.2006\\n12.DeepBoltzmannmachine(SalakhutdinovandHinton2009a,)\\n13.GPU-accelerateddeepbeliefnetwork(,) Raina e t al.2009\\n14.Unsupervisedconvolutionalnetwork( ,) Jarrett e t al.2009\\n15.GPU-acceleratedmultilayerperceptron( ,) Ciresan e t al.2010\\n16.OMP-1network( ,) CoatesandNg2011\\n17.Distributedautoencoder(,) Le e t al.2012\\n18.Multi-GPUconvolutionalnetwork( ,) Krizhevsky e t al.2012\\n19.COTSHPCunsupervisedconvolutionalnetwork( ,) Coates e t al.2013\\n20.GoogLeNet( ,) Szegedy e t al.2014a\\n2 7', 'CHAPTER1.INTRODUCTION\\n2010 2011 2012 2013 2014 2015\\nYear000 .005 .010 .015 .020 .025 .030 .ILSVRC  classiﬁcationerrorrate\\nFigure1.12:SincedeepnetworksreachedthescalenecessarytocompeteintheImageNet\\nLargeScaleVisualRecognitionChallenge,theyhaveconsistentlywonthecompetition\\neveryyear,andyieldedlowerandlowererrorrateseachtime.\\xa0DatafromRussakovsky\\ne t a l . e t a l . ()and2014b He().2015\\n2 8']}, 'DeepLearning-chapter2.pdf': {'text': ['C h a p t e r 2\\nL i n e ar A l ge b ra\\nLinearalgebraisabranchofmathematics thatiswidelyusedthroughoutscience\\nandengineering.However,becauselinearalgebraisaformofcontinuousrather\\nthandiscretemathematics,manycomputerscientistshavelittleexperiencewithit.\\nAgoodunderstandingoflinearalgebraisessentialforunderstandingandworking\\nwithmanymachinelearningalgorithms,especiallydeeplearningalgorithms.We\\nthereforeprecedeourintroductiontodeeplearningwithafocusedpresentationof\\nthekeylinearalgebraprerequisites.\\nIfyouarealreadyfamiliarwithlinearalgebra,feelfreetoskipthischapter.If\\nyouhavepreviousexperiencewiththeseconceptsbutneedadetailedreference\\nsheettoreviewkeyformulas,werecommend TheMatrixCookbook(Petersenand\\nPedersen2006,).Ifyouhavenoexposureatalltolinearalgebra,thischapter\\nwillteachyouenoughtoreadthisbook,butwehighlyrecommendthatyoualso\\nconsultanotherresourcefocusedexclusivelyonteachinglinearalgebra,suchas\\nShilov1977().Thischapterwillcompletelyomitmanyimportantlinearalgebra\\ntopicsthatarenotessentialforunderstandingdeeplearning.\\n2.1Scalars,Vectors,MatricesandTensors\\nThestudyoflinearalgebrainvolvesseveraltypesofmathematical objects:\\n•Scalars:Ascalarisjustasinglenumber,incontrasttomostoftheother\\nobjectsstudiedinlinearalgebra,whichareusuallyarraysofmultiplenumbers.\\nWewritescalarsinitalics.Weusuallygivescalarslower-casevariablenames.\\nWhenweintroducethem,wespecifywhatkindofnumbertheyare.For\\n31', 'CHAPTER2.LINEARALGEBRA\\nexample,wemightsay“Let s∈ Rbetheslopeoftheline,”whiledeﬁninga\\nreal-valuedscalar,or“Let n∈ Nbethenumberofunits,”whiledeﬁninga\\nnaturalnumberscalar.\\n•Vectors:\\xa0Avectorisanarrayofnumbers.Thenumbersarearrangedin\\norder.Wecanidentifyeachindividualnumberbyitsindexinthatordering.\\nTypicallywegivevectorslowercasenameswritteninboldtypeface,such\\nasx.Theelementsofthevectorareidentiﬁedbywritingitsnameinitalic\\ntypeface,withasubscript.Theﬁrstelementofxis x 1,thesecondelement\\nis x 2andsoon.Wealsoneedtosaywhatkindofnumbersarestoredin\\nthevector.Ifeachelementisin R,andthevectorhas nelements,thenthe\\nvectorliesinthesetformedbytakingtheCartesianproductof R ntimes,\\ndenotedas Rn.Whenweneedtoexplicitlyidentifytheelementsofavector,\\nwewritethemasacolumnenclosedinsquarebrackets:\\nx=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0x 1\\nx 2\\n...\\nx n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb. (2.1)\\nWecanthinkofvectorsasidentifyingpointsinspace,witheachelement\\ngivingthecoordinatealongadiﬀerentaxis.\\nSometimesweneedtoindexasetofelementsofavector.Inthiscase,we\\ndeﬁneasetcontainingtheindicesandwritethesetasasubscript.For\\nexample,toaccess x 1, x 3and x 6,wedeﬁnetheset S={1 ,3 ,6}andwrite\\nx S.Weusethe−signtoindexthecomplementofaset.Forexamplex − 1is\\nthevectorcontainingallelementsofxexceptfor x 1,andx − Sisthevector\\ncontainingalloftheelementsofexceptforx x 1, x 3and x 6.\\n•Matrices:Amatrixisa2-Darrayofnumbers,soeachelementisidentiﬁed\\nbytwoindicesinsteadofjustone.Weusuallygivematricesupper-case\\nvariablenameswithboldtypeface,suchasA.Ifareal-valuedmatrixAhas\\naheightof mandawidthof n,thenwesaythatA∈ Rm n ×.\\xa0Weusually\\nidentifytheelementsofamatrixusingitsnameinitalicbutnotboldfont,\\nandtheindicesarelistedwithseparatingcommas.Forexample, A 1 1 ,isthe\\nupperleftentryofAand A m , nisthebottomrightentry.Wecanidentifyall\\nofthenumberswithverticalcoordinate ibywritinga“”forthehorizontal :\\ncoordinate.Forexample,A i , :denotesthehorizontalcrosssectionofAwith\\nverticalcoordinate i.Thisisknownasthe i-throwofA.Likewise,A : , iis\\n3 2', 'CHAPTER2.LINEARALGEBRA\\nA =\\uf8ee\\n\\uf8f0A 1 1 , A 1 2 ,\\nA 2 1 , A 2 2 ,\\nA 3 1 , A 3 2 ,\\uf8f9\\n\\uf8fb ⇒ A\\ue021=\\ue025A 1 1 , A 2 1 , A 3 1 ,\\nA 1 2 , A 2 2 , A 3 2 ,\\ue026\\nFigure2.1:Thetransposeofthematrixcanbethoughtofasamirrorimageacrossthe\\nmaindiagonal.\\nthe-thof.Whenweneedtoexplicitlyidentifytheelementsof icolumnA\\namatrix,wewritethemasanarrayenclosedinsquarebrackets:\\n\\ue014A 1 1 , A 1 2 ,\\nA 2 1 , A 2 2 ,\\ue015\\n. (2.2)\\nSometimeswemayneedtoindexmatrix-valuedexpressionsthatarenotjust\\nasingleletter.Inthiscase,weusesubscriptsaftertheexpression,butdo\\nnotconvertanythingtolowercase.Forexample, f(A) i , jgiveselement( i , j)\\nofthematrixcomputedbyapplyingthefunctionto. fA\\n•Tensors:Insomecaseswewillneedanarraywithmorethantwoaxes.\\nInthegeneralcase,anarrayofnumbersarrangedonaregulargridwitha\\nvariablenumberofaxesisknownasatensor.Wedenoteatensornamed“A”\\nwiththistypeface: A.Weidentifytheelementof Aatcoordinates ( i , j , k)\\nbywriting A i , j , k.\\nOneimportantoperationonmatricesisthetranspose.\\xa0Thetransposeofa\\nmatrixisthemirrorimageofthematrixacrossadiagonalline,calledthemain\\ndiagonal,runningdownandtotheright,startingfromitsupperleftcorner.See\\nﬁgureforagraphicaldepictionofthisoperation.Wedenotethetransposeofa 2.1\\nmatrixasAA\\ue03e,anditisdeﬁnedsuchthat\\n(A\\ue03e) i , j= A j , i . (2.3)\\nVectorscanbethoughtofasmatricesthatcontainonlyonecolumn.The\\ntransposeofavectoristhereforeamatrixwithonlyonerow.Sometimeswe\\n3 3', 'CHAPTER2.LINEARALGEBRA\\ndeﬁneavectorbywritingoutitselementsinthetextinlineasarowmatrix,\\nthenusingthetransposeoperatortoturnitintoastandardcolumnvector,e.g.,\\nx= [ x 1 , x 2 , x 3]\\ue03e.\\nAscalarcanbethoughtofasamatrixwithonlyasingleentry.Fromthis,we\\ncanseethatascalarisitsowntranspose: a a= \\ue03e.\\nWecanaddmatricestoeachother,aslongastheyhavethesameshape,just\\nbyaddingtheircorrespondingelements: whereCAB = + C i , j= A i , j+ B i , j .\\nWecanalsoaddascalartoamatrixormultiplyamatrixbyascalar,just\\nbyperformingthatoperationoneachelementofamatrix:D= a·B+ cwhere\\nD i , j= a B· i , j+ c.\\nInthecontextofdeeplearning,wealsousesomelessconventionalnotation.\\nWeallowtheadditionofmatrixandavector,yieldinganothermatrix:C=A+b,\\nwhere C i , j= A i , j+ b j.Inotherwords,thevectorbisaddedtoeachrowofthe\\nmatrix.Thisshorthandeliminatestheneedtodeﬁneamatrixwithbcopiedinto\\neachrowbeforedoingtheaddition.Thisimplicitcopyingofbtomanylocations\\niscalled .broadcasting\\n2.2MultiplyingMatricesandVectors\\nOneofthemostimportantoperationsinvolvingmatricesismultiplication oftwo\\nmatrices.ThematrixproductofmatricesAandBisathirdmatrixC.In\\norderforthisproducttobedeﬁned,Amusthavethesamenumberofcolumnsas\\nBhasrows.IfAisofshape m n×andBisofshape n p×,thenCisofshape\\nm p×.Wecanwritethematrixproductjustbyplacingtwoormorematrices\\ntogether,e.g.\\nCAB= . (2.4)\\nTheproductoperationisdeﬁnedby\\nC i , j=\\ue058\\nkA i , k B k, j . (2.5)\\nNotethatthestandardproductoftwomatricesisjustamatrixcontaining not\\ntheproductoftheindividualelements.Suchanoperationexistsandiscalledthe\\nelement-wiseproductHadamardproduct or ,andisdenotedas.AB\\ue00c\\nThedotproductbetweentwovectorsxandyofthesamedimensionality\\nisthematrixproductx\\ue03ey.WecanthinkofthematrixproductC=ABas\\ncomputing C i , jasthedotproductbetweenrowofandcolumnof. iA jB\\n3 4', 'CHAPTER2.LINEARALGEBRA\\nMatrixproductoperationshavemanyusefulpropertiesthatmakemathematical\\nanalysis\\xa0ofmatrices\\xa0moreconvenient.For\\xa0example,\\xa0matrix\\xa0m ultiplication\\xa0is\\ndistributive:\\nABCABAC (+) = + . (2.6)\\nItisalsoassociative:\\nABCABC ( ) = ( ) . (2.7)\\nMatrixmultiplication iscommutative(thecondition not AB=BAdoesnot\\nalwayshold),unlikescalarmultiplication. However,thedotproductbetweentwo\\nvectorsiscommutative:\\nx\\ue03eyy= \\ue03ex . (2.8)\\nThetransposeofamatrixproducthasasimpleform:\\n( )AB\\ue03e= B\\ue03eA\\ue03e. (2.9)\\nThisallowsustodemonstrateequation,byexploitingthefactthatthevalue 2.8\\nofsuchaproductisascalarandthereforeequaltoitsowntranspose:\\nx\\ue03ey=\\ue010\\nx\\ue03ey\\ue011\\ue03e\\n= y\\ue03ex . (2.10)\\nSincethefocusofthistextbookisnotlinearalgebra,wedonotattemptto\\ndevelopacomprehensivelistofusefulpropertiesofthematrixproducthere,but\\nthereadershouldbeawarethatmanymoreexist.\\nWenowknowenoughlinearalgebranotationtowritedownasystemoflinear\\nequations:\\nAxb= (2.11)\\nwhereA∈ Rm n ×isaknownmatrix,b∈ Rmisaknownvector,andx∈ Rnisa\\nvectorofunknownvariableswewouldliketosolvefor.Eachelement x iofxisone\\noftheseunknownvariables.EachrowofAandeachelementofbprovideanother\\nconstraint.Wecanrewriteequationas:2.11\\nA 1 : ,x= b 1 (2.12)\\nA 2 : ,x= b 2 (2.13)\\n. . . (2.14)\\nA m , :x= b m (2.15)\\nor,evenmoreexplicitly,as:\\nA 1 1 , x 1+A 1 2 , x 2+ +···A 1 , n x n= b 1 (2.16)\\n3 5', 'CHAPTER2.LINEARALGEBRA\\n\\uf8ee\\n\\uf8f0100\\n010\\n001\\uf8f9\\n\\uf8fb\\nFigure2.2:Exampleidentitymatrix:ThisisI 3.\\nA 2 1 , x 1+A 2 2 , x 2+ +···A 2 , n x n= b 2 (2.17)\\n. . . (2.18)\\nA m , 1 x 1+A m , 2 x 2+ +···A m , n x n= b m . (2.19)\\nMatrix-vectorproductnotationprovidesamorecompactrepresentationfor\\nequationsofthisform.\\n2.3IdentityandInverseMatrices\\nLinearalgebraoﬀersapowerfultoolcalledmatrixinversionthatallowsusto\\nanalyticallysolveequationformanyvaluesof. 2.11 A\\nTodescribematrixinversion,weﬁrstneedtodeﬁnetheconceptofanidentity\\nmatrix.Anidentitymatrixisamatrixthatdoesnotchangeanyvectorwhenwe\\nmultiplythatvectorbythatmatrix.Wedenotetheidentitymatrixthatpreserves\\nn-dimensionalvectorsasI n.Formally,I n∈ Rn n ×,and\\n∀∈x Rn,I nxx= . (2.20)\\nThestructureoftheidentitymatrixissimple:alloftheentriesalongthemain\\ndiagonalare1,whilealloftheotherentriesarezero.Seeﬁgureforanexample.2.2\\nThematrixinverseofAisdenotedasA− 1,anditisdeﬁnedasthematrix\\nsuchthat\\nA− 1AI= n . (2.21)\\nWecannowsolveequationbythefollowingsteps: 2.11\\nAxb= (2.22)\\nA− 1AxA= − 1b (2.23)\\nI nxA= − 1b (2.24)\\n3 6', 'CHAPTER2.LINEARALGEBRA\\nxA= − 1b . (2.25)\\nOfcourse,thisprocessdependsonitbeingpossibletoﬁndA− 1.Wediscuss\\ntheconditionsfortheexistenceofA− 1inthefollowingsection.\\nWhenA− 1exists,severaldiﬀerentalgorithmsexistforﬁndingitinclosedform.\\nIntheory,thesameinversematrixcanthenbeusedtosolvetheequationmany\\ntimesfordiﬀerentvaluesofb.However,A− 1isprimarilyusefulasatheoretical\\ntool,andshouldnotactuallybeusedinpracticeformostsoftwareapplications.\\nBecauseA− 1canberepresentedwithonlylimitedprecisiononadigitalcomputer,\\nalgorithmsthatmakeuseofthevalueofbcanusuallyobtainmoreaccurate\\nestimatesof.x\\n2.4LinearDependenceandSpan\\nInorderforA− 1toexist,equationmusthaveexactlyonesolutionforevery 2.11\\nvalueofb.However,itisalsopossibleforthesystemofequationstohaveno\\nsolutionsorinﬁnitelymanysolutionsforsomevaluesofb.\\xa0Itisnotpossibleto\\nhavemorethanonebutlessthaninﬁnitelymanysolutionsforaparticularb;if\\nbothandaresolutionsthen xy\\nzxy = α+(1 )− α (2.26)\\nisalsoasolutionforanyreal. α\\nToanalyzehowmanysolutionstheequationhas,wecanthinkofthecolumns\\nofAasspecifyingdiﬀerentdirectionswecantravelfromtheorigin(thepoint\\nspeciﬁedbythevectorofallzeros),anddeterminehowmanywaysthereareof\\nreachingb.Inthisview,eachelementofxspeciﬁeshowfarweshouldtravelin\\neachofthesedirections,with x ispecifyinghowfartomoveinthedirectionof\\ncolumn: i\\nAx=\\ue058\\nix iA : , i . (2.27)\\nIngeneral,thiskindofoperationiscalledalinearcombination.Formally,a\\nlinearcombinationofsomesetofvectors{v( 1 ), . . . ,v( ) n}isgivenbymultiplying\\neachvectorv( ) ibyacorrespondingscalarcoeﬃcientandaddingtheresults:\\n\\ue058\\nic iv( ) i. (2.28)\\nThespanofasetofvectorsisthesetofallpointsobtainablebylinearcombination\\noftheoriginalvectors.\\n3 7', 'CHAPTER2.LINEARALGEBRA\\nDeterminingwhetherAx=bhasasolutionthusamountstotestingwhetherb\\nisinthespanofthecolumnsofA.Thisparticularspanisknownasthecolumn\\nspacerangeortheof.A\\nInorderforthesystemAx=btohaveasolutionforallvaluesofb∈ Rm,\\nwethereforerequirethatthecolumnspaceofAbeallof Rm.Ifanypointin Rm\\nisexcludedfromthecolumnspace,thatpointisapotentialvalueofbthathas\\nnosolution.TherequirementthatthecolumnspaceofAbeallof Rmimplies\\nimmediately thatAmusthaveatleast mcolumns,i.e., n m≥.\\xa0Otherwise, the\\ndimensionalityofthecolumnspacewouldbelessthan m.Forexample,considera\\n3×2matrix.Thetargetbis3-D,butxisonly2-D,somodifyingthevalueofx\\natbestallowsustotraceouta2-Dplanewithin R3.Theequationhasasolution\\nifandonlyifliesonthatplane.b\\nHaving n m≥isonlyanecessaryconditionforeverypointtohaveasolution.\\nItisnotasuﬃcientcondition,becauseitispossibleforsomeofthecolumnsto\\nberedundant.Considera2×2matrixwherebothofthecolumnsareidentical.\\nThishasthesamecolumnspaceasa2×1matrixcontainingonlyonecopyofthe\\nreplicatedcolumn.Inotherwords,thecolumnspaceisstilljustaline,andfailsto\\nencompassallof R2,eventhoughtherearetwocolumns.\\nFormally,thiskindofredundancyisknownaslineardependence.Asetof\\nvectorsislinearlyindependentifnovectorinthesetisalinearcombination\\noftheothervectors.\\xa0Ifweaddavectortoasetthatisalinearcombinationof\\ntheothervectorsintheset,thenewvectordoesnotaddanypointstotheset’s\\nspan.Thismeansthatforthecolumnspaceofthematrixtoencompassallof Rm,\\nthematrixmustcontainatleastonesetof mlinearlyindependentcolumns.This\\nconditionisbothnecessaryandsuﬃcientforequationtohaveasolutionfor 2.11\\neveryvalueofb.Notethattherequirementisforasettohaveexactly mlinear\\nindependentcolumns,notatleast m.Nosetof m-dimensionalvectorscanhave\\nmorethan mmutuallylinearlyindependentcolumns,butamatrixwithmorethan\\nmcolumnsmayhavemorethanonesuchset.\\nInorderforthematrixtohaveaninverse,weadditionallyneedtoensurethat\\nequationhasonesolutionforeachvalueof 2.11 atmost b.Todoso,weneedto\\nensurethatthematrixhasatmost mcolumns.Otherwisethereismorethanone\\nwayofparametrizing eachsolution.\\nTogether,thismeansthatthematrixmustbesquare,thatis,werequirethat\\nm= nandthatallofthecolumnsmustbelinearlyindependent.Asquarematrix\\nwithlinearlydependentcolumnsisknownas.singular\\nIfAisnotsquareorissquarebutsingular,itcanstillbepossibletosolvethe\\nequation.However,wecannotusethemethodofmatrixinversiontoﬁndthe\\n3 8', 'CHAPTER2.LINEARALGEBRA\\nsolution.\\nSofarwehavediscussedmatrixinversesasbeingmultipliedontheleft.Itis\\nalsopossibletodeﬁneaninversethatismultipliedontheright:\\nAA− 1= I . (2.29)\\nForsquarematrices,theleftinverseandrightinverseareequal.\\n2.5Norms\\nSometimesweneedtomeasurethesizeofavector.Inmachinelearning,weusually\\nmeasurethesizeofvectorsusingafunctioncalledanorm.Formally,the Lpnorm\\nisgivenby\\n||||x p=\\ue020\\ue058\\ni| x i|p\\ue021 1\\np\\n(2.30)\\nfor p , p . ∈ R≥1\\nNorms,includingthe Lpnorm,arefunctionsmappingvectorstonon-negative\\nvalues.Onanintuitivelevel,thenormofavectorxmeasuresthedistancefrom\\ntheorigintothepointx.Morerigorously,anormisanyfunction fthatsatisﬁes\\nthefollowingproperties:\\n• ⇒ f() = 0 xx= 0\\n• ≤ f(+) xy f f ()+x ()y(thetriangleinequality)\\n•∀∈ || α R , f α(x) = α f()x\\nThe L2norm,with p= 2,isknownastheEuclideannorm.Itissimplythe\\nEuclideandistancefromtheorigintothepointidentiﬁedbyx.The L2normis\\nusedsofrequentlyinmachinelearningthatitisoftendenotedsimplyas||||x,with\\nthesubscriptomitted.Itisalsocommontomeasurethesizeofavectorusing 2\\nthesquared L2norm,whichcanbecalculatedsimplyasx\\ue03ex.\\nThesquared L2normismoreconvenienttoworkwithmathematically and\\ncomputationally thanthe L2normitself.Forexample,thederivativesofthe\\nsquared L2normwithrespecttoeachelementofxeachdependonlyonthe\\ncorrespondingelementofx,whileallofthederivativesofthe L2normdepend\\nontheentirevector.Inmanycontexts,thesquared L2normmaybeundesirable\\nbecauseitincreasesveryslowlyneartheorigin.Inseveralmachinelearning\\n3 9', 'CHAPTER2.LINEARALGEBRA\\napplications,itisimportanttodiscriminatebetweenelementsthatareexactly\\nzeroandelementsthataresmallbutnonzero.Inthesecases,weturntoafunction\\nthatgrowsatthesamerateinalllocations,butretainsmathematical simplicity:\\nthe L1norm.The L1normmaybesimpliﬁedto\\n||||x 1=\\ue058\\ni| x i| . (2.31)\\nThe L1normiscommonlyusedinmachinelearningwhenthediﬀerencebetween\\nzeroandnonzeroelementsisveryimportant.Everytimeanelementofxmoves\\nawayfrom0by,the \\ue00f L1normincreasesby. \\ue00f\\nWesometimesmeasurethesizeofthevectorbycountingitsnumberofnonzero\\nelements.Someauthorsrefertothisfunctionasthe“ L0norm,”butthisisincorrect\\nterminology.Thenumberofnon-zeroentriesinavectorisnotanorm,because\\nscalingthevectorby αdoesnotchangethenumberofnonzeroentries.\\xa0The L1\\nnormisoftenusedasasubstituteforthenumberofnonzeroentries.\\nOneothernormthatcommonlyarisesinmachinelearningisthe L∞norm,\\nalsoknownasthemaxnorm.Thisnormsimpliﬁestotheabsolutevalueofthe\\nelementwiththelargestmagnitudeinthevector,\\n||||x ∞= max\\ni| x i| . (2.32)\\nSometimeswemayalsowishtomeasurethesizeofamatrix.Inthecontext\\nofdeeplearning,themostcommonwaytodothisiswiththeotherwiseobscure\\nFrobeniusnorm:\\n|||| A F=\\ue073\\ue058\\ni , jA2\\ni , j , (2.33)\\nwhichisanalogoustothe L2normofavector.\\nThedotproductoftwovectorscanberewrittenintermsofnorms.Speciﬁcally,\\nx\\ue03eyx= |||| 2||||y 2cos θ (2.34)\\nwhereistheanglebetweenand. θ xy\\n2.6SpecialKindsofMatricesandVectors\\nSomespecialkindsofmatricesandvectorsareparticularlyuseful.\\nDiagonalmatricesconsistmostlyofzerosandhavenon-zeroentriesonlyalong\\nthemaindiagonal.\\xa0Formally,amatrixDisdiagonalifandonlyif D i , j=0for\\n4 0', 'CHAPTER2.LINEARALGEBRA\\nall i\\ue036= j.\\xa0Wehavealreadyseenoneexampleofadiagonalmatrix:\\xa0theidentity\\nmatrix,whereallofthediagonalentriesare1.Wewritediag(v) todenoteasquare\\ndiagonalmatrixwhosediagonalentriesaregivenbytheentriesofthevectorv.\\nDiagonalmatricesareofinterestinpartbecausemultiplyingbyadiagonalmatrix\\nisverycomputationally eﬃcient.Tocomputediag(v)x,weonlyneedtoscaleeach\\nelement x iby v i.Inotherwords,diag(v)x=vx\\ue00c.Invertingasquarediagonal\\nmatrixisalsoeﬃcient.Theinverseexistsonlyifeverydiagonalentryisnonzero,\\nandinthatcase,diag(v)− 1=diag([1 /v 1 , . . . ,1 /v n]\\ue03e).Inmanycases,wemay\\nderivesomeverygeneralmachinelearningalgorithmintermsofarbitrarymatrices,\\nbutobtainalessexpensive(andlessdescriptive)algorithmbyrestrictingsome\\nmatricestobediagonal.\\nNotalldiagonalmatricesneedbesquare.Itispossibletoconstructarectangular\\ndiagonalmatrix.Non-squarediagonalmatricesdonothaveinversesbutitisstill\\npossibletomultiplybythemcheaply.Foranon-squarediagonalmatrixD,the\\nproductDxwillinvolvescalingeachelementofx,andeitherconcatenating some\\nzerostotheresultifDistallerthanitiswide,ordiscardingsomeofthelast\\nelementsofthevectorifiswiderthanitistall. D\\nA matrixisanymatrixthatisequaltoitsowntranspose: symmetric\\nAA= \\ue03e. (2.35)\\nSymmetricmatricesoftenarisewhentheentriesaregeneratedbysomefunctionof\\ntwoargumentsthatdoesnotdependontheorderofthearguments.Forexample,\\nifAisamatrixofdistancemeasurements,withA i , jgivingthedistancefrompoint\\nitopoint,then jA i , j= A j , ibecausedistancefunctionsaresymmetric.\\nA isavectorwith : unitvectorunitnorm\\n||||x 2= 1 . (2.36)\\nAvectorxandavectoryareorthogonaltoeachotherifx\\ue03ey= 0.Ifboth\\nvectorshavenonzeronorm,thismeansthattheyareata90degreeangletoeach\\nother.In Rn,atmost nvectorsmaybemutuallyorthogonalwithnonzeronorm.\\nIfthevectorsarenotonlyorthogonalbutalsohaveunitnorm,wecallthem\\northonormal.\\nAnorthogonalmatrixisasquarematrixwhoserowsaremutuallyorthonor-\\nmalandwhosecolumnsaremutuallyorthonormal:\\nA\\ue03eAAA= \\ue03e= I . (2.37)\\n4 1', 'CHAPTER2.LINEARALGEBRA\\nThisimpliesthat\\nA− 1= A\\ue03e, (2.38)\\nsoorthogonalmatricesareofinterestbecausetheirinverseisverycheaptocompute.\\nPaycarefulattentiontothedeﬁnitionoforthogonalmatrices.Counterintuitively,\\ntheirrowsarenotmerelyorthogonalbutfullyorthonormal. Thereisnospecial\\ntermforamatrixwhoserowsorcolumnsareorthogonalbutnotorthonormal.\\n2.7Eigendecomposition\\nManymathematical objectscanbeunderstoodbetterbybreakingtheminto\\nconstituentparts,orﬁndingsomepropertiesofthemthatareuniversal,notcaused\\nbythewaywechoosetorepresentthem.\\nForexample,integerscanbedecomposedintoprimefactors.Thewaywe\\nrepresentthenumberwillchangedependingonwhetherwewriteitinbaseten 12\\norinbinary,butitwillalwaysbetruethat12 = 2×2×3.Fromthisrepresentation\\nwecanconcludeusefulproperties,suchasthatisnotdivisibleby,orthatany 12 5\\nintegermultipleofwillbedivisibleby. 12 3\\nMuchaswecandiscoversomethingaboutthetruenatureofanintegerby\\ndecomposingitintoprimefactors,wecanalsodecomposematricesinwaysthat\\nshowusinformationabouttheirfunctionalpropertiesthatisnotobviousfromthe\\nrepresentationofthematrixasanarrayofelements.\\nOneofthemostwidelyusedkindsofmatrixdecompositioniscalledeigen-\\ndecomposition,inwhichwedecomposeamatrixintoasetofeigenvectorsand\\neigenvalues.\\nAneigenvectorofasquarematrixAisanon-zerovectorvsuchthatmulti-\\nplicationbyaltersonlythescaleof: A v\\nAvv= λ . (2.39)\\nThescalar λisknownastheeigenvaluecorrespondingtothiseigenvector.(One\\ncanalsoﬁndalefteigenvectorsuchthatv\\ue03eA= λv\\ue03e,\\xa0butweareusually\\nconcernedwithrighteigenvectors).\\nIfvisaneigenvectorofA,thensoisanyrescaledvector svfor s , s ∈ R\\ue036= 0.\\nMoreover, svstillhasthesameeigenvalue.Forthisreason,weusuallyonlylook\\nforuniteigenvectors.\\nSupposethatamatrixAhas nlinearlyindependenteigenvectors,{v( 1 ), . . . ,\\nv( ) n},withcorrespondingeigenvalues { λ 1 , . . . , λ n}.Wemayconcatenateallofthe\\n4 2', 'CHAPTER2.LINEARALGEBRA\\n\\x00\\ue033 \\x00\\ue032 \\x00\\ue031 \\ue030 \\ue031 \\ue032 \\ue033\\n\\ue078\\ue030\\x00\\ue033\\x00\\ue032\\x00\\ue031\\ue030\\ue031\\ue032\\ue033\\ue078\\ue031\\ue076\\ue028\\ue031 \\ue029\\n\\ue076\\ue028\\ue032 \\ue029\\ue042 \\ue065 \\ue066 \\ue06f \\ue072 \\ue065 \\ue020 \\ue06d \\ue075 \\ue06c \\ue074 \\ue069 \\ue070 \\ue06c \\ue069 \\ue063 \\ue061 \\ue074 \\ue069 \\ue06f \\ue06e\\n\\x00\\ue033 \\x00\\ue032 \\x00\\ue031 \\ue030 \\ue031 \\ue032 \\ue033\\n\\ue078\\ue030\\n\\ue030\\x00\\ue033\\x00\\ue032\\x00\\ue031\\ue030\\ue031\\ue032\\ue033\\ue078\\ue030\\n\\ue031\\ue076\\ue028\\ue031 \\ue029\\ue0b8\\ue031 \\ue076\\ue028\\ue031 \\ue029\\n\\ue076\\ue028\\ue032 \\ue029\\ue0b8\\ue032\\ue076\\ue028\\ue032 \\ue029\\ue041 \\ue066 \\ue074 \\ue065 \\ue072 \\ue020 \\ue06d \\ue075 \\ue06c \\ue074 \\ue069 \\ue070 \\ue06c \\ue069 \\ue063 \\ue061 \\ue074 \\ue069 \\ue06f \\ue06e\\ue045 \\ue066 \\ue066 \\ue065 \\ue063 \\ue074 \\ue020 \\ue06f\\ue066 \\ue020 \\ue065 \\ue069 \\ue067 \\ue065 \\ue06e \\ue076 \\ue065 \\ue063 \\ue074 \\ue06f\\ue072 \\ue073 \\ue020 \\ue061\\ue06e \\ue064 \\ue020 \\ue065 \\ue069 \\ue067\\ue065 \\ue06e \\ue076 \\ue061\\ue06c \\ue075 \\ue065 \\ue073\\nFigure2.3:Anexampleoftheeﬀectofeigenvectorsandeigenvalues.Here,wehave\\namatrixAwithtwoorthonormaleigenvectors,v( 1 )witheigenvalue λ 1andv( 2 )with\\neigenvalue λ 2. ( L e f t )Weplotthesetofallunitvectorsu∈ R2asaunitcircle. ( R i g h t )We\\nplotthesetofallpointsAu.ByobservingthewaythatAdistortstheunitcircle,we\\ncanseethatitscalesspaceindirectionv( ) iby λ i.\\neigenvectorstoformamatrixVwithoneeigenvectorpercolumn:V= [v( 1 ), . . . ,\\nv( ) n].Likewise,wecanconcatenatetheeigenvaluestoformavectorλ= [ λ 1 , . . . ,\\nλ n]\\ue03e.The ofisthengivenby eigendecompositionA\\nAVλV = diag()− 1. (2.40)\\nWehaveseenthatconstructingmatriceswithspeciﬁceigenvaluesandeigenvec-\\ntorsallowsustostretchspaceindesireddirections.\\xa0Ho wever,weoftenwantto\\ndecomposematricesintotheireigenvaluesandeigenvectors.Doingsocanhelp\\nustoanalyzecertainpropertiesofthematrix,muchasdecomposinganinteger\\nintoitsprimefactorscanhelpusunderstandthebehaviorofthatinteger.\\nNoteverymatrixcanbedecomposedintoeigenvaluesandeigenvectors.Insome\\n4 3', 'CHAPTER2.LINEARALGEBRA\\ncases,thedecompositionexists,butmayinvolvecomplexratherthanrealnumbers.\\nFortunately,inthisbook,weusuallyneedtodecomposeonlyaspeciﬁcclassof\\nmatricesthathaveasimpledecomposition.Speciﬁcally,everyrealsymmetric\\nmatrixcanbedecomposedintoanexpressionusingonlyreal-valuedeigenvectors\\nandeigenvalues:\\nAQQ = Λ\\ue03e, (2.41)\\nwhereQisanorthogonalmatrixcomposedofeigenvectorsofA,and Λisa\\ndiagonalmatrix.TheeigenvalueΛ i , iisassociatedwiththeeigenvectorincolumn i\\nofQ,denotedasQ : , i.BecauseQisanorthogonalmatrix,wecanthinkofAas\\nscalingspaceby λ iindirectionv( ) i.Seeﬁgureforanexample.2.3\\nWhileanyrealsymmetricmatrixAisguaranteedtohaveaneigendecomposi-\\ntion,theeigendecompositionmaynotbeunique.Ifanytwoormoreeigenvectors\\nsharethesameeigenvalue,thenanysetoforthogonalvectorslyingintheirspan\\narealsoeigenvectorswiththateigenvalue,andwecouldequivalentlychooseaQ\\nusingthoseeigenvectorsinstead.Byconvention,weusuallysorttheentriesof Λ\\nindescendingorder.Underthisconvention,theeigendecompositionisuniqueonly\\nifalloftheeigenvaluesareunique.\\nTheeigendecompositionof\\xa0amatrix\\xa0tellsus\\xa0many\\xa0usefulfactsabout\\xa0the\\nmatrix.Thematrixissingularifandonlyifanyoftheeigenvaluesarezero.\\nTheeigendecomposition ofarealsymmetricmatrixcanalsobeusedtooptimize\\nquadraticexpressionsoftheform f(x) =x\\ue03eAxsubjectto||||x 2= 1.Wheneverx\\nisequaltoaneigenvectorofA, ftakesonthevalueofthecorrespondingeigenvalue.\\nThemaximumvalueof fwithintheconstraintregionisthemaximumeigenvalue\\nanditsminimumvaluewithintheconstraintregionistheminimumeigenvalue.\\nAmatrixwhoseeigenvaluesareallpositiveiscalledpositivedeﬁnite.A\\nmatrixwhoseeigenvaluesareallpositiveorzero-valuediscalledpositivesemideﬁ-\\nnite.Likewise,ifalleigenvaluesarenegative,thematrixisnegativedeﬁnite,and\\nifalleigenvaluesarenegativeorzero-valued,itisnegativesemideﬁnite.Positive\\nsemideﬁnitematricesareinterestingbecausetheyguaranteethat∀xx ,\\ue03eAx≥0.\\nPositivedeﬁnitematricesadditionallyguaranteethatx\\ue03eAxx = 0 ⇒ = 0.\\n2.8SingularValueDecomposition\\nInsection,wesawhowtodecomposeamatrixintoeigenvectorsandeigenvalues. 2.7\\nThesingularvaluedecomposition(SVD)providesanotherwaytofactorize\\namatrix,intosingularvectorsandsingularvalues.TheSVDallowsusto\\ndiscoversomeofthesamekindofinformationastheeigendecomposition.However,\\n4 4', 'CHAPTER2.LINEARALGEBRA\\ntheSVDismoregenerallyapplicable.Everyrealmatrixhasasingularvalue\\ndecomposition,butthesameisnottrueoftheeigenvaluedecomposition.For\\nexample,ifamatrixisnotsquare,theeigendecompositionisnotdeﬁned,andwe\\nmustuseasingularvaluedecompositioninstead.\\nRecallthattheeigendecompositioninvolvesanalyzingamatrixAtodiscover\\namatrixVofeigenvectorsandavectorofeigenvaluesλsuchthatwecanrewrite\\nAas\\nAVλV = diag()− 1. (2.42)\\nThesingularvaluedecompositionissimilar,exceptthistimewewillwriteA\\nasaproductofthreematrices:\\nAUDV = \\ue03e. (2.43)\\nSupposethatAisan m n×matrix.ThenUisdeﬁnedtobean m m×matrix,\\nD V tobeanmatrix,and m n× tobeanmatrix. n n×\\nEachofthesematricesisdeﬁnedtohaveaspecialstructure.ThematricesU\\nandVarebothdeﬁnedtobeorthogonalmatrices.ThematrixDisdeﬁnedtobe\\nadiagonalmatrix.Notethatisnotnecessarilysquare. D\\nTheelementsalongthediagonalofDareknownasthesingularvaluesof\\nthematrixA.ThecolumnsofUareknownastheleft-singularvectors.The\\ncolumnsofareknownasasthe V right-singularvectors.\\nWecanactuallyinterpretthesingularvaluedecompositionofAintermsof\\ntheeigendecomposition offunctionsofA.Theleft-singularvectorsofAarethe\\neigenvectorsofAA\\ue03e.Theright-singularvectorsofAaretheeigenvectorsofA\\ue03eA.\\nThenon-zerosingularvaluesofAarethesquarerootsoftheeigenvaluesofA\\ue03eA.\\nThesameistrueforAA\\ue03e.\\nPerhapsthemostusefulfeatureoftheSVDisthatwecanuseittopartially\\ngeneralizematrixinversiontonon-squarematrices,aswewillseeinthenext\\nsection.\\n2.9TheMoore-PenrosePseudoinverse\\nMatrixinversionisnotdeﬁnedformatricesthatarenotsquare.Supposewewant\\ntomakealeft-inverseofamatrix,sothatwecansolvealinearequation BA\\nAxy= (2.44)\\n4 5', 'CHAPTER2.LINEARALGEBRA\\nbyleft-multiplyingeachsidetoobtain\\nxBy= . (2.45)\\nDependingonthestructureoftheproblem,itmaynotbepossibletodesigna\\nuniquemappingfromto.AB\\nIfAistallerthanitiswide,\\xa0thenitispossibleforthisequationtohave\\nnosolution.IfAiswiderthanitistall,thentherecouldbemultiplepossible\\nsolutions.\\nTheMoore-Penrosepseudoinverseallowsustomakesomeheadwayin\\nthesecases.Thepseudoinverseofisdeﬁnedasamatrix A\\nA+=lim\\nα \\ue026 0(A\\ue03eAI+ α)− 1A\\ue03e. (2.46)\\nPracticalalgorithmsforcomputingthepseudoinversearenotbasedonthisdeﬁni-\\ntion,butrathertheformula\\nA+= VD+U\\ue03e, (2.47)\\nwhereU,DandVarethesingularvaluedecompositionofA,andthepseudoinverse\\nD+ofadiagonalmatrixDisobtainedbytakingthereciprocalofitsnon-zero\\nelementsthentakingthetransposeoftheresultingmatrix.\\nWhenAhasmorecolumnsthanrows,thensolvingalinearequationusingthe\\npseudoinverseprovidesoneofthemanypossiblesolutions.Speciﬁcally,itprovides\\nthesolutionx=A+ywithminimalEuclideannorm ||||x 2amongallpossible\\nsolutions.\\nWhenAhasmorerowsthancolumns,itispossiblefortheretobenosolution.\\nInthiscase,usingthepseudoinversegivesusthexforwhichAxisascloseas\\npossibletointermsofEuclideannorm y ||−||Axy 2.\\n2.10TheTraceOperator\\nThetraceoperatorgivesthesumofallofthediagonalentriesofamatrix:\\nTr() =A\\ue058\\niA i , i . (2.48)\\nThetraceoperatorisusefulforavarietyofreasons.Someoperationsthatare\\ndiﬃculttospecifywithoutresortingtosummationnotationcanbespeciﬁedusing\\n4 6', 'CHAPTER2.LINEARALGEBRA\\nmatrixproductsandthetraceoperator.Forexample,thetraceoperatorprovides\\nanalternativewayofwritingtheFrobeniusnormofamatrix:\\n|||| A F=\\ue071\\nTr(AA\\ue03e) . (2.49)\\nWritinganexpressionintermsofthetraceoperatoropensupopportunitiesto\\nmanipulatetheexpressionusingmanyusefulidentities.\\xa0Forexample,thetrace\\noperatorisinvarianttothetransposeoperator:\\nTr() = Tr(AA\\ue03e) . (2.50)\\nThetraceofasquarematrixcomposedofmanyfactorsisalsoinvariantto\\nmovingthelastfactorintotheﬁrstposition,iftheshapesofthecorresponding\\nmatricesallowtheresultingproducttobedeﬁned:\\nTr( ) = Tr( ) = Tr( ) ABCCABBCA (2.51)\\normoregenerally,\\nTr(n\\ue059\\ni = 1F( ) i) = Tr(F( ) nn − 1\\ue059\\ni = 1F( ) i) . (2.52)\\nThisinvariancetocyclicpermutationholdseveniftheresultingproducthasa\\ndiﬀerentshape.Forexample,forA∈ Rm n ×andB∈ Rn m ×,wehave\\nTr( ) = Tr( )ABBA (2.53)\\neventhoughAB∈ Rm m ×andBA∈ Rn n ×.\\nAnotherusefulfacttokeepinmindisthatascalarisitsowntrace: a=Tr( a).\\n2.11TheDeterminant\\nThedeterminant ofa\\xa0squarematrix,\\xa0denoted det(A),\\xa0isa\\xa0functionmapping\\nmatricesto\\xa0realscalars.Thedeterminant isequal\\xa0totheproductof\\xa0allthe\\neigenvaluesofthematrix.Theabsolutevalueofthedeterminantcanbethought\\nofasameasureofhowmuchmultiplicationbythematrixexpandsorcontracts\\nspace.Ifthedeterminantis0,thenspaceiscontractedcompletelyalongatleast\\nonedimension,causingittoloseallofitsvolume.Ifthedeterminantis1,then\\nthetransformationpreservesvolume.\\n4 7', 'CHAPTER2.LINEARALGEBRA\\n2.12Example:PrincipalComponentsAnalysis\\nOnesimplemachinelearningalgorithm,principalcomponentsanalysisorPCA\\ncanbederivedusingonlyknowledgeofbasiclinearalgebra.\\nSupposewehaveacollectionof mpoints{x( 1 ), . . . ,x( ) m}in Rn.Supposewe\\nwouldliketoapplylossycompressiontothesepoints.Lossycompressionmeans\\nstoringthepointsinawaythatrequireslessmemorybutmaylosesomeprecision.\\nWewouldliketoloseaslittleprecisionaspossible.\\nOnewaywecanencodethesepointsistorepresentalower-dimensionalversion\\nofthem.Foreachpointx( ) i∈ Rnwewillﬁndacorrespondingcodevectorc( ) i∈ Rl.\\nIf lissmallerthan n,itwilltakelessmemorytostorethecodepointsthanthe\\noriginaldata.Wewillwanttoﬁndsomeencodingfunctionthatproducesthecode\\nforaninput, f(x) =c,andadecodingfunctionthatproducesthereconstructed\\ninputgivenitscode, .xx ≈ g f(())\\nPCAisdeﬁnedbyourchoiceofthedecodingfunction.Speciﬁcally,tomakethe\\ndecoderverysimple,wechoosetousematrixmultiplicationtomapthecodeback\\ninto Rn.Let,where g() = cDcD∈ Rn l ×isthematrixdeﬁningthedecoding.\\nComputingtheoptimalcodeforthisdecodercouldbeadiﬃcultproblem.To\\nkeeptheencodingproblemeasy,PCAconstrainsthecolumnsofDtobeorthogonal\\ntoeachother.(NotethatDisstillnottechnically“anorthogonalmatrix”unless\\nl n= )\\nWiththeproblemasdescribedsofar,manysolutionsarepossible,becausewe\\ncanincreasethescaleofD : , iifwedecrease c iproportionallyforallpoints.Togive\\ntheproblemauniquesolution,weconstrainallofthecolumnsoftohaveunitD\\nnorm.\\nInordertoturnthisbasicideaintoanalgorithmwecanimplement,theﬁrst\\nthingweneedtodoisﬁgureouthowtogeneratetheoptimalcodepointc∗for\\neachinputpointx.Onewaytodothisistominimizethedistancebetweenthe\\ninputpointxanditsreconstruction, g(c∗).Wecanmeasurethisdistanceusinga\\nnorm.Intheprincipalcomponentsalgorithm,weusethe L2norm:\\nc∗= argmin\\nc||− ||x g()c 2 . (2.54)\\nWecanswitchtothesquared L2norminsteadofthe L2normitself,because\\nbothareminimizedbythesamevalueofc.Bothareminimizedbythesame\\nvalueofcbecausethe L2normisnon-negative andthesquaringoperationis\\n4 8', 'CHAPTER2.LINEARALGEBRA\\nmonotonically increasingfornon-negative arguments.\\nc∗= argmin\\nc||− ||x g()c2\\n2 . (2.55)\\nThefunctionbeingminimizedsimpliﬁesto\\n( ())x− gc\\ue03e( ())x− gc (2.56)\\n(bythedeﬁnitionofthe L2norm,equation)2.30\\n= x\\ue03exx−\\ue03eg g ()c−()c\\ue03exc+( g)\\ue03eg()c (2.57)\\n(bythedistributiveproperty)\\n= x\\ue03exx−2\\ue03eg g ()+c ()c\\ue03eg()c (2.58)\\n(becausethescalar g()c\\ue03exisequaltothetransposeofitself).\\nWecannowchangethefunctionbeingminimizedagain,toomittheﬁrstterm,\\nsincethistermdoesnotdependon:c\\nc∗= argmin\\nc−2x\\ue03eg g ()+c ()c\\ue03eg .()c (2.59)\\nTomakefurtherprogress,wemustsubstituteinthedeﬁnitionof: g()c\\nc∗= argmin\\nc−2x\\ue03eDcc+\\ue03eD\\ue03eDc (2.60)\\n= argmin\\nc−2x\\ue03eDcc+\\ue03eI lc (2.61)\\n(bytheorthogonalityandunitnormconstraintson)D\\n= argmin\\nc−2x\\ue03eDcc+\\ue03ec (2.62)\\nWecansolvethisoptimization problemusingvectorcalculus(seesectionif4.3\\nyoudonotknowhowtodothis):\\n∇ c(2−x\\ue03eDcc+\\ue03ec) = 0 (2.63)\\n−2D\\ue03exc+2= 0 (2.64)\\ncD= \\ue03ex . (2.65)\\n4 9', 'CHAPTER2.LINEARALGEBRA\\nThismakesthealgorithmeﬃcient:\\xa0wecanoptimallyencodexjustusinga\\nmatrix-vectoroperation.Toencodeavector,weapplytheencoderfunction\\nf() = xD\\ue03ex . (2.66)\\nUsingafurthermatrixmultiplication, wecanalsodeﬁnethePCAreconstruction\\noperation:\\nr g f () = x (()) = xDD\\ue03ex . (2.67)\\nNext,weneedtochoosetheencodingmatrixD.Todoso,werevisittheidea\\nofminimizingthe L2distancebetweeninputsandreconstructions.Sincewewill\\nusethesamematrixDtodecodeallofthepoints,wecannolongerconsiderthe\\npointsinisolation.Instead,wemustminimizetheFrobeniusnormofthematrix\\noferrorscomputedoveralldimensionsandallpoints:\\nD∗= argmin\\nD\\ue073\\ue058\\ni , j\\ue010\\nx( ) i\\nj− r(x( ) i) j\\ue0112\\nsubjecttoD\\ue03eDI= l(2.68)\\nToderivethealgorithmforﬁndingD∗,wewillstartbyconsideringthecase\\nwhere l= 1.Inthiscase,Disjustasinglevector,d.Substitutingequation2.67\\nintoequationandsimplifyinginto,theproblemreducesto 2.68 Dd\\nd∗= argmin\\nd\\ue058\\ni||x( ) i−dd\\ue03ex( ) i||2\\n2subjectto||||d 2= 1 .(2.69)\\nTheaboveformulationisthemostdirectwayofperformingthesubstitution,\\nbutisnotthemoststylisticallypleasingwaytowritetheequation.Itplacesthe\\nscalarvalued\\ue03ex( ) iontherightofthevectord.Itismoreconventionaltowrite\\nscalarcoeﬃcientsontheleftofvectortheyoperateon.Wethereforeusuallywrite\\nsuchaformulaas\\nd∗= argmin\\nd\\ue058\\ni||x( ) i−d\\ue03ex( ) id||2\\n2subjectto||||d 2= 1 ,(2.70)\\nor,exploitingthefactthatascalarisitsowntranspose,as\\nd∗= argmin\\nd\\ue058\\ni||x( ) i−x( ) i \\ue03edd||2\\n2subjectto||||d 2= 1 .(2.71)\\nThereadershouldaimtobecomefamiliarwithsuchcosmeticrearrangements .\\n5 0', 'CHAPTER2.LINEARALGEBRA\\nAtthispoint,itcanbehelpfultorewritetheproblemintermsofasingle\\ndesignmatrixofexamples,ratherthanasasumoverseparateexamplevectors.\\nThiswillallowustousemorecompactnotation.LetX∈ Rm n ×bethematrix\\ndeﬁnedbystackingallofthevectorsdescribingthepoints,suchthatX i , :=x( ) i\\ue03e.\\nWecannowrewritetheproblemas\\nd∗= argmin\\nd||−XXdd\\ue03e||2\\nFsubjecttod\\ue03ed= 1 .(2.72)\\nDisregardingtheconstraintforthemoment,wecansimplifytheFrobeniusnorm\\nportionasfollows:\\nargmin\\nd||−XXdd\\ue03e||2\\nF (2.73)\\n= argmin\\ndTr\\ue012\\ue010\\nXXdd −\\ue03e\\ue011\\ue03e\\ue010\\nXXdd −\\ue03e\\ue011\\ue013\\n(2.74)\\n(byequation)2.49\\n= argmin\\ndTr(X\\ue03eXX−\\ue03eXdd\\ue03e−dd\\ue03eX\\ue03eXdd+\\ue03eX\\ue03eXdd\\ue03e)(2.75)\\n= argmin\\ndTr(X\\ue03eX)Tr(−X\\ue03eXdd\\ue03e)Tr(−dd\\ue03eX\\ue03eX)+Tr(dd\\ue03eX\\ue03eXdd\\ue03e)\\n(2.76)\\n= argmin\\nd−Tr(X\\ue03eXdd\\ue03e)Tr(−dd\\ue03eX\\ue03eX)+Tr(dd\\ue03eX\\ue03eXdd\\ue03e)(2.77)\\n(becausetermsnotinvolvingdonotaﬀectthe) d argmin\\n= argmin\\nd−2Tr(X\\ue03eXdd\\ue03e)+Tr(dd\\ue03eX\\ue03eXdd\\ue03e)(2.78)\\n(becausewecancycletheorderofthematricesinsideatrace,equation)2.52\\n= argmin\\nd−2Tr(X\\ue03eXdd\\ue03e)+Tr(X\\ue03eXdd\\ue03edd\\ue03e)(2.79)\\n(usingthesamepropertyagain)\\nAtthispoint,were-introducetheconstraint:\\nargmin\\nd−2Tr(X\\ue03eXdd\\ue03e)+Tr(X\\ue03eXdd\\ue03edd\\ue03e)subjecttod\\ue03ed= 1(2.80)\\n= argmin\\nd−2Tr(X\\ue03eXdd\\ue03e)+Tr(X\\ue03eXdd\\ue03e)subjecttod\\ue03ed= 1(2.81)\\n(duetotheconstraint)\\n= argmin\\nd−Tr(X\\ue03eXdd\\ue03e)subjecttod\\ue03ed= 1(2.82)\\n5 1', 'CHAPTER2.LINEARALGEBRA\\n= argmax\\ndTr(X\\ue03eXdd\\ue03e)subjecttod\\ue03ed= 1(2.83)\\n= argmax\\ndTr(d\\ue03eX\\ue03eXdd )subjectto\\ue03ed= 1(2.84)\\nThisoptimizationproblemmaybesolvedusingeigendecomposition.Speciﬁcally,\\ntheoptimaldisgivenbytheeigenvectorofX\\ue03eXcorrespondingtothelargest\\neigenvalue.\\nThisderivationisspeciﬁctothecaseof l=1andrecoversonlytheﬁrst\\nprincipalcomponent.Moregenerally,whenwewishtorecoverabasisofprincipal\\ncomponents,thematrixDisgivenbythe leigenvectorscorrespondingtothe\\nlargesteigenvalues.Thismaybeshownusingproofbyinduction.Werecommend\\nwritingthisproofasanexercise.\\nLinearalgebraisoneofthefundamentalmathematical disciplinesthatis\\nnecessarytounderstanddeeplearning.Anotherkeyareaofmathematics thatis\\nubiquitousinmachinelearningisprobabilitytheory,presentednext.\\n5 2']}, 'DeepLearning-chapter-3.pdf': {'text': ['C h a p t e r 3\\nProbabilityandInformation\\nTheory\\nInthischapter,wedescribeprobabilitytheoryandinformationtheory.\\nProbabilitytheoryisamathematical frameworkforrepresentinguncertain\\nstatements.Itprovidesameansofquantifyinguncertaintyandaxiomsforderiving\\nnewuncertainstatements.Inartiﬁcialintelligenceapplications,weuseprobability\\ntheoryintwomajorways.First,thelawsofprobabilitytellushowAIsystems\\nshouldreason,sowedesignouralgorithmstocomputeorapproximate various\\nexpressionsderivedusingprobabilitytheory.Second,wecanuseprobabilityand\\nstatisticstotheoreticallyanalyzethebehaviorofproposedAIsystems.\\nProbabilitytheoryisafundamentaltoolofmanydisciplinesofscienceand\\nengineering.Weprovidethischaptertoensurethatreaderswhosebackgroundis\\nprimarilyinsoftwareengineeringwithlimitedexposuretoprobabilitytheorycan\\nunderstandthematerialinthisbook.\\nWhileprobabilitytheoryallowsustomakeuncertainstatementsandreasonin\\nthepresenceofuncertainty,informationtheoryallowsustoquantifytheamount\\nofuncertaintyinaprobabilitydistribution.\\nIfyouarealreadyfamiliarwithprobabilitytheoryandinformationtheory,you\\nmaywishtoskipallofthischapterexceptforsection,whichdescribesthe 3.14\\ngraphsweusetodescribestructuredprobabilisticmodelsformachinelearning.If\\nyouhaveabsolutelynopriorexperiencewiththesesubjects,thischaptershould\\nbesuﬃcienttosuccessfullycarryoutdeeplearningresearchprojects,butwedo\\nsuggestthatyouconsultanadditionalresource,suchasJaynes2003().\\n53', 'CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\n3.1WhyProbability?\\nManybranchesofcomputersciencedealmostlywithentitiesthatareentirely\\ndeterministicandcertain.AprogrammercanusuallysafelyassumethataCPUwill\\nexecuteeachmachineinstructionﬂawlessly.Errorsinhardwaredooccur,butare\\nrareenoughthatmostsoftwareapplicationsdonotneedtobedesignedtoaccount\\nforthem.Giventhatmanycomputerscientistsandsoftwareengineersworkina\\nrelativelycleanandcertainenvironment,itcanbesurprisingthatmachinelearning\\nmakesheavyuseofprobabilitytheory.\\nThisisbecausemachinelearningmustalwaysdealwithuncertainquantities,\\nandsometimesmayalsoneedtodealwithstochastic(non-determinis tic)quantities.\\nUncertaintyandstochasticitycanarisefrommanysources.Researchershavemade\\ncompellingargumentsforquantifyinguncertaintyusingprobabilitysinceatleast\\nthe1980s.Manyoftheargumentspresentedherearesummarizedfromorinspired\\nbyPearl1988().\\nNearlyallactivitiesrequiresomeabilitytoreasoninthepresenceofuncertainty.\\nInfact,beyondmathematical statementsthataretruebydeﬁnition,itisdiﬃcult\\ntothinkofanypropositionthatisabsolutelytrueoranyeventthatisabsolutely\\nguaranteedtooccur.\\nTherearethreepossiblesourcesofuncertainty:\\n1.Inherentstochasticityinthesystembeingmodeled.Forexample,most\\ninterpretationsofquantummechanicsdescribethedynamicsofsubatomic\\nparticlesasbeingprobabilistic.Wecanalsocreatetheoreticalscenariosthat\\nwepostulatetohaverandomdynamics,suchasahypothetical cardgame\\nwhereweassumethatthecardsaretrulyshuﬄedintoarandomorder.\\n2.Incompleteobservability.Evendeterministicsystemscanappearstochastic\\nwhenwecannotobserveallofthevariablesthatdrivethebehaviorofthe\\nsystem.Forexample,intheMontyHallproblem,agameshowcontestantis\\naskedtochoosebetweenthreedoorsandwinsaprizeheldbehindthechosen\\ndoor.Twodoorsleadtoagoatwhileathirdleadstoacar.\\xa0Theoutcome\\ngiventhecontestant’schoiceisdeterministic,butfromthecontestant’spoint\\nofview,theoutcomeisuncertain.\\n3.Incompletemodeling.Whenweuseamodelthatmustdiscardsomeof\\nthe\\xa0information wehave\\xa0observed,\\xa0the\\xa0discarded\\xa0i nformationresults\\xa0in\\nuncertaintyinthemodel’spredictions. Forexample,supposewebuilda\\nrobotthatcanexactlyobservethelocationofeveryobjectaroundit.Ifthe\\n54', 'CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\nrobotdiscretizesspacewhenpredictingthefuturelocationoftheseobjects,\\nthenthediscretizationmakestherobotimmediatelybecomeuncertainabout\\ntheprecisepositionofobjects:\\xa0eachobjectcouldbeanywherewithinthe\\ndiscretecellthatitwasobservedtooccupy.\\nInmanycases,itismorepracticaltouseasimplebutuncertainrulerather\\nthanacomplexbutcertainone,evenifthetrueruleisdeterministicandour\\nmodelingsystemhastheﬁdelitytoaccommodateacomplexrule.Forexample,the\\nsimplerule“Mostbirdsﬂy”ischeaptodevelopandisbroadlyuseful,whilearule\\noftheform,“Birdsﬂy,exceptforveryyoungbirdsthathavenotyetlearnedto\\nﬂy,sickorinjuredbirdsthathavelosttheabilitytoﬂy,ﬂightlessspeciesofbirds\\nincludingthecassowary,ostrichandkiwi...”\\xa0isexpensivetodevelop,maintainand\\ncommunicate,andafterallofthiseﬀortisstillverybrittleandpronetofailure.\\nWhileitshouldbeclearthatweneedameansofrepresentingandreasoning\\naboutuncertainty,itisnotimmediatelyobviousthatprobabilitytheorycanprovide\\nallofthetoolswewantforartiﬁcialintelligenceapplications.Probabilitytheory\\nwasoriginallydevelopedtoanalyzethefrequenciesofevents.Itiseasytosee\\nhowprobabilitytheorycanbeusedtostudyeventslikedrawingacertainhandof\\ncardsinagameofpoker.Thesekindsofeventsareoftenrepeatable.\\xa0Whenwe\\nsaythatanoutcomehasaprobabilitypofoccurring,itmeansthatifwerepeated\\ntheexperiment(e.g.,drawahandofcards)inﬁnitelymanytimes,thenproportion\\npoftherepetitionswouldresultinthatoutcome.Thiskindofreasoningdoesnot\\nseemimmediatelyapplicabletopropositionsthatarenotrepeatable.Ifadoctor\\nanalyzesapatientandsaysthatthepatienthasa40%chanceofhavingtheﬂu,\\nthismeanssomethingverydiﬀerent—wecannotmakeinﬁnitelymanyreplicasof\\nthepatient,noristhereanyreasontobelievethatdiﬀerentreplicasofthepatient\\nwouldpresentwiththesamesymptomsyethavevaryingunderlyingconditions.In\\nthecaseofthedoctordiagnosingthepatient,weuseprobabilitytorepresenta\\ndegr e e o f b e l i e f,with1indicatingabsolutecertaintythatthepatienthastheﬂu\\nand0indicatingabsolutecertaintythatthepatientdoesnothavetheﬂu.\\xa0The\\nformerkindofprobability,relateddirectlytotheratesatwhicheventsoccur,is\\nknownas f r e q uen t i st pr o babili t y,whilethelatter,relatedtoqualitativelevels\\nofcertainty,isknownas B ay e si an pr o babili t y.\\nIfwelistseveralpropertiesthatweexpectcommonsensereasoningabout\\nuncertaintytohave,thentheonlywaytosatisfythosepropertiesistotreat\\nBayesianprobabilities asbehavingexactlythesameasfrequentistprobabilities.\\nForexample,ifwewanttocomputetheprobabilitythataplayerwillwinapoker\\ngamegiventhatshehasacertainsetofcards,weuseexactlythesameformulas\\naswhenwecomputetheprobabilitythatapatienthasadiseasegiventhatshe\\n55', 'CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\nhascertainsymptoms.Formoredetailsaboutwhyasmallsetofcommonsense\\nassumptionsimpliesthatthesameaxiomsmustcontrolbothkindsofprobability,\\nsee(). Ramsey1926\\nProbabilitycanbeseenastheextensionoflogictodealwithuncertainty.Logic\\nprovidesasetofformalrulesfordeterminingwhatpropositionsareimpliedto\\nbetrueorfalsegiventheassumptionthatsomeothersetofpropositionsistrue\\norfalse.Probabilitytheoryprovidesasetofformalrulesfordeterminingthe\\nlikelihoodofapropositionbeingtruegiventhelikelihoodofotherpropositions.\\n3.2RandomVariables\\nA r andom v ar i abl eisavariablethatcantakeondiﬀerentvaluesrandomly.We\\ntypicallydenotetherandomvariableitselfwithalowercaseletterinplaintypeface,\\nandthevaluesitcantakeonwithlowercasescriptletters.Forexample,x 1andx 2\\narebothpossiblevaluesthattherandomvariablexcantakeon.Forvector-valued\\nvariables,wewouldwritetherandomvariableas xandoneofitsvaluesas x.On\\nitsown,arandomvariableisjustadescriptionofthestatesthatarepossible;it\\nmustbecoupledwithaprobabilitydistributionthatspeciﬁeshowlikelyeachof\\nthesestatesare.\\nRandomvariablesmaybediscreteorcontinuous.Adiscreterandomvariable\\nisonethathasaﬁniteorcountablyinﬁnitenumberofstates.Notethatthese\\nstatesarenotnecessarilytheintegers;theycanalsojustbenamedstatesthat\\narenotconsideredtohaveanynumericalvalue.Acontinuousrandomvariableis\\nassociatedwitharealvalue.\\n3.3ProbabilityDistributions\\nA pr o babili t y di st r i but i o nisadescriptionofhowlikelyarandomvariableor\\nsetofrandomvariablesistotakeoneachofitspossiblestates.Thewaywe\\ndescribeprobabilitydistributionsdependsonwhetherthevariablesarediscreteor\\ncontinuous.\\n3.3.1DiscreteVariablesandProbabilityMassFunctions\\nAprobabilitydistributionoverdiscretevariablesmaybedescribedusinga pr o ba-\\nbi l i t y m ass f unc t i o n(PMF).Wetypicallydenoteprobabilitymassfunctionswith\\nacapitalP.Oftenweassociateeachrandomvariablewithadiﬀerentprobability\\n56', 'CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\nmassfunctionandthereadermustinferwhichprobabilitymassfunctiontouse\\nbasedontheidentityoftherandomvariable,ratherthanthenameofthefunction;\\nP P ()xisusuallynotthesameas()y.\\nTheprobabilitymassfunctionmapsfromastateofarandomvariableto\\ntheprobabilityofthatrandomvariabletakingonthatstate.Theprobability\\nthatx=xisdenotedasP(x),withaprobabilityof1indicatingthatx=xis\\ncertainandaprobabilityof0indicatingthatx=xisimpossible.Sometimes\\ntodisambiguatewhichPMFtouse,wewritethenameoftherandomvariable\\nexplicitly:P(x=x).Sometimeswedeﬁneavariableﬁrst,thenuse∼notationto\\nspecifywhichdistributionitfollowslater:xx. ∼P()\\nProbabilitymassfunctionscanactonmanyvariablesatthesametime.Such\\naprobabilitydistributionovermanyvariablesisknownasa j o i n t pr o babili t y\\ndi st r i but i o n.P(x=x,y=y)denotestheprobabilitythatx=xandy=y\\nsimultaneously.Wemayalsowrite forbrevity. Px,y()\\nTobeaprobabilitymassfunctiononarandomvariablex,afunctionPmust\\nsatisfythefollowingproperties:\\n•Thedomainofmustbethesetofallpossiblestatesofx. P\\n•∀∈xx,0≤P(x)≤1.Animpossibleeventhasprobabilityandnostatecan 0 \\nbelessprobablethanthat.Likewise,aneventthatisguaranteedtohappen\\nhasprobability,andnostatecanhaveagreaterchanceofoccurring. 1\\n•\\ue050\\nx ∈ xP(x) = 1.Werefertothispropertyasbeing nor m al i z e d.Without\\nthisproperty,wecouldobtainprobabilities greaterthanonebycomputing\\ntheprobabilityofoneofmanyeventsoccurring.\\nForexample,considerasinglediscreterandomvariablexwithkdiﬀerent\\nstates.Wecanplacea uni f o r m di st r i but i o nonx—thatis,makeeachofits\\nstatesequallylikely—bysettingitsprobabilitymassfunctionto\\nPx (= x i) =1\\nk(3.1)\\nforalli.Wecanseethatthisﬁtstherequirementsforaprobabilitymassfunction.\\nThevalue1\\nkispositivebecauseisapositiveinteger.Wealsoseethat k\\n\\ue058\\niPx (= x i) =\\ue058\\ni1\\nk=k\\nk= 1, (3.2)\\nsothedistributionisproperlynormalized.\\n57', 'CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\n3.3.2ContinuousVariablesandProbabilityDensityFunctions\\nWhenworkingwithcontinuousrandomvariables,wedescribeprobabilitydistri-\\nbutionsusinga pr o babili t y densit y f unc t i o n ( P D F)ratherthanaprobability\\nmassfunction.Tobeaprobabilitydensityfunction,afunctionpmustsatisfythe\\nfollowingproperties:\\n•Thedomainofmustbethesetofallpossiblestatesofx. p\\n•∀∈ ≥ ≤ xx,px() 0 () . p Notethatwedonotrequirex 1.\\n•\\ue052\\npxdx()= 1.\\nAprobabilitydensityfunctionp(x)doesnotgivetheprobabilityofaspeciﬁc\\nstatedirectly,insteadtheprobabilityoflandinginsideaninﬁnitesimalregionwith\\nvolumeisgivenby. δx pxδx()\\nWecanintegratethedensityfunctiontoﬁndtheactualprobabilitymassofa\\nsetofpoints.Speciﬁcally,theprobabilitythatxliesinsomeset Sisgivenbythe\\nintegralofp(x)overthatset.Intheunivariateexample,theprobabilitythatx\\nliesintheintervalisgivenby []a,b\\ue052\\n[ ] a , bpxdx().\\nForanexampleofaprobabilitydensityfunctioncorrespondingtoaspeciﬁc\\nprobabilitydensityoveracontinuousrandomvariable,considerauniformdistribu-\\ntiononanintervaloftherealnumbers.Wecandothiswithafunctionu(x;a,b),\\nwhereaandbaretheendpointsoftheinterval,withb>a.The“;”notationmeans\\n“parametrized by”;weconsiderxtobetheargumentofthefunction,whileaand\\nbareparametersthatdeﬁnethefunction.Toensurethatthereisnoprobability\\nmassoutsidetheinterval,wesayu(x;a,b)=0forallx\\ue036∈[a,b] [.Withina,b],\\nuxa,b (;) =1\\nb a −.Wecanseethatthisisnonnegativeeverywhere.Additionally,it\\nintegratesto1.Weoftendenotethatxfollowstheuniformdistributionon[a,b]\\nbywritingx. ∼Ua,b()\\n3.4MarginalProbability\\nSometimesweknowtheprobabilitydistributionoverasetofvariablesandwewant\\ntoknowtheprobabilitydistributionoverjustasubsetofthem.Theprobability\\ndistributionoverthesubsetisknownasthe distribution. m ar g i nal pr o babili t y\\nForexample,supposewehavediscreterandomvariablesxandy,andweknow\\nP,(xy.Wecanﬁndxwiththe : ) P() sum r ul e\\n∀∈xxx,P(= ) =x\\ue058\\nyPx,y. (= xy= ) (3.3)\\n58', 'CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\nThename“marginalprobability”comesfromtheprocessofcomputingmarginal\\nprobabilities onpaper.WhenthevaluesofP(xy,)arewritteninagridwith\\ndiﬀerentvaluesofxinrowsanddiﬀerentvaluesofyincolumns,itisnaturalto\\nsumacrossarowofthegrid,thenwriteP(x)inthemarginofthepaperjustto\\ntherightoftherow.\\nForcontinuousvariables,weneedtouseintegrationinsteadofsummation:\\npx() =\\ue05a\\npx,ydy. () (3.4)\\n3.5ConditionalProbability\\nInmanycases,weareinterestedintheprobabilityofsomeevent,giventhatsome\\nothereventhashappened.Thisiscalleda c o ndi t i o n a l pr o babili t y.Wedenote\\ntheconditionalprobabilitythaty=ygivenx=xasP(y=y|x=x).This\\nconditionalprobabilitycanbecomputedwiththeformula\\nPyx (= y |x= ) =Py,x (= yx= )\\nPx (= x ). (3.5)\\nTheconditionalprobabilityisonlydeﬁnedwhenP(x=x)>0.Wecannotcompute\\ntheconditionalprobabilityconditionedonaneventthatneverhappens.\\nItisimportantnottoconfuseconditionalprobabilitywithcomputingwhat\\nwouldhappenifsomeactionwereundertaken.Theconditionalprobabilitythat\\napersonisfromGermanygiventhattheyspeakGermanisquitehigh,butif\\narandomlyselectedpersonistaughttospeakGerman,theircountryoforigin\\ndoesnotchange.Computingtheconsequencesofanactioniscalledmakingan\\ni n t e r v e n t i o n q uer y.Interventionqueriesarethedomainof c ausal m o del i ng,\\nwhichwedonotexploreinthisbook.\\n3.6TheChainRuleofConditionalProbabilities\\nAnyjointprobabilitydistributionovermanyrandomvariablesmaybedecomposed\\nintoconditionaldistributionsoveronlyonevariable:\\nP(x( 1 ),...,x( ) n) = (Px( 1 ))Πn\\ni = 2P(x( ) i|x( 1 ),...,x( 1 ) i −).(3.6)\\nThisobservationisknownasthe c hai n r ul eor pr o duc t r ul eofprobability.\\nItfollowsimmediatelyfromthedeﬁnitionofconditionalprobabilityinequation.3.5\\n59', 'CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\nForexample,applyingthedeﬁnitiontwice,weget\\nP,,P,P, (abc)= (ab|c)(bc)\\nP,PP (bc)= ( )bc| ()c\\nP,,P,PP. (abc)= (ab|c)( )bc| ()c\\n3.7IndependenceandConditionalIndependence\\nTworandomvariablesxandyare i ndep e nden tiftheirprobabilitydistribution\\ncanbeexpressedasaproductoftwofactors,oneinvolvingonlyxandoneinvolving\\nonlyy:\\n∀∈ ∈xx,yyxyxy (3.7) ,p(= x,= ) = (yp= )(xp= )y.\\nTworandomvariablesxandyare c o ndi t i o n a l l y i ndep e nden tgivenarandom\\nvariableziftheconditionalprobabilitydistributionoverxandyfactorizesinthis\\nwayforeveryvalueofz:\\n∀∈ ∈ ∈ | | | xx,yy,zzxy,p(= x,= yzx = ) = (zp= xzy = )(zp= yz= )z.\\n(3.8)\\nWe\\xa0candenoteindependence\\xa0andconditionalindependence\\xa0with compact\\nnotation:xy⊥meansthatxandyareindependent,whilexyz ⊥|meansthatx\\nandyareconditionallyindependentgivenz.\\n3.8Expectation,VarianceandCovariance\\nThe e x p e c t at i o nor e x p e c t e d v al ueofsomefunctionf(x)withrespecttoa\\nprobabilitydistributionP(x)istheaverageormeanvaluethatftakesonwhenx\\nisdrawnfrom.Fordiscretevariablesthiscanbecomputedwithasummation: P\\nE x ∼ P[()] =fx\\ue058\\nxPxfx, ()() (3.9)\\nwhileforcontinuousvariables,itiscomputedwithanintegral:\\nE x ∼ p[()] =fx\\ue05a\\npxfxdx. ()() (3.10)\\n60', 'CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\nWhentheidentityofthedistributionisclearfromthecontext,wemaysimply\\nwritethenameoftherandomvariablethattheexpectationisover,asin E x[f(x)].\\nIfitisclearwhichrandomvariabletheexpectationisover,wemayomitthe\\nsubscriptentirely,asin E[f(x)].Bydefault,wecanassumethat E[·]averagesover\\nthevaluesofalltherandomvariablesinsidethebrackets.Likewise,whenthereis\\nnoambiguity,wemayomitthesquarebrackets.\\nExpectationsarelinear,forexample,\\nE x[()+ ()] = αfxβgxα E x[()]+fxβ E x[()]gx, (3.11)\\nwhenandarenotdependenton. αβ x\\nThe v ar i anc egivesameasureofhowmuchthevaluesofafunctionofarandom\\nvariablexvaryaswesamplediﬀerentvaluesofxfromitsprobabilitydistribution:\\nVar(()) = fx E\\ue068\\n(() [()]) fx− Efx2\\ue069\\n. (3.12)\\nWhenthevarianceislow,thevaluesoff(x)clusterneartheirexpectedvalue.The\\nsquarerootofthevarianceisknownasthe . st andar d dev i at i o n\\nThe c o v ar i anc egivessomesenseofhowmuchtwovaluesarelinearlyrelated\\ntoeachother,aswellasthescaleofthesevariables:\\nCov(()()) = [(() [()])(() [()])] fx,gy Efx− Efxgy− Egy.(3.13)\\nHighabsolutevaluesofthecovariancemeanthatthevalueschangeverymuch\\nandarebothfarfromtheirrespectivemeansatthesametime.Ifthesignofthe\\ncovarianceispositive,thenbothvariablestendtotakeonrelativelyhighvalues\\nsimultaneously.Ifthesignofthecovarianceisnegative,thenonevariabletendsto\\ntakeonarelativelyhighvalueatthetimesthattheothertakesonarelatively\\nlowvalueandviceversa.Othermeasuressuchas c o r r e l at i o nnormalizethe\\ncontributionofeachvariableinordertomeasureonlyhowmuchthevariablesare\\nrelated,ratherthanalsobeingaﬀectedbythescaleoftheseparatevariables.\\nThenotionsofcovarianceanddependencearerelated,butareinfactdistinct\\nconcepts.Theyarerelatedbecausetwovariablesthatareindependenthavezero\\ncovariance,andtwovariablesthathavenon-zerocovariancearedependent.How-\\never,independence isadistinctpropertyfromcovariance.Fortwovariablestohave\\nzerocovariance,theremustbenolineardependencebetweenthem.Independence\\nisastrongerrequirementthanzerocovariance,becauseindependencealsoexcludes\\nnonlinearrelationships.Itispossiblefortwovariablestobedependentbuthave\\nzerocovariance.Forexample,supposeweﬁrstsamplearealnumberxfroma\\nuniformdistributionovertheinterval[−1,1].Wenextsamplearandomvariable\\n61', 'CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\ns.Withprobability1\\n2,wechoosethevalueofstobe.Otherwise,wechoose 1\\nthevalueofstobe−1.Wecanthengeneratearandomvariableybyassigning\\ny=sx.Clearly,xandyarenotindependent,becausexcompletelydetermines\\nthemagnitudeof.However,y Cov() = 0x,y.\\nThe c o v ar i anc e m at r i xofarandomvector x∈ Rnisannn×matrix,such\\nthat\\nCov() x i , j= Cov(x i,x j). (3.14)\\nThediagonalelementsofthecovariancegivethevariance:\\nCov(x i,x i) = Var(x i). (3.15)\\n3.9CommonProbabilityDistributions\\nSeveralsimpleprobabilitydistributionsareusefulinmanycontextsinmachine\\nlearning.\\n3.9.1BernoulliDistribution\\nThe B e r noul l idistributionisadistributionoverasinglebinaryrandomvariable.\\nItiscontrolledbyasingleparameterφ∈[0,1],whichgivestheprobabilityofthe\\nrandomvariablebeingequalto1.Ithasthefollowingproperties:\\nP φ (= 1) = x (3.16)\\nP φ (= 0) = 1x − (3.17)\\nPxφ (= x ) = x(1 )−φ1 − x(3.18)\\nE x[] = xφ (3.19)\\nVar x() = (1 )xφ−φ (3.20)\\n3.9.2MultinoulliDistribution\\nThe m ul t i noull ior c at e g o r i c a ldistributionisadistributionoverasinglediscrete\\nvariablewithkdiﬀerentstates,wherekisﬁnite.1Themultinoullidistributionis\\n1“Multinoulli”isatermthatwasrecentlycoinedbyGustavoLacerdoandpopularizedby\\nMurphy2012().Themultinoullidistributionisaspecialcaseofthe m u lt in om ia ldistribution.\\nAmultinomialdistributionisthedistributionovervectorsin{0,...,n}krepresentinghowmany\\ntimeseachofthekcategoriesisvisitedwhennsamplesaredrawnfromamultinoullidistribution.\\nManytextsusetheterm“multinomial”torefertomultinoullidistributionswithoutclarifying\\nthattheyreferonlytothecase. n= 1\\n62', 'CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\nparametrized byavector p∈[0,1]k − 1,wherep igivestheprobabilityofthei-th\\nstate.Theﬁnal,k-thstate’sprobabilityisgivenby1− 1\\ue03ep.Notethatwemust\\nconstrain 1\\ue03ep≤1.Multinoullidistributionsareoftenusedtorefertodistributions\\novercategoriesofobjects,sowedonotusuallyassumethatstate1hasnumerical\\nvalue1,etc.Forthisreason,wedonotusuallyneedtocomputetheexpectation\\norvarianceofmultinoulli-dis tributedrandomvariables.\\nTheBernoulliandmultinoullidistributionsaresuﬃcienttodescribeanydistri-\\nbutionovertheirdomain.\\xa0They areabletodescribeanydistributionovertheir\\ndomainnotsomuchbecausetheyareparticularlypowerfulbutratherbecause\\ntheirdomainissimple;theymodeldiscretevariablesforwhichitisfeasibleto\\nenumerateallofthestates.Whendealingwithcontinuousvariables,thereare\\nuncountablymanystates,soanydistributiondescribedbyasmallnumberof\\nparametersmustimposestrictlimitsonthedistribution.\\n3.9.3GaussianDistribution\\nThemostcommonlyuseddistributionoverrealnumbersisthe nor m al di st r i bu-\\nt i o n,alsoknownasthe : G aussian di st r i but i o n\\nN(;xµ,σ2) =\\ue072\\n1\\n2πσ2exp\\ue012\\n−1\\n2σ2( )xµ−2\\ue013\\n.(3.21)\\nSeeﬁgureforaplotofthedensityfunction. 3.1\\nThetwoparameters µ∈ Randσ∈(0,∞)controlthenormaldistribution.\\nTheparameterµgivesthecoordinateofthecentralpeak.Thisisalsothemeanof\\nthedistribution: E[x] =µ.Thestandarddeviationofthedistributionisgivenby\\nσ,andthevariancebyσ2.\\nWhenweevaluatethePDF,weneedtosquareandinvertσ.Whenweneedto\\nfrequentlyevaluatethePDFwithdiﬀerentparametervalues,amoreeﬃcientway\\nofparametrizing thedistributionistouseaparameterβ∈(0,∞)tocontrolthe\\npr e c i si o norinversevarianceofthedistribution:\\nN(;xµ,β− 1) =\\ue072\\nβ\\n2πexp\\ue012\\n−1\\n2βxµ (−)2\\ue013\\n. (3.22)\\nNormaldistributionsareasensiblechoiceformanyapplications.Intheabsence\\nofpriorknowledgeaboutwhatformadistributionovertherealnumbersshould\\ntake,thenormaldistributionisagooddefaultchoicefortwomajorreasons.\\n63', 'CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\n− − − − 20 . 15 . 10 . 05 00 05 10 15 20 . . . . . .\\nx000 .005 .010 .015 .020 .025 .030 .035 .040 .p(x)Maximumat= x µ\\nInﬂectionpointsat\\nx µ σ = ±\\nFigure3.1:Thenormaldistribution:ThenormaldistributionN(x;µ,σ2)exhibits\\naclassic“bellcurve”shape,withthexcoordinateofitscentralpeakgivenbyµ,and\\nthewidthofitspeakcontrolledbyσ.Inthisexample,wedepictthestandardnormal\\ndistribution,withand. µ= 0σ= 1\\nFirst,manydistributionswewishtomodelaretrulyclosetobeingnormal\\ndistributions.The c e n t r al l i m i t t heor e mshowsthatthesumofmanyindepen-\\ndentrandomvariablesisapproximatelynormallydistributed.Thismeansthat\\ninpractice,manycomplicatedsystemscanbemodeledsuccessfullyasnormally\\ndistributednoise,evenifthesystemcanbedecomposedintopartswithmore\\nstructuredbehavior.\\nSecond,outofallpossibleprobabilitydistributionswiththesamevariance,\\nthenormaldistributionencodesthemaximumamountofuncertaintyoverthe\\nrealnumbers.Wecanthusthinkofthenormaldistributionasbeingtheone\\nthatinsertstheleastamountofpriorknowledgeintoamodel.Fullydeveloping\\nandjustifyingthisidearequiresmoremathematical tools,andispostponedto\\nsection.19.4.2\\nThenormaldistributiongeneralizesto Rn,inwhichcaseitisknownasthe\\nm ul t i v ar i at e nor m al di st r i but i o n.Itmaybeparametrized withapositive\\ndeﬁnitesymmetricmatrix: Σ\\nN(; ) = x µ, Σ\\ue073\\n1\\n(2)πndet() Σexp\\ue012\\n−1\\n2( ) x µ−\\ue03eΣ− 1( ) x µ−\\ue013\\n.(3.23)\\n64', 'CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\nTheparameter µstillgivesthemeanofthedistribution,thoughnowitis\\nvector-valued.Theparameter Σgivesthecovariancematrixofthedistribution.\\nAsintheunivariatecase,whenwewishtoevaluatethePDFseveraltimesfor\\nmanydiﬀerentvaluesoftheparameters,thecovarianceisnotacomputationally\\neﬃcientwaytoparametrizethedistribution,sinceweneedtoinvert Σtoevaluate\\nthePDF.Wecaninsteadusea : pr e c i si o n m at r i x β\\nN(; x µ β,− 1) =\\ue073\\ndet() β\\n(2)πnexp\\ue012\\n−1\\n2( ) x µ−\\ue03eβ x µ (−)\\ue013\\n.(3.24)\\nWeoftenﬁxthecovariancematrixtobeadiagonalmatrix.Anevensimpler\\nversionisthe i sot r o pi cGaussiandistribution,whosecovariancematrixisascalar\\ntimestheidentitymatrix.\\n3.9.4ExponentialandLaplaceDistributions\\nInthecontextofdeeplearning,weoftenwanttohaveaprobabilitydistribution\\nwithasharppointatx=0.Toaccomplishthis,wecanusethe e x p o nen t i al\\ndi st r i but i o n:\\npxλλ (;) = 1 x ≥ 0exp( )−λx. (3.25)\\nTheexponentialdistributionusestheindicatorfunction 1 x ≥ 0toassignprobability\\nzerotoallnegativevaluesof.x\\nAcloselyrelatedprobabilitydistributionthatallowsustoplaceasharppeak\\nofprobabilitymassatanarbitrarypointistheµ L apl ac e di st r i but i o n\\nLaplace(;) =xµ,γ1\\n2γexp\\ue012\\n−|−|xµ\\nγ\\ue013\\n. (3.26)\\n3.9.5TheDiracDistributionandEmpiricalDistribution\\nInsomecases,wewishtospecifythatallofthemassinaprobabilitydistribution\\nclustersaroundasinglepoint.ThiscanbeaccomplishedbydeﬁningaPDFusing\\ntheDiracdeltafunction,:δx()\\npxδxµ. () = (−) (3.27)\\nTheDiracdeltafunctionisdeﬁnedsuchthatitiszero-valuedeverywhereexcept\\n0,yetintegratesto1.TheDiracdeltafunctionisnotanordinaryfunctionthat\\nassociateseachvaluexwithareal-valuedoutput,insteaditisadiﬀerentkindof\\n65', 'CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\nmathematical objectcalleda g e ner al i z e d f unc t i o nthatisdeﬁnedintermsofits\\npropertieswhenintegrated.WecanthinkoftheDiracdeltafunctionasbeingthe\\nlimitpointofaseriesoffunctionsthatputlessandlessmassonallpointsother\\nthanzero.\\nBydeﬁningp(x)tobeδshiftedby−µweobtainaninﬁnitelynarrowand\\ninﬁnitelyhighpeakofprobabilitymasswhere.xµ= \\nAcommonuseoftheDiracdeltadistributionisasacomponentofan e m pi r i c a l\\ndi st r i but i o n,\\nˆp() = x1\\nmm\\ue058\\ni = 1δ( x x−( ) i) (3.28)\\nwhichputsprobabilitymass1\\nmoneachofthempoints x( 1 ),..., x( ) mforminga\\ngivendatasetorcollectionofsamples.TheDiracdeltadistributionisonlynecessary\\ntodeﬁnetheempiricaldistributionovercontinuousvariables.Fordiscretevariables,\\nthesituationissimpler:anempiricaldistributioncanbeconceptualized asa\\nmultinoullidistribution,withaprobabilityassociatedtoeachpossibleinputvalue\\nthatissimplyequaltothe e m pi r i c a l f r e q uenc yofthatvalueinthetrainingset.\\nWecanviewtheempiricaldistributionformedfromadatasetoftraining\\nexamplesasspecifyingthedistributionthatwesamplefromwhenwetrainamodel\\nonthisdataset.\\xa0Anotherimportantperspectiveontheempiricaldistributionis\\nthatitistheprobabilitydensitythatmaximizesthelikelihoodofthetrainingdata\\n(seesection).5.5\\n3.9.6MixturesofDistributions\\nItisalsocommontodeﬁneprobabilitydistributionsbycombiningothersimpler\\nprobabilitydistributions.Onecommon\\xa0wayof\\xa0combining\\xa0distributionsis\\xa0to\\nconstructa m i x t ur e di st r i but i o n.Amixturedistributionismadeupofseveral\\ncomponentdistributions.Oneachtrial,thechoiceofwhichcomponentdistribution\\ngeneratesthesampleisdeterminedbysamplingacomponentidentityfroma\\nmultinoullidistribution:\\nP() =x\\ue058\\niPiPi (= c )( = xc| ) (3.29)\\nwherecisthemultinoullidistributionovercomponentidentities. P()\\nWehavealreadyseenoneexampleofamixturedistribution:theempirical\\ndistributionoverreal-valuedvariablesisamixturedistributionwithoneDirac\\ncomponentforeachtrainingexample.\\n66', 'CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\nThemixturemodelisonesimplestrategyforcombiningprobabilitydistributions\\ntocreatearicherdistribution.Inchapter,weexploretheartofbuildingcomplex 16\\nprobabilitydistributionsfromsimpleonesinmoredetail.\\nThemixturemodelallowsustobrieﬂyglimpseaconceptthatwillbeof\\nparamountimportancelater—the l at e n t v ar i abl e.Alatentvariableisarandom\\nvariablethatwecannotobservedirectly.Thecomponentidentityvariablecofthe\\nmixturemodelprovidesanexample.Latentvariablesmayberelatedtoxthrough\\nthejointdistribution,inthiscase,P(xc,) =P(xc|)P(c).ThedistributionP(c)\\noverthelatentvariableandthedistributionP(xc|)relatingthelatentvariables\\ntothevisiblevariablesdeterminestheshapeofthedistributionP(x)eventhough\\nitispossibletodescribeP(x)withoutreferencetothelatentvariable.Latent\\nvariablesarediscussedfurtherinsection.16.5\\nAverypowerfulandcommontypeofmixturemodelisthe G aussian m i x t ur e\\nmodel,inwhichthecomponentsp( x|c=i)areGaussians.Eachcomponenthas\\naseparatelyparametrized mean µ( ) iandcovariance Σ( ) i.Somemixturescanhave\\nmoreconstraints.Forexample,thecovariancescouldbesharedacrosscomponents\\nviatheconstraint Σ( ) i= Σ,i∀.AswithasingleGaussiandistribution,themixture\\nofGaussiansmightconstrainthecovariancematrixforeachcomponenttobe\\ndiagonalorisotropic.\\nInadditiontothemeansandcovariances,theparametersofaGaussianmixture\\nspecifythe pr i o r pr o babili t yα i=P(c=i) giventoeachcomponenti.Theword\\n“prior”indicatesthatitexpressesthemodel’sbeliefsaboutc b e f o r eithasobserved\\nx.Bycomparison,P(c| x)isa p o st e r i o r pr o babili t y,becauseitiscomputed\\na f t e robservationof x.AGaussianmixturemodelisa uni v e r sal appr o x i m a t o r\\nofdensities,inthesensethatanysmoothdensitycanbeapproximatedwithany\\nspeciﬁc,non-zeroamountoferrorbyaGaussianmixturemodelwithenough\\ncomponents.\\nFigureshowssamplesfromaGaussianmixturemodel. 3.2\\n3.10UsefulPropertiesofCommonFunctions\\nCertainfunctionsariseoftenwhileworkingwithprobabilitydistributions,especially\\ntheprobabilitydistributionsusedindeeplearningmodels.\\nOneofthesefunctionsisthe : l o g i st i c si g m o i d\\nσx() =1\\n1+exp()−x. (3.30)\\nThelogisticsigmoidiscommonlyusedtoproducetheφparameterofaBernoulli\\n67', 'CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\nx 1x 2\\nFigure3.2:\\xa0SamplesfromaGaussianmixturemodel.Inthisexample,therearethree\\ncomponents.Fromlefttoright,theﬁrstcomponenthasanisotropiccovariancematrix,\\nmeaningithasthesameamountofvarianceineachdirection.Thesecondhasadiagonal\\ncovariancematrix,meaningitcancontrolthevarianceseparatelyalongeachaxis-aligned\\ndirection.Thisexamplehasmorevariancealongthex 2axisthanalongthex 1axis.The\\nthirdcomponenthasafull-rankcovariancematrix,allowingittocontrolthevariance\\nseparatelyalonganarbitrarybasisofdirections.\\ndistributionbecauseitsrangeis(0,1),whichlieswithinthevalidrangeofvalues\\nfortheφparameter.Seeﬁgureforagraphofthesigmoidfunction.The 3.3\\nsigmoidfunction sat ur at e swhenitsargumentisverypositiveorverynegative,\\nmeaningthatthefunctionbecomesveryﬂatandinsensitivetosmallchangesinits\\ninput.\\nAnothercommonlyencounteredfunctionisthe sof t pl usfunction(,Dugas e t a l .\\n2001):\\nζx x. () = log(1+exp()) (3.31)\\nThesoftplusfunctioncanbeusefulforproducingtheβorσparameterofanormal\\ndistributionbecauseitsrangeis(0,∞).Italsoarisescommonlywhenmanipulating\\nexpressionsinvolvingsigmoids.Thenameofthesoftplusfunctioncomesfromthe\\nfactthatitisasmoothedor“softened”versionof\\nx+= max(0),x. (3.32)\\nSeeﬁgureforagraphofthesoftplusfunction. 3.4\\nThefollowingpropertiesareallusefulenoughthatyoumaywishtomemorize\\nthem:\\n68', 'CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\n− − 1 0 5 0 5 1 0\\nx0 0 .0 2 .0 4 .0 6 .0 8 .1 0 .σ x ( )\\nFigure3.3:Thelogisticsigmoidfunction.\\n− − 1 0 5 0 5 1 0\\nx024681 0ζ x ( )\\nFigure3.4:Thesoftplusfunction.\\n69', 'CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\nσx() =exp()x\\nexp()+exp(0)x(3.33)\\nd\\ndxσxσxσx () = ()(1−()) (3.34)\\n1 () = () −σxσ−x (3.35)\\nlog() = () σx −ζ−x (3.36)\\nd\\ndxζxσx () = () (3.37)\\n∀∈x(01),,σ− 1() = logx\\ue012x\\n1−x\\ue013\\n(3.38)\\n∀x>,ζ0− 1() = log(exp()1) x x− (3.39)\\nζx() =\\ue05ax\\n− ∞σydy() (3.40)\\nζxζxx ()−(−) = (3.41)\\nThefunctionσ− 1(x)iscalledthe l o g i tinstatistics,butthistermismorerarely\\nusedinmachinelearning.\\nEquationprovidesextrajustiﬁcationforthename“softplus.”Thesoftplus 3.41\\nfunctionisintendedasasmoothedversionofthe p o si t i v e par tfunction,x+=\\nmax{0,x}.Thepositivepartfunctionisthecounterpartofthe negat i v e par t\\nfunction,x−=max{0,x−}.Toobtainasmoothfunctionthatisanalogoustothe\\nnegativepart,onecanuseζ(−x).Justasxcanberecoveredfromitspositivepart\\nandnegativepartviatheidentityx+−x−=x,itisalsopossibletorecoverx\\nusingthesamerelationshipbetweenand,asshowninequation. ζx()ζx(−) 3.41\\n3.11Bayes’Rule\\nWeoftenﬁndourselvesinasituationwhereweknowP(yx|)andneedtoknow\\nP(xy|).Fortunately,ifwealsoknowP(x),wecancomputethedesiredquantity\\nusing B a y e s’ r ul e:\\nP( ) =xy|PP()x( )yx|\\nP()y. (3.42)\\nNotethatwhileP(y)appearsintheformula,itisusuallyfeasibletocompute\\nP() =y\\ue050\\nxPxPx P (y|)(),sowedonotneedtobeginwithknowledgeof()y.\\n70', 'CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\nBayes’ruleis\\xa0straightforwardto\\xa0derivefrom\\xa0thedeﬁnitionofconditional\\nprobability,butitisusefultoknowthenameofthisformulasincemanytexts\\nrefertoitbyname.ItisnamedaftertheReverendThomasBayes,whoﬁrst\\ndiscoveredaspecialcaseoftheformula.Thegeneralversionpresentedherewas\\nindependentlydiscoveredbyPierre-SimonLaplace.\\n3.12TechnicalDetailsofContinuousVariables\\nAproperformalunderstandingofcontinuousrandomvariablesandprobability\\ndensityfunctionsrequiresdevelopingprobabilitytheoryintermsofabranchof\\nmathematics knownas m e asur e t heor y.Measuretheoryisbeyondthescopeof\\nthistextbook,butwecanbrieﬂysketchsomeoftheissuesthatmeasuretheoryis\\nemployedtoresolve.\\nInsection,wesawthattheprobabilityofacontinuousvector-valued 3.3.2 x\\nlyinginsomeset Sisgivenbytheintegralofp( x)overtheset S.Somechoices\\nofset Scanproduceparadoxes.Forexample,itispossibletoconstructtwosets\\nS 1and S 2suchthatp( x∈ S 1) +p( x∈ S 2)>1but S 1∩ S 2=∅.Thesesets\\naregenerallyconstructedmakingveryheavyuseoftheinﬁniteprecisionofreal\\nnumbers,forexamplebymakingfractal-shapedsetsorsetsthataredeﬁnedby\\ntransformingthesetofrationalnumbers.2Oneofthekeycontributionsofmeasure\\ntheoryistoprovideacharacterization ofthesetofsetsthatwecancomputethe\\nprobabilityofwithoutencounteringparadoxes.\\xa0Inthisbook,weonlyintegrate\\noversetswithrelativelysimpledescriptions,sothisaspectofmeasuretheorynever\\nbecomesarelevantconcern.\\nForourpurposes,measuretheoryismoreusefulfordescribingtheoremsthat\\napplytomostpointsin Rnbutdonotapplytosomecornercases.Measuretheory\\nprovidesarigorouswayofdescribingthatasetofpointsisnegligiblysmall.Such\\nasetissaidtohave m e asur e z e r o.Wedonotformallydeﬁnethisconceptinthis\\ntextbook.Forourpurposes,itissuﬃcienttounderstandtheintuitionthataset\\nofmeasurezerooccupiesnovolumeinthespacewearemeasuring.Forexample,\\nwithin R2,alinehasmeasurezero,whileaﬁlledpolygonhaspositivemeasure.\\nLikewise,anindividualpointhasmeasurezero.Anyunionofcountablymanysets\\nthateachhavemeasurezeroalsohasmeasurezero(sothesetofalltherational\\nnumbershasmeasurezero,forinstance).\\nAnotherusefultermfrommeasuretheoryis al m o st e v e r y wher e.Aproperty\\nthatholdsalmosteverywhereholdsthroughoutallofspaceexceptforonasetof\\n2TheBanach-Tarskitheoremprovidesafunexampleofsuchsets.\\n71', 'CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\nmeasurezero.Becausetheexceptionsoccupyanegligibleamountofspace,they\\ncanbesafelyignoredformanyapplications.Someimportantresultsinprobability\\ntheoryholdforalldiscretevaluesbutonlyhold“almosteverywhere”forcontinuous\\nvalues.\\nAnothertechnicaldetailofcontinuousvariablesrelatestohandlingcontinuous\\nrandomvariablesthataredeterministicfunctionsofoneanother.Supposewehave\\ntworandomvariables, xand y,suchthat y=g( x),wheregisaninvertible,con-\\ntinuous,diﬀerentiabletransformation.Onemightexpectthatp y( y) =p x(g− 1( y)).\\nThisisactuallynotthecase.\\nAsasimpleexample,supposewehavescalarrandomvariablesxandy.Suppose\\ny=x\\n2andx∼U(0,1).Ifweusetherulep y(y)=p x(2y)thenp ywillbe0\\neverywhereexcepttheinterval[0,1\\n2] 1 ,anditwillbeonthisinterval.Thismeans\\n\\ue05a\\np y()=ydy1\\n2, (3.43)\\nwhichviolatesthedeﬁnitionofaprobabilitydistribution.Thisisacommonmistake.\\nTheproblemwiththisapproachisthatitfailstoaccountforthedistortionof\\nspaceintroducedbythefunctiong.Recallthattheprobabilityof xlyinginan\\ninﬁnitesimallysmallregionwithvolumeδ xisgivenbyp( x)δ x.Sincegcanexpand\\norcontractspace,theinﬁnitesimalvolumesurrounding xin xspacemayhave\\ndiﬀerentvolumeinspace. y\\nToseehowtocorrecttheproblem,wereturntothescalarcase.Weneedto\\npreservetheproperty\\n|p y(())= gxdy||p x()xdx.| (3.44)\\nSolvingfromthis,weobtain\\np y() = yp x(g− 1())y\\ue00c\\ue00c\\ue00c\\ue00c∂x\\n∂y\\ue00c\\ue00c\\ue00c\\ue00c(3.45)\\norequivalently\\np x() = xp y(())gx\\ue00c\\ue00c\\ue00c\\ue00c∂gx()\\n∂x\\ue00c\\ue00c\\ue00c\\ue00c. (3.46)\\nInhigherdimensions,thederivativegeneralizestothedeterminantofthe J ac o bi an\\nm at r i x—thematrixwithJ i , j=∂ x i\\n∂ y j.Thus,forreal-valuedvectorsand, x y\\np x() = xp y(())g x\\ue00c\\ue00c\\ue00c\\ue00cdet\\ue012∂g() x\\n∂ x\\ue013 \\ue00c\\ue00c\\ue00c\\ue00c. (3.47)\\n72', 'CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\n3.13InformationTheory\\nInformationtheory\\xa0isa\\xa0branchof\\xa0appliedmathematics\\xa0thatrevolvesaround\\nquantifyinghowmuchinformationispresentinasignal.Itwasoriginallyinvented\\ntostudysendingmessagesfromdiscretealphabetsoveranoisychannel,suchas\\ncommunicationviaradiotransmission.Inthiscontext,informationtheorytellshow\\ntodesignoptimalcodesandcalculatetheexpectedlengthofmessagessampledfrom\\nspeciﬁcprobabilitydistributionsusingvariousencodingschemes.Inthecontextof\\nmachinelearning,wecanalsoapplyinformationtheorytocontinuousvariables\\nwheresomeofthesemessagelengthinterpretations donotapply.Thisﬁeldis\\nfundamentaltomanyareasofelectricalengineeringandcomputerscience.Inthis\\ntextbook,wemostlyuseafewkeyideasfrominformationtheorytocharacterize\\nprobabilitydistributionsorquantifysimilaritybetweenprobabilitydistributions.\\nFormoredetailoninformationtheory,seeCoverandThomas2006MacKay ()or\\n().2003\\nThebasicintuitionbehindinformationtheoryisthatlearningthatanunlikely\\neventhas\\xa0occurredismoreinformativethanlearningthata\\xa0likely\\xa0eventhas\\noccurred.Amessagesaying“thesunrosethismorning”issouninformative as\\ntobeunnecessarytosend,butamessagesaying“therewasasolareclipsethis\\nmorning”isveryinformative.\\nWewouldliketoquantifyinformationinawaythatformalizesthisintuition.\\nSpeciﬁcally,\\n•Likelyeventsshouldhavelowinformationcontent,andintheextremecase,\\neventsthatareguaranteedtohappenshouldhavenoinformationcontent\\nwhatsoever.\\n•Lesslikelyeventsshouldhavehigherinformationcontent.\\n•Independenteventsshouldhaveadditiveinformation. Forexample,ﬁnding\\noutthatatossedcoinhascomeupasheadstwiceshouldconveytwiceas\\nmuchinformationasﬁndingoutthatatossedcoinhascomeupasheads\\nonce.\\nInordertosatisfyallthreeoftheseproperties,wedeﬁnethe se l f - i nf o r m a t i o n\\nofaneventxtobe = x\\nIxPx. () = log− () (3.48)\\nInthisbook,wealwaysuselogtomeanthenaturallogarithm,withbasee.Our\\ndeﬁnitionofI(x)isthereforewritteninunitsof nat s.Onenatistheamountof\\n73', 'CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\ninformationgainedbyobservinganeventofprobability1\\ne.Othertextsusebase-2\\nlogarithmsandunitscalled bi t sor shannons;informationmeasuredinbitsis\\njustarescalingofinformationmeasuredinnats.\\nWhenxiscontinuous,weusethesamedeﬁnitionofinformationbyanalogy,\\nbutsomeofthepropertiesfromthediscretecasearelost.Forexample,anevent\\nwithunitdensitystillhaszeroinformation, despitenotbeinganeventthatis\\nguaranteedtooccur.\\nSelf-information dealsonlywithasingleoutcome.Wecanquantifytheamount\\nofuncertaintyinanentireprobabilitydistributionusingthe Shannon e nt r o p y:\\nH() = x E x ∼ P[()] = Ix − E x ∼ P[log()]Px. (3.49)\\nalsodenotedH(P).Inotherwords,theShannonentropyofadistributionisthe\\nexpectedamountofinformationinaneventdrawnfromthatdistribution.Itgives\\nalowerboundonthenumberofbits(ifthelogarithmisbase2,otherwisetheunits\\narediﬀerent)neededonaveragetoencodesymbolsdrawnfromadistributionP.\\nDistributionsthatarenearlydeterministic(wheretheoutcomeisnearlycertain)\\nhavelowentropy;distributionsthatareclosertouniformhavehighentropy.See\\nﬁgureforademonstration.When 3.5 xiscontinuous,theShannonentropyis\\nknownasthe di ﬀ e r e n t i al e nt r o p y.\\nIfwehavetwoseparateprobabilitydistributionsP(x)andQ(x)overthesame\\nrandomvariablex,wecanmeasurehowdiﬀerentthesetwodistributionsareusing\\nthe K ul l bac k - L e i bl e r ( K L ) di v e r g e nc e:\\nD K L( ) = PQ\\ue06b E x ∼ P\\ue014\\nlogPx()\\nQx()\\ue015\\n= E x ∼ P[log()log()] Px−Qx.(3.50)\\nInthecaseofdiscretevariables,itistheextraamountofinformation(measured\\ninbitsifweusethebaselogarithm,butinmachinelearningweusuallyusenats 2\\nandthenaturallogarithm)neededtosendamessagecontainingsymbolsdrawn\\nfromprobabilitydistributionP,whenweuseacodethatwasdesignedtominimize\\nthelengthofmessagesdrawnfromprobabilitydistribution.Q\\nTheKLdivergencehasmanyusefulproperties,mostnotablythatitisnon-\\nnegative.TheKLdivergenceis0ifandonlyifPandQarethesamedistributionin\\nthecaseofdiscretevariables,orequal“almosteverywhere”inthecaseofcontinuous\\nvariables.BecausetheKLdivergenceisnon-negativeandmeasuresthediﬀerence\\nbetweentwodistributions,itisoftenconceptualized asmeasuringsomesortof\\ndistancebetweenthesedistributions.However,itisnotatruedistancemeasure\\nbecauseitisnotsymmetric:D K L(PQ\\ue06b)\\ue036=D K L(QP\\ue06b)forsomePandQ.\\xa0This\\n74', 'CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\n0 0 0 2 0 4 0 6 0 8 1 0 . . . . . .\\np0 0 .0 1 .0 2 .0 3 .0 4 .0 5 .0 6 .0 7 .Sha nno n e ntr o p y i n na t s\\nFigure3.5:Thisplotshowshowdistributionsthatareclosertodeterministichavelow\\nShannonentropywhiledistributionsthatareclosetouniformhavehighShannonentropy.\\nOnthehorizontalaxis,weplotp,theprobabilityofabinaryrandomvariablebeingequal\\nto.Theentropyisgivenby 1 (p−1)log(1−p)−pplog.Whenpisnear0,thedistribution\\nisnearlydeterministic,becausetherandomvariableisnearlyalways0.Whenpisnear1,\\nthedistributionisnearlydeterministic,becausetherandomvariableisnearlyalways1.\\nWhenp= 0.5,theentropyismaximal,becausethedistributionisuniformoverthetwo\\noutcomes.\\nasymmetrymeansthatthereareimportantconsequencestothechoiceofwhether\\ntouseD K L( )PQ\\ue06borD K L( )QP\\ue06b.Seeﬁgureformoredetail.3.6\\nAquantitythatiscloselyrelatedtotheKLdivergenceisthe c r o ss-en t r o p y\\nH(P,Q) =H(P)+D K L(PQ\\ue06b),whichissimilartotheKLdivergencebutlacking\\nthetermontheleft:\\nHP,Q( ) = − E x ∼ Plog()Qx. (3.51)\\nMinimizingthecross-entropywithrespecttoQisequivalenttominimizingthe\\nKLdivergence,becausedoesnotparticipateintheomittedterm. Q\\nWhencomputingmanyofthesequantities,itiscommontoencounterexpres-\\nsionsoftheform0log0.Byconvention,inthecontextofinformationtheory,we\\ntreattheseexpressionsaslim x → 0xxlog= 0.\\n3.14StructuredProbabilisticModels\\nMachinelearningalgorithmsofteninvolveprobabilitydistributionsoveravery\\nlargenumberofrandomvariables.Often,theseprobabilitydistributionsinvolve\\ndirectinteractionsbetweenrelativelyfewvariables.Usingasinglefunctionto\\n75', 'CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\nxProbability Densityq∗= argminq D K L() p q \\ue06b\\np x()\\nq∗() x\\nxProbability Densityq∗= argminq D K L() q p \\ue06b\\np() x\\nq∗() x\\nFigure3.6:TheKLdivergenceisasymmetric.Supposewehaveadistributionp(x)and\\nwishtoapproximateitwithanotherdistributionq(x).Wehavethechoiceofminimizing\\neitherD KL(pq\\ue06b)orD KL(qp\\ue06b).Weillustratetheeﬀectofthischoiceusingamixtureof\\ntwoGaussiansforp,andasingleGaussianforq.\\xa0Thechoiceofwhichdirectionofthe\\nKLdivergencetouseisproblem-dependent.Someapplicationsrequireanapproximation\\nthatusuallyplaceshighprobabilityanywherethatthetruedistributionplaceshigh\\nprobability,whileotherapplicationsrequireanapproximationthatrarelyplaceshigh\\nprobabilityanywherethatthetruedistributionplaceslowprobability.Thechoiceofthe\\ndirectionoftheKLdivergencereﬂectswhichoftheseconsiderationstakespriorityforeach\\napplication. ( L e f t )TheeﬀectofminimizingD KL(pq\\ue06b).Inthiscase,weselectaqthathas\\nhighprobabilitywherephashighprobability.Whenphasmultiplemodes,qchoosesto\\nblurthemodestogether,inordertoputhighprobabilitymassonallofthem. ( R i g h t )The\\neﬀectofminimizingD KL(qp\\ue06b).Inthiscase,weselectaqthathaslowprobabilitywhere\\nphaslowprobability.Whenphasmultiplemodesthataresuﬃcientlywidelyseparated,\\nasinthisﬁgure,theKLdivergenceisminimizedbychoosingasinglemode,inorderto\\navoidputtingprobabilitymassinthelow-probabilityareasbetweenmodesofp.Here,we\\nillustratetheoutcomewhenqischosentoemphasizetheleftmode.Wecouldalsohave\\nachievedanequalvalueoftheKLdivergencebychoosingtherightmode.Ifthemodes\\narenotseparatedbyasuﬃcientlystronglowprobabilityregion,thenthisdirectionofthe\\nKLdivergencecanstillchoosetoblurthemodes.\\n76', 'CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\ndescribetheentirejointprobabilitydistributioncanbeveryineﬃcient(both\\ncomputationally andstatistically).\\nInsteadofusingasinglefunctiontorepresentaprobabilitydistribution,we\\ncansplitaprobabilitydistributionintomanyfactorsthatwemultiplytogether.\\nForexample,supposewehavethreerandomvariables:a,bandc.Supposethat\\nainﬂuencesthevalueofbandbinﬂuencesthevalueofc,butthataandcare\\nindependentgivenb.Wecanrepresenttheprobabilitydistributionoverallthree\\nvariablesasaproductofprobabilitydistributionsovertwovariables:\\np,,ppp. (abc) = ()a( )ba|( )cb| (3.52)\\nThesefactorizationscangreatlyreducethenumberofparametersneeded\\ntodescribethedistribution.Eachfactorusesanumberofparametersthatis\\nexponentialinthenumberofvariablesinthefactor.Thismeansthatwecangreatly\\nreducethecostofrepresentingadistributionifweareabletoﬁndafactorization\\nintodistributionsoverfewervariables.\\nWecandescribethesekindsoffactorizationsusinggraphs.Hereweusetheword\\n“graph”inthesenseofgraphtheory:asetofverticesthatmaybeconnectedtoeach\\notherwithedges.Whenwerepresentthefactorizationofaprobabilitydistribution\\nwithagraph,wecallita st r uc t ur e d pr o babili s t i c m o delor g r aphic al m o del.\\nTherearetwomainkindsofstructuredprobabilisticmodels:directedand\\nundirected.Bothkindsofgraphicalmodelsuseagraph Ginwhicheachnode\\ninthegraphcorrespondstoarandomvariable,\\xa0and anedgeconnectingtwo\\nrandomvariablesmeansthattheprobabilitydistributionisabletorepresentdirect\\ninteractionsbetweenthosetworandomvariables.\\nD i r e c t e dmodelsuse\\xa0graphswithdirectededges,\\xa0andtheyrepresentfac-\\ntorizationsintoconditionalprobabilitydistributions,asintheexampleabove.\\nSpeciﬁcally,adirectedmodelcontainsonefactorforeveryrandomvariablex iin\\nthedistribution,andthatfactorconsistsoftheconditionaldistributionoverx i\\ngiventheparentsofx i,denotedPa G(x i):\\np() = x\\ue059\\nip(x i|Pa G(x i)). (3.53)\\nSeeﬁgureforanexampleofadirectedgraphandthefactorizationofprobability 3.7\\ndistributionsitrepresents.\\nU ndi r e c t e dmodelsusegraphswithundirectededges,andtheyrepresent\\nfactorizationsintoasetoffunctions;unlikeinthedirectedcase,thesefunctions\\n77', 'CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\naa\\nccbb\\needd\\nFigure3.7:Adirectedgraphicalmodeloverrandomvariablesa,b,c,dande.Thisgraph\\ncorrespondstoprobabilitydistributionsthatcanbefactoredas\\np,,,,ppp,pp. (abcde) = ()a( )ba|(ca|b)( )db|( )ec| (3.54)\\nThisgraphallowsustoquicklyseesomepropertiesofthedistribution.Forexample,a\\nandcinteractdirectly,butaandeinteractonlyindirectlyviac.\\nareusuallynotprobabilitydistributionsofanykind.Anysetofnodesthatareall\\nconnectedtoeachotherinGiscalledaclique.Eachclique C( ) iinanundirected\\nmodelisassociatedwithafactorφ( ) i(C( ) i).Thesefactorsarejustfunctions,not\\nprobabilitydistributions.Theoutputofeachfactormustbenon-negative, but\\nthereisnoconstraintthatthefactormustsumorintegrateto1likeaprobability\\ndistribution.\\nTheprobabilityofaconﬁgurationofrandomvariablesis pr o p o r t i o naltothe\\nproductofallofthesefactors—assignmentsthatresultinlargerfactorvaluesare\\nmorelikely.Ofcourse,thereisnoguaranteethatthisproductwillsumto1.We\\nthereforedividebyanormalizingconstantZ,deﬁnedtobethesumorintegral\\noverallstatesoftheproductoftheφfunctions,inordertoobtainanormalized\\nprobabilitydistribution:\\np() = x1\\nZ\\ue059\\niφ( ) i\\ue010\\nC( ) i\\ue011\\n. (3.55)\\nSeeﬁgureforanexampleofanundirectedgraphandthefactorizationof 3.8\\nprobabilitydistributionsitrepresents.\\nKeep\\xa0inmind\\xa0thatthese\\xa0graphicalrepresentationsof\\xa0factorizations are\\xa0a\\nlanguagefordescribingprobabilitydistributions.Theyarenotmutuallyexclusive\\nfamiliesofprobabilitydistributions.Beingdirectedorundirectedisnotaproperty\\nofaprobabilitydistribution;itisapropertyofaparticular desc r i pti o nofa\\n78', 'CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\naa\\nccbb\\needd\\nFigure3.8:Anundirectedgraphicalmodeloverrandomvariablesa,b,c,dande.This\\ngraphcorrespondstoprobabilitydistributionsthatcanbefactoredas\\np,,,, (abcde) =1\\nZφ( 1 )( )abc,,φ( 2 )()bd,φ( 3 )()ce,. (3.56)\\nThisgraphallowsustoquicklyseesomepropertiesofthedistribution.Forexample,a\\nandcinteractdirectly,butaandeinteractonlyindirectlyviac.\\nprobabilitydistribution,butanyprobabilitydistributionmaybedescribedinboth\\nways.\\nThroughoutpartsandofthisbook,wewillusestructuredprobabilistic III\\nmodelsmerelyasalanguagetodescribewhichdirectprobabilisticrelationships\\ndiﬀerentmachinelearningalgorithmschoosetorepresent.Nofurtherunderstanding\\nofstructuredprobabilisticmodelsisneededuntilthediscussionofresearchtopics,\\ninpart,wherewewillexplorestructuredprobabilisticmodelsinmuchgreater III\\ndetail.\\nThischapterhasreviewedthebasicconceptsofprobabilitytheorythatare\\nmostrelevanttodeeplearning.Onemoresetoffundamentalmathematical tools\\nremains:numericalmethods.\\n79']}, '12_distributed_dl.pdf': {'text': ['Distributed Deep Learning\\nSlides by Amir H. Payberah and Jim Dowling', 'The need for Distributed Training of DNNs\\n1 / 66', 'Growth in the Size of Deep Neural Networks\\nNvidia Link\\n2 / 66', 'Training Deep Neural Networks\\n▶Computationally intensive\\n▶Time consuming\\n[https://cloud.google.com/tpu/docs/images/inceptionv3onc--oview.png]\\n3 / 66', 'Why?\\n▶Massive amount of training dataset\\n▶Large number of parameters\\n4 / 66', 'Accuracy vs. Data/Model Size\\n[Jeff Dean at AI Frontiers: Trends and Developments in Deep Learning Research]\\n5 / 66', 'Accuracy vs. Data/Model Size\\n[Jeff Dean at AI Frontiers: Trends and Developments in Deep Learning Research]\\n6 / 66', 'Accuracy vs. Data/Model Size\\n[Jeff Dean at AI Frontiers: Trends and Developments in Deep Learning Research]\\n7 / 66', 'Fundamentals of Machine Learning\\n8 / 66', 'Training Dataset\\n▶E.g., tabular data, image, text, etc.\\n9 / 66', 'Model\\n▶E.g., linear models, neural networks, etc.\\n▶^ y=fw(x)\\n10 / 66', 'Loss function\\n▶How good ^ yis able to predict the expected outcome y.\\n▶J(w) =Pm\\ni=1l(yi,^ yi)\\n▶E.g., J(w) =1\\nmPm\\ni=1(yi−^ yi)2\\n11 / 66', 'Objective\\n▶Minimize the loss function\\n▶arg min wJ(w)\\n▶J(w) =Pm\\ni=1l(yi,^ yi)\\n12 / 66', 'Training\\n▶J(w) =Pm\\ni=1l(yi,^ yi)\\n▶Gradient descent, i.e., w:=w−η∇J(w)\\n▶Stochastic gradient descent, i.e., w:=w−η~ gJ(w)\\n•~ g: gradient at a randomly chosen point.\\n▶Mini-barch gradient descent, i.e., w:=w−η~ gBJ(w)\\n•~ g: gradient with respect to a set of Brandomly chosen points.\\n13 / 66', 'Let’s Scale the Learning\\n14 / 66', 'Scalable Training\\n▶Data parallelism\\n▶Model parallelism\\n15 / 66', 'Data Parallelism\\n16 / 66', 'Data Parallelization (1/4)\\n▶Replicate a whole model on every device.\\n▶Train all replicas simultaneously, using a different mini-batch for each.\\n[Tang et al., Communication-Efficient Distributed Deep Learning: A Comprehensive Survey, 2020]\\n17 / 66', 'Data Parallelization (2/4)\\n▶kdevices\\n▶Jj(w) =Pbj\\ni=1l(yi,^ yi),∀j=1,2,···,k\\n▶~ gBJj(w): gradient of Jj(w) with respect to a set of Brandomly chosen points at\\ndevice j.\\n▶Compute ~ gBJj(w) on each device j.\\n[Tang et al., Communication-Efficient Distributed Deep Learning: A Comprehensive Survey, 2020]\\n18 / 66', 'Data Parallelization (3/4)\\n▶Compute the mean of the gradients.\\n▶~ gBJ(w) =1\\nkPk\\nj=1~ gBJj(w)\\n[Tang et al., Communication-Efficient Distributed Deep Learning: A Comprehensive Survey, 2020]\\n19 / 66', 'Data Parallelization (4/4)\\n▶Update the model.\\n▶w:=w−η~ gBJ(w)\\n[Tang et al., Communication-Efficient Distributed Deep Learning: A Comprehensive Survey, 2020]\\n20 / 66', 'Data Parallelization Design Issues\\n▶The aggregation algorithm\\n▶Communication synchronization and frequency\\n▶Communication compression\\n21 / 66', 'The Aggregation Algorithm\\n22 / 66', 'The Aggregation Algorithm\\n▶How to aggregate gradients (compute the mean of the gradients)?\\n▶Centralized - parameter server\\n▶Decentralized - all-reduce\\n▶Decentralized - gossip\\n23 / 66', 'Aggregation - Centralized - Parameter Server\\n▶Store the model parameters outside of the workers.\\n▶Workers periodically report their computed parameters or parameter updates to a\\n(set of) parameter server(s) (PSs).\\n[Tang et al., Communication-Efficient Distributed Deep Learning: A Comprehensive Survey, 2020]\\n24 / 66', 'Aggregation - Distributed - All-Reduce\\n▶Mirror all the model parameters across all workers (no PS).\\n▶Workers exchange parameter updates directly via an allreduce operation.\\n[Tang et al., Communication-Efficient Distributed Deep Learning: A Comprehensive Survey, 2020]\\n25 / 66', 'Aggregation - Distributed - Gossip\\n▶No PS, and no global model.\\n▶Every worker communicates updates with their neighbors.\\n▶The consistency of parameters across all workers only at the end of the algorithm.\\n[Tang et al., Communication-Efficient Distributed Deep Learning: A Comprehensive Survey, 2020]\\n26 / 66', 'Reduce and AllReduce (1/2)\\n▶Reduce: reducing a set of numbers into a smaller set of numbers via a function.\\n▶E.g., sum([1, 2, 3, 4, 5]) = 15\\n▶Reduce takes an array of input elements on each process and returns an array of\\noutput elements to the root process.\\n[https://mpitutorial.com/tutorials/mpi-reduce-and-allreduce]\\n27 / 66', 'Reduce and AllReduce (2/2)\\n▶AllReduce stores reduced results across all processes rather than the root process.\\n[https://mpitutorial.com/tutorials/mpi-reduce-and-allreduce]\\n28 / 66', 'AllReduce Example\\nInitial state After AllReduce operation\\n[https://towardsdatascience.com/visual-intuition-on-ring-allreduce-for-distributed-deep-learning-d1f34b4911da]\\n29 / 66', 'AllReduce Implementation\\n▶All-to-all allreduce\\n▶Master-worker allreduce\\n▶Tree allreduce\\n▶Round-robin allreduce\\n▶Butterfly allreduce\\n▶Ring allreduce\\n30 / 66', 'AllReduce Implementation - All-to-All AllReduce\\n▶Send the array of data to each other.\\n▶Apply the reduction operation on each process.\\n▶Too many unnecessary messages.\\n[https://towardsdatascience.com/visual-intuition-on-ring-allreduce-for-distributed-deep-learning-d1f34b4911da]\\n31 / 66', 'AllReduce Implementation - Master-Worker AllReduce\\n▶Selecting one process as a master, gather all arrays into the master.\\n▶Perform reduction operations locally in the master.\\n▶Distribute the result to the other processes.\\n▶The master becomes a bottleneck (not scalable).\\n[https://towardsdatascience.com/visual-intuition-on-ring-allreduce-for-distributed-deep-learning-d1f34b4911da]\\n32 / 66', 'AllReduce Implementation - Other implementations\\n▶Some try to minimize bandwidth.\\n▶Some try to minimize latency.\\n[Zhao H. et al., arXiv:1312.3020, 2013]\\n33 / 66', 'AllReduce Implementation - Ring-AllReduce (1/6)\\n▶The Ring-Allreduce has two phases:\\n1. First, the share-reduce phase\\n2. Then, the share-only phase\\n34 / 66', 'AllReduce Implementation - Ring-AllReduce (2/6)\\n▶In the share-reduce phase, each process psends data to the process (p+1)%m\\n•mis the number of processes, and %is the modulo operator.\\n▶The array of data on each process is divided to mchunks ( m=4here).\\n▶Each one of these chunks will be indexed by igoing forward.\\n[https://towardsdatascience.com/visual-intuition-on-ring-allreduce-for-distributed-deep-learning-d1f34b4911da]\\n35 / 66', 'AllReduce Implementation - Ring-AllReduce (3/6)\\n▶In the first share-reduce step, process Asends a0to process B.\\n▶Process Bsends b1to process C, etc.\\n[https://towardsdatascience.com/visual-intuition-on-ring-allreduce-for-distributed-deep-learning-d1f34b4911da]\\n36 / 66', 'AllReduce Implementation - Ring-AllReduce (4/6)\\n▶When each process receives the data from the previous process, it applies the reduce\\noperator (e.g., sum)\\n•The reduce operator should be associative and commutative.\\n▶It then proceeds to send it to the next process in the ring.\\n[https://towardsdatascience.com/visual-intuition-on-ring-allreduce-for-distributed-deep-learning-d1f34b4911da]\\n37 / 66', 'AllReduce Implementation - Ring-AllReduce (5/6)\\n▶The share-reduce phase finishes when each process holds the complete reduction of\\nchunk i.\\n▶At this point each process holds a part of the end result.\\n[https://towardsdatascience.com/visual-intuition-on-ring-allreduce-for-distributed-deep-learning-d1f34b4911da]\\n38 / 66', 'AllReduce Implementation - Ring-AllReduce (6/6)\\n▶The share-only step is the same process of sharing the data in a ring-like fashion\\nwithout applying the reduce operation.\\n▶This consolidates the result of each chunk in every process.\\n[https://towardsdatascience.com/visual-intuition-on-ring-allreduce-for-distributed-deep-learning-d1f34b4911da]\\n39 / 66', 'Master-Worker AllReduce vs. Ring-AllReduce\\n▶N: number of elements, m: number of processes\\n▶Master-Worker AllReduce\\n•First each process sends Nelements to the master: N×(m−1) messages.\\n•Then the master sends the results back to the process: another N×(m−1) messages.\\n•Total network traffic is 2(N×(m−1)), which is proportional to m.\\n▶Ring-AllReduce\\n•In the share-reduce step each process sendsN\\nmelements, and it does it m−1times:\\nN\\nm×(m−1) messages.\\n•On the share-only step, each process sends the result for the chunk it calculated: another\\nN\\nm×(m−1) messages.\\n•Total network traffic is 2(N\\nm×(m−1)).\\n40 / 66', 'Communication Synchronization and Frequency\\n41 / 66', 'Synchronization\\n▶When to synchronize the parameters among the parallel workers?\\n42 / 66', 'Communication Synchronization (1/2)\\n▶Synchronizing the model replicas in data-parallel training requires communication\\n•between workers, in allreduce\\n•between workers and parameter servers, in the centralized architecture\\n▶The communication synchronization decides how frequently all local models are syn-\\nchronized with others.\\n43 / 66', 'Communication Synchronization (2/2)\\n▶It will influence:\\n•The communication traffic\\n•The performance\\n•The convergence of model training\\n▶There is a trade-off between the communication traffic and the convergence.\\n44 / 66', 'Reducing Synchronization Overhead\\n▶Two directions for improvement:\\n1. To relax the synchronization among all workers.\\n2. The frequency of communication can be reduced by more computation in one\\niteration.\\n45 / 66', 'Communication Synchronization Models\\n▶Synchronous\\n▶Stale-synchronous\\n▶Asynchronous\\n▶Local SGD\\n46 / 66', 'Communication Synchronization - Synchronous\\n▶After each iteration, the workers synchronize their parameter updates.\\n▶Every worker must wait for all workers to finish the transmission of all parameters in\\nthe current iteration, before the next training.\\n▶Stragglers can influence the overall system throughput.\\n▶High communication cost that limits the system scalability.\\n[Tang et al., Communication-Efficient Distributed Deep Learning: A Comprehensive Survey, 2020]\\n47 / 66', 'Communication Synchronization - Stale Synchronous (1/2)\\n▶Alleviate the straggler problem without losing synchronization.\\n▶The faster workers to do more updates than the slower workers to reduce the waiting\\ntime of the faster workers.\\n▶Staleness bounded barrier to limit the iteration gap between the fastest worker and\\nthe slowest worker.\\n[Tang et al., Communication-Efficient Distributed Deep Learning: A Comprehensive Survey, 2020]\\n48 / 66', 'Communication Synchronization - Stale Synchronous (2/2)\\n▶For a maximum staleness bound s, the update formula of worker iat iteration t+1:\\n▶wi,t+1:=w0−η(Pt\\nk=1Pn\\nj=1Gj,k+Pt\\nk=t−sGi,k+P\\n(j,k)∈Si,t+1Gj,k)\\n▶The update has three parts:\\n1. Guaranteed pre-window updates from clock 1totover all workers.\\n2. Guaranteed read-my-writes in-window updates made by the querying worker i.\\n3. Best-effort in-window updates. Si,t+1is some subset of the updates from other workers\\nduring period [ t−s].\\n[Tang et al., Communication-Efficient Distributed Deep Learning: A Comprehensive Survey, 2020]\\n49 / 66', 'Communication Synchronization - Asynchronous (1/2)\\n▶It completely eliminates the synchronization.\\n▶Each work transmits its gradients to the PS after it calculates the gradients.\\n▶The PS updates the global model without waiting for the other workers.\\n[Tang et al., Communication-Efficient Distributed Deep Learning: A Comprehensive Survey, 2020]\\n50 / 66', 'Communication Synchronization - Asynchronous (2/2)\\n▶wt+1:=wt−ηPn\\ni=1Gi,t−τk,i\\n▶τk,iis the time delay between the moment when worker icalculates the gradient at\\nthe current iteration.\\n[Tang et al., Communication-Efficient Distributed Deep Learning: A Comprehensive Survey, 2020]\\n51 / 66', 'Communication Synchronization - Local SGD\\n▶All workers run several iterations, and then averages all local models into the newest\\nglobal model.\\n▶IfITrepresents the synchronization timestamps, then:\\nwi,t+1=\\x1awi,t−ηGi,t ift+1/∈ I T\\nwi,t−η1\\nnPn\\ni=1Gi,tift+1∈ I T\\n[Tang et al., Communication-Efficient Distributed Deep Learning: A Comprehensive Survey, 2020]\\n52 / 66', 'Communication Compression\\n53 / 66', 'Communication Compression\\n▶Reduce the communication traffic with little impact on the model convergence.\\n▶Compress the exchanged gradients or models before transmitting across the network.\\n▶Quantization\\n▶Sparsification\\n54 / 66', 'Communication Compression - Quantization\\n▶Useing lower bits to represent the data.\\n▶The gradients are of low precision.\\n[Tang et al., Communication-Efficient Distributed Deep Learning: A Comprehensive Survey, 2020]\\n55 / 66', 'Communication Compression - Sparsification\\n▶Reducing the number of elements that are transmitted at each iteration.\\n▶Only significant gradients are required to update the model parameter to guarantee\\nthe convergence of the training.\\n▶E.g., the zero-valued elements are no need to transmit.\\n[Tang et al., Communication-Efficient Distributed Deep Learning: A Comprehensive Survey, 2020]\\n56 / 66', 'Model Parallelism\\n57 / 66', 'Model Parallelization\\n▶The model is split across multiple devices.\\n▶Depends on the architecture of the NN.\\n[Tang et al., Communication-Efficient Distributed Deep Learning: A Comprehensive Survey, 2020]\\n58 / 66', 'DeepSpeed Zero\\n59 / 66', 'DeepSpeed Zero\\n60 / 66', 'DeepSpeed Zero\\n61 / 66', 'DeepSpeed Zero\\n62 / 66', 'DeepSpeed Zero\\n63 / 66', 'Summary\\n64 / 66', 'Summary\\n▶Scalability matters\\n▶Parallelization\\n▶Data Parallelization\\n•Parameter server vs. AllReduce\\n•Synchronized vs. asynchronized\\n▶Model Parallelization\\n•DeepSpeed-Zero\\n65 / 66', 'Thanks!\\n66 / 66']}, '11_automl.pdf': {'text': ['Automated Machine Learning (AutoML)\\nSlides by Amir H. Payberah\\npayberah@kth.se', 'The Machine Learning Process\\n▶Building an ML model is an iterative, complex, and time-consuming process.\\n▶It can take a lot of trial and error.\\n[Elshawi et al., Automated Machine Learning: State-of-The-Art and Open Challenges, 2019]\\n1 / 63', 'Automated vs. Manual Machine Learning\\n▶AutoML: build models in a data-driven, intelligent, and purposeful way.\\n[Joaquin Vanschoren, Automatic Machine Learning - A Tutorial]\\n2 / 63', '3 / 63', 'AutoML Subproblems - Neural Architecture Search\\n▶Represent and search all pipelines or neural nets, e.g., neural layers, interconnections,\\netc.\\n[Joaquin Vanschoren, Automatic Machine Learning - A Tutorial]\\n4 / 63', 'AutoML Subproblems - Hyperparameter Optimization\\n▶Which hyperparameters are important? How to optimize them?\\n[Joaquin Vanschoren, Automatic Machine Learning - A Tutorial]\\n5 / 63', 'AutoML Subproblems - Meta-learning\\n▶How can we transfer experience from previous tasks?\\n▶Don’t start from scratch (search space is too large).\\n[Joaquin Vanschoren, Automatic Machine Learning - A Tutorial]\\n6 / 63', '7 / 63', 'Hyper-Parameter Optimization (HPO)\\n8 / 63', 'AutoML Definition\\n▶Adenotes a ML algorithm with mhyperparameters.\\n▶{A1,A2,···,An}is a set of ML algorithms.\\n▶Λjis the domain of jth hyperparameter.\\n▶Λ=Λ1×Λ2× ··· × Λmis the overall hyperparameter configuration space.\\n▶θ∈Λis a vector of hyperparameters.\\n▶J(θ,Xtrain,Xvalid) is the loss of the ML model created by θ, trained on Xtrain, and\\nvalidated on Xvalid.\\n▶Find the configuration that minimizes the expected loss on a dataset Xtrain:\\nθ∗= arg min θ∈ΛE(Xtrain,Xvalid )∼XJ(θ,Xtrain,Xvalid)\\n9 / 63', 'Types of Hyperparameters\\n▶Continuous\\n•E.g., learning rate\\n▶Integer\\n•E.g., number of hidden units\\n▶Categorical\\n•E.g., choice of operator (Convolution, MaxPooling, DropOut, etc.)\\n•E.g., choice of activation function (ReLU, Leaky ReLU, tanh, etc.)\\n▶Conditional\\n•E.g., convolution kernel size, if convolution layer is selected\\n10 / 63', 'Hyper-Parameter Optimization\\n▶Black-box optimization\\n•Grid search\\n•Random search\\n•Population-based search\\n•Bayesian optimization\\n▶Multi-fidelity optimization\\n•Modeling learning curve\\n•Bandit based\\n11 / 63', 'Hyper-Parameter Optimization\\n▶Black-box optimization\\n•Grid search\\n•Random search\\n•Population-based search\\n•Bayesian optimization\\n▶Multi-fidelity optimization\\n•Modeling learning curve\\n•Bandit based\\n12 / 63', 'Black-box Optimization - Grid and Random Search\\n[Hutter et al., Automated Machine Learning, 2019]\\n13 / 63', 'Black-box Optimization - Population-based Search\\n▶They maintain a population, i.e., a set of configurations.\\n▶Improve this population to obtain a new generation of better configurations.\\n▶Achieve this by applying:\\n•Local perturbations (so-called mutations)\\n•Combinations of different members (so-called crossover)\\n▶E.g., genetic algorithms, evolutionary algorithms, particle swarm optimization\\n14 / 63', 'Black-box Optimization - Bayesian Optimization (1/3)\\n▶Start with a few (random) hyperparameter configurations.\\n▶Fit a surrogate model to predict other configurations.\\n▶An acquisition function drives the proposition of new points to test, in an exploration\\nand exploitation trade-off.\\n▶Sample for the best configuration under that function.\\n[Hutter et al., Automated Machine Learning, 2019]\\n15 / 63', 'Black-box Optimization - Bayesian Optimization (2/3)\\n[Hutter et al., Automated Machine Learning, 2019]\\n16 / 63', 'Black-box Optimization - Bayesian Optimization (3/3)\\n[Hutter et al., Automated Machine Learning, 2019]\\n17 / 63', 'Hyper-Parameter Optimization\\n▶Black-box optimization\\n•Grid search\\n•Random search\\n•Population-based search\\n•Bayesian optimization\\n▶Multi-fidelity optimization\\n•Modeling learning curve\\n•Bandit based\\n18 / 63', 'Multi-fidelity Optimization\\n▶Massive dataset sizes and complex models make blackbox performance evaluation\\nexpensive.\\n▶Probe a hyperparameter configuration on a small subset.\\n▶Multi-fidelity methods use low fidelity approximations of the actual loss function to\\nminimize.\\n▶These approximations introduce a tradeoff between optimization performance and\\nruntime.\\n19 / 63', 'Multi-fidelity Optimization - Modeling Learning Curves\\n▶Learning curve extrapolation is used in predicting early termination for a particular\\nconfiguration.\\n▶Models learning curves during hyper-parameter optimization.\\n▶Decides whether to allocate more resources or to stop the training procedure for a\\nparticular configuration.\\n▶The learning process is terminated if the performance of the predicted configuration\\nis less than the performance of the best model trained so far in the optimization\\nprocess.\\n20 / 63', 'Multi-fidelity Optimization - Bandit-Based\\n▶Successive halving algorithm (SHA)\\n▶HyperBand\\n21 / 63', 'Multi-fidelity Optimization - SHA (1/4)\\n▶Train on small subsets, infer which regions may be interesting to evaluate in more\\ndepth.\\n▶Randomly sample candidates and evaluate on a small data sample.\\n▶E.g., retrain the 50% best candidates on twice the data.\\n[Hutter et al., Automated Machine Learning, 2019]\\n22 / 63', 'Multi-fidelity Optimization - SHA (2/4)\\n▶Successive halving for eight algorithms/configurations.\\n▶After evaluating all algorithms on 1/8 of the total budget, half of them are dropped\\nand the budget given to the remaining algorithms is doubled.\\n[Hutter et al., Automated Machine Learning, 2019]\\n23 / 63', 'Multi-fidelity Optimization - SHA (3/4)\\n24 / 63', 'Multi-fidelity Optimization - SHA (4/4)\\n▶Successive halving suffers from the budget-vs-number of configurations trade off.\\n▶Given a total budget, the user has to decide beforehand whether:\\n•to try many configurations and only assign a small budget to each, or\\n•to try only a few and assign them a larger budget.\\n▶Assigning too small a budget can result in prematurely terminating good configura-\\ntions.\\n▶Assigning too large a budget can result in running poor configurations too long and\\nthereby wasting resources.\\n25 / 63', 'Multi-fidelity Optimization - HyperBand (1/2)\\n▶HyperBand combats SHA problem when selecting from randomly sampled configu-\\nrations.\\n▶It divides the total budget into several combinations of number of configurations vs.\\nbudget for each.\\n▶Then it calls SHA on each set of random configurations.\\n26 / 63', 'Multi-fidelity Optimization - HyperBand (2/2)\\n▶The inner loop invokes SHA for fixed values of nandr.\\n▶The outer loop iterates over different values of nandr.\\n27 / 63', 'Neural Architecture Search (NAS)\\n28 / 63', 'Neural Architecture Search\\n▶The process of automating architecture engineering.\\n▶Search space: which architectures can be represented in principle.\\n▶Search strategy: how to explore the search space.\\n▶Performance estimation: to perform a standard training and validation of the archi-\\ntecture on data.\\n[Hutter et al., Automated Machine Learning, 2019]\\n29 / 63', 'Search Space\\n30 / 63', 'Search Space\\n▶Which neural architectures a NAS approach might discover.\\n▶Chain-structured neural network\\n▶Multi-branch networks\\n▶Repeated motifs\\n31 / 63', 'Chain-Structured Neural Network\\n▶A sequence of nlayers.\\n▶The i’th layer Lireceives its input from layer i−1and its output\\nserves as the input for layer i+1.\\n▶Parameters of the search space:\\n•The (maximum) number of layers n.\\n•The type of operation every layer can execute, e.g., pooling, conv.\\n•Hyperparameters associated with the operation, e.g., number of filters,\\nkernel size and strides for a convolutional layer.\\n32 / 63', 'Multi-Branch Networks\\n▶The input of layer i: a function gi(Lout\\ni−1,···,Lout\\n0) of previous layer outputs.\\n▶Special cases:\\n•The chain-structured networks: gi(Lout\\ni−1,···,Lout\\n0) =Lout\\ni−1\\n•Residual networks, where previous layer outputs are summed:\\ngi(Lout\\ni−1,···,Lout\\n0) =Lout\\ni−1+Lout\\ni,j<i\\n•DenseNets, where previous layer outputs are out concatenated:\\ngi(Lout\\ni−1,···,Lout\\n0) =concat (Lout\\ni−1,···,Lout\\n0)\\n33 / 63', 'Repeated Motifs\\n▶Normal cell: preservers the\\ndimensionality of the input.\\n▶Reduction cell: reduces the\\nspatial dimension.\\n34 / 63', 'Search Strategy\\n35 / 63', 'Search Strategy\\n▶Random search\\n▶Reinforcement learning\\n▶Gradient-based optimization\\n▶Bayesian optimization\\n▶Evolutionary methods\\n36 / 63', 'Random Search\\n▶For each node in the DAG, determine what decisions must be made.\\n•Choose a node as input and a corresponding operation to apply to generate the\\noutput of the node.\\n•E.g., node ican take the outputs of nodes 0to node i−1as input.\\n•E.g., choose an operation, e.g., tanh, relu, sigmoid to apply to the output of node i.\\n▶Sample uniformly from the set of possible choices for each decision that needs to be\\nmade.\\n▶Moving from node to node.\\n[Li et al., Random Search and Reproducibility for Neural Architecture Search, 2020]\\n37 / 63', 'Evolutionary Methods\\n▶Evolves a population of models, i.e., a set of (possibly trained) networks.\\n▶In every evolution step, at least one model from the population is sampled and serves\\nas a parent to generate offsprings by applying mutations to it.\\n•E.g., adding or removing a layer, altering the hyperparameters of a layer, adding skip\\nconnections, etc.\\n▶After training the offsprings, their fitness (e.g., performance on a validation set) is\\nevaluated and they are added to the population.\\n▶Evolutionary methods differ in how they sample parents, update populations, and\\ngenerate offsprings.\\n38 / 63', 'Reinforcement Learning\\n▶Action: the generation of a neural architecture.\\n▶Action space: the search space.\\n▶Reward: based on an estimate of the performance of the trained architecture on\\nunseen data.\\n▶Policy: different approaches.\\n39 / 63', 'Gradient-based Optimization\\n▶The previous methods search over a discrete set of candidate architectures.\\n▶Here, it relaxes the search space to be continuous, so that the architecture can be\\noptimized with respect to its validation set performance by gradient descent.\\n▶We relax the categorical choice of a particular operation to a softmax over all possible\\noperations.\\n[Liu et al., DARTS: Differentiable Architecture Search, 2019]\\n40 / 63', 'Bayesian Optimization (1/3)\\n▶Find the architecture a∈Athat maximizes f(a).\\n▶Choose several architectures from Aat random and evaluating f(a) for each of them.\\n▶Based on these results, iteratively choose new architectures to evaluate.\\n▶The full algorithm: Trounds of choosing an architecture aiand computing f(ai).\\n▶The output is the architecture a∗with the largest value of f(a∗) among all those\\nthat were tried in the previous rounds.\\n41 / 63', 'Bayesian Optimization (2/3)\\n▶Choose the next architecture in round i+1, given f(a1),···,f(ai).\\n▶Assume f:A→[0,1] follows a Gaussian Process (GP).\\n▶Makes an assumption about the distribution f(A).\\n▶The assumptions about the mean and variance of f(A) are constantly being updated\\nas the algorithm gathers more data in the form of f(a1),···,f(ai).\\n▶Chooses the architecture with the greatest chance of giving a large improvement.\\n▶The algorithm chooses ai+1= arg max a∈Amax(0,E[f(a)−f∗]) = arg max a∈AE[f(a)].\\n▶f∗is the best accuracy observed so far.\\n42 / 63', 'Bayesian Optimization (3/3)\\n▶The top graph: three evaluations of f(blue circles), an estimate of f(solid red line),\\nand confidence intervals (dotted red lines).\\n▶The bottom graph: the expected improvement value for each architecture. The\\narchitecture with the largest expected improvement is chosen (blue x).\\n[https://medium.com/abacus-ai/an-introduction-to-bayesian-optimization-for-neural-architecture-search-d324830ec781]\\n43 / 63', 'Performance Estimation\\n44 / 63', 'Performance Estimation\\n▶The search strategies need to estimate the performance of a given architecture A\\nthey consider.\\n▶The simplest way of doing this is to train Aon training data and evaluate its perfor-\\nmance on validation data.\\n▶However, training each architecture to be evaluated from scratch frequently yields\\ncomputational demands in the order of thousands of GPU days for NAS.\\n45 / 63', 'Reduce the Computational Burden\\n▶Low-fidelity approximation\\n▶Learning curve extrapolation\\n▶One-shot architecture\\n46 / 63', 'BOHB: Robust and Efficient Hyperparameter\\nOptimization at Scale\\n47 / 63', 'BOHB: Bayesian Optimization and Hyperband\\n▶Bayesian optimization (BO): for choosing the configuration to evaluate\\n▶Hyperband (HB): for deciding how to allocate budgets\\n48 / 63', 'Bayesian Optimization vs. Random Search\\n▶BO advantage: much improved final performance\\n49 / 63', 'Hyperband vs. Random Search\\n▶HB advantage: much improved anytime performance\\n50 / 63', 'Combining Bayesian Optimization and Hyperband\\n▶Best of both worlds: strong anytime and final performance\\n51 / 63', 'HBOB Algorithm\\n▶Relies on HB to determine how many configurations to evaluate with which budget.\\n▶Replaces the random selection of configurations at the beginning of each HB iteration\\nby a BO model-based search.\\n▶Once the desired number of configurations for the iteration is reached, the SHA\\nprocedure is carried out using these configurations.\\n52 / 63', 'A System for Massively Parallel\\nHyperparameter Tuning\\n53 / 63', 'SHA\\n▶SHA allocates a small budget to each configuration, evaluate all configurations and\\nkeep the top1\\nρ.\\n▶It then increases the budget per configuration by a factor of ρ.\\n▶Repeats until the maximum per-configuration budget of Ris reached.\\n▶SHA requires the number of configurations, a min and max resource, a reduction\\nfactor, and a minimum early-stopping rate.\\n54 / 63', 'Asynchronous SHA (ASHA)\\n▶ASHA is a technique to parallelize SHA, leveraging asynchrony to mitigate stragglers\\nand maximize parallelism.\\n▶ASHA promotes configurations to the next rung whenever possible, instead of waiting\\nfor a rung to complete before proceeding to the next rung.\\n▶If no promotions are possible, ASHA simply adds a configuration to the base rung,\\nso that more configurations can be promoted to the upper rungs.\\n▶Given its asynchronous nature it does not require the user to pre-specify the number\\nof configurations to evaluate, but it otherwise requires the same inputs as SHA.\\n55 / 63', 'DARTS: Differentiable Architecture Search\\n56 / 63', 'Differentiable ARchiTecture Search (DARTS)\\n▶Instead of searching over a discrete set of candidate architectures, we relax the search\\nspace to be continuous.\\n▶The architecture can be optimized with respect to its validation set performance by\\ngradient descent.\\n57 / 63', 'Search Space\\n▶It searches for a computation cell as the building block of the final architecture.\\n▶A cell is a DAG consisting of an ordered sequence of Nnodes.\\n▶Each node x(i)is a latent representation (e.g. a feature map in CNNs).\\n▶Each directed edge ( i,j) is associated with some operation o(i,j)that transforms\\nx(i).\\n▶Each intermediate node is computed based on all of its predecessors:\\nx(j)=P\\ni<jo(i,j)(xi)\\n58 / 63', 'Continuous Relaxation and Optimization\\n▶LetObe a set of candidate operations, where each operation represents some func-\\ntionoto be applied to x(i).\\n▶To make the search space continuous, it relaxes the categorical choice of a particular\\noperation to a softmax over all possible operations:\\no(i,j)(x) =P\\no∈Oexp(α(i,j)\\no)P\\no′∈Oexp(α(i,j)\\no′)o(x)\\n▶The operation mixing weights for a pair of nodes ( i,j) are parameterized by a vector\\nα(i,j)of dimension |O|.\\n▶At the end of search, a discrete architecture can be obtained by replacing each mixed\\noperation o(i,j)with the most likely operation, i.e., o(i,j)= arg max o∈Oα(i,j)\\no.\\n59 / 63', 'Summary\\n60 / 63', 'Summary\\n▶Hyperparameter optimization\\n•Black-box optimization\\n•Multi-fidelity optimization\\n▶Nural architecture search\\n•Search space\\n•Search strategy\\n•Performance estimation\\n61 / 63', 'Reference\\n▶Elshawi et al., Automated Machine Learning: State-of-The-Art and Open Challenges,\\n2019\\n▶Falkner et al., BOHB: Robust and Efficient Hyperparameter Optimization at Scale,\\n2018\\n▶Li et al., A System for Massively Parallel Hyperparameter Tuning, 2020\\n▶Liu et al., DARTS: Differentable Architecture Search, 2019\\n62 / 63', 'Questions?\\n63 / 63']}, '10_real_time_ml.pdf': {'text': ['Real-Time Machine Learning Systems\\nJim Dowling\\njdowling@kth.se', 'Real-Time and Interactive Systems with SLOs\\n2 / 51', 'Higher latency reduces usage of Interactive Systems\\n3 / 51', 'Example: Hugging Face Spaces with Stable Diffusion\\n4 / 51', 'Reduce Model Inference Latency with Distillation\\n5 / 51', 'Personalized Models require History and Context (Feature\\nStore)\\n6 / 51', 'Feature pipelines write to both the Online/Offline Feature\\nStores\\n7 / 51', 'When writing, Ensure Consistency Between Offline and Online\\nFeature Stores\\n8 / 51', 'The Online Feature Store\\n9 / 51', 'Hopsworks: Write to Feature Groups, Read from Feature Views\\n10 / 51', 'Real-Time APIs for Writing and Reading to the Feature Store\\n11 / 51', 'Streaming/Training/Online-Inference Pipelines\\n12 / 51', 'Programming Frameworks for Streaming Feature Pipelines\\n13 / 51', 'Streaming Feature Pipelines - Windows (1/2)\\n14 / 51', 'Streaming Feature Pipelines - Windows (2/2)\\n15 / 51', 'PySpark Streaming Program\\n16 / 51', 'On-Demand Features\\n17 / 51', 'On-Demand Features - Example\\n18 / 51', 'Online or Interactive Applications\\n19 / 51', 'Making an Application “Intelligent” with a Model and Features\\n20 / 51', 'Online Inference and the Online Feature Store\\n21 / 51', 'KServe - Open-Source Model Serving Infrastructure\\n22 / 51', 'Example transformer.py program\\n23 / 51', 'Example predictor.py program\\n24 / 51', 'Putting it together in an Operational ML System\\n25 / 51', 'Online Feature/Prediction Monitoring\\n26 / 51', 'What should you monitor in a ML System?\\n27 / 51', 'Feature Drift\\n28 / 51', 'Feature Group Drift\\n29 / 51', 'Training Data Feature Shift\\n30 / 51', 'Models Degrade in Quality over Time\\n31 / 51', 'Label Shift\\n32 / 51', 'Concept Drift - where the model degrades over time\\n33 / 51', 'How to handle Concept Drift - Retrain models\\n34 / 51', 'Algorithm for Detecting and Retraining models\\n35 / 51', 'TikTok - online model retraining for recommendations\\n36 / 51', 'Real-Time Personalized\\nRecommendations/Search\\n37 / 51', 'Real-Time Personalized Recommender/Search Service\\n38 / 51', 'Embeddings can be used for Similarity Search with a VectorDB\\n39 / 51', 'What about Multi-Modal Similarity Search?\\n40 / 51', 'Training data for our Two-Tower Model will be User-Item\\nInteractions\\n41 / 51', 'Two Tower Model for Personalized Recommendations/Search\\n42 / 51', 'Build the ANN Index on Items. Similarity Search with user\\nqueries on it.\\n43 / 51', 'Build the ANN Index on Items. Similarity Search with user\\nqueries on it.\\nClick here to show animated insertions and lookups in a ANN with a Two-Tower\\nModel\\n44 / 51', 'Real-Time Retrieval and Ranking\\n45 / 51', 'Real-Time Retrieval and Ranking Online Infrastructure\\n46 / 51', 'Offline Infrastructure for Retrieval and Ranking\\n47 / 51', 'Model Training for Embeddings and Ranking Model\\n48 / 51', 'Extended Retrieval and Ranking Architecture\\n49 / 51', 'Model Serving with VectorDB and Feature Store\\n50 / 51', 'References\\n▶Real-time machine learning: challenges and solutions by Chip Huyen\\n▶Concept Drift by FastForwardLabs\\n▶Scale faster with less code using Two Tower with Merlin by Nvidia\\n51 / 51']}, '09_mlops.pdf': {'text': ['Machine Learning Operations (MLOps)\\nJim Dowling\\njdowling@kth.se', 'MLOps for Developing Machine Learning Products\\n▶Get to a working ML system with a baseline ASAP, so that you can iteratively improve\\nit.\\n▶A goal of MLOps is to improve both iteration speed and quality when developing ML\\n2 / 65', 'MLOps: what does Improving Iteration Speed mean?\\n▶Safe incremental updates: make small changes to your source code with confidence\\nthat your changes will not break anything (downstream clients, deployments on dif-\\nferent platforms), performance regressions, etc)\\n▶Tighter iteration loop: the time taken to run tests or experiments should not dominate\\nthe time taken to make the source code changes\\n▶A faster iteration loop makes developers happier and more productive\\n3 / 65', 'MLOps: iteratively Develop and Test ML Systems\\n▶ML-enabled products evolve over time :\\n•The available input data (features) change over time\\n•The target you are trying to predict changes over time\\n•With the help of automation, how can quickly and reliably develop, test, and deploy\\nML-enabled products without affecting their ongoing operation?\\n▶We should aim to automate the testing and deployment of ML-enabled Products\\n4 / 65', 'MLOps: Automated Testing to Improve ML Product Quality\\n▶The goal is to be able to reliably build:\\n•trustworthy features using feature pipelines and data\\n•a trustworthy model using your trustworthy features\\n•an AI-enabled product using trustworthy models and features\\n▶To this end, features and models must be tested\\n▶Tests should run automatically as part of a CI/CD workflow\\n5 / 65', 'Prediction Feedback to Improve ML Product Quality\\n▶Acquire user feedback with a user-interface to quickly improve the quality of your\\nML-enabled product and model\\n▶Log predictions and features to enable developers to quickly find and understand the\\nroot cause of poor quality predictions\\n▶Compare historical predictions with outcomes (or proxy metrics for outcomes) to\\ninform when a model is stale\\n▶Monitor feature or label drift to identify when a model needs to be re-trained\\n6 / 65', 'DevOps for reliable software development\\n▶DevOps is a set of practices, tools, and a cultural philosophy that automate and in-\\ntegrate the processes between software development and IT teams. Key technologies\\nareversion control, automated testing, versioning of production deployments.\\n7 / 65', 'Changes in either source code or Data can break your ML\\nProduct\\n▶In DevOps, changes in source code trigger automated testing and deployment.\\n▶In MLOps, changes in either source code or incoming data trigger automated testing\\nand deployment.\\n8 / 65', 'A Complete MLOps Platform with Automated Testing\\n9 / 65', 'Lineage in MLOps\\n10 / 65', 'What was the root cause for the introduction of model bias?\\n11 / 65', 'Lineage in Hopsworks: from Data to Features to Models to\\nDeployments\\n12 / 65', 'Reproducible ML Assets makes for better Data Science\\n13 / 65', 'Reverse Lineage for Batch Inference Pipelines\\n14 / 65', 'Reverse Lineage for Online Inference\\n15 / 65', 'Data Versioning in Apache Hudi\\n▶Lineage involves storing metadata about both state and pipeline executions of ver-\\nsioned ML Assets, enabling the discovery of the provenance of any given ML asset.\\n▶Lineage facilitates Debugging, Analyzing, Cleaning of ML Assets and Pipelines, and\\nReproducing ML Assets.\\n▶If a stateful ML asset supports time-travel, you can track and recover its state at\\na point in time in the past. Git provides time-travel for source code. Hudi provide\\ntime-travel for data commits in cached Feature Groups in Hopsworks.\\n16 / 65', 'Versioning of ML Assets\\n17 / 65', 'Mutability of ML Assets in Hopsworks\\n18 / 65', 'Versioning of ML Assets in Hopsworks\\n19 / 65', 'Handling Versioning Challenges in Hopsworks\\n20 / 65', 'Versioning of Source Code\\nPackaging of Pipelines\\n21 / 65', 'Packaging Pipelines as Installable Python Artifacts\\n22 / 65', 'Manage OS package dependenices for Pipelines\\n23 / 65', 'Reuse Versioned Feature Code for Prod/Backfill Pipelines\\n24 / 65', 'Reuse On-Demand feature code in Training/Inference Pipelines\\n25 / 65', 'Model-Specific Transformation Pipelines in Scikit-Learn\\n26 / 65', 'Versioning of Data: Schemas and Commits\\n27 / 65', 'Schema Versioning (Data Contracts)\\n28 / 65', 'Data Versioning with Git\\n29 / 65', 'Data Versions in feature groups with Apache Hudi\\n30 / 65', 'Unit tests for Feature Logic\\nand integration tests for Feature Pipelines\\n31 / 65', 'ML Test Score Criteria by D. Sculley et al\\n32 / 65', 'ML Test Score Criteria by D. Sculley et al\\n33 / 65', 'Where can we add tests to Operational ML Systems?\\n34 / 65', 'Where can we add tests to Operational ML Systems?\\n35 / 65', 'Offline and Online Tests for Operational ML Systems\\n36 / 65', 'Unit tests for Features with Pytest\\n37 / 65', 'Refactor Feature Engineering Code into Testable Functions\\n38 / 65', 'Write Unit Tests for the Feature Functions\\n39 / 65', 'Recommended Pytest directory structure\\n40 / 65', 'Feature Pipeline Tests in a CI/CD Setup\\n41 / 65', 'Data Validation with Great Expectations\\n42 / 65', 'Data Validation with Great Expectations in Feature Pipelines\\n43 / 65', 'Feature Data Validation Rules\\n44 / 65', 'Great Expectations and Pandas DataFrames\\n45 / 65', 'Great Expectations and Pandas DataFrames\\n46 / 65', 'Feature Pipeline CI/CD Setup with Great Expectations\\n47 / 65', 'Testing Training Pipelines\\nand Model Deployments\\n48 / 65', 'Testing Training Pipelines and Models\\n49 / 65', 'Evaluating Models and Testing Training Pipelines\\n50 / 65', 'Model Performance Evaluation\\n51 / 65', 'Model Tests\\n52 / 65', 'Test a Model for Bias with Evaluation Sets\\n53 / 65', 'Integration (End-to-End) Tests for Training Pipelines\\n54 / 65', 'A/B Testing Model Deployments (Blue/Green Rollouts)\\n55 / 65', 'The “Big Red Button” enabled by MLOps\\n56 / 65', 'Model and Feature Monitoring\\n57 / 65', 'Data for AI Flywheel\\n58 / 65', 'Feature and Prediction Logging\\n59 / 65', 'Monitor Features, Labels, Predictions, Outcomes for Drift\\n60 / 65', 'What is practical to measure for Data Drift?\\n61 / 65', 'Case Study\\n62 / 65', 'Lyft Model/Feature Monitoring\\n63 / 65', 'Lyft - Performance Drift Detection\\n64 / 65', 'References\\n▶Reliable Machine Learning: Applying SRE Principles to ML in Production, Murphy\\net al, O’Reilly\\n▶Designing Machine Learning Systems: An Iterative Process for Production-Ready\\nApplications, Chip Huyen, O’Reilly\\n65 / 65']}, '08_feature_stores.pdf': {'text': ['Feature Stores for Machine Learning\\nJim Dowling\\njdowling@kth.se', 'Enterprise Data and Feature Store\\n2 / 54', 'Example e-commerce marketing data model\\n3 / 54', 'Data modelling: Fact and Dimension Tables\\n▶A popular Data Model for Data Warehouses is to have Fact and Dimension Tables\\n▶Examples of Facts: purchases, user clicks, user searches, songs played, embeddings\\n(recent user searches/sessions)\\n▶Examples of Dimensions: click dimension, location dimension, time dimension, cus-\\ntomer dimension, song dimension\\n▶Business events are modelled as Facts (aka measurements)\\n▶Idenify and save dimensions for your facts that are useful for analysis or prediction\\nservices\\n▶Dimensions can be thought of as the columns you would expect to “group by”\\n4 / 54', 'Example credit-card fraud facts and dimensions in a Star\\nSchema\\n5 / 54', 'Example credit-card fraud tables\\n6 / 54', 'Our credit-card fraud tables are updated at different cadences\\n7 / 54', 'Our credit-card fraud tables are in 3rd normal form\\n8 / 54', 'The primary keys for our credit-card fraud tables\\n9 / 54', 'Credit card number - the Join key for our credit-card fraud\\ntables\\n10 / 54', 'Feature Pipeline for Credit Card Fraud Features\\n11 / 54', 'Credit Card Transactions Feature Group\\n12 / 54', 'Credit Card Transactions Feature Group - One Big Table\\n13 / 54', 'Credit Card Transactions - 4 hr Aggregations Feature Group\\n14 / 54', 'Decouple feature pipelines from Models with a Feature Store\\n15 / 54', 'One complex feature pipeline with both backfill and production\\n16 / 54', 'Separate feature pipeline for backfill and production\\n17 / 54', 'Separate feature pipeline for backfill and production with\\nshared code\\n18 / 54', 'Feature pipelines write DataFrames to Feature Groups\\n19 / 54', 'Create a Feature Group in Hopsworks with Python\\n20 / 54', 'Feature Group - primary keys\\n21 / 54', 'Feature Group - Event Time\\n22 / 54', 'Feature Group - Event Time is not Ingestion Time\\n23 / 54', 'Feature Group - Online Enabled\\n24 / 54', 'Feature Groups are stored internally with Hive (offline),\\nMySQL (online) schemas\\n25 / 54', 'Partitions: Efficient Queries over Offline Feature Groups storing\\nlarge amounts of data\\n26 / 54', 'Compute descriptive statistics over numerical features,\\ndistributions for categorical features\\n27 / 54', 'Storing Labels in Feature Groups\\n28 / 54', 'Feature Selection\\n29 / 54', 'Feature Selection with Scikit-Learn\\n30 / 54', 'Feature Selection with Uber’s XRay Framework\\n31 / 54', 'Feature Selection with a Feature View\\n32 / 54', 'Join Features together to create a Feature View\\n33 / 54', 'Point-in-Time Correct Joins needed to create Training Data\\n34 / 54', 'Create a Feature View\\n35 / 54', 'Create a Feature View from your Selected Features\\n36 / 54', 'Feature View Offline API: Create Training Data or Batch\\nInference Data\\n37 / 54', 'Feature View Offline API: Create Training Data\\n38 / 54', 'Random or Time-Series Split into Train/Test sets?\\n39 / 54', 'Feature View Online API: Retrieve Feature Vectors for Online\\nModels\\n40 / 54', 'Feature Selection Pipeline\\n41 / 54', 'Model-Specific Transformations can be applied by Feature\\nViews (1/3)\\n▶Transformation functions are applied to features to (1) make their data compatible\\nwith the model training algorithm or (2) to improve model performance\\n▶Transformation functions typically use state computed on the train set (e.g., the\\narithmetic mean is used to normalize a numerical feature or the number of categories\\nis used to one-hot encode acategorical variable)\\n▶Model-specific transformations functions need identical implementations in the\\ntraining and inference pipelines. If the implementations differ, you may introduce\\ntraining-inference skew.\\n▶Training-inference skew is difficult to diagnose and fix, and causes models to perform\\npoorly.\\n42 / 54', 'Model-Specific Transformations can be applied by Feature\\nViews (2/3)\\n43 / 54', 'Model-Specific Transformations can be applied by Feature\\nViews (3/3)\\n44 / 54', 'Consistent Training/Inference Transformations with\\nScikit-Learn\\n45 / 54', 'Use Pretrained Models and Transfer Learning, where\\nappropriate\\n46 / 54', 'Typical steps in a training pipeline that uses a Feature Store\\n47 / 54', 'Experiment tracking tools help manage your training pipelines\\n48 / 54', 'Common training pipeline pattern when using a Feature Store\\n49 / 54', 'Training Pipeline output - save your model to a Model Registry\\n50 / 54', 'Hopsworks is both a Metadata and Artifact Store\\n51 / 54', 'Batch Inference Pipeline uses features from the Feature Store\\n52 / 54', 'Batch Inference Pipeline Code for Scoring Data from Last 24\\nhours\\n53 / 54', 'References\\n▶Feature Group Concepts, Feature Group Guide, API Docs for Feature Groups-\\nhttps://docs.hopsworks.ai\\n▶Data models - star schema - https://www.databricks.com/glossary/star-schema\\n▶Credit Card Fraud - https://www.kaggle.com/datasets/kartik2112/fraud-detection\\n54 / 54']}, '07_rnn_transformers_2023.pdf': {'text': ['RNNs and Transformers\\nJim Dowling\\njdowling@kth.se\\n2022-11-23\\nSlides by Francisco J. Pena, Amir H. Payberah, and Jim Dowling', 'Language Modeling (1/2)\\n▶Language modeling is the task of predicting what word comes next.\\n3 / 67', 'Language Modeling (2/2)\\n▶More formally: given a sequence of words x(1),x(2),···,x(t), compute the probability\\ndistribution of the next word x(t+1):\\np(x(t+1)=wj|x(t),···x(1))\\n▶wjis a word in vocabulary V={w1,···,wv}.\\n4 / 67', 'n-gram Language Models\\n▶the students opened their\\n▶How to learn a Language Model?\\n▶Learn a n-gram Language Model!\\n▶A n-gram is a chunk of n consecutive words.\\n•Unigrams: \"the\", \"students\", \"opened\", \"their\"\\n•Bigrams: \"the students\", \"students opened\", \"opened their\"\\n•Trigrams: \"the students opened\", \"students opened their\"\\n•4-grams: \"the students opened their\"\\n▶Collect statistics about how frequent different n-grams are, and use these to predict\\nnext word.\\n5 / 67', 'n-gram Language Models - Example\\n▶Suppose we are learning a 4-gram Language Model.\\n•x(t+1)depends only on the preceding 3 words {x(t),x(t−1),x(t−2)}.\\np(wj|students opened their) =students opened their wj\\nstudents opened their\\n▶In the corpus:\\n•\"students opened their\" occurred 1000 times\\n•\"students opened their books occurred 400 times:\\np(books|students opened their ) =0.4\\n•\"students opened their exams occurred 100 times:\\np(exams|students opened their ) =0.1\\n6 / 67', 'Problems with n-gram Language Models - Sparsity\\np(wj|students opened their) =students opened their wj\\nstudents opened their\\n▶What if \"students opened their w j\"never occurred in data? Then wjhas prob-\\nability 0!\\n▶What if \"students opened their\" never occurred in data? Then we can’t calcu-\\nlate probability for any wj!\\n▶Increasing nmakes sparsity problems worse.\\n•Typically we can’t have nbigger than 5.\\n7 / 67', 'Problems with n-gram Language Models - Storage\\np(wj|students opened their) =students opened their wj\\nstudents opened their\\n▶For\"students opened their w j\", we need to store count for all possible 4-grams.\\n▶The model size is in the order of O(exp(n)).\\n▶Increasing nmakes model size huge.\\n8 / 67', 'Can We Build a Neural Language Model? (1/3)\\n▶Recall the Language Modeling task:\\n•Input: sequence of words x(1),x(2),···,x(t)\\n•Output: probability dist of the next word p(x(t+1)=wj|x(t),···,x(1))\\n▶One-Hot encoding\\n•Represent a categorical variable as a binary vector.\\n•All recodes are zero, except the index of the integer, which is one.\\n•Each embedded word e(t)=E⊺x(t)is a one-hot vector of size vocabulary size.\\n9 / 67', 'Can We Build a Neural Language Model? (2/3)\\n▶A MLP model\\n•Input: words x(1),x(2),x(3),x(4)\\n•Input layer: one-hot vectors e(1),e(2),e(3),e(4)\\n•Hidden layer: h=f(w⊺e),fis an activation function.\\n•Output: ^y=softmax (v⊺h)\\n10 / 67', 'Can We Build a Neural Language Model? (3/3)\\n▶Improvements over n-gram LM:\\n•No sparsity problem\\n•Model size is O(n) notO(exp(n))\\n▶Remaining problems:\\n•It is fixed 4 in our example, which is small\\n•We need a neural architecture that can process\\nany length input\\n11 / 67', 'Recurrent Neural Networks (RNN)\\n12 / 67', 'Recurrent Neural Networks (1/4)\\n▶The idea behind Recurrent neural networks (RNN) is to make use of sequential data.\\n•Until here, we assume that all inputs (and outputs) are independent of each other.\\n•Independent input (output) is a bad idea for many tasks, e.g., predicting the next word\\nin a sentence (it’s better to know which words came before it).\\n▶They can analyze time series data and predict the future.\\n▶They can work on sequences of arbitrary lengths, rather than on fixed-sized inputs.\\n13 / 67', 'Recurrent Neural Networks (2/4)\\n▶Neurons in an RNN have connections pointing backward.\\n▶RNNs have memory, which captures information about what has been calculated so\\nfar.\\n14 / 67', 'Recurrent Neural Networks (3/4)\\n▶Unfolding the network: represent a network against the time axis.\\n•We write out the network for the complete sequence.\\n▶For example, if the sequence we care about is a sentence of three words, the network\\nwould be unfolded into a 3-layer neural network.\\n•One layer for each word.\\n15 / 67', 'Recurrent Neural Networks (4/4)\\n▶h(t)=f(u⊺x(t)+wh(t−1)), where fis an activation function, e.g., tanh orReLU .\\n▶^ y(t)=g(vh(t)), where gcan be the softmax function.\\n▶cost (y(t),^ y(t)) =cross entropy (y(t),^ y(t)) =−Py(t)log^ y(t)\\n▶y(t)is the correct word at time step t, and ^ y(t)is the prediction.\\n16 / 67', 'Recurrent Neurons - Weights (1/4)\\n▶Each recurrent neuron has three sets of weights: u,w, and v.\\n17 / 67', 'Recurrent Neurons - Weights (2/4)\\n▶u: the weights for the inputs x(t).\\n▶x(t): is the input at time step t.\\n▶For example, x(1)could be a one-hot vector corresponding to the first word of a\\nsentence.\\n18 / 67', 'Recurrent Neurons - Weights (3/4)\\n▶w: the weights for the hidden state of the previous time step h(t−1).\\n▶h(t): is the hidden state (memory) at time step t.\\n•h(t)=tanh (u⊺x(t)+wh(t−1))\\n•h(0)is the initial hidden state.\\n19 / 67', 'Recurrent Neurons - Weights (4/4)\\n▶v: the weights for the hidden state of the current time step h(t).\\n▶^y(t)is the output at step t.\\n▶^y(t)=softmax (vh(t))\\n▶For example, if we wanted to predict the next word in a sentence, it would be a\\nvector of probabilities across our vocabulary.\\n20 / 67', 'Layers of Recurrent Neurons\\n▶At each time step t, every neuron of a layer receives both the input vector x(t)and\\nthe output vector from the previous time step h(t−1).\\nh(t)=tanh (u⊺x(t)+w⊺h(t−1))\\ny(t)=sigmoid (v⊺h(t))\\n21 / 67', 'Deep RNN\\n▶Stacking multiple layers of cells gives you a deep RNN.\\n22 / 67', 'RNN Design Patterns\\n27 / 67', 'RNN Design Patterns - Sequence-to-Vector\\n▶Sequence-to-vector network: takes a sequence of inputs, and ignore all outputs except\\nfor the last one.\\n▶E.g., you could feed the network a sequence of words corresponding to a movie\\nreview, and the network would output a sentiment score.\\n28 / 67', 'RNN Design Patterns - Vector-to-Sequence\\n▶Vector-to-sequence network: takes a single input at the first time step, and let it\\noutput a sequence.\\n▶E.g., the input could be an image, and the output could be a caption for that image.\\n29 / 67', 'RNN Design Patterns - Sequence-to-Sequence\\n▶Sequence-to-sequence network: takes a sequence of inputs and produce a sequence\\nof outputs.\\n▶Useful for predicting time series such as stock prices: you feed it the prices over the\\nlast N days, and it must output the prices shifted by one day into the future.\\n▶Here, both input sequences and output sequences have the same length.\\n30 / 67', 'RNN Design Patterns - Encoder-Decoder\\n▶Encoder-decoder network: a sequence-to-vector network (encoder), followed by a\\nvector-to-sequence network (decoder).\\n▶E.g., translating a sentence from one language to another.\\n▶You would feed the network a sentence in one language, the encoder would convert\\nthis sentence into a single vector representation, and then the decoder would decode\\nthis vector into a sentence in another language.\\n31 / 67', 'RNN Problems\\n▶Sometimes we only need to look at recent information to perform the present task.\\n•E.g., predicting the next word based on the previous ones.\\n▶In such cases, where the gap between the relevant information and the place that\\nit’s needed is small, RNNs can learn to use the past information.\\n▶But, as that gap grows, RNNs become unable to learn to connect the information.\\n▶RNNs may suffer from the vanishing/exploding gradients problem.\\n64 / 67', 'RNN References\\n▶Ian Goodfellow et al., Deep Learning (Ch. 10)\\n▶Aur´ elien G´ eron, Hands-On Machine Learning (Ch. 15)\\n▶Understanding LSTM Networks\\nhttp://colah.github.io/posts/2015-08-Understanding-LSTMs\\n▶CS224d: Deep Learning for Natural Language Processing\\nhttp://cs224d.stanford.edu\\n65 / 67', 'Word Embeddings\\nProblem: Word embeddings are context-free\\n[Peltarion, 2020]\\n6/52', 'Word Embeddings\\nProblem: Word embeddings are context-free\\n[Peltarion, 2020]\\n7/52', 'Word Embeddings\\nProblem: Word embeddings are context-free\\nSolution: Createcontextualized representation\\n[Peltarion, 2020]\\n8/52', 'From RNNs to Transformers\\n9/52', 'Problems with RNNs - Motivation for Transformers\\nISequential computations prevents parallelization\\nIDespite GRUs and LSTMs, RNNs still need attention mechanisms to deal with long\\nrange dependencies\\nIAttention gives us access to any state…Maybe we don’t need the costly recursion?\\nIThen NLP can have deep models, solves our computer vision envy!\\n10/52', 'Attention is all you need! [Vaswani, 2017]\\nISequence-to-sequence model for\\nMachine Translation\\nIEncoder-decoder architecture\\nIMulti-headed self-attention\\n\\x0fModels context and no locality\\nbias\\n[Vaswani et al., 2017]\\n11/52', 'Transformers Step-by-Step\\n12/52', 'Understanding the Transformer: Step-by-Step\\n[Alammar, 2018]\\n13/52', 'Understanding the Transformer: Step-by-Step\\nNo recursion, instead\\nstacking encoder and\\ndecoderblocks\\nIOriginally: 6 layers\\nIBERT base: 12 layers\\nIBERT large: 24 layers\\nIGPT2-XL: 48 layers\\nIGPT3: 96 layers\\n[Alammar, 2018]\\n14/52', 'The Encoder and Decoder Blocks\\n[Alammar, 2018]\\n15/52', 'The Encoder Block\\n[Alammar, 2018]\\n16/52', 'Attention Preliminaries\\nMimics the retrieval of a value vifor a query qbased on a key kiin a database,\\nbut in a probabilistic fashion\\n17/52', 'Dot-Product Attention\\nIQueries, keys and values are vectors\\nIOutput is a weighted sum of the values\\nIWeights are are computed as the scaled dot-product (similarity) between\\nthe query and the keys\\nAttention (q;K;V) =X\\niSimilarity (q;ki)\\x01vi=X\\nieq\\x01ki/pdk\\nP\\njeq\\x01kj/pdkviOutput is a\\nrow-vector\\nICan stack multiple queries into a matrix Q\\nAttention (Q;K;V) =softmax\\x12QK>\\npdk\\x13\\nVOutput is again\\na matrix\\nISelf-attention: Let the word embeddings be the queries, keys and values,\\ni.e.let the words select each other\\n18/52', 'Self-Attention Mechanism\\n[Alammar, 2018]\\n19/52', 'Self-Attention Mechanism\\n[Alammar, 2018]\\n20/52', 'Self-Attention Mechanism in Matrix Notation\\n[Alammar, 2018]\\n21/52', 'Multi-Headed Self-Attention\\n[Alammar, 2018]\\n22/52', 'Multi-Headed Self-Attention\\n[Alammar, 2018]\\n23/52', 'Self-Attention: Putting It All Together\\n[Alammar, 2018]\\n24/52', 'Attention Visualized\\n[Alammar, 2018]\\n25/52', 'The Full Encoder Block\\nEncoder block consisting of:\\nIMulti-headed self-attention\\nIFeedforward NN (FC 2 layers)\\nISkip connections\\nILayer normalization - Similar to\\nbatch normalization but computed\\nover features (words/tokens) for a\\nsingle sample\\n[Alammar, 2018]\\n26/52', 'Encoder-Decoder Architecture - Small Example\\n[Alammar, 2018]\\n27/52', 'Positional Encodings\\nEncoder block consisting of:\\nIAttentionmechanismhasnolocality\\nbias -no notion of word order\\nIAdd positional encodings to input\\nembeddings to let model learn rel-\\native positioning\\nPE(pos ;2i) =sin\\x10pos\\n100002i/dmodel\\x11\\nPE(pos ;2i+ 1) =cos\\x10pos\\n100002i/dmodel\\x11\\n[Alammar, 2018]\\n28/52', 'Positional Encodings\\n[Kazemnejad, 2019]\\n29/52', 'Let’s start the encoding!\\n[Alammar, 2018]\\n30/52', 'Decoding procedure\\n[Alammar, 2018]\\n31/52', 'Producing the output text\\nEncoder block consisting of:\\nIThe output from the decoder is\\npassed through a final fully con-\\nnectedlinear layer with asoftmax\\nactivation function\\nIProduces a probability distribution\\nover the pre-defined vocabulary of\\noutput words (tokens)\\nIGreedydecoding picksthewordwith\\nthe highest probability at each time\\nstep\\n[Alammar, 2018]\\n32/52', 'Training Objective\\n[Alammar, 2018]\\n33/52', 'Complexity Comparison\\n[Vaswani et al., 2017]\\n34/52', 'Results\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModelBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [15] 23.75\\nDeep-Att + PosUnk [32] 39.2 1.0·1020\\nGNMT + RL [31] 24.6 39.92 2.3·10191.4·1020\\nConvS2S [8] 25.16 40.46 9.6·10181.5·1020\\nMoE [26] 26.03 40.56 2.0·10191.2·1020\\nDeep-Att + PosUnk Ensemble [32] 40.4 8.0·1020\\nGNMT + RL Ensemble [31] 26.30 41.16 1.8·10201.1·1021\\nConvS2S Ensemble [8] 26.36 41.29 7.7·10191.2·1021\\nTransformer (base model) 27.3 38.1 3.3·1018\\nTransformer (big) 28.4 41.0 2.3·1019\\nLabel Smoothing During training, we employed label smoothing of value ✏ls=0.1[30]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop=0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4and length penalty ↵=0.6[31]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [31].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of ﬂoating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision ﬂoating-point capacity of each GPU5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8\\n[Vaswani et al., 2017]\\n35/52', 'BERT\\n36/52', 'BERT\\nBidirectional EncoderRepresentations\\nfromTransformers\\nISelf-supervised pre-training of\\nTransformers encoder for language\\nunderstanding\\nIFine-tuning for specific downstream\\ntask\\n37/52', 'BERT Training Procedure\\n[Devlin et al., 2018]\\n38/52', 'BERT Training Objectives\\nMasked Language Modelling\\nNext Sentence prediction\\n[Devlin et al., 2018]\\n39/52', 'BERT Fine-Tuning Examples\\nSentence\\nClassification\\nQuestion\\nAnswering\\nNamed Entity\\nRecognition\\n[Devlin et al., 2018]\\n40/52', 'How good are transformers?\\nIScaling up models size and amount of training data helps a lot\\nIBest model is 10B (!!) parameters\\nITwo models have already surpassed human performance!!!\\nIExactpre-training objective (MLM, NSP, corruption) doesn’t matter too\\nmuch\\nISuperGLUE benchmark:\\n[Raffel et al., 2019]\\n41/52', 'Practical Examples\\n42/52', 'BERT in low-latency production settings\\n[Devlin, 2020]\\n43/52', 'Distillation\\nIModern pre-trained language\\nmodels are hugeand very\\ncomputationally expensive\\nIHow are these companies applying\\nthem to low-latency applications?\\nIDistillation!\\n\\x0fTrain SOTA teacher model\\n(pre-training + fine-tuning)\\n\\x0fTrain smaller student model that\\nmimicsthe teacher’s output on a\\nlarge dataset on unlabeled data\\nIDistillation works muchbetter than\\npre-training + fine-tuning with\\nsmaller model\\n[Devlin, 2020] [Turc, 2020]\\n44/52', 'Transformers in TensorFlow using HuggingFace\\nITheHuggingFace Library contains a\\nmajority of the recent pre-trained\\nState-of-the-art NLP models, as well as\\nover 4 000 community uploaded models\\nIWorks with both TensorFlow andPyTorch\\n45/52', 'Transformers in TensorFlow using HuggingFace\\nfromtransformers importBertTokenizerFast, TFBertForSequenceClassification\\nfromdatasets importload_dataset\\nimporttensorflow astf\\ndataset =load_dataset( \"imdb\").shuffle()\\ntokenizer =BertTokenizerFast .from_pretrained( \\'bert-base-uncased\\' )\\nmodel=TFBertForSequenceClassification .from_pretrained( \\'bert-base-uncased\\' , num_labels =2)\\ntrain_encodings =tokenizer(dataset[ \\'train\\'][\\'text\\'], truncation =True, padding =True)\\ntrain_dataset =tf.data.Dataset.from_tensor_slices(( dict(train_encodings), dataset[ \\'train\\'][\\'label\\']))\\nval_dataset =...//Analogously\\noptimizer =tf.keras.optimizers .Adam(learning_rate =5e-5)\\nmodel.compile(optimizer =optimizer, loss =model.compute_loss)\\nmodel.fit(train_dataset .batch(16), epochs =3, batch_size =16)\\nmodel.evaluate(val_dataset .batch(16), verbose =0)\\n46/52', 'Transformers in TensorFlow using HuggingFace\\nfromtransformers importBertTokenizerFast, TFBertForSequenceClassification}\\nfromdatasets importload_dataset\\nimporttensorflow astf\\ndataset =load_dataset( \"imdb\").shuffle()\\ntokenizer =BertTokenizerFast .from_pretrained( \\'bert-base-uncased\\' )\\nmodel=TFBertForSequenceClassification .from_pretrained( \\'bert-base-uncased\\' , num_labels =2)\\ntrain_encodings =tokenizer(dataset[ \\'train\\'][\\'text\\'], truncation =True, padding =True)\\ntrain_dataset =tf.data.Dataset.from_tensor_slices(( dict(train_encodings), dataset[ \\'train\\'][\\'label\\']))\\nval_dataset =...//Analogously\\noptimizer =tf.keras.optimizers .Adam(learning_rate =5e-5)\\nmodel.compile(optimizer =optimizer, loss =model.compute_loss)\\nmodel.fit(train_dataset .batch(16), epochs =3, batch_size =16)\\nmodel.evaluate(val_dataset .batch(16), verbose =0)\\n47/52', 'Transformers in TensorFlow using HuggingFace\\nfromtransformers importBertTokenizerFast, TFBertForSequenceClassification}\\nfromdatasets importload_dataset\\nimporttensorflow astf\\ndataset =load_dataset( \"imdb\").shuffle()\\ntokenizer =BertTokenizerFast .from_pretrained( \\'bert-base-uncased\\' )\\nmodel=TFBertForSequenceClassification .from_pretrained( \\'bert-base-uncased\\' , num_labels =2)\\ntrain_encodings =tokenizer(dataset[ \\'train\\'][\\'text\\'], truncation =True, padding =True)\\ntrain_dataset =tf.data.Dataset.from_tensor_slices(( dict(train_encodings), dataset[ \\'train\\'][\\'label\\']))\\nval_dataset =...//Analogously\\noptimizer =tf.keras.optimizers .Adam(learning_rate =5e-5)\\nmodel.compile(optimizer =optimizer, loss =model.compute_loss)\\nmodel.fit(train_dataset .batch(16), epochs =3, batch_size =16)\\nmodel.evaluate(val_dataset .batch(16), verbose =0)\\n48/52', 'Transformers in TensorFlow using HuggingFace\\nfromtransformers importBertTokenizerFast, TFBertForSequenceClassification}\\nfromdatasets importload_dataset\\nimporttensorflow astf\\ndataset =load_dataset( \"imdb\").shuffle()\\ntokenizer =BertTokenizerFast .from_pretrained( \\'bert-base-uncased\\' )\\nmodel=TFBertForSequenceClassification .from_pretrained( \\'bert-base-uncased\\' , num_labels =2)\\ntrain_encodings =tokenizer(dataset[ \\'train\\'][\\'text\\'], truncation =True, padding =True)\\ntrain_dataset =tf.data.Dataset.from_tensor_slices(( dict(train_encodings), dataset[ \\'train\\'][\\'label\\']))\\nval_dataset =...//Analogously\\noptimizer =tf.keras.optimizers .Adam(learning_rate =5e-5)\\nmodel.compile(optimizer =optimizer, loss =model.compute_loss)\\nmodel.fit(train_dataset .batch(16), epochs =3, batch_size =16)\\nmodel.evaluate(val_dataset .batch(16), verbose =0)\\n49/52', 'Wrap Up\\n50/52', 'Summary\\nITransformers have blown other\\narchitectures out of the water for\\nNLP\\nIGet rid of recurrence and rely on\\nself-attention\\nINLP pre-training using Masked\\nLanguage Modelling\\nIMost recent improvements using\\nlarger models andmore data\\nIDistillation can make model serving\\nand inference more tractable\\n51/52']}, '06_cnn_printable.pdf': {'text': ['Introduction\\nJim Dowling\\njdowling@kth.se\\n2022-11-10\\nSlides by Amir H. Payberah', 'Let’s Start With An Example\\n1 / 1', 'MNIST Dataset\\n▶Handwritten digits in the MNIST dataset are 28x28 pixel greyscale images.\\n2 / 1', 'One-Layer Network For Classifying MNIST (1/4)\\n[https://github.com/GoogleCloudPlatform/tensorflow-without-a-phd]\\n3 / 1', 'One-Layer Network For Classifying MNIST (2/4)\\n▶Let’s make a one-layer neural network for classifying digits.\\n▶Each neuron in a neural network:\\n•Does a weighted sum of all of its inputs\\n•Adds a bias\\n•Feeds the result through some non-linear activation function, e.g., softmax.\\n4 / 1', 'One-Layer Network For Classifying MNIST (3/4)\\n▶Assume we have a batch of 100 images as the input.\\n▶Using the first column of the weights matrix W, we compute the weighted sum of\\nall the pixels of the first image.\\n•The first neuron:\\nL0,0=w0,0x(1)\\n0+w1,0x(1)\\n1+···+w783 ,0x(1)\\n783\\n•The 2nd neuron until the 10th:\\nL0,1=w0,1x(1)\\n0+w1,1x(1)\\n1+···+w783 ,1x(1)\\n783\\n···\\nL0,9=w0,9x(1)\\n0+w1,9x(1)\\n1+···+w783 ,9x(1)\\n783\\n•Repeat the operation for the other 99 images,\\ni.e.,x(2)···x(100)\\n5 / 1', 'One-Layer Network For Classifying MNIST (4/4)\\n▶Each neuron must now add its bias.\\n▶Apply the softmax activation function for each instance x(i).\\n▶For each input instance x(i):Li=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0Li,0\\nLi,1\\n...\\nLi,9\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n▶^yi=softmax (Li+b)\\n6 / 1', 'How Good the Predictions Are?\\n▶Define the cost function J(W) as the cross-entropy of what the network tells us ( ˆ yi)\\nand what we know to be the truth ( yi), for each instance x(i).\\n▶Compute the partial derivatives of the cross-entropy with respect to all the weights\\nand all the biases, ∇WJ(W).\\n▶Update weights and biases by a fraction of the gradient W(next )=W−η∇WJ(W)\\n7 / 1', 'mnist = tf.keras.datasets.mnist\\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\\nmodel = tf.keras.Sequential([\\ntf.keras.layers.Flatten(input_shape=(28, 28)),\\ntf.keras.layers.Dense(10, activation=’softmax’)\\n])\\nmodel.compile(optimizer=’sgd’, loss=’sparse_categorical_crossentropy’, metrics=[’accuracy’])\\nmodel.fit(x_train, y_train, batch_size=100, epochs=10)\\nmodel.evaluate(x_test, y_test)\\n8 / 1', '9 / 1', 'Some Improvement (1/5)\\n▶Add more layers to improve the accuracy.\\n▶On intermediate layers we will use the the sigmoid activation function.\\n▶We keep softmax as the activation function on the last layer.\\n[https://github.com/GoogleCloudPlatform/tensorflow-without-a-phd]\\n10 / 1', 'Some Improvement (2/5)\\n▶Network initialization. e.g., using He initialization.\\n▶Better optimizer, e.g., using Adam optimizer.\\n[https://github.com/GoogleCloudPlatform/tensorflow-without-a-phd]\\n11 / 1', 'Some Improvement (3/5)\\n▶Better activation function, e.g., using ReLU (z) = max( 0,z).\\n[https://github.com/GoogleCloudPlatform/tensorflow-without-a-phd]\\n12 / 1', 'Some Improvement (4/5)\\n▶Overcome overfitting, e.g., using dropout.\\n[https://github.com/GoogleCloudPlatform/tensorflow-without-a-phd]\\n13 / 1', 'Some Improvement (5/5)\\n▶Start fast and decay the learning rate exponentially.\\n▶You can do this with the tf.keras.callbacks.LearningRateScheduler callback.\\n[https://github.com/GoogleCloudPlatform/tensorflow-without-a-phd]\\n14 / 1', 'model = tf.keras.models.Sequential([\\ntf.keras.layers.Flatten(input_shape=(28, 28)),\\ntf.keras.layers.Dense(128, kernel_initializer=\"he_normal\", activation=’relu’),\\ntf.keras.layers.Dropout(0.2),\\ntf.keras.layers.Dense(10, activation=’softmax’)\\n])\\n# lr decay function\\ndef lr_decay(epoch):\\nreturn 0.01 * math.pow(0.6, epoch)\\n# lr schedule callback\\nlr_decay_callback = tf.keras.callbacks.LearningRateScheduler(lr_decay, verbose=True)\\nmodel.compile(optimizer=’adam’, loss=’sparse_categorical_crossentropy’, metrics=[’accuracy’],\\ncallbacks=[lr_decay_callback])\\nmodel.fit(x_train, y_train, batch_size=100, epochs=10)\\nmodel.evaluate(x_test, y_test)\\n15 / 1', 'Vanilla Deep Neural Networks Challenges (1/2)\\n▶Pixels of each image were flattened into a single vector (really bad idea).\\n▶Vanilla deep neural networks do not scale.\\n•In MNIST, images are black-and-white 28x28 pixel images: 28×28=784weights.\\n▶Handwritten digits are made of shapes and we discarded the shape information when\\nwe flattened the pixels.\\n16 / 1', 'Vanilla Deep Neural Networks Challenges (2/2)\\n▶Difficult to recognize objects.\\n▶Rotation\\n▶Lighting: objects may look different depending on the level of external lighting.\\n▶Deformation: objects can be deformed in a variety of non-affine ways.\\n▶Scale variation: visual classes often exhibit variation in their size.\\n▶Viewpoint invariance.\\n17 / 1', 'Tackle the Challenges\\n▶Convolutional neural networks (CNN) can tackle the vanilla model challenges.\\n▶CNN is a type of neural network that can take advantage of shape information.\\n▶It applies a series of filters to the raw pixel data of an image to extract and learn\\nhigher-level features, which the model can then use for classification.\\n18 / 1', 'Filters and Convolution Operations\\n19 / 1', 'Brain Visual Cortex Inspired CNNs\\n▶1959, David H. Hubel and Torsten Wiesel.\\n▶Many neurons in the visual cortex have a small local receptive field.\\n▶They react only to visual stimuli located in a limited region of the visual field.\\n20 / 1', 'Receptive Fields and Filters\\n▶Imagine a flashlight that is shining over the top left of the image.\\n▶The region that it is shining over is called the receptive field.\\n▶This flashlight is called a filter.\\n▶A filter is a set of weights.\\n▶A filter is a feature detector, e.g., straight edges, simple colors, and curves.\\n[https://adeshpande3.github.io/A-Beginner’s-Guide-To-Understanding-Convolutional-Neural-Networks]\\n21 / 1', 'Filters Example (1/3)\\n[https://adeshpande3.github.io/A-Beginner’s-Guide-To-Understanding-Convolutional-Neural-Networks]\\n22 / 1', 'Filters Example (2/3)\\n[https://adeshpande3.github.io/A-Beginner’s-Guide-To-Understanding-Convolutional-Neural-Networks]\\n23 / 1', 'Filters Example (3/3)\\n[https://adeshpande3.github.io/A-Beginner’s-Guide-To-Understanding-Convolutional-Neural-Networks]\\n24 / 1', 'Convolution Operation\\n▶Convolution takes a filter and multiplying it over the entire area of an input image.\\n▶Imagine this flashlight (filter) sliding across all the areas of the input image.\\n[https://adeshpande3.github.io/A-Beginner’s-Guide-To-Understanding-Convolutional-Neural-Networks]\\n25 / 1', 'Convolution Operation - 2D Example\\n26 / 1', 'Convolutional Neural Network (CNN)\\n27 / 1', 'CNN Components (1/2)\\n▶Convolutional layers: apply a specified number of convolution filters to the image.\\n▶Pooling layers: downsample the image data extracted by the convolutional layers to\\nreduce the dimensionality of the feature map in order to decrease processing time.\\n▶Dense layers: a fully connected layer that performs classification on the features\\nextracted by the convolutional layers and downsampled by the pooling layers.\\n28 / 1', 'CNN Components (2/2)\\n▶A CNN is composed of a stack of convolutional modules.\\n▶Each module consists of a convolutional layer followed by a pooling layer.\\n▶The last module is followed by one or more dense layers that perform classification.\\n▶The final dense layer contains a single node for each target class in the model, with\\na softmax activation function.\\n29 / 1', 'Convolutional Layer\\n30 / 1', 'Convolutional Layer (1/4)\\n▶Sparse interactions\\n▶Each neuron in the convolutional layers is only connected to pixels in its receptive\\nfield (not every single pixel).\\n31 / 1', 'Convolutional Layer (2/4)\\n▶Each neuron applies filters on its receptive field.\\n•Calculates a weighted sum of the input pixels in the receptive field.\\n▶Adds a bias, and feeds the result through its activation function to the next layer.\\n▶The output of this layer is a feature map (activation map)\\n32 / 1', 'Convolutional Layer (3/4)\\n▶Parameter sharing\\n▶All neurons of a convolutional layer reuse the same weights.\\n▶They apply the same filter in different positions.\\n▶Whereas in a fully-connected network, each neuron had its own set of weights.\\n33 / 1', 'Convolutional Layer (4/4)\\n▶Assume the filter size (kernel size) is fw×fh.\\n•fhandfware the height and width of the receptive field, respectively.\\n▶A neuron in row iand column jof a given layer is connected to the outputs of the\\nneurons in the previous layer in rows itoi+fh−1, and columns jtoj+fw−1.\\n34 / 1', 'Padding\\n▶What will happen if you apply a 5x5 filter to a 32x32 input volume?\\n•The output volume would be 28x28.\\n•The spatial dimensions decrease.\\n▶Zero padding: in order for a layer to have the same height and width as the previous\\nlayer, it is common to add zeros around the inputs.\\n▶In TensorFlow, padding can be either SAME orVALID to have zero padding or not.\\n35 / 1', 'Stride\\n▶The distance between two consecutive receptive fields is called the stride.\\n▶The stride controls how the filter convolves around the input volume.\\n▶Assume shandsware the vertical and horizontal strides, then, a neuron located in\\nrowiand column jin a layer is connected to the outputs of the neurons in the\\nprevious layer located in rows i×shtoi×sh+fh−1, and columns j×swto\\nj×sw+fw−1.\\n36 / 1', 'Stacking Multiple Feature Maps\\n▶Up to now, we represented each convolutional layer with a single feature map.\\n▶Each convolutional layer can be composed of several feature maps of equal sizes.\\n▶Input images are also composed of multiple sublayers: one per color channel.\\n▶A convolutional layer simultaneously applies multiple filters to its inputs.\\n37 / 1', 'Activation Function\\n▶After calculating a weighted sum of the input pixels in the receptive fields, and adding\\nbiases, each neuron feeds the result through its ReLU activation function to the next\\nlayer.\\n▶The purpose of this activation function is to add non linearity to the system.\\n38 / 1', 'Pooling Layer\\n39 / 1', 'Pooling Layer (1/2)\\n▶After the activation functions, we can apply a pooling layer.\\n▶Its goal is to subsample (shrink) the input image.\\n•To reduce the computational load, the memory usage, and the number of parameters.\\n40 / 1', 'Pooling Layer (2/2)\\n▶Each neuron in a pooling layer is connected to the outputs of a receptive field in the\\nprevious layer.\\n▶A pooling neuron has no weights.\\n▶It aggregates the inputs using an aggregation function such as the max or mean.\\n41 / 1', 'Fully Connected Layer\\n42 / 1', 'Fully Connected Layer\\n▶This layer takes an input from the last convolution module, and outputs an Ndimen-\\nsional vector.\\n•Nis the number of classes that the model has to choose from.\\n▶For example, if you wanted a digit classification model, Nwould be 10.\\n▶Each number in this Ndimensional vector represents the probability of a certain class.\\n43 / 1', 'Flattening\\n▶We need to convert the output of the convolutional part of the CNN into a 1D\\nfeature vector.\\n▶This operation is called flattening.\\n▶It gets the output of the convolutional layers, flattens all its structure to create a\\nsingle long feature vector to be used by the dense layer for the final classification.\\n44 / 1', 'Example\\n45 / 1', 'A Toy ConvNet: X’s and O’s\\n46 / 1', 'For Example\\n47 / 1', 'Trickier Cases\\n48 / 1', 'Deciding is Hard\\n49 / 1', 'What Computers See\\n50 / 1', 'Computers are Literal\\n51 / 1', 'ConvNets Match Pieces of the Image\\n52 / 1', 'Filters Match Pieces of the Image\\n53 / 1', 'Filters Match Pieces of the Image\\n54 / 1', 'Filters Match Pieces of the Image\\n55 / 1', 'Filters Match Pieces of the Image\\n56 / 1', 'Filters Match Pieces of the Image\\n57 / 1', 'Filters Match Pieces of the Image\\n58 / 1', 'Filtering: The Math Behind the Match\\n59 / 1', 'Filtering: The Math Behind the Match\\n60 / 1', 'Filtering: The Math Behind the Match\\n61 / 1', 'Filtering: The Math Behind the Match\\n62 / 1', 'Filtering: The Math Behind the Match\\n63 / 1', 'Filtering: The Math Behind the Match\\n64 / 1', 'Filtering: The Math Behind the Match\\n65 / 1', 'Filtering: The Math Behind the Match\\n66 / 1', 'Filtering: The Math Behind the Match\\n67 / 1', 'Filtering: The Math Behind the Match\\n68 / 1', 'Filtering: The Math Behind the Match\\n69 / 1', 'Filtering: The Math Behind the Match\\n70 / 1', 'Filtering: The Math Behind the Match\\n71 / 1', 'Filtering: The Math Behind the Match\\n72 / 1', 'Filtering: The Math Behind the Match\\n73 / 1', 'Convolution: Trying Every Possible Match\\n74 / 1', 'Three Filters Here, So Three Images Out\\n75 / 1', 'Convolution Layer\\n▶One image becomes a stack of filtered images.\\n76 / 1', 'Rectified Linear Units (ReLUs)\\n77 / 1', 'Rectified Linear Units (ReLUs)\\n78 / 1', 'Rectified Linear Units (ReLUs)\\n79 / 1', 'Rectified Linear Units (ReLUs)\\n80 / 1', 'ReLU Layer\\n▶A stack of images becomes a stack of images with no negative values.\\n81 / 1', 'Pooling: Shrinking the Image Stack\\n82 / 1', 'Pooling: Shrinking the Image Stack\\n83 / 1', 'Pooling: Shrinking the Image Stack\\n84 / 1', 'Pooling: Shrinking the Image Stack\\n85 / 1', 'Pooling: Shrinking the Image Stack\\n86 / 1', 'Pooling: Shrinking the Image Stack\\n87 / 1', 'Repeat For All the Filtered Images\\n88 / 1', 'Layers Get Stacked\\n▶The output of one becomes the input of the next.\\n89 / 1', 'Deep Stacking\\n90 / 1', 'Fully Connected Layer\\n▶Flattening the outputs before giving them to the fully connected layer.\\n91 / 1', 'Fully Connected Layer\\n92 / 1', 'Fully Connected Layer\\n93 / 1', 'Fully Connected Layer\\n94 / 1', 'Fully Connected Layer\\n95 / 1', 'Fully Connected Layer\\n96 / 1', 'Fully Connected Layer\\n97 / 1', 'Fully Connected Layer\\n98 / 1', 'Fully Connected Layer\\n99 / 1', 'Putting It All Together\\n100 / 1', '101 / 1', 'One more example\\n▶A conv layer.\\n▶Computes 2 feature maps.\\n▶Filters: 3x3 with stride of 2.\\n▶Input tensor shape: [ 7,7,3].\\n▶Output tensor shape: [ 3,3,2].\\n[http://cs231n.github.io/convolutional-networks]\\n102 / 1', '103 / 1', 'CNN in TensorFlow\\n104 / 1', 'CNN in TensorFlow (1/7)\\n▶A CNN for the MNIST dataset with the following network.\\n▶Conv. layer 1: computes 32 feature maps using a 5x5 filter with ReLU activation.\\n▶Pooling layer 1: max pooling layer with a 2x2 filter and stride of 2.\\n▶Conv. layer 2: computes 64 feature maps using a 5x5 filter.\\n▶Pooling layer 2: max pooling layer with a 2x2 filter and stride of 2.\\n▶Dense layer: densely connected layer with 1024 neurons.\\n▶Softmax layer\\n105 / 1', 'CNN in TensorFlow (2/7)\\n▶Conv. layer 1: computes 32 feature maps using a 5x5 filter with ReLU activation.\\n▶Padding same is added to preserve width and height.\\n▶Input tensor shape: [ batch size ,28,28,1]\\n▶Output tensor shape: [ batch size ,28,28,32]\\n# MNIST images are 28x28 pixels, and have one color channel: [28, 28, 1]\\ntf.keras.layers.Conv2D(kernel_size=5, filters=32, activation=’relu’, padding=’same’,\\ninput_shape=[28, 28, 1])\\n106 / 1', 'CNN in TensorFlow (3/7)\\n▶Pooling layer 1: max pooling layer with a 2x2 filter and stride of 2.\\n▶Input tensor shape: [ batch size ,28,28,32]\\n▶Output tensor shape: [ batch size ,14,14,32]\\ntf.keras.layers.MaxPooling2D(pool_size=2, strides=2)\\n107 / 1', 'CNN in TensorFlow (4/7)\\n▶Conv. layer 2: computes 64 feature maps using a 5x5 filter.\\n▶Padding same is added to preserve width and height.\\n▶Input tensor shape: [ batch size ,14,14,32]\\n▶Output tensor shape: [ batch size ,14,14,64]\\ntf.keras.layers.Conv2D(kernel_size=5, filters=64, activation=’relu’, padding=’same’)\\n108 / 1', 'CNN in TensorFlow (5/7)\\n▶Pooling layer 2: max pooling layer with a 2x2 filter and stride of 2.\\n▶Input tensor shape: [ batch size ,14,14,64]\\n▶Output tensor shape: [ batch size ,7,7,64]\\ntf.keras.layers.MaxPooling2D(pool_size=2, strides=2)\\n109 / 1', 'CNN in TensorFlow (6/7)\\n▶Flatten tensor into a batch of vectors.\\n•Input tensor shape: [ batch size ,7,7,64]\\n•Output tensor shape: [ batch size ,7∗7∗64]\\ntf.keras.layers.Flatten()\\n▶Dense layer: densely connected layer with 1024 neurons.\\n•Input tensor shape: [ batch size ,7∗7∗64]\\n•Output tensor shape: [ batch size ,1024 ]\\ntf.keras.layers.Dense(1024, activation=’relu’)\\n▶Softmax layer: softmax layer with 10 neurons.\\n•Input tensor shape: [ batch size ,1024 ]\\n•Output tensor shape: [ batch size ,10]\\ntf.keras.layers.Dense(10, activation=’softmax’)\\n110 / 1', 'CNN in TensorFlow (7/7)\\nmodel = tf.keras.Sequential([\\ntf.keras.layers.Conv2D(kernel_size=5, filters=32, activation=’relu’, padding=’same’,\\ninput_shape=[28, 28, 1]),\\ntf.keras.layers.MaxPooling2D(pool_size=2, strides=2),\\ntf.keras.layers.Conv2D(kernel_size=5, filters=64, activation=’relu’, padding=’same’),\\ntf.keras.layers.MaxPooling2D(pool_size=2, strides=2),\\ntf.keras.layers.Flatten(),\\ntf.keras.layers.Dense(1024, activation=’relu’),\\ntf.keras.layers.Dense(10, activation=’softmax’)\\n])\\n111 / 1', '112 / 1', 'Training CNNs\\n113 / 1', 'Training CNN (1/4)\\n▶Let’s see how to use backpropagation on a single convolutional layer.\\n▶Assume we have an input Xof size 3x3 and a single filter Wof size 2x2.\\n▶No padding and stride = 1.\\n▶It generates an output Hof size 2x2.\\n114 / 1', 'Training CNN (2/4)\\n▶Forward pass\\nh11=W11X11+W12X12+W21X21+W22X22\\nh12=W11X12+W12X13+W21X22+W22X23\\nh21=W11X21+W12X22+W21X31+W22X32\\nh22=W11X22+W12X23+W21X32+W22X33\\n115 / 1', 'Training CNN (3/4)\\n▶Backward pass\\n▶Eis the error: E=Eh11+Eh12+Eh21+Eh22\\n∂E\\n∂W11=∂Eh11\\n∂h11∂h11\\n∂W11+∂Eh12\\n∂h12∂h12\\n∂W11+∂Eh21\\n∂h21∂h21\\n∂W11+∂Eh22\\n∂h22∂h22\\n∂W11\\n∂E\\n∂W12=∂Eh11\\n∂h11∂h11\\n∂W12+∂Eh12\\n∂h12∂h12\\n∂W12+∂Eh21\\n∂h21∂h21\\n∂W12+∂Eh22\\n∂h22∂h22\\n∂W12\\n∂E\\n∂W21=∂Eh11\\n∂h11∂h11\\n∂W21+∂Eh12\\n∂h12∂h12\\n∂W21+∂Eh21\\n∂h21∂h21\\n∂W21+∂Eh22\\n∂h22∂h22\\n∂W21\\n∂E\\n∂W22=∂Eh11\\n∂h11∂h11\\n∂W22+∂Eh12\\n∂h12∂h12\\n∂W22+∂Eh21\\n∂h21∂h21\\n∂W22+∂Eh22\\n∂h22∂h22\\n∂W22\\n116 / 1', 'Training CNN (4/4)\\n▶Update the wights W\\nW(next )\\n11 =W11−η∂E\\n∂W11\\nW(next )\\n12 =W12−η∂E\\n∂W12\\nW(next )\\n21 =W21−η∂E\\n∂W21\\nW(next )\\n22 =W22−η∂E\\n∂W22\\n117 / 1', 'Summary\\n118 / 1', 'Summary\\n▶Receptive fields and filters\\n▶Convolution operation\\n▶Padding and strides\\n▶Pooling layer\\n▶Flattening, dropout, dense\\n119 / 1', 'Reference\\n▶Tensorflow and Deep Learning without a PhD\\nhttps://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist\\n▶Ian Goodfellow et al., Deep Learning (Ch. 9)\\n▶Aur´ elien G´ eron, Hands-On Machine Learning (Ch. 14)\\n120 / 1', 'Questions?\\n121 / 1']}, '05_training_feedforward_networks_printable.pdf': {'text': ['Introduction\\nJim Dowling\\njdowling@kth.se\\n2022-11-14\\nSlides by Amir H. Payberah', 'The Course Web Page\\nhttps://id2223kth.github.io\\nhttps://tinyurl.com/6s5jy46a\\n1 / 71', 'Feedforward Neural Network Architecture\\n▶A feedforward neural network is composed of:\\n•One input layer\\n•One or more hidden layers\\n•One final output layer\\n2 / 71', 'Feedforward Network in TensorFlow\\nn_output = 3\\nn_hidden = 4\\nn_features = 2\\nmodel = keras.models.Sequential()\\nmodel.add(keras.layers.Dense(n_hidden, input_shape=(n_features,), activation=\"relu\"))\\nmodel.add(keras.layers.Dense(n_output, activation=\"softmax\"))\\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\\nmodel.fit(X_train, y_train, epochs=30)\\n3 / 71', 'Challenges of Training Feedforward Neural Networks\\n▶Challenges ...\\n▶Overfitting: risk of overfitting a model with large number of parameters.\\n▶Vanishing/exploding gradients: hard to train lower layers.\\n▶Training speed: slow training with large networks.\\n4 / 71', 'Overfitting\\n5 / 71', 'High Degree of Freedom and Overfitting Problem\\n▶With large number of parameters, a network has a high degree of freedom.\\n▶It can fit a huge variety of complex datasets.\\n▶This flexibility also means that it is prone to overfitting on training set.\\n▶Let’s reduce the degree of freedom a model.\\n6 / 71', 'Avoiding Overfitting\\n▶Early stopping\\n▶ l1 and l2 regularization\\n▶Max-norm regularization\\n▶Dropout\\n▶Data augmentation\\n7 / 71', 'Avoiding Overfitting\\n▶Early stopping\\n▶ l1 and l2 regularization\\n▶Max-norm regularization\\n▶Dropout\\n▶Data augmentation\\n8 / 71', 'Early Stopping (1/2)\\n▶As the training steps go by, its prediction error on the training/validation set naturally\\ngoes down.\\n▶After a while the validation error stops decreasing and starts to go back up.\\n•The model has started to overfit the training data.\\n▶In the early stopping, we stop training when the validation error reaches a minimum.\\n9 / 71', 'Early Stopping (2/2)\\nfrom tensorflow.keras.callbacks import EarlyStopping\\nmodel = tf.keras.models.Sequential(...)\\nmodel.compile(optimizer=’sgd’, loss=’sparse_categorical_crossentropy’, metrics=[’accuracy’])\\nearlystop_callback = EarlyStopping(monitor=’accuracy’, min_delta=0.05, patience=1)\\nmodel.fit(x_train, y_train, epochs=500, callbacks=[earlystop_callback])\\n10 / 71', 'Avoiding Overfitting\\n▶Early stopping\\n▶ l1 and l2 regularization\\n▶Max-norm regularization\\n▶Dropout\\n▶Data augmentation\\n11 / 71', 'l1 and l2 Regularization (1/3)\\n▶Penalize large values of weights wj.\\n~J(w) =J(w) +λR(w)\\n▶Two questions:\\n1. How should we define R(w)?\\n2. How do we determine λ?\\n12 / 71', 'l1 and l2 Regularization (2/3)\\n▶ l1 regression: R(w) =λPn\\ni=1|wi|is added to the cost function.\\n~J(w) =J(w) +λnX\\ni=1|wi|\\nkeras.layers.Dense(100, activation=\"relu\", kernel_regularizer=keras.regularizers.l1(0.1))\\n13 / 71', 'l1 and l2 Regularization (3/3)\\n▶ l2 regression: R(w) =λPn\\ni=1w2\\niis added to the cost function.\\n~J(w) =J(w) +λnX\\ni=1w2\\ni\\nkeras.layers.Dense(100, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.01))\\n14 / 71', 'Avoiding Overfitting\\n▶Early stopping\\n▶ l1 and l2 regularization\\n▶Max-norm regularization\\n▶Dropout\\n▶Data augmentation\\n15 / 71', 'Max-Norm Regularization\\n▶Max-norm regularization: constrains the weights wjof the incoming connections for\\neach neuron j.\\n•Prevents them from getting too large.\\n▶After each training step, clip wjas below, if ||wj||2>r:\\nwj←wjr\\n||wj||2\\n•ris the max-norm hyperparameter\\n•||wj||2= (P\\niw2\\ni,j)1\\n2=q\\nw2\\n1,j+w2\\n2,j+···+w2\\nn,j\\nkeras.layers.Dense(100, activation=\"relu\", kernel_constraint=keras.constraints.max_norm(1.))\\n16 / 71', 'Avoiding Overfitting\\n▶Early stopping\\n▶ l1 and l2 regularization\\n▶Max-norm regularization\\n▶Dropout\\n▶Data augmentation\\n17 / 71', 'Dropout (1/4)\\n▶Would a company perform better if its employees were told to toss a coin every\\nmorning to decide whether or not to go to work?\\n18 / 71', 'Dropout (2/4)\\n▶At each training step, each neuron drops out temporarily with a probability p.\\n•The hyperparameter pis called the dropout rate.\\n•A neuron will be entirely ignored during this training step.\\n•It may be active during the next step.\\n•Exclude the output neurons.\\n▶After training, neurons don’t get dropped anymore.\\n19 / 71', 'Dropout (3/4)\\n▶Each neuron can be either present or absent.\\n▶2Npossible networks, where Nis the total\\nnumber of droppable neurons.\\n•N=4in this figure.\\n20 / 71', 'Dropout (4/4)\\nmodel = keras.models.Sequential([\\nkeras.layers.Flatten(input_shape=[28, 28]),\\nkeras.layers.Dropout(rate=0.2),\\nkeras.layers.Dense(128, activation=\"relu\"),\\nkeras.layers.Dropout(rate=0.2),\\nkeras.layers.Dense(10, activation=\"softmax\")\\n])\\n21 / 71', 'Avoiding Overfitting\\n▶Early stopping\\n▶ l1 and l2 regularization\\n▶Max-norm regularization\\n▶Dropout\\n▶Data augmentation\\n22 / 71', 'Data Augmentation\\n▶One way to make a model generalize better is to train it on more data.\\n▶This will reduce overfitting.\\n▶Create fake data and add it to the training set.\\n•E.g., in an image classification we can slightly\\nshift, rotate and resize an image.\\n•Add the resulting pictures to the training set.\\n23 / 71', 'Vanishing/Exploding Gradients\\n24 / 71', 'Vanishing/Exploding Gradients Problem (1/4)\\n▶The backpropagation goes from output to input layer, and propagates the error\\ngradient on the way.\\nw(next )=w−η∂J(w)\\n∂w\\n▶Gradients often get smaller and smaller as the algorithm progresses down to the lower\\nlayers.\\n▶As a result, the gradient descent update leaves the lower layer connection weights\\nvirtually unchanged.\\n▶This is called the vanishing gradients problem.\\n25 / 71', 'Vanishing/Exploding Gradients Problem (2/4)\\n▶Assume a network with just a single neuron in each layer.\\n•w1,w2,···are the weights\\n•b1,b2,···are the biases\\n•Cis the cost function\\n▶The output ajfrom the jth neuron is σ(zj).\\n•σis the sigmoid activation function\\n•zj=wjaj−1+bj\\n•E.g., a4=σ(z4) =sigmoid (w4a3+b4)\\n26 / 71', 'Vanishing/Exploding Gradients Problem (3/4)\\n▶Lets compute the gradient associated to the first hidden neuron (∂C\\n∂b1).\\n∂C\\n∂b1=∂C\\n∂a4×∂a4\\n∂z4×∂z4\\n∂a3×∂a3\\n∂z3×∂z3\\n∂a2×∂a2\\n∂z2×∂z2\\n∂a1×∂a1\\n∂z1×∂z1\\n∂b1\\n∂C\\n∂b1=∂C\\n∂a4×∂a4\\n∂z4×∂w4a3+b4\\n∂a3×∂a3\\n∂z3×∂w3a2+b3\\n∂a2×∂a2\\n∂z2×∂w2a1+b2\\n∂a1×∂a1\\n∂z1×∂w1a0+b1\\n∂b1\\n∂C\\n∂b1=∂C\\n∂a4×∂a4\\n∂z4×w4×∂a3\\n∂z3×w3×∂a2\\n∂z2× ×w2×∂a1\\n∂z1×1\\n27 / 71', 'Vanishing/Exploding Gradients Problem (4/4)\\n▶Now, consider∂C\\n∂b3.\\n∂C\\n∂b3=∂C\\n∂a4×∂a4\\n∂z4×w4×∂a3\\n∂z3\\n∂C\\n∂b1=∂C\\n∂a4×∂a4\\n∂z4×w4×∂a3\\n∂z3×w3×∂a2\\n∂z2×w2×∂a1\\n∂z1×1\\n▶Assume w3×∂a2\\n∂z2<1\\n4andw2×∂a1\\n∂z1<1\\n4\\n•The gradient∂C\\n∂b1be a factor of 16 (or more) smaller than∂C\\n∂b3.\\n•This is the essential origin of the vanishing gradient problem.\\n28 / 71', 'Overcoming the Vanishing Gradient\\n▶Parameter initialization strategies\\n▶Nonsaturating activation function\\n▶Batch normalization\\n▶Gradient clipping\\n29 / 71', 'Overcoming the Vanishing Gradient\\n▶Parameter initiazlization strategies\\n▶Nonsaturating activation function\\n▶Batch normalization\\n▶Gradient clipping\\n30 / 71', 'Parameter Initialization Strategies (1/4)\\n▶The non-linearity of a neural network causes the cost functions to become non-\\nconvex.\\n▶The stochastic gradient descent on non-convex cost functions performs is sensitive\\nto the values of the initial parameters.\\n▶Designing initialization strategies is a difficult task.\\n31 / 71', 'Parameter Initialization Strategies (2/4)\\n▶The initial parameters need to break symmetry between different units.\\n▶Two hidden units with the same activation function connected to the same inputs,\\nmust have different initial parameters.\\n•The goal of having each unit compute a different function.\\n▶It motivates random initialization of the parameters.\\n•Typically, we set the biases to constants, and initialize only the weights randomly.\\n32 / 71', 'Parameter Initialization Strategies (3/4)\\n▶We need the signals to flow properly in both directions.\\n▶The Glorot and Bengio initialization proposed that:\\n•The variance of the outputs of each layer to be equal to the variance of its inputs.\\n•The gradients to have equal variance before and after flowing through a layer in the\\nreverse direction.\\n▶It is not possible to guarantee both unless each layer has an equal number of inputs\\nand neurons.\\n▶Based on the Xavier initialization, the weights are initialized using normal distribution\\nwith mean 0 and the following standard deviation.\\n33 / 71', 'Parameter Initialization Strategies (4/4)\\n▶fan inandfan outare the number of inputs and neurons for the layer whose weights\\nare being initialized.\\n▶fan avg=2\\nfan in+fan out\\n▶Glorot initialization, for none, logistic, sigmoid, and tanh: σ2=1\\nfan avg\\n▶He initialization, for ReLU: σ2=2\\nfan in\\nkeras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")\\n34 / 71', 'Overcoming the Vanishing Gradient\\n▶Parameter initiazlization strategies\\n▶Nonsaturating activation function\\n▶Batch normalization\\n▶Gradient clipping\\n35 / 71', 'Nonsaturating Activation Functions (1/4)\\n▶ReLU (z) = max( 0,z)\\n▶The dying ReLUs problem.\\n•During training, some neurons stop outputting anything other than 0.\\n•E.g., when the weighted sum of the neuron’s inputs is negative, it starts outputting 0.\\n▶Use leaky ReLU instead: LeakyReLU α(z) = max( αz,z).\\n•αis the slope of the function for z<0.\\n36 / 71', 'Nonsaturating Activation Functions (2/4)\\n▶Randomized Leaky ReLU (RReLU)\\n•αis picked randomly during training, and it is fixed during testing.\\n▶Parametric Leaky ReLU (PReLU)\\n•Learn αduring training (instead of being a hyperparameter).\\n▶Exponential Linear Unit (ELU)\\nELUα(z) =\\x1aα(exp(z)−1) if z<0\\nz ifz≥0\\n37 / 71', 'Nonsaturating Activation Functions (3/4)\\n▶Which activation function should we use?\\n▶In general logistic <tanh <ReLU <leaky ReLU (and its variants) <ELU\\n▶If you care about runtime performance, then leaky ReLUs works better than ELUs.\\n38 / 71', 'Nonsaturating Activation Functions (4/4)\\n# elu\\nkeras.layers.Dense(10, activation=\"elu\")\\n# leaky relu\\nmodel = keras.models.Sequential([\\nkeras.layers.Flatten(input_shape=[28, 28]),\\nkeras.layers.Dense(128, kernel_initializer=\"he_normal\"),\\nkeras.layers.LeakyReLU(),\\nkeras.layers.Dense(10, activation=\"softmax\")\\n])\\n39 / 71', 'Overcoming the Vanishing Gradient\\n▶Parameter initiazlization strategies\\n▶Nonsaturating activation function\\n▶Batch normalization\\n▶Gradient clipping\\n40 / 71', 'Batch Normalization (1/4)\\n▶The gradient is used to update each parameter, under the assumption that the other\\nlayers do not change.\\n•In practice, we update all of the layers simultaneously.\\n•However, unexpected results can happen.\\n▶Batch normalization makes the learning of layers in the network more independent\\nof each other.\\n•It is a technique to address the problem that the distribution of each layer’s inputs\\nchanges during training, as the parameters of the previous layers change.\\n▶The technique consists of adding an operation in the model just before the activation\\nfunction of each layer.\\n41 / 71', 'Batch Normalization (2/4)\\n▶It’s zero-centering and normalizing the inputs, then scaling and shifting the result.\\n•Estimates the inputs’ mean and standard deviation of the current mini-batch.\\nµB=1\\nmBmBX\\ni=1x(i)\\nσ2\\nB=1\\nmBmBX\\ni=1(x(i)−µB)2\\n▶µB: the empirical mean, evaluated over the whole mini-batch B.\\n▶σB: the empirical standard deviation, also evaluated over the whole mini-batch.\\n▶mB: the number of instances in the mini-batch.\\n42 / 71', 'Batch Normalization (3/4)\\n^x(i)=x(i)−µBp\\nσ2\\nB+ϵ\\nz(i)=γ^x(i)+β\\n▶^x(i): the zero-centered and normalized input.\\n▶z(i): the output of the BN operation, which is a scaled and shifted version of the\\ninputs.\\n▶γ: the scaling parameter vector for the layer.\\n▶β: the shifting parameter (offset) vector for the layer.\\n▶ϵ: a tiny number to avoid division by zero.\\n▶⊗: represents the element-wise multiplication.\\n43 / 71', 'Batch Normalization (4/4)\\nmodel = keras.models.Sequential([\\nkeras.layers.Flatten(input_shape=[28, 28]),\\nkeras.layers.BatchNormalization(),\\nkeras.layers.Dense(128, activation=\"relu\"),\\nkeras.layers.BatchNormalization(),\\nkeras.layers.Dense(10, activation=\"softmax\")\\n])\\n44 / 71', 'Overcoming the Vanishing Gradient\\n▶Parameter initiazlization strategies\\n▶Nonsaturating activation function\\n▶Batch normalization\\n▶Gradient clipping\\n45 / 71', 'Gradient Clipping\\n▶Gradient clipping: clip the gradients during backpropagation so that they never ex-\\nceed some threshold.\\noptimizer = keras.optimizers.SGD(clipvalue=1.0)\\nmodel.compile(loss=\"mse\", optimizer=optimizer)\\n▶Setting the clipvalue orclipnorm argument when creating an optimizer.\\n▶clipvalue=1.0 andclipnorm=1.0 : values between -1.0 and 1.0.\\n▶clipvalue=1.0 : [0.9,100.0]⇒[0.9,1.0]\\n▶clipnorm=1.0 : [0.9,100.0]⇒[0.00899964 ,0.9999595]\\n46 / 71', 'Training Speed\\n47 / 71', 'Regular Gradient Descent Optimization (1/2)\\n▶Gradient descent optimization algorithm\\n▶It updates the weights w(next )\\ni =wi−η∂J(w)\\n∂wi\\n▶Better optimization algorithms to improve the training speed\\n48 / 71', 'Regular Gradient Descent Optimization (2/2)\\nn_output = 3\\nn_hidden = 4\\nn_features = 2\\nmodel = keras.models.Sequential()\\nmodel.add(keras.layers.Dense(n_hidden, input_shape=(n_features,), activation=\"relu\"))\\nmodel.add(keras.layers.Dense(n_output, activation=\"softmax\"))\\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\\nmodel.fit(X_train, y_train, epochs=30)\\n49 / 71', 'Optimization Algorithms\\n▶Momentum\\n▶Nesterov momentum\\n▶AdaGrad\\n▶RMSProp\\n▶Adam Optimization\\n50 / 71', 'Optimization Algorithms\\n▶Momentum\\n▶Nesterov momentum\\n▶AdaGrad\\n▶RMSProp\\n▶Adam optimization\\n51 / 71', 'Momentum (1/3)\\n▶Momentum is a concept from physics: an object in motion will have a tendency to\\nkeep moving.\\n▶It measures the resistance to change in motion.\\n•The higher momentum an object has, the harder it is to stop it.\\n52 / 71', 'Momentum (2/3)\\n▶This is the very simple idea behind momentum optimization.\\n▶We can see the change in the parameters was motion: w(next )\\ni =wi−η∂J(w)\\n∂wi\\n▶We can thus use the concept of momentum to give the update process a tendency\\nto keep moving in the same direction.\\n▶It can help to escape from bad local minima pits.\\n53 / 71', 'Momentum (3/3)\\n▶Regular gradient descent optimization: w(next )\\ni =wi−η∂J(w)\\n∂wi\\n▶Momentum optimization cares about what previous gradients were.\\n▶At each iteration, it adds the local gradient to the momentum vector m.\\nmi=βmi+η∂J(w)\\n∂wi\\nw(next )\\ni =wi−mi\\n▶βis called momentum, ans it is between 0 and 1.\\noptimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)\\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\\n54 / 71', 'Optimization Algorithms\\n▶Momentum\\n▶Nesterov momentum\\n▶AdaGrad\\n▶RMSProp\\n▶Adam optimization\\n55 / 71', 'Nesterov Momentum (1/2)\\n▶Nesterov Momentum is a small variant to Momentum optimization.\\n▶Faster than vanilla Momentum optimization.\\n▶∇1represents the gradient of the cost function measured at the starting point w,\\nand∇2represents the gradient at the point located at w+βm.\\n56 / 71', 'Nesterov Momentum (2/2)\\n▶Measure the gradient of the cost function slightly ahead in the direction of the\\nmomentum (not at the local position).\\nmi=βmi+η∂J(w+βm)\\n∂wi\\nw(next )\\ni =wi−mi\\noptimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)\\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\\n57 / 71', 'Optimization Algorithms\\n▶Momentum\\n▶Nesterov momentum\\n▶AdaGrad\\n▶RMSProp\\n▶Adam optimization\\n58 / 71', 'AdaGrad (1/2)\\n▶AdaGrad keeps track of a learning rate for each parameter.\\n▶Adapts the learning rate over time (adaptive learning rate).\\n▶Decays the learning rate faster for steep dimensions than for dimensions with gentler\\nslopes.\\n59 / 71', 'AdaGrad (2/2)\\n▶For each feature wi, we do the following steps:\\nsi=si+ (∂J(w)\\n∂wi)2\\nw(next )\\ni =wi−η√si+ϵ∂J(w)\\n∂wi\\noptimizer = keras.optimizers.Adagrad(lr=0.001)\\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\\n60 / 71', 'Optimization Algorithms\\n▶Momentum\\n▶Nesterov momentum\\n▶AdaGrad\\n▶RMSProp\\n▶Adam optimization\\n61 / 71', 'RMSProp (1/2)\\n▶AdaGrad often stops too early when training neural networks.\\n▶The learning rate gets scaled down so much that the algorithm ends up stopping\\nentirely before reaching the global optimum.\\n▶The RMSProp fixed the AdaGrad problem.\\n▶It is like the AdaGrad problem, but accumulates only the gradients from the most\\nrecent iterations (not from the beginning of training).\\n62 / 71', 'RMSProp (2/2)\\n▶For each feature wi, we do the following steps:\\nsi=βsi+ (1−β)(∂J(w)\\n∂wi)2\\nw(next )\\ni =wi−η√si+ϵ∂J(w)\\n∂wi\\noptimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)\\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\\n63 / 71', 'Optimization Algorithms\\n▶Momentum\\n▶Nesterov momentum\\n▶AdaGrad\\n▶RMSProp\\n▶Adam optimization\\n64 / 71', 'Adam Optimization (1/3)\\n▶Adam (Adaptive moment estimation) combines the ideas of Momentum optimization\\nand RMSProp.\\n▶Like Momentum optimization, it keeps track of an exponentially decaying average of\\npast gradients.\\n▶Like RMSProp, it keeps track of an exponentially decaying average of past squared\\ngradients.\\n65 / 71', 'Adam Optimization (2/3)\\n1.m(next )=β1m+ (1−β1)∇wJ(w)\\n2.s(next )=β2s+ (1−β2)∇wJ(w)⊗ ∇ wJ(w)\\n3.m(next )=m\\n1−βT\\n1\\n4.s(next )=s\\n1−βT\\n2\\n5.w(next )=w−ηm⊘√\\ns+ϵ\\n▶⊗and⊘represent the element-wise multiplication and division.\\n▶Steps 1, 2, and 5: similar to both Momentum optimization and RMSProp.\\n▶Steps 3 and 4: since mandsare initialized at 0, they will be biased toward 0 at the\\nbeginning of training, so these two steps will help boost mandsat the beginning of\\ntraining.\\n66 / 71', 'Adam Optimization (3/3)\\noptimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\\n67 / 71', 'Summary\\n68 / 71', 'Summary\\n▶Overfitting\\n•Early stopping, l1 and l2 regularization, max-norm regularization\\n•Dropout, data augmentation\\n▶Vanishing gradient\\n•Parameter initialization, nonsaturating activation functions\\n•Batch normalization, gradient clipping\\n▶Training speed\\n•Momentum, nesterov momentum, AdaGrad\\n•RMSProp, Adam optimization\\n69 / 71', 'Reference\\n▶Ian Goodfellow et al., Deep Learning (Ch. 7, 8)\\n▶Aur´ elien G´ eron, Hands-On Machine Learning (Ch. 11)\\n70 / 71', 'Questions?\\n71 / 71']}, '04_deep_feedforward_networks_printable.pdf': {'text': ['Introduction\\nJim Dowling\\njdowling@kth.se\\n2022-11-10\\nSlides by Amir H. Payberah', 'Biological Neurons (1/2)\\n▶Brain architecture has inspired artificial neural networks.\\n▶A biological neuron is composed of\\n•Cell body, many dendrites (branching extensions), one axon (long extension), synapses\\n▶Biological neurons receive signals from other neurons via these synapses.\\n▶When a neuron receives a sufficient number of signals within a few milliseconds, it\\nfires its own signals.\\n1 / 58', 'Biological Neurons (2/2)\\n▶Biological neurons are organized in a vast network of billions of neurons.\\n▶Each neuron typically is connected to thousands of other neurons.\\n2 / 58', 'A Simple Artificial Neural Network\\n▶One or more binary inputs and one binary output\\n▶Activates its output when more than a certain number of its inputs are active.\\n[A. Geron, O’Reilly Media, 2017]\\n3 / 58', 'The Linear Threshold Unit (LTU)\\n▶Inputs of a LTU are numbers (not binary).\\n▶Each input connection is associated with a weight.\\n▶Computes a weighted sum of its inputs and applies a step function to that sum.\\n▶z=w1x1+w2x2+···+wnxn=w⊺x\\n▶^ y=step (z) =step (w⊺x)\\n4 / 58', 'The Perceptron\\n▶The perceptron is a single layer of LTUs.\\n▶The input neurons output whatever input they are fed.\\n▶A bias neuron, which just outputs 1 all the time.\\n▶If we use logistic function (sigmoid) instead of a step function, it computes a con-\\ntinuous output.\\n5 / 58', 'How is a Perceptron Trained? (1/2)\\n▶The Perceptron training algorithm is inspired by Hebb’s rule.\\n▶When a biological neuron often triggers another neuron, the connection between\\nthese two neurons grows stronger.\\n6 / 58', 'How is a Perceptron Trained? (2/2)\\n▶Feed one training instance xto each neuron jat a time and make its prediction ^ y.\\n▶Update the connection weights.\\n^ yj=σ(w⊺\\njx+b)\\nJ(wj) =cross entropy (yj,^ yj)\\nw(next )\\ni,j=wi,j−η∂J(wj)\\nwi\\n▶wi,j: the weight between neurons iandj.\\n▶xi: the ith input value.\\n▶^ yj: the jth predicted output value.\\n▶yj: the jth true output value.\\n▶η: the learning rate.\\n7 / 58', 'Perceptron in TensorFlow\\n8 / 58', 'Perceptron in TensorFlow\\nn_neurons = 3\\nn_features = 2\\nmodel = keras.models.Sequential()\\nmodel.add(keras.layers.Dense(n_neurons, input_shape=(n_features,), activation=\"softmax\"))\\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\\nmodel.fit(X_train, y_train, epochs=30)\\n9 / 58', 'Multi-Layer Perceptron (MLP)\\n10 / 58', 'Perceptron Weakness (1/2)\\n▶Incapable of solving some trivial problems, e.g., XOR classification problem. Why?\\nX=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f00 0\\n0 1\\n1 0\\n1 1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fby=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f00\\n1\\n1\\n0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n11 / 58', 'Perceptron Weakness (2/2)\\nX=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f00 0\\n0 1\\n1 0\\n1 1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fby=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f00\\n1\\n1\\n0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb^ y=step (z),z=w1x1+w2x2+b\\nJ(w) =1\\n4X\\nx∈X(^ y(x)−y(x))2\\n▶If we minimize J(w), we obtain w1=0,w2=0, and b=1\\n2.\\n▶But, the model outputs 0.5 everywhere.\\n12 / 58', 'Multi-Layer Perceptron (MLP)\\n▶The limitations of Perceptrons can be eliminated by stacking multiple Perceptrons.\\n▶The resulting network is called a Multi-Layer Perceptron (MLP) or deep feedforward\\nneural network.\\n13 / 58', 'Feedforward Neural Network Architecture\\n▶A feedforward neural network is composed of:\\n•One input layer\\n•One or more hidden layers\\n•One final output layer\\n▶Every layer except the output layer includes a bias neuron and is fully connected to\\nthe next layer.\\n14 / 58', 'How Does it Work?\\n▶The model is associated with a directed acyclic graph\\ndescribing how the functions are composed together.\\n▶E.g., assume a network with just a single neuron in each layer.\\n▶Also assume we have three functions f(1),f(2), and\\nf(3)connected in a chain: ^ y=f(x) =f(3)(f(2)(f(1)(x)))\\n▶f(1)is called the first layer of the network.\\n▶f(2)is called the second layer, and so on.\\n▶The length of the chain gives the depth of the model.\\n15 / 58', 'XOR with Feedforward Neural Network (1/3)\\nX=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f00 0\\n0 1\\n1 0\\n1 1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fby=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f00\\n1\\n1\\n0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fbWx=\\x141 1\\n1 1\\x15\\nbx=\\x14−1.5\\n−0.5\\x15\\n16 / 58', 'XOR with Feedforward Neural Network (2/3)\\nout h=XW⊺\\nx+bx=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0−1.5−0.5\\n−0.5 0 .5\\n−0.5 0 .5\\n0.5 1 .5\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fbh=step (out h) =\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f00 0\\n0 1\\n0 1\\n1 1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\nwh=\\x14−1\\n1\\x15\\nbh=−0.5\\n17 / 58', 'XOR with Feedforward Neural Network (3/3)\\nout=w⊺\\nhh+bh=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0−0.5\\n0.5\\n0.5\\n−0.5\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fbstep (out) =\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f00\\n1\\n1\\n0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n18 / 58', 'How to Learn Model Parameters W?\\n19 / 58', 'Feedforward Neural Network - Cost Function\\n▶We use the cross-entropy (minimizing the negative log-likelihood) between the train-\\ning data yand the model’s predictions ^ yas the cost function.\\ncost (y,^ y) =−X\\njyjlog(^ yj)\\n20 / 58', 'Gradient-Based Learning (1/2)\\n▶The most significant difference between the linear models we have seen so far and\\nfeedforward neural network?\\n▶The non-linearity of a neural network causes its cost functions to become non-convex.\\n▶Linear models, with convex cost function, guarantee to find global minimum.\\n•Convex optimization converges starting from any initial parameters.\\n21 / 58', 'Gradient-Based Learning (2/2)\\n▶Stochastic gradient descent applied to non-convex cost functions has no such con-\\nvergence guarantee.\\n▶It is sensitive to the values of the initial parameters.\\n▶For feedforward neural networks, it is important to initialize all weights to small\\nrandom values.\\n▶The biases may be initialized to zero or to small positive values.\\n22 / 58', 'Training Feedforward Neural Networks\\n▶How to train a feedforward neural network?\\n▶For each training instance x(i)the algorithm does the following steps:\\n1. Forward pass: make a prediction (compute ^ y(i)=f(x(i))).\\n2. Measure the error (compute cost (^ y(i),y(i))).\\n3. Backward pass: go through each layer in reverse to measure the error contribution from\\neach connection.\\n4. Tweak the connection weights to reduce the error (update Wandb).\\n▶It’s called the backpropagation training algorithm\\n23 / 58', 'Output Unit (1/3)\\n▶Linear units in neurons of the output layer.\\n▶Output function: ^ yj=w⊺\\njh+bj.\\n▶Cost function: minimizing the mean squared error.\\n24 / 58', 'Output Unit (2/3)\\n▶Sigmoid units in neurons of the output layer (binomial classification).\\n▶Output function: ^ yj=σ(w⊺\\njh+bj).\\n▶Cost function: minimizing the cross-entropy.\\n25 / 58', 'Output Unit (3/3)\\n▶Softmax units in neurons of the output layer (multinomial classification).\\n▶Output function: ^ yj=softmax (w⊺\\njh+bj).\\n▶Cost function: minimizing the cross-entropy.\\n26 / 58', 'Hidden Units\\n▶In order for the backpropagation algorithm to work properly, we need to replace the\\nstep function with other activation functions. Why?\\n▶Alternative activation functions:\\n1. Logistic function (sigmoid): σ(z) =1\\n1+e−z\\n2. Hyperbolic tangent function: tanh (z) =2σ(2z)−1\\n3. Rectified linear units (ReLUs): ReLU (z) = max( 0,z)\\n27 / 58', 'Feedforward Network in TensorFlow\\n28 / 58', 'Feedforward Network in TensorFlow\\nn_output = 3\\nn_hidden = 4\\nn_features = 2\\nmodel = keras.models.Sequential()\\nmodel.add(keras.layers.Dense(n_hidden, input_shape=(n_features,), activation=\"relu\"))\\nmodel.add(keras.layers.Dense(n_output, activation=\"softmax\"))\\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\\nmodel.fit(X_train, y_train, epochs=30)\\n29 / 58', 'Dive into Backpropagation Algorithm\\n30 / 58', 'Chain Rule of Calculus (1/2)\\n▶Assume x∈R, and two functions fandg, and also assume y=g(x) and z=\\nf(y) =f(g(x)).\\n▶The chain rule of calculus is used to compute the derivatives of functions, e.g., z,\\nformed by composing other functions, e.g., g.\\n▶Then the chain rule states thatdz\\ndx=dz\\ndydy\\ndx\\n▶Example:\\nz=f(y) =5y4andy=g(x) =x3+7\\ndz\\ndx=dz\\ndydy\\ndx\\ndz\\ndy=20y3anddy\\ndx=3x2\\ndz\\ndx=20y3×3x2=20(x3+7)×3x2\\n31 / 58', 'Chain Rule of Calculus (2/2)\\n▶Two paths chain rule.\\nz=f(y1,y2) where y1=g(x) and y2=h(x)\\n∂z\\n∂x=∂z\\n∂y1∂y1\\n∂x+∂z\\n∂y2∂y2\\n∂x\\n32 / 58', 'Backpropagation\\n▶Backpropagation training algorithm for MLPs\\n▶The algorithm repeats the following steps:\\n1. Forward pass\\n2. Backward pass\\n33 / 58', 'Backpropagation - Forward Pass\\n▶Calculates outputs given input patterns.\\n▶For each training instance\\n•Feeds it to the network and computes the output of every neuron in each consecutive\\nlayer.\\n•Measures the network’s output error (i.e., the difference between the true and the\\npredicted output of the network)\\n•Computes how much each neuron in the last hidden layer contributed to each output\\nneuron’s error.\\n34 / 58', 'Backpropagation - Backward Pass\\n▶Updates weights by calculating gradients.\\n▶Measures how much of these error contributions came from each neuron in the\\nprevious hidden layer\\n•Proceeds until the algorithm reaches the input layer.\\n▶The last step is the gradient descent step on all the connection weights in the network,\\nusing the error gradients measured earlier.\\n35 / 58', 'Backpropagation Example\\n▶Two inputs, two hidden, and two output neurons.\\n▶Bias in hidden and output neurons.\\n▶Logistic activation in all the neurons.\\n▶Squared error function as the cost function.\\n36 / 58', 'Backpropagation - Forward Pass (1/3)\\n▶Compute the output of the hidden layer\\nnet h1=w1x1+w2x2+b1=0.15×0.05+0.2×0.1+0.35=0.3775\\nout h1=1\\n1+eneth1=1\\n1+e0.3775=0.59327\\nout h2=0.59688\\n37 / 58', 'Backpropagation - Forward Pass (2/3)\\n▶Compute the output of the output layer\\nnet o1=w5out h1+w6out h2+b2=0.4×0.59327 +0.45×0.59688 +0.6=1.1059\\nout o1=1\\n1+eneto1=1\\n1+e1.1059=0.75136\\nout o2=0.77292\\n38 / 58', 'Backpropagation - Forward Pass (3/3)\\n▶Calculate the error for each output\\nEo1=1\\n2(target o1−output o1)2=1\\n2(0.01−0.75136 )2=0.27481\\nEo2=0.02356\\nEtotal =X1\\n2(target −output )2=Eo1+Eo2=0.27481 +0.02356 =0.29837\\n39 / 58', 'Backpropagation - Backward Pass - Output Layer (1/6)\\n▶Consider w5\\n▶We want to know how much a change in w5affects the total error (∂Etotal\\n∂w5)\\n▶Applying the chain rule\\n∂Etotal\\n∂w5=∂Etotal\\n∂out o1×∂out o1\\n∂net o1×∂net o1\\n∂w5\\n40 / 58', 'Backpropagation - Backward Pass - Output Layer (2/6)\\n▶First, how much does the total error change with respect to the output? (∂Etotal\\n∂out o1)\\n∂Etotal\\n∂w5=∂Etotal\\n∂out o1×∂out o1\\n∂net o1×∂net o1\\n∂w5\\nEtotal =1\\n2(target o1−out o1)2+1\\n2(target o2−out o2)2\\n∂Etotal\\n∂out o1=−21\\n2(target o1−out o1) =−(0.01−0.75136 ) =0.74136\\n41 / 58', 'Backpropagation - Backward Pass - Output Layer (3/6)\\n▶Next, how much does the out o1change with respect to its total input net o1?\\n(∂out o1\\n∂net o1)\\n∂Etotal\\n∂w5=∂Etotal\\n∂out o1×∂out o1\\n∂net o1×∂net o1\\n∂w5\\nout o1=1\\n1+e−neto1\\n∂out o1\\n∂net o1=out o1(1−out o1) =0.75136 (1−0.75136 ) =0.18681\\n42 / 58', 'Backpropagation - Backward Pass - Output Layer (4/6)\\n▶Finally, how much does the total net o1change with respect to w5? (∂net o1\\n∂w5)\\n∂Etotal\\n∂w5=∂Etotal\\n∂out o1×∂out o1\\n∂net o1×∂net o1\\n∂w5\\nnet o1=w5×out h1+w6×out h2+b2\\n∂net o1\\n∂w5=out h1=0.59327\\n43 / 58', 'Backpropagation - Backward Pass - Output Layer (5/6)\\n▶Putting it all together:\\n∂Etotal\\n∂w5=∂Etotal\\n∂out o1×∂out o1\\n∂net o1×∂net o1\\n∂w5\\n∂Etotal\\n∂w5=0.74136 ×0.18681 ×0.59327 =0.08216\\n44 / 58', 'Backpropagation - Backward Pass - Output Layer (6/6)\\n▶To decrease the error, we subtract this value from the current weight.\\n▶We assume that the learning rate is η=0.5.\\nw(next )\\n5 =w5−η×∂Etotal\\n∂w5=0.4−0.5×0.08216 =0.35891\\nw(next )\\n6 =0.40866\\nw(next )\\n7 =0.5113\\nw(next )\\n8 =0.56137\\n45 / 58', 'Backpropagation - Backward Pass - Hidden Layer (1/8)\\n▶Continue the backwards pass by calculating new values for w1,w2,w3, and w4.\\n▶Forw1we have:\\n∂Etotal\\n∂w1=∂Etotal\\n∂out h1×∂out h1\\n∂net h1×∂net h1\\n∂w1\\n46 / 58', 'Backpropagation - Backward Pass - Hidden Layer (2/8)\\n▶Here, the output of each hidden layer neuron contributes to the output of multiple\\noutput neurons.\\n▶E.g., out h1affects both out o1andout o2, so∂Etotal\\n∂out h1needs to take into consideration\\nits effect on the both output neurons.\\n∂Etotal\\n∂w1=∂Etotal\\n∂out h1×∂out h1\\n∂net h1×∂net h1\\n∂w1\\n∂Etotal\\n∂out h1=∂Eo1\\n∂out h1+∂Eo2\\n∂out h1\\n47 / 58', 'Backpropagation - Backward Pass - Hidden Layer (3/8)\\n▶Starting with∂Eo1\\n∂out h1\\n∂Etotal\\n∂out h1=∂Eo1\\n∂out h1+∂Eo2\\n∂out h1\\n∂Eo1\\n∂out h1=∂Eo1\\n∂out o1×∂out o1\\n∂net o1×∂net o1\\n∂out h1\\n∂Eo1\\n∂out o1=0.74136 ,∂out o1\\n∂net o1=0.18681\\nnet o1=w5×out h1+w6×out h2+b2\\n∂net o1\\n∂out h1=w5=0.40\\n48 / 58', 'Backpropagation - Backward Pass - Hidden Layer (4/8)\\n▶Plugging them together.\\n∂Eo1\\n∂out h1=∂Eo1\\n∂out o1×∂out o1\\n∂net o1×∂net o1\\n∂out h1=0.74136 ×0.18681 ×0.40=0.0554\\n∂Eo2\\n∂out h1=−0.01905\\n∂Etotal\\n∂out h1=∂Eo1\\n∂out h1+∂Eo2\\n∂out h1=0.0554 +−0.01905 =0.03635\\n49 / 58', 'Backpropagation - Backward Pass - Hidden Layer (5/8)\\n▶Now we need to figure out∂out h1\\n∂net h1.\\n∂Etotal\\n∂w1=∂Etotal\\n∂out h1×∂out h1\\n∂net h1×∂net h1\\n∂w1\\nout h1=1\\n1+e−neth1\\n∂out h1\\n∂net h1=out h1(1−out h1) =0.59327 (1−0.59327 ) =0.2413\\n50 / 58', 'Backpropagation - Backward Pass - Hidden Layer (6/8)\\n▶And then∂net h1\\n∂w1.\\n∂Etotal\\n∂w1=∂Etotal\\n∂out h1×∂out h1\\n∂net h1×∂net h1\\n∂w1\\nnet h1=w1x1+w2x2+b1\\n∂net h1\\n∂w1=x1=0.05\\n51 / 58', 'Backpropagation - Backward Pass - Hidden Layer (7/8)\\n▶Putting it all together.\\n∂Etotal\\n∂w1=∂Etotal\\n∂out h1×∂out h1\\n∂net h1×∂net h1\\n∂w1\\n∂Etotal\\n∂w1=0.03635 ×0.2413×0.05=0.00043\\n52 / 58', 'Backpropagation - Backward Pass - Hidden Layer (8/8)\\n▶We can now update w1.\\n▶Repeating this for w2,w3, and w4.\\nw(next )\\n1 =w1−η×∂Etotal\\n∂w1=0.15−0.5×0.00043 =0.14978\\nw(next )\\n2 =0.19956\\nw(next )\\n3 =0.24975\\nw(next )\\n4 =0.2995\\n53 / 58', 'Summary\\n54 / 58', 'Summary\\n▶LTU\\n▶Perceptron\\n▶Perceptron weakness\\n▶MLP and feedforward neural network\\n▶Gradient-based learning\\n▶Backpropagation: forward pass and backward pass\\n▶Output unit: linear, sigmoid, softmax\\n▶Hidden units: sigmoid, tanh, relu\\n55 / 58', 'Reference\\n▶Ian Goodfellow et al., Deep Learning (Ch. 6)\\n▶Aur´ elien G´ eron, Hands-On Machine Learning (Ch. 10)\\n56 / 58', 'Questions?\\n57 / 58']}, '03_ml_classification_regression_trees.pdf': {'text': ['Introduction\\nJim Dowling\\njdowling@kth.se\\n2022-11-6\\nSlides by Amir H. Payberah', '1 / 86', 'The Housing Price Example (1/3)\\n▶Given the dataset of mhouses.\\nLiving area No. of bedrooms Price\\n2104 3 400\\n1600 3 330\\n2400 3 369\\n.........\\n▶Predict the prices of other houses, as a function of the size of living area and number\\nof bedrooms?\\n2 / 86', 'The Housing Price Example (2/3)\\nLiving area No. of bedrooms Price\\n2104 3 400\\n1600 3 330\\n2400 3 369\\n.........\\nx(1)=\\x142104\\n3\\x15\\ny(1)=400 x(2)=\\x141600\\n3\\x15\\ny(2)=330 x(3)=\\x142400\\n3\\x15\\ny(3)=369\\nX=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0x(1)⊺\\nx(2)⊺\\nx(3)⊺\\n...\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f02104 3\\n1600 3\\n2400 3\\n......\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fby=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0400\\n330\\n369\\n...\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n▶x(i)∈R2:x(i)\\n1is the living area, and x(i)\\n2is the number of bedrooms of the ith\\nhouse in the training set.\\n3 / 86', 'The Housing Price Example (3/3)\\nLiving area No. of bedrooms Price\\n2104 3 400\\n1600 3 330\\n2400 3 369\\n.........\\n▶Predict the prices of other houses ^ yas a function of the size of their living areas x1,\\nand number of bedrooms x2, i.e., ^ y=f(x1,x2)\\n▶E.g., what is ^ y, ifx1=4000 andx2=4?\\n▶As an initial choice: ^ y=fw(x) =w1x1+w2x2\\n4 / 86', 'Linear Regression\\n5 / 86', 'Linear Regression (1/2)\\n▶Our goal: to build a system that takes input x∈Rnand predicts output ^ y∈R.\\n▶In linear regression, the output ^ yis a linear function of the input x.\\n^ y=fw(x) =w1x1+w2x2+···+wnxn\\n^ y=w⊺x\\n•^ y: the predicted value\\n•n: the number of features\\n•xi: the ith feature value\\n•wj: the jth model parameter ( w∈Rn)\\n6 / 86', 'Linear Regression (2/2)\\n▶Linear regression often has one additional parameter, called intercept b:\\n^ y=w⊺x+b\\n▶Instead of adding the bias parameter b, we can augment xwith an extra entry that\\nis always set to 1.\\n^ y=fw(x) =w0x0+w1x1+w2x2+···+wnxn,where x0=1\\n7 / 86', 'Linear Regression - Model Parameters\\n^ y=fw(x) =w0x0+w1x1+w2x2+···+wnxn\\n▶Parameters w∈Rnare values that control the behavior of the model.\\n▶ware a set of weights that determine how each feature affects the prediction.\\n•wi>0: increasing the value of the feature xi, increases the value of our prediction ^ y.\\n•wi<0: increasing the value of the feature xi, decreases the value of our prediction ^ y.\\n•wi=0: the value of the feature xi, has no effect on the prediction ^ y.\\n8 / 86', 'How can you learn Model Parameters w?\\n9 / 86', 'Linear Regression - Cost Function (1/2)\\n▶One reasonable model should make ^ yclose to y, at least for the training dataset.\\n▶Residual: the difference between the dependent variable yand the predicted value ^ y.\\nr(i)=y(i)−^ y(i)\\n10 / 86', 'Linear Regression - Cost Function (2/2)\\n▶Cost function J(w)\\n•For each value of the w, it measures how close the ^ y(i)is to the corresponding y(i).\\n•We can define J(w) as the mean squared error (MSE):\\nJ(w) =MSE(w) =1\\nmmX\\ni(^ y(i)−y(i))2\\n=E[(^ y−y)2] =1\\nm||^y−y||2\\n2\\n11 / 86', 'How can you learn Model Parameters?\\n▶We want to choose wso as to minimize J(w).\\n▶Two approaches to find w:\\n•Normal equation - closed form solution\\n•Gradient descent - iterative optimization\\n12 / 86', 'Normal Equation\\n13 / 86', 'Derivatives and Gradient (1/4)\\n[https://mathequality.wordpress.com/2012/09/26/derivative-dance-gangnam-style/]\\n14 / 86', 'Derivatives and Gradient (2/4)\\n▶The first derivative of f(x), shown as f′(x), shows the slope of the tangent line to\\nthe function at the poa x.\\n▶f(x) =x2⇒f′(x) =2x\\n▶Iff(x) is increasing, then f′(x)>0\\n▶Iff(x) is decreasing, then f′(x)<0\\n▶Iff(x) is at local minimum/maximum,\\nthen f′(x) =0\\n15 / 86', 'Derivatives and Gradient (3/4)\\n▶What if a function has multiple arguments, e.g., f(x1,x2,···,xn)\\n▶Partial derivatives: the derivative with respect to a particular argument.\\n•∂f\\n∂x1, the derivative with respect to x1\\n•∂f\\n∂x2, the derivative with respect to x2\\n▶∂f\\n∂xi: shows how much the function fwill change, if we change xi.\\n▶Gradient: the vector of all partial derivatives for a function f.\\n∇xf(x) =\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0∂f\\n∂x1∂f\\n∂x2...\\n∂f\\n∂xn\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n16 / 86', 'Derivatives and Gradient (4/4)\\n▶What is the gradient of f(x1,x2,x3) =x1−x1x2+x2\\n3?\\n∇xf(x) =\\uf8ee\\n\\uf8ef\\uf8f0∂\\n∂x1(x1−x1x2+x2\\n3)\\n∂\\n∂x2(x1−x1x2+x2\\n3)\\n∂\\n∂x3(x1−x1x2+x2\\n3)\\uf8f9\\n\\uf8fa\\uf8fb=\\uf8ee\\n\\uf8f01−x2\\n−x1\\n2x3\\uf8f9\\n\\uf8fb\\n17 / 86', 'Normal Equation\\n▶To minimize J(w), we can simply solve for where its gradient is 0: ∇wJ(w) =0\\n^ y=w⊺x\\n...\\n...\\n⇒w= (X⊺X)−1X⊺y\\n18 / 86', 'Normal Equation - Example (1/7)\\nLiving area No. of bedrooms Price\\n2104 3 400\\n1600 3 330\\n2400 3 369\\n1416 2 232\\n3000 4 540\\n▶Predict the value of ^ y, when x1=4000 andx2=4.\\n▶We should find w0,w1, and w2in^ y=w0+w1x1+w2x2.\\n▶w= (X⊺X)−1X⊺y.\\n19 / 86', 'Normal Equation - Example (2/7)\\nLiving area No. of bedrooms Price\\n2104 3 400\\n1600 3 330\\n2400 3 369\\n1416 2 232\\n3000 4 540\\nX=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f012104 3\\n11600 3\\n12400 3\\n11416 2\\n13000 4\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fby=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0400\\n330\\n369\\n232\\n540\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n20 / 86', 'Normal Equation - Example (3/7)\\nX⊺X=\\uf8ee\\n\\uf8f01 1 1 1 1\\n2104 1600 2400 1416 3000\\n3 3 3 2 4\\uf8f9\\n\\uf8fb\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f01 2104 3\\n1 1600 3\\n1 2400 3\\n1 1416 2\\n1 3000 4\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb=\\uf8ee\\n\\uf8f05 10520 15\\n10520 23751872 33144\\n15 33144 47\\uf8f9\\n\\uf8fb\\n21 / 86', 'Normal Equation - Example (4/7)\\n(X⊺X)−1=\\uf8ee\\n\\uf8f04.90366455e +00 7 .48766737e −04−2.09302326e +00\\n7.48766737e −04 2 .75281889e −06−2.18023256e −03\\n−2.09302326e +00−2.18023256e −03 2 .22674419e +00\\uf8f9\\n\\uf8fb\\n22 / 86', 'Normal Equation - Example (5/7)\\nX⊺y=\\uf8ee\\n\\uf8f01 1 1 1 1\\n2104 1600 2400 1416 3000\\n3 3 3 2 4\\uf8f9\\n\\uf8fb\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0400\\n330\\n369\\n232\\n540\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb=\\uf8ee\\n\\uf8f01871\\n4203712\\n5921\\uf8f9\\n\\uf8fb\\n23 / 86', 'Normal Equation - Example (6/7)\\nw= (X⊺X)−1X⊺y=\\uf8ee\\n\\uf8f04.90366455e +00 7 .48766737e −04−2.09302326e +00\\n7.48766737e −04 2 .75281889e −06−2.18023256e −03\\n−2.09302326e +00−2.18023256e −03 2 .22674419e +00\\uf8f9\\n\\uf8fb\\uf8ee\\n\\uf8f01871\\n4203712\\n5921\\uf8f9\\n\\uf8fb\\n=\\uf8ee\\n\\uf8f0−7.04346018 e+ 01\\n6.38433756 e−02\\n1.03436047 e+ 02\\uf8f9\\n\\uf8fb\\n24 / 86', 'Normal Equation - Example (7/7)\\n▶Predict the value of y, when x1=4000 andx2=4.\\n^ y=−7.04346018e +01+6.38433756e −02×4000 +1.03436047e +02×4≈599\\n25 / 86', 'Gradient Descent\\n26 / 86', 'Gradient Descent (1/2)\\n▶Gradient descent is a generic optimization algorithm capable of finding optimal so-\\nlutions to a wide range of problems.\\n▶The idea: to tweak parameters iteratively in order to minimize a cost function.\\n27 / 86', 'Gradient Descent (2/2)\\n▶Suppose you are lost in the mountains in a dense fog.\\n▶You can only feel the slope of the ground below your feet.\\n▶A strategy to get to the bottom of the valley is to go downhill in the direction of the\\nsteepest slope.\\n28 / 86', 'Gradient Descent - Iterative Optimization Algorithm\\n▶Choose a starting point, e.g., filling wwith random values.\\n▶If the stopping criterion is true return the current solution, otherwise continue.\\n▶Find a descent direction, a direction in which the function value decreases near the\\ncurrent point.\\n▶Determine the step size, the length of a step in the given direction.\\n29 / 86', 'Gradient Descent - Key Points\\n▶Stopping criterion\\n▶Descent direction\\n▶Step size (learning rate)\\n30 / 86', 'Gradient Descent - Stopping Criterion\\n▶The cost function minimum property: the gradient has to be zero.\\n∇wJ(w) =0\\n31 / 86', 'Gradient Descent - Descent Direction (1/2)\\n▶Direction in which the function value decreases near the current point.\\n▶Find the direction of descent (slope).\\n▶Example:\\nJ(w) =w2\\n∂J(w)\\n∂w=2w=−2atw=−1\\n32 / 86', 'Gradient Descent - Descent Direction (2/2)\\n▶Follow the opposite direction of the slope.\\n33 / 86', 'Gradient Descent - Learning Rate\\n▶Learning rate: the length of steps.\\n▶If it is too small: many iterations to converge.\\n▶If it is too high: the algorithm might diverge.\\n34 / 86', 'Gradient Descent - How to Learn Model Parameters w?\\n▶Goal: find wthat minimizes J(w) =Pm\\ni=1(w⊺x(i)−y(i))2.\\n▶Start at a random point, and repeat the following steps, until the stopping criterion\\nis satisfied:\\n1. Determine a descent direction∂J(w)\\n∂w\\n2. Choose a step size η\\n3. Update the parameters: w(next )=w−η∂J(w)\\n∂w\\n(should be done for all parameters simultanously)\\n35 / 86', 'Gradient Descent - Different Algorithms\\n▶Batch gradient descent (all samples)\\n▶Stochastic gradient descent (1 sample)\\n▶Mini-batch gradient descent (a mini-batch of samples - e.g., 200 samples)\\n[https://towardsdatascience.com/gradient-descent-algorithm-and-its-variants-10f652806a3]\\n36 / 86', 'Mini-Batch Gradient Descent\\n▶Batch gradient descent: at each step, it computes the gradients based on the full\\ntraining set.\\n▶Stochastic gradient descent: at each step, it computes the gradients based on just\\none instance.\\n▶Mini-batch gradient descent: at each step, it computes the gradients based on small\\nrandom sets of instances called mini-batches.\\n37 / 86', 'Comparison of Algorithms for Linear Regression\\n38 / 86', 'Generalization\\n39 / 86', 'Training Data and Test Data\\n▶Split data into a training set and a test set.\\n▶Use training set when training a machine learning model.\\n•Compute training error on the training set.\\n•Try to reduce this training error.\\n▶Use test set to measure the accuracy of the model.\\n•Test error is the error when you run the trained model on test data (new data).\\n40 / 86', 'Generalization\\n▶Generalization: make a model that performs well on test data.\\n•Have a small test error.\\n▶Challenges\\n1. Make the training error small.\\n2. Make the gap between training and test error small.\\n41 / 86', 'More About The Test Error\\n▶The test error is defined as the expected value of the error on test set.\\nMSE=1\\nkkX\\ni(^ y(i)−y(i))2,k: the num. of instances in the test set\\n=E[(^ y−y)2]\\n▶A model’s test error can be expressed as the sum of bias and variance.\\nE[(^ y−y)2] =Bias [^ y,y]2+Var[^ y] +ε2\\n42 / 86', '43 / 86', 'Bias and Underfitting\\n▶Bias: the expected deviation from the true value of the function.\\nBias [^ y,y] =E[^ y]−y\\n▶A high-bias model is most likely to underfit the training data.\\n•High error value on the training set.\\n▶Underfitting happens when the model is too simple to learn the underlying structure\\nof the data.\\n44 / 86', 'Variance and Overfitting\\n▶Variance: how much a model changes if you train it on a different training set.\\nVar[^ y] =E[(^ y−E[^ y])2]\\n▶A high-variance model is most likely to overfit the training data.\\n•The gap between the training error and test error is too large.\\n▶Overfitting happens when the model is too complex relative to the amount and\\nnoisiness of the training data.\\n45 / 86', 'The Bias/Variance Tradeoff (1/2)\\n▶Assume a model with two parameters w0(intercept) and w1(slope): ^ y=w0+w1x\\n▶They give the learning algorithm two degrees of freedom.\\n▶We tweak both the w0andw1to adapt the model to the training data.\\n▶If we forced w0=0, the algorithm would have only one degree of freedom and would\\nhave a much harder time fitting the data properly.\\n46 / 86', 'The Bias/Variance Tradeoff (2/2)\\n▶Increasing degrees of freedom will typically increase its variance and reduce its bias.\\n▶Decreasing degrees of freedom increases its bias and reduces its variance.\\n▶This is why it is called a tradeoff.\\n[https://ml.berkeley.edu/blog/2017/07/13/tutorial-4]\\n47 / 86', 'Regularization (1/2)\\n▶One way to reduce the risk of overfitting is to have fewer degrees of freedom.\\n▶Regularization is a technique to reduce the risk of overfitting.\\n▶For a linear model, regularization is achieved by constraining the weights of the\\nmodel.\\nJ(w) =MSE(w) +λR(w)\\n48 / 86', 'Regularization (2/2)\\n▶Lasso regression ( l1):R(w) =λPn\\ni=1|wi|is added to the cost function:\\nJ(w) =MSE(w) +λnX\\ni=1|wi|\\n▶Ridge regression ( l2):R(w) =λPn\\ni=1w2\\niis added to the cost function.\\nJ(w) =MSE(w) +λnX\\ni=1w2\\ni\\n▶ElasticNet: a middle ground between l1 and l2 regularization.\\nJ(w) =MSE(w) +αλnX\\ni=1|wi|+ (1−α)λnX\\ni=1w2\\ni\\n49 / 86', 'Hyperparameters\\n50 / 86', 'Hyperparameters and Validation Sets (1/2)\\n▶Hyperparameters are settings that we can use to control the behavior of a learning\\nalgorithm.\\n▶The values of hyperparameters are not adapted by the learning algorithm itself.\\n•E.g., the αandλvalues for regularization.\\n▶We do not learn the hyperparameter.\\n•It is not appropriate to learn that hyperparameter on the training set.\\n•If learned on the training set, such hyperparameters would always result in overfitting.\\n51 / 86', 'Hyperparameters and Validation Sets (2/2)\\n▶To find hyperparameters, we need a validation set of examples that the training\\nalgorithm does not observe.\\n▶We construct the validation set from the training data (not the test data).\\n▶We split the training data into two disjoint subsets:\\n1. One is used to learn the parameters.\\n2. The other one (the validation set) is used to estimate the test error during or after\\ntraining, allowing for the hyperparameters to be updated accordingly.\\n52 / 86', 'Cross-Validation\\n▶Cross-validation: a technique to avoid wasting too much training data in validation\\nsets.\\n▶The training set is split into complementary subsets.\\n▶Each model is trained against a different combination of these subsets and validated\\nagainst the remaining parts.\\n▶Once the model type and hyperparameters have been selected, a final model is trained\\nusing these hyperparameters on the full training set, and the test error is measured\\non the test set.\\n53 / 86', 'Regression Summary\\n▶Linear regression model ^ y=w⊺x\\n•Learning parameters w\\n•Cost function J(w)\\n•Learn parameters: normal equation, gradient descent (batch, stochastic, mini-batch)\\n▶Generalization\\n•Overfitting vs. underfitting\\n•Bias vs. variance\\n•Regularization: Lasso regression, Ridge regression, ElasticNet\\n▶Hyperparameters and cross-validation\\n54 / 86', 'Classification\\n55 / 86', 'Evaluation of Classification Models (1/3)\\n▶In a classification problem, there exists a true output yand a model-generated pre-\\ndicted output ^ yfor each data point.\\n▶The results for each instance point can be assigned to one of four categories:\\n•True Positive (TP)\\n•True Negative (TN)\\n•False Positive (FP)\\n•False Negative (FN)\\n56 / 86', 'Evaluation of Classification Models (2/3)\\n▶True Positive (TP): the label yis positive and prediction ^ yis also positive.\\n▶True Negative (TN): the label yis negative and prediction ^ yis also negative.\\n57 / 86', 'Evaluation of Classification Models (3/3)\\n▶False Positive (FP): the label yis negative but prediction ^ yis positive (type I error).\\n▶False Negative (FN): the label yis positive but prediction ^ yis negative (type II error).\\n58 / 86', 'Why Pure Accuracy Is Not A Good Metric?\\n▶Accuracy: how close the prediction is to the true value.\\n▶Assume a highly unbalanced dataset\\n▶E.g., a dataset where 95% of the data points are not fraud and 5% of the data points\\nare fraud.\\n▶A a naive classifier that predicts not fraud, regardless of input, will be 95% accurate.\\n▶For this reason, metrics like precision and recall are typically used.\\n59 / 86', 'Precision\\n▶It is the accuracy of the positive predictions.\\nPrecision =p(y=1|^ y=1) =TP\\nTP+FP\\n60 / 86', 'Recall\\n▶Is is the ratio of positive instances that are correctly detected by the classifier.\\n▶Also called sensitivity or true positive rate (TPR).\\nRecall =p(^ y=1|y=1) =TP\\nTP+FN\\n61 / 86', 'F1 Score\\n▶F1 score: combine precision and recall into a single metric.\\n▶TheF1 score is the harmonic mean of precision and recall.\\n▶Whereas the regular mean treats all values equally, the harmonic mean gives much\\nmore weight to low values.\\n▶F1 only gets high score if both recall and precision are high.\\nF1=2\\n1\\nprecision+1\\nrecall\\n62 / 86', 'Confusion Matrix\\n▶The confusion matrix is K×K, where Kis the number of classes.\\n▶It shows the number of correct and incorrect predictions made by the classification\\nmodel compared to the actual outcomes in the data.\\n63 / 86', 'Confusion Matrix - Example\\nTP=3,TN=5,FP=1,FN=2\\nPrecision =TP\\nTP+FP=3\\n3+1=3\\n4\\nRecall (TPR) =TP\\nTP+FN=3\\n3+2=3\\n5\\nFPR=FP\\nTN+FP=1\\n5+1=5\\n6\\n64 / 86', 'Precision-Recall Tradeoff\\n▶Precision-recall tradeoff: increasing precision reduces recall, and vice versa.\\n▶Assume a classifier that detects number 5 from the other digits.\\n•If an instance score is greater than a threshold, it assigns it to the positive class,\\notherwise to the negative class.\\n▶Raising the threshold (move it to the arrow on the right), the false positive (the 6)\\nbecomes a true negative, thereby increasing precision.\\n▶Lowering the threshold increases recall and reduces precision.\\n65 / 86', 'The ROC Curve (1/2)\\n▶True positive rate (TPR) (recall): p(^ y=1|y=1)\\n▶False positive rate (FPR): p(^ y=1|y=0)\\n▶The receiver operating characteristic (ROC) curves summarize the trade-off between\\nthe TPR and FPR for a model using different probability thresholds.\\n66 / 86', 'The ROC Curve (2/2)\\n▶Here is a tradeoff: the higher the TPR, the more FPR the classifier produces.\\n▶The dotted line represents the ROC curve of a purely random classifier.\\n▶A good classifier moves toward the top-left corner.\\n▶Area under the curve (AUC)\\n67 / 86', 'Decision Trees\\n68 / 86', 'Buying Computer Example (1/3)\\n▶Given the dataset of mpeople.\\nid age income student credit rating buys computer\\n1 youth high no fair no\\n2 youth high no excellent no\\n3middleage high no fair yes\\n4 senior medium no fair yes\\n5 senior low yes fair yes\\n..................\\n▶Predict if a new person buys a computer?\\n▶Given an instance x(i), e.g., x(i)\\n1=senior ,x(i)\\n2=medium ,x(i)\\n3=no, and x(i)\\n4=\\nfair , then y(i)=?\\n69 / 86', 'Buying Computer Example (2/3)\\nid age income student credit rating buys computer\\n1 youth high no fair no\\n2 youth high no excellent no\\n3 middleage high no fair yes\\n4 senior medium no fair yes\\n5 senior low yes fair yes\\n..................\\n70 / 86', 'Buying Computer Example (3/3)\\n▶Given an input instance x(i), for which the class label y(i)is unknown.\\n▶The attribute values of the input (e.g., ageorincome ) are tested.\\n▶A path is traced from the root to a leaf node, which holds the class prediction for\\nthat input.\\n▶E.g., input x(i)with x(i)\\n1=senior ,x(i)\\n2=medium ,x(i)\\n3=no, and x(i)\\n4=fair .\\n71 / 86', 'Decision Trees\\n72 / 86', 'Decision Tree\\n▶A decision tree is a flowchart-like tree structure.\\n•The topmost node: represents the root\\n•Each internal node: denotes a test on an attribute\\n•Each branch: represents an outcome of the test\\n•Each leaf: holds a class label\\n73 / 86', 'Training Algorithm (1/2)\\n▶Decision trees are constructed in a top-down recursive divide-and-conquer manner.\\n▶The algorithm is called with the following parameters.\\n•Data partition D: initially the complete set of training data and labels D= (X,y).\\n•Feature list: list of features {x(i)\\n1,···,x(i)\\nn}of each data instance x(i).\\n•Feature selection method: determines the splitting criterion.\\n74 / 86', 'Training Algorithm (2/2)\\n▶1. The tree starts as a single node, N, representing the training data instances D.\\n▶2. If all instances xinDare all of the same class, then node Nbecomes a leaf.\\n▶3. The algorithm calls feature selection method to determine the splitting criterion.\\n•Indicates (i) the splitting feature xk, and (ii) a split-point or a splitting subset.\\n•The instances in Dare partitioned accordingly.\\n▶4. The algorithm repeats the same process recursively to form a decision tree.\\n75 / 86', 'Training Algorithm - Termination Conditions\\n▶The training algorithm stops only when any one of the following conditions is true.\\n▶1. All the instances in partition Dat a node Nbelong to the same class.\\n•It is labeled with that class.\\n▶2. No remaining features on which the instances may be further partitioned.\\n▶3. There are no instances for a given branch, that is, a partition Djis empty.\\n▶In conditions 2 and 3:\\n•Convert node Ninto a leaf.\\n•Label it either with the most common class in D.\\n•Or, the class distribution of the node tuples may be stored.\\n76 / 86', 'Ensembles\\n77 / 86', 'Wisdom of the Crowd\\n▶Ask a complex question to thousands of random people, then aggregate their answers.\\n▶In many cases, this aggregated answer is better than an expert’s answer.\\n▶This is called the wisdom of the crowd.\\n▶Similarly, the aggregated estimations of a group of estimators (e.g., classifiers or\\nregressors), often gets better estimations than with the best individual estimator.\\n▶A group of estimators is an ensemble, and this technique is called Ensemble Learning.\\n78 / 86', 'Ensemble Learning\\n▶Two main categories of ensemble learning algorithms.\\n▶Bagging\\n•Use the same training algorithm for every estimator, but to train them on different\\nrandom subsets of the training set.\\n•E.g., random forest\\n▶Boosting\\n•Train estimators sequentially, each trying to correct its predecessor.\\n•E.g., adaboost and gradient boosting\\n79 / 86', 'Random Forest\\n▶Random forest builds multiple decision trees that are most of the time trained with\\nthe bagging method.\\n▶It, then, merges the trees together to get a more accurate and stable prediction.\\n80 / 86', 'AdaBoost\\n▶AdaBoost: train a new estimator by paying more attention to the training instances\\nthat the predecessor underfitted.\\n▶Each estimator is trained on a random subset of the total training set.\\n▶AdaBoost assigns a weight to each training instance, which determines the probability\\nthat each instance should appear in the training set.\\n81 / 86', 'Gradient Boosting (1/3)\\n▶Just like AdaBoost, Gradient Boosting works by sequentially adding estimators to an\\nensemble, each one correcting its predecessor.\\n▶However, instead of tweaking the instance weights at every iteration, this method\\ntries to fit the new estimator to the residual errors made by the previous estimator.\\n82 / 86', 'Gradient Boosting (2/3)\\n▶Let’s go through a regression example using Gradient Boosted Regression Trees.\\n▶Fit the first estimator on the training set.\\ntree_reg1 = DecisionTreeRegressor(max_depth=2)\\ntree_reg1.fit(X, y)\\n▶Now train the second estimator on the residual errors made by the first estimator.\\ny2 = y - tree_reg1.predict(X)\\ntree_reg2 = DecisionTreeRegressor(max_depth=2)\\ntree_reg2.fit(X, y2)\\n83 / 86', 'Gradient Boosting (3/3)\\n▶Then we train the third estimator on the residual errors made by the second estimator.\\ny3 = y2 - tree_reg2.predict(X)\\ntree_reg3 = DecisionTreeRegressor(max_depth=2)\\ntree_reg3.fit(X, y3)\\n▶Now we have an ensemble containing three trees.\\n▶It can make predictions on a new instance simply by adding up the predictions of all\\nthe trees.\\ny_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\\n84 / 86', 'Summary\\n▶Decision tree\\n•Top-down training algorithm\\n•Termination condition\\n▶Ensemble models\\n•Bagging: random forest\\n•Boosting: AdaBoost, Gradient Boosting\\n85 / 86', 'Reference\\n▶Ian Goodfellow et al., Deep Learning (Ch. 4, 5)\\n▶Aur´ elien G´ eron, Hands-On Machine Learning (Ch. 2, 3, 4)\\n86 / 86', 'Questions?\\n87 / 86']}, '02_serverless_ml.pdf': {'text': ['Serverless Machine Learning\\nJim Dowling\\njdowling@kth.se\\n2022-11-04', 'Enterprise AI Value Chain\\n1 / 54', 'Modern Enterprise Data and ML Infrastructure\\n2 / 54', 'Monolithic ML Pipeline\\n3 / 54', 'Problems with Monolithic ML Pipelines\\n▶They are often not modular - their components are not modular and cannot be\\nindependently scaled or deployed on different hardware (e.g., CPUs for feature engi-\\nneering, GPUs for model training).\\n▶They are difficult to test - production software needs automated tests to ensure\\nfeatures and models are of high quality.\\n▶They tightly couple the execution of feature engineering, model training, and infer-\\nence steps - running them in the same pipeline program at the same time.\\n▶They do not promote reuse of features/models/code. The code for computing fea-\\ntures (feature logic) cannot be easily disentangled from its pipeline jungle.\\n4 / 54', 'Modularity enables more Robust and Scalable Systems\\nModular water pipes in a Google Datacenter. Instead of one giant water pipe (our\\nmonolithic notebook), separate water pipes reduce the blast radius if one fails. Color\\ncoding makes it easier to debug problems in a damaged water pipe.\\n5 / 54', 'Pipelines as Modular Programs\\n▶Modularity involves structuring your code such that its functionality is separated into\\nindependent classes and/or functions that can be more easily reused and tested.\\n▶Modules should be placed in accessible classes or functions, keeping them small and\\neasy to understand and document.\\n▶Modules enable code to be more easily reused in different pipelines.\\n▶Modules enable code to be more easily independently tested, enabling the easier and\\nearlier discovery of bugs.\\n6 / 54', 'Supervised ML Pipeline Stages\\ntrain (features ,labels )−>model\\nmodel (features )−>predictions\\n7 / 54', 'ML Pipeline Stages in a Serverless Machine Learning System\\n8 / 54', 'ML Pipeline Stages - Data Sources\\n9 / 54', 'Connect to Data Sources and Read Raw Data\\n▶Discover data sources, securely connect to heterogeneous data sources\\n▶Manage dependencies such as connectors and drivers\\n▶Manage connection information securely: network endpoint, database/table names,\\nauthentication credentials such as API keys or credentials (username/password)\\n10 / 54', 'Heterogeneous Data Sources\\n11 / 54', 'File Formats for different Data Sources\\n12 / 54', 'ML Pipeline Stages - Feature Pipelines\\n13 / 54', 'Feature Pipelines\\n14 / 54', 'Feature Pipelines\\n▶A feature pipeline is a program that orchestrates the execution of feature engineering\\nsteps on input data to create feature values.\\nExamples of feature engineering steps:\\n▶Clean, validate, data\\n▶Data de-duplication, pseudononymization, data wrangling\\n▶Feature extraction, aggregations, dimensionality reduction, feature binning, feature\\ncrosses\\n15 / 54', 'Tabular Data\\n16 / 54', 'Tabular Data as Features, Labels, Entity (or Primary) Keys,\\nEvent Time\\n17 / 54', 'Tabular Data in Pandas\\n18 / 54', 'Exploratory Data Analysis in Pandas\\n19 / 54', 'Aggregations in Pandas\\n20 / 54', 'Rolling Windows in Pandas\\n21 / 54', 'Feature binning\\n22 / 54', 'Feature Crosses\\n▶A feature cross is a synthetic feature formed by multiplying (crossing) two or more\\nfeatures. By multiplying features together, you encode nonlinearity in the feature\\nspace.\\n▶For example, imagine we are looking for credit card fraud activity within a geographic\\nregion (e.g., a city district), how would we capture that as a feature?\\n▶We could cross to a geographic area (binned latitude and binned longitude - a grid\\nidentifying a city district) with the level of credit card activity within that geographic\\narea.\\n23 / 54', 'Embeddings as Features\\n▶An embedding is a lower dimension representation of a sparse input that retains some\\nof the semantics of the input.\\n▶An embedding store (vector database) stores semantically similar inputs close to-\\ngether in the embedding space. You can implement “similarity search” by finding\\nembeddings close in embedding space. You can even apply arithmetic on embeddings\\nto discover semantic relationships.\\n24 / 54', 'ML Pipeline Stages - Feature Store\\n25 / 54', 'Store Features\\nThere are two general ways people manage features and labels for both training and\\nserving:\\n▶(1) Compute features on-demand as part of the model training or batch inference\\npipeline.\\n▶(2) Use a feature store to store the features so that they can be reused across\\ndifferent models for both training and inference. For online models that require\\nfeatures with either historical or contextual information , feature stores are typically\\nused.\\n26 / 54', 'ML Pipeline Stages - Training Pipelines\\n27 / 54', 'Feature Types\\nReference: https://www.hopsworks.ai/post/feature-types-for-machine-learning\\n28 / 54', 'Feature Types Taxonomy\\n29 / 54', 'Model Training Pipelines\\n30 / 54', 'Model-Dependent Transformations\\nReference: https://developers.google.com/machine-learning/data-\\nprep/transform/introduction\\n31 / 54', 'Transformations in Pandas\\n32 / 54', 'Different types of Transformations\\n33 / 54', 'Model Training with Train and Test Sets\\n34 / 54', 'Model Training with Train and Test Sets in Scikti-Learn\\n35 / 54', 'Model Training is an Iterative Process\\n36 / 54', 'Model-Centric Iteration to Improve Model Performance\\nPossible steps to improve your model performance:\\n▶Try out a different supervised ML learning algorithm (e.g., random forest, feedforward\\ndeep neural network, Gradient-boosted decision tree)\\n▶Try out new combinations of hyperparameters (e.g., number of training epochs, the\\nlearning rate, number of layers in a deep neural network, adjust regularizations such\\nas Dropout or BatchNorm)\\n▶Evaluate your model on a validation set (keeping a separate holdout test set for final\\nmodel performance evaluation)\\n37 / 54', 'Data-Centric Iteration to Improve Model Performance\\nSteps to improve your model\\n▶Add or remove features to or from your model (feature selection)\\n▶Add more training data\\n▶Remove poor quality training samples\\n▶Improve the quality of existing training samples (e.g., using Cleanlab or Snorkel)\\n▶Rank the importance of the training samples (Active Learning)\\n38 / 54', 'Train, Validation, and Test Sets\\n▶Random splits of the training data when the data is not time-series data\\n▶Time-series splits of the training data when the data is time-series data\\n39 / 54', 'Model Training is an Iterative Process\\n40 / 54', 'ML Pipeline Stages - Inference Pipelines\\n41 / 54', 'Batch Inference Pipeline\\n42 / 54', 'Online Inference Pipeline\\n43 / 54', 'Serverless ML with Python\\n▶Write Feature, Training, and Inference Pipelines in Python\\n▶Orchestrate the execution of Pipelines using Serverless Compute Platforms\\n▶Store features and models in a serverless feature/model store\\n▶Run a User Interface (UI), written in Python, on serverless infrastructure\\n44 / 54', 'Serverless Compute Platforms\\n45 / 54', 'Serverless Feature Stores and Model Registry/Serving\\nFeature Stores\\n▶Hopsworks\\nModel Registry and Serving\\n▶Hopsworks\\n▶AWS Sagemaker\\n▶Databricks\\n▶Google Vertex\\n46 / 54', 'Serverless User Interfaces\\n▶Hugging Faces Spaces\\n▶Streamlit Cloud\\n47 / 54', 'Iris Flower Dataset\\nhttps://github.com/ID2223KTH/id2223kth.github.io/tree/master/src/serverless-ml-\\nintro\\n▶4 input features: sepal length, sepal width, petal length, petal width\\n▶label (target): Iris Flower Type (one of Setosa, Versicolor, Virginica)\\n▶Only 150 samples in the dataset\\n48 / 54', 'Serverless Iris with Modal, Hopsworks, and Hugging Face\\n49 / 54', 'Iris Flowers: Feature Pipeline with Modal and Hopsworks\\nimport os\\nimport modal\\nstub = modal.Stub()\\nhopsworks_image = modal.Image.debian_slim().pip_install([\"hopsworks\"])\\n@stub.function(image=hopsworks_image, schedule=modal.Period(days=1), \\\\\\nsecret=modal.Secret.from_name(\"jim-hopsworks-ai\"))\\ndef f():\\nimport hopsworks\\nimport pandas as pd\\nproject = hopsworks.login()\\nfs = project.get_feature_store()\\niris_df = pd.read_csv(\"https://repo.hops.works/master/hopsworks-tutorials/data/iris.csv\")\\niris_fg = fs.get_or_create_feature_group( name=\"iris_modal\", version=1,\\nprimary_key=[\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\"],\\ndescription=\"Iris flower dataset\")\\niris_fg.insert(iris_df)\\nif __name__ == \"__main__\":\\nwith stub.run():\\nf()\\n50 / 54', 'Training Pipeline with Modal and Hopsworks\\n@stub.function(image=hopsworks_image, schedule=modal.Period(days=1),\\\\\\nsecret=modal.Secret.from_name(\"jim-hopsworks-ai\"))\\ndef f():\\n# lots of imports\\nproject = hopsworks.login()\\nfs = project.get_feature_store()\\ntry:\\nfeature_view = fs.get_feature_view(name=\"iris_modal\", version=1)\\nexcept:\\niris_fg = fs.get_feature_group(name=\"iris_modal\", version=1)\\nquery = iris_fg.select_all()\\nfeature_view = fs.create_feature_view(name=\"iris_modal\",\\nversion=1,\\ndescription=\"Read from Iris flower dataset\",\\nlabels=[\"variety\"],\\nquery=query)\\nX_train, X_test, y_train, y_test = feature_view.train_test_split(0.2)\\nmodel = KNeighborsClassifier(n_neighbors=2)\\nmodel.fit(X_train, y_train.values.ravel())\\n51 / 54', 'Training Pipeline (ctd)\\ny_pred = model.predict(X_test)\\nmetrics = classification_report(y_test, y_pred, output_dict=True)\\nresults = confusion_matrix(y_test, y_pred)\\ndf_cm = pd.DataFrame(results, [’True Setosa’, ’True Versicolor’, ’True Virginica’],\\n[’Pred Setosa’, ’Pred Versicolor’, ’Pred Virginica’])\\ncm = sns.heatmap(df_cm, annot=True)\\nfig = cm.get_figure()\\njoblib.dump(model, \"iris_model/iris_model.pkl\")\\nfig.savefig(\"iris_model/confusion_matrix.png\")\\ninput_schema = Schema(X_train)\\noutput_schema = Schema(y_train)\\nmodel_schema = ModelSchema(input_schema, output_schema)\\nmr = project.get_model_registry()\\niris_model = mr.python.create_model(\\nname=\"iris_modal\",\\nmetrics={\"accuracy\" : metrics[’accuracy’]},\\nmodel_schema=model_schema,\\ndescription=\"Iris Flower Predictor\")\\niris_model.save(\"iris_model\")\\n52 / 54', 'Interactive Inference Pipeline with Hugging Face/Hopsworks\\nmodel = mr.get_model(\"iris_modal\", version=1)\\nmodel_dir = model.download()\\nmodel = joblib.load(model_dir + \"/iris_model.pkl\")\\ndef iris(sepal_length, sepal_width, petal_length, petal_width):\\ninput_list = []\\ninput_list.append(sepal_length)\\ninput_list.append(sepal_width)\\ninput_list.append(petal_length)\\ninput_list.append(petal_width)\\nres = model.predict(np.asarray(input_list).reshape(1, -1))\\nflower_url = \"https://raw.githubusercontent.com/.../assets/\" + res[0] + \".png\"\\nreturn Image.open(requests.get(flower_url, stream=True).raw)\\ndemo = gr.Interface(\\nfn=iris, title=\"Iris Flower Predictive Analytics\", allow_flagging=\"never\",\\ndescription=\"Experiment with sepal/petal lengths/widths to predict which flower it is.\",\\ninputs=[ gr.inputs.Number(default=1.0, label=\"sepal length (cm)\"),\\ngr.inputs.Number(default=1.0, label=\"sepal width (cm)\"),\\ngr.inputs.Number(default=1.0, label=\"petal length (cm)\"),\\ngr.inputs.Number(default=1.0, label=\"petal width (cm)\"),],\\noutputs=gr.Image(type=\"pil\"))\\ndemo.launch()\\n53 / 54', 'Questions?\\nAcknowledgements\\nSome of the images are used with permission from Hopsworks AB.\\n54 / 54']}, '01_introduction.pdf': {'text': ['Introduction\\nJim Dowling\\njdowling@kth.se\\nCourse Assistants: Fabien Schmidt, Javier Ron, Ahmad Al-Shishtawy\\n2023-10-30\\nSlides by Amir H. Payberah and Jim Dowling', 'Course Information\\n1 / 79', 'Course Objective\\n▶This course has a system-based focus.\\n▶Learn the theory of machine learning and deep learning.\\n▶Learn the practical aspects of building machine learning and deep learning algorithms\\nusing data parallel programming platforms, such as TensorFlow.\\n2 / 79', '————————————————\\n3 / 79', 'Topics Covered in the Course\\n▶Part 1: large scale deep learning\\n•TensorFlow\\n•Deep Neural Networks (DNN)\\n•Different DNN architectures, e.g., CNNs, RNNs, Autoencoders, GAN\\n•Distributed learning\\n▶Part 2: large scale machine learning\\n•Serverless Machine Learning\\n•MLOps (machine learning operations)\\n•Feature Stores for Machine Learning\\n•Distributed Training of DNNs\\n3 / 79', 'Intended Learning Outcomes (ILOs)\\n▶ILO1: explain the principles of ML/DL algorithms and apply their techniques to solve\\nproblems.\\n▶ILO2: demonstrate an ability to design DNN architectures and explain challenges to\\nscaling their training and inference with increasing compute and data.\\n▶ILO3: explain the principles of scaling machine learning systems.\\n▶ILO4: implement scalable ML/DL systems.\\n4 / 79', 'The Course Assessment\\n▶Task1: the Examination on 8th January 2023(A-F)\\n▶Task2: the lab assignments (A-F)\\n▶Task3: the final project (P/F)\\n5 / 79', 'How Each ILO is Assessed?\\nTask1 Task2 Task3\\nILO1 x\\nILO2 x x x\\nILO3 x x\\nILO4 x x\\n6 / 79', 'Task1: The Exam (A-F)\\n▶Questions about the lectures and the labs.\\n▶Some sample questions for the exam will be distributed in mid December.\\n▶The examination is graded (A-F).\\n7 / 79', 'Task2: The Lab Assignments (A-F)\\n▶Two lab assignments: source code and oral presentation.\\n▶E: source code\\n▶D: source code + half questions (basic)\\n▶C: source code + all questions (basic)\\n▶B: source code + half questions (basic and advanced)\\n▶A: source code + all questions (basic and advanced)\\n8 / 79', 'Task3: The Final Project (A-F)\\n▶One final project: source code and oral presentation.\\n▶Proposed by students and confirmed by the teacher:\\n▶Source code and documentation as a README.md by the students of the project\\n▶5 minute Presentation of the project by the students.\\n▶20% of total grade bonus for projects graded as excellent.\\n9 / 79', 'The Final Grade\\n▶The final grade is the weighted average of the Exam (0.3), two labs (0.15 each). A\\nbonus of .09 points will be added for projects graded ’excellent’.\\n▶To compute it, map A-E to 5-1, and take the average.\\n▶The floating values are rounded up, if they are more than half, otherwise they are\\nrounded down.\\n•E.g., 3.6 will be rounded to 4, and 4.5 will be rounded to 4.\\n▶A late submission will reduce your grade level by one. That is, A will become B, B\\nwill become C, and so on.\\n▶To pass the course, you need to take at least E in all the assignments.\\n10 / 79', 'How do you submit the Assignments?\\n▶Through the Canvas site.\\n▶Students will work in groups of two on Task 2 and Task 3.\\n11 / 79', 'The Course Material\\n▶Hands-on machine learning with Scikit-Learn and TensorFlow, 2nd Edition, A. Geron,\\nO’Reilly Media, 2019\\n▶Deep learning, I. Goodfellow et al., Cambridge: MIT press, 2016\\n12 / 79', 'The Course Web Page\\nhttps://id2223kth.github.io\\n13 / 79', 'Supervised Machine Learning 101\\n14 / 79', 'Features in Machine Learning\\n15 / 79', 'Supervised Machine Learning (ML)\\n16 / 79', 'A Linear Model can classify the Fruit (Classifier)\\n17 / 79', 'The Decision Boundary\\n18 / 79', 'Source code for a small sample of data with a decision tree\\nclassifier\\n19 / 79', 'But wait, apples can also be red!\\n20 / 79', 'It’s harder to separate Green Apples, Oranges, and Red Apples\\nwith just 2 colors (red and green)\\n21 / 79', 'From two dimensions to millions of dimensions\\n▶Can we just add more features? Yes. If you add weight and smoothness, you can\\nseparate apples and oranges. We could plot our fruit in 3d or even 4d and find a\\nplane that separates apples and oranges\\n▶You can add many more features (dimensions). With the caveat that too many\\ndimensions can lead to overfitting. Overfitting means your model is not good at\\ngeneralizing to correctly predict new fruit examples (it would work well for the training\\ndata, but not unseen (new) examples)\\n▶In image classification, each pixel is a feature. That’s millions of features for a single\\nHD image. Deep learning can be used to train models with millions of features.\\n22 / 79', 'Not all properties with predictive power should be features\\n23 / 79', 'Lots of features: deep learning for apple classification works\\n24 / 79', 'A model can also be trained to predict a number (regression)\\n25 / 79', 'Predicting Surf Height at a beach – Classification or\\nRegression?\\n26 / 79', 'What is Supervised Machine Learning, then?\\n▶With our Apple/Orange classifier, we used (features, label) examples to train a model\\nto find a decision boundary.\\n▶Then when a new fruit arrived, we could use the model to predict if the new piece\\nof fruit is an apple or an orange.\\n▶We can generalize to say that supervised machine learning is concerned with:\\n▶extracting a pattern from labeled data (features) to a model\\n▶using that model to make predictions for new unlabeled data (features)\\n27 / 79', 'Tradtional ML courses vs ID2223\\n▶Static Datasets, where Features for ML are correct and unbiased\\n▶The goal is to optimize your model with a model evaluation metric (accuracy) to\\ncommunicate the value of your model\\n▶Data never stops coming and it comes from heterogeneous data sources\\n▶Communicate the value of your model as a Prediction Service - one that can be scaled\\nand deployed using MLOps (Machine Learning Operations) best practices (versioning,\\nautomated testing/deployment)\\n28 / 79', 'Feature Engineering is often treated like this “helpful” guide to\\ndrawing a Barn Owl\\n29 / 79', 'Extract the features from the input data. We will study feature\\nengineering at scale.\\nRaw Data Extracted Feature Method\\nHotel room bookings Weekly vacancy level Aggregation\\nUser’s web session history Session history embedding Dimensionality Reduction\\nUser’s date of birth Binning\\nHourly spot electricity prices Scale into range [0, 1] Normalization\\nUser’s home country Binary number of country One Hot Encoding\\n30 / 79', 'Many Enterprises have walled gardens between teams building\\nproduction ML Systems\\n31 / 79', 'With infrastructure support (Serverless Machine Learning),\\ndevelopers need less infrastructural skills to deploy ML Systems\\n32 / 79', 'Monolithic ML Pipeline\\n33 / 79', 'Refactor the Monolithic ML Pipeline to Scale your ML Systems\\n34 / 79', 'Machine Learning and Deep Learning\\n35 / 79', 'Learning Algorithms\\n▶A ML algorithm is an algorithm that is able to learn from data.\\n▶What is learning?\\n▶A computer program is said to learn from experience E with respect to some class of\\ntasks T and performance measure P, if its performance at tasks in T, as measured\\nby P, improves with experience E. (Tom M. Mitchell)\\n36 / 79', 'Learning Algorithms - Example 1\\n▶A spam filter that can learn to flag spam given examples of spam emails and examples\\nof regular emails.\\n▶Task T: flag spam for new emails\\n▶Experience E: the training data\\n▶Performance measure P: the ratio of correctly classified emails\\n[https://bit.ly/2oiplYM]\\n37 / 79', 'Learning Algorithms - Example 2\\n▶Given dataset of prices of 500 houses, how can we learn to predict the prices of other\\nhouses, as a function of the size of their living areas?\\n▶Task T: predict the price\\n▶Experience E: the dataset of living areas and prices\\n▶Performance measure P: the difference between the predicted price and the real price\\n[https://bit.ly/2MyiJUy]\\n38 / 79', 'Types of Machine Learning Algorithms\\n▶Supervised learning\\n•Input data is labeled, e.g., spam/not-spam or a stock price at a time.\\n•Regression vs. classification\\n▶Unsupervised learning\\n•Input data is unlabeled.\\n•Find hidden structures in data.\\n39 / 79', 'AI Generations - Deep Learning\\n▶For many tasks, it is difficult to know what features should be extracted\\n▶Use machine learning to discover the mapping from representation to output\\n[https://bit.ly/2woLEzs]\\n40 / 79', 'Image Classification with Deep Learning\\n▶For image classification, where each pixel is a feature, Deep Learning can do the\\nfeature extraction as part of the learning algorithm.\\n41 / 79', 'Sheepdog or Mop\\n42 / 79', 'Chihuahua or Muffin\\n43 / 79', 'Barn Owl or Apple\\n44 / 79', 'Training Deep Neural Networks\\n▶Computationally intensive\\n▶Time consuming\\n[https://cloud.google.com/tpu/docs/images/inceptionv3onc--oview.png]\\n45 / 79', 'Why?\\n▶Massive amount of training dataset\\n▶Large number of parameters\\n46 / 79', 'Accuracy vs. Data/Model Size\\n[Jeff Dean at AI Frontiers: Trends and Developments in Deep Learning Research]\\n47 / 79', 'Accuracy vs. Data/Model Size\\n[Jeff Dean at AI Frontiers: Trends and Developments in Deep Learning Research]\\n48 / 79', 'Accuracy vs. Data/Model Size\\n[Jeff Dean at AI Frontiers: Trends and Developments in Deep Learning Research]\\n49 / 79', 'Why Does Deep Learning Work Now?\\n▶Huge quantity of data\\n▶Tremendous increase in computing power\\n▶Better training algorithms\\n50 / 79', 'Linear Algebra Review\\n51 / 79', 'Vector\\n▶A vector is an array of numbers.\\n▶Notation:\\n•Denoted by bold lowercase letters, e.g., x.\\n•xidenotes the ith entry.\\nx=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0x1\\nx2\\n...\\nxn\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n52 / 79', 'Matrix and Tensor\\n▶A matrix is a 2-D array of numbers.\\n▶A tensor is an array with more than two axes.\\n▶Notation:\\n•Denoted by bold uppercase letters, e.g., A.\\n•aijdenotes the entry in ith row and jth column.\\n•IfAism×n, it has mrows and ncolumns.\\nA=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0a1,1a1,2a1,3. . . a1,n\\na2,1a2,2a2,3. . . a2,n\\n...............\\nam,1am,2am,3. . . am,n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n53 / 79', 'Matrix Addition and Subtraction\\n▶The matrices must have the same dimensions.\\nA=\\x14a b\\nc d\\x15\\n+\\x14e f\\ng h\\x15\\n=\\x14a+e b +f\\nc+g d +h\\x15\\n54 / 79', 'Matrix Product\\n▶The matrix product of matrices AandBis a third matrix C, where C=AB.\\n▶IfAis of shape m×nandBis of shape n×p, then Cis of shape m×p.\\ncij=X\\nkaikbkj\\n▶Properties\\n•Associative: ( AB)C=A(BC)\\n•Not commutative: AB̸=BA\\n[https://en.wikipedia.org/wiki/Matrix multiplication]\\n55 / 79', 'Matrix Transpose\\n▶Swap the rows and columns of a matrix.\\nA=\\uf8ee\\n\\uf8f0a b\\nc d\\ne f\\uf8f9\\n\\uf8fb⇒A⊺=\\x14a c e\\nb d f\\x15\\n▶Properties\\n•Aij=A⊺\\nji\\n•IfAism×n, then A⊺isn×m\\n•(A+B)⊺=A⊺+B⊺\\n•(AB)⊺=B⊺A⊺\\n56 / 79', 'Inverse of a Matrix\\n▶IfAis a square matrix, its inverse is called A−1.\\nAA−1=A−1A=I\\n▶Where I, the identity matrix, is a diagonal matrix with all 1’s on the diagonal.\\nI2=\\x141 0\\n0 1\\x15\\nI3=\\uf8ee\\n\\uf8f01 0 0\\n0 1 0\\n0 0 1\\uf8f9\\n\\uf8fb\\n57 / 79', 'LpNorm for Vectors\\n▶We can measure the size of vectors using a norm function.\\n▶Norms are functions mapping vectors to non-negative values.\\n▶L1norm\\n||x||1=X\\ni|xi|\\n▶L2norm\\n||x||2= (X\\ni|xi|2)1\\n2=q\\nx2\\n1+x2\\n2+···+x2n\\n▶Lpnorm\\n||x||p= (X\\ni|xi|p)1\\np\\n58 / 79', 'Probability Review\\n59 / 79', 'Random Variables\\n▶Random variable: a variable that can take on different values randomly.\\n▶Random variables may be discrete or continuous.\\n•Discrete random variable: finite or countably infinite number of states\\n•Continuous random variable: real value\\n▶Notation:\\n•Denoted by an upper case letter, e.g., X\\n•Values of a random variable Xare denoted by lower case letters, e.g., xandy.\\n60 / 79', 'Probability Distributions\\n▶Probability distribution: how likely a random variable is to take on each of its possible\\nstates.\\n•E.g., the random variable Xdenotes the outcome of a coin toss.\\n•The probability distribution of Xwould take the value 0.5forX=head , and 0.5for\\nY=tail (assuming the coin is fair).\\n▶The way we describe probability distributions depends on whether the variables are\\ndiscrete or continuous.\\n61 / 79', 'Discrete Variables\\n▶Probability mass function (PMF): the probability distribution of a discrete random\\nvariable X.\\n▶Notation: denoted by a lowercase p.\\n•E.g., p(x) =1indicates that X=xis certain\\n•E.g., p(x) =0indicates that X=xis impossible\\n▶Properties:\\n•The domain Dofpmust be the set of all possible states of X\\n•∀x∈D(X),0≤p(x)≤1\\n•P\\nx∈D(X)p(x) =1\\n62 / 79', 'Independence\\n▶Two random variables XandYare independent, if their probability distribution can\\nbe expressed as their products.\\n∀x∈D(X),y∈D(Y),p(X=x,Y=y) =p(X=x)p(Y=y)\\n▶E.g., if a coin is tossed and a single 6-sided die is rolled, then the probability of\\nlanding on the head side of the coin and rolling a 3 on the die is:\\np(X=head ,Y=3) =p(X=head )p(Y=3) =1\\n2×1\\n6=1\\n12\\n63 / 79', 'Conditional Probability\\n▶Conditional probability: the probability of an event given that another event has\\noccurred.\\np(Y=y|X=x) =p(Y=y,X=x)\\np(X=x)\\n▶E.g., if 60% of the class passed both labs and 80% of the class passed the first labs,\\nthen what percent of those who passed the first lab also passed the second lab?\\n•E.g., XandYrandom variables for the first and the second labs, respectively.\\np(Y=lab2|X=lab1 ) =p(Y=lab2 ,X=lab1 )\\np(X=lab1 )=0.6\\n0.8=3\\n4\\n64 / 79', 'Expectation\\n▶The expected value of a random variable Xwith respect to a probability distribution\\np(X) is the average value that Xtakes on when it is drawn from p(X).\\nEx∼p[X] =X\\nxp(x)x\\n▶E.g., If X:{1,2,3}, and p(X=1) =0.3,p(X=2) =0.5,p(X=3) =0.2\\n•E[X] =0.3×1+0.5×2+0.2×3=1.9\\n65 / 79', 'Variance and Standard Deviation\\n▶The variance gives a measure of how much the values of a random variable Xvary\\nas we sample it from its probability distribution p(X).\\nVar(X) =E[(X−E[X])2]\\nVar(X) =X\\nxp(x)(x−E[X])2\\n▶E.g., If X:{1,2,3}, and p(X=1) =0.3,p(X=2) =0.5,p(X=3) =0.2\\n•E[X] =0.3×1+0.5×2+0.2×3=1.9\\n•Var(X) =0.3(1−1.9)2+0.5(2−1.9)2+0.2(3−1.9)2=0.49\\n▶The standard deviation, shown by σ, is the square root of the variance.\\n66 / 79', 'Covariance (1/2)\\n▶The covariance gives some sense of how much two values are linearly related to each\\nother.\\nCov(X,Y) =E[(X−E[X])(Y−E[Y])]\\nCov(X,Y) =X X\\n(x,y)p(x,y)(x−E[X])(y−E[Y])\\n67 / 79', 'Covariance (2/2)\\nY\\np(X, Y) 1 2 3 p(X)\\n1 1/4 1/4 0 1/2\\nX 2 0 1/4 1/4 1/2\\np(Y) 1/4 1/2 1/4 1\\nE[X] =1\\n2×1+1\\n2×2=3\\n2E[Y] =1\\n4×1+1\\n2×2+1\\n4×3=2\\nCov(X,Y) =X X\\n(x,y)p(x,y)(x−E[X])(y−E[Y])\\n=1\\n4(1−3\\n2)(1−2) +1\\n4(1−3\\n2)(2−2) +0(1−3\\n2)(3−2)\\n+0(2−3\\n2)(1−2) +1\\n4(2−3\\n2)(2−2) +1\\n4(2−3\\n2)(3−2) =1\\n4\\n68 / 79', 'Correlation Coefficient\\n▶The Correlation coefficient is a quantity that measures the strength of the association\\n(or dependence) between two random variables, e.g., XandY.\\nρ(X,Y) =Cov(X,Y)\\nσ(X)σ(Y)\\n69 / 79', 'Probability and Likelihood (1/2)\\n▶LetX:{x(1),x(2),···,x(m)}be a discrete random variable drawn independently from\\na distribution probability pdepending on a parameter θ.\\n•For six tosses of a coin, X:{h,t,t,t,h,t},h: head, and t: tail.\\n•Suppose you have a coin with probability θto land heads and ( 1−θ) to land tails.\\n▶p(X|θ=2\\n3) is the probability of Xgiven θ=2\\n3.\\n▶p(X=h|θ) is the likelihood of θgiven X=h.\\n▶Likelihood ( L): a function of the parameters ( θ) of a probability model, given specific\\nobserved data, e.g., X=h.\\nL(θ|X) =p(X|θ)\\n70 / 79', 'Probability and Likelihood (2/2)\\n▶The likelihood differs from that of a probability.\\n▶A probability p(X|θ) refers to the occurrence of future events.\\n▶A likelihood L(θ|X) refers to past events with known outcomes.\\n71 / 79', 'Maximum Likelihood Estimator\\n▶If samples in Xare independent we have:\\nL(θ|X) =p(X|θ) =p(x(1),x(2),···,x(m)|θ)\\n=p(x(1)|θ)p(x(2)|θ)···p(x(m)|θ) =mY\\ni=1p(x(i)|θ)\\n▶The maximum likelihood estimator (MLE): what is the most likely value of θgiven\\nthe training set?\\n^θMLE= arg max\\nθL(θ|X) = arg max\\nθmY\\ni=1p(x(i)|θ)\\n72 / 79', 'Maximum Likelihood Estimator - Example\\n▶Six tosses of a coin, with the following model:\\n•Possible outcomes: h with probability of θ, and t with probability ( 1−θ).\\n•Results of coin tosses are independent of one another.\\n▶Data: X:{h,t,t,t,h,t}\\n▶The likelihood is\\nL(θ|X) =p(X|θ)\\n=p(X=h|θ)p(X=t|θ)p(X=t|θ)p(X=t|θ)p(X=h|θ)p(X=t|θ)\\n=θ(1−θ)(1−θ)(1−θ)θ(1−θ)\\n=θ2(1−θ)4\\n▶^θis the value of θthat maximizes the likelihood:\\n^θMLE= arg max\\nθL(θ|X) =2\\n2+4\\n73 / 79', 'Log-Likelihood\\n▶The MLE product is prone to numerical underflow.\\n^θMLE= arg max\\nθL(θ|X) = arg max\\nθmY\\ni=1p(x(i)|θ)\\n▶To overcome this problem we can use the logarithm of the likelihood.\\n•It does not change its arg max, but transforms a product into a sum.\\n^θMLE= arg max\\nθmX\\ni=1logp (x(i)|θ)\\n74 / 79', 'Negative Log-Likelihood\\n▶Likelihood: L(θ|X) =Qm\\ni=1p(x(i)|θ)\\n▶Log-Likelihood: logL (θ|X) =logQm\\ni=1p(x(i)|θ) =Pm\\ni=1logp (x(i)|θ)\\n▶Negative Log-Likelihood: −logL (θ|X) =−Pm\\ni=1logp (x(i)|θ)\\n▶Negative log-likelihood is also called the cross-entropy\\n75 / 79', 'Cross-Entropy\\n▶Coss-entropy: quantify the difference (error) between two probability distributions.\\n▶How close is the predicted distribution to the true distribution?\\nH(p,q) =−X\\nxp(x)log(q(x))\\n▶Where pis the true distribution, and qthe predicted distribution.\\n76 / 79', 'Cross-Entropy - Example\\n▶Six tosses of a coin: X:{h,t,t,t,h,t}\\n▶The true distribution p:p(h) =2\\n6andp(t) =4\\n6\\n▶The predicted distribution q: h with probability of θ, and t with probability ( 1−θ).\\n▶Cross entropy: H(p,q) =−P\\nxp(x)log(q(x))\\n=−p(h)log(q(h))−p(t)log(q(t)) =−2\\n6log(θ)−4\\n6log(1−θ)\\n▶Likelihood: θ2(1−θ)4\\n▶Negative log likelihood: −log(θ2(1−θ)4) =−2log (θ)−4log (1−θ)\\n77 / 79', 'References\\n▶Ian Goodfellow et al., Deep Learning (Ch. 1, 2, 3)\\n78 / 79', 'Questions?\\nAcknowledgements\\nSome of the pictures were copied from the book Hands-On Machine Learning\\nwith Scikit-Learn and TensorFlow, Aurelien Geron, O’Reilly Media, 2017.\\n79 / 79']}}\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "\n",
    "transcriptions = [] \n",
    "embeddings = {}\n",
    "\n",
    "for file in new_files:\n",
    "    pdfReader = PyPDF2.PdfReader(file[\"title\"])\n",
    "\n",
    "    count = len(pdfReader.pages)\n",
    "    output = \"\"\n",
    "    embeddings[file[\"title\"]] = {\"text\":[]}\n",
    "    for i in range(count):\n",
    "        pageObj = pdfReader.pages[i]\n",
    "       \n",
    "        extr = pageObj.extract_text()\n",
    "        embeddings[file[\"title\"]][\"text\"].append(extr)\n",
    "        output += \"\\n\" + extr\n",
    "        \n",
    "    transcriptions.append(output)\n",
    "print(embeddings)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '_io.StringIO'>\n",
      "<class '_io.StringIO'>\n",
      "<class '_io.StringIO'>\n",
      "<class '_io.StringIO'>\n",
      "<class '_io.StringIO'>\n",
      "<class '_io.StringIO'>\n",
      "<class '_io.StringIO'>\n",
      "<class '_io.StringIO'>\n",
      "<class '_io.StringIO'>\n",
      "<class '_io.StringIO'>\n",
      "<class '_io.StringIO'>\n",
      "<class '_io.StringIO'>\n",
      "<class '_io.StringIO'>\n",
      "<class '_io.StringIO'>\n",
      "<class '_io.StringIO'>\n",
      "<class '_io.StringIO'>\n",
      "<class '_io.StringIO'>\n",
      "<class '_io.StringIO'>\n",
      "<class '_io.StringIO'>\n",
      "<class '_io.StringIO'>\n",
      "<class '_io.StringIO'>\n",
      "<class '_io.StringIO'>\n"
     ]
    }
   ],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import XMLConverter, HTMLConverter, TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "import io\n",
    "import os\n",
    "\n",
    "transcriptions = [];\n",
    "emb = {\"source\":[],\"page\":[],\"content\":[]}\n",
    "for file in new_files:\n",
    "    embeddings[file[\"title\"]] = {\"text\":[]}\n",
    "    fp = open(file[\"title\"], 'rb')\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    print(type(retstr))\n",
    "    codec = 'utf-16'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    full_file = \"\"\n",
    "    page_no = 0\n",
    "    for pageNumber, page in enumerate(PDFPage.get_pages(fp)):\n",
    "        if pageNumber == page_no:\n",
    "            interpreter.process_page(page)\n",
    "\n",
    "            data = retstr.getvalue()\n",
    "            data = ' '.join(data.splitlines())\n",
    "            full_file += data + \"\\n\"\n",
    "            data = data.split(\". \")\n",
    "            for e in data:\n",
    "                emb[\"source\"].append(file[\"title\"])\n",
    "                emb[\"page\"].append(pageNumber)\n",
    "                emb[\"content\"].append(data)\n",
    "                #embeddings[file[\"title\"]][\"text\"].append(data)\n",
    "            retstr.truncate(0)\n",
    "            retstr.seek(0)\n",
    "        page_no += 1\n",
    "    transcriptions.append(full_file)\n",
    "\n",
    "#text = extract_text(\"DeepLearning-chapter-3.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9571\n"
     ]
    }
   ],
   "source": [
    "print(len(emb[\"source\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"questions\": [\n",
      "    \"What are recurrent neural networks?\",\n",
      "    \"Why are recurrent networks specialized for processing sequences?\",\n",
      "    \"What is the advantage of parameter sharing in recurrent neural networks?\",\n",
      "    \"Give an example of how recurrent neural networks can recognize information at different positions within a sequence.\",\n",
      "    \"How do recurrent neural networks differ from fully connected feedforward networks?\",\n",
      "    \"What is the concept of parameter sharing in recurrent neural networks?\",\n",
      "    \"What is the difference between the parameter sharing in recurrent networks and convolutional networks?\",\n",
      "    \"How do recurrent neural networks handle sequence length and time in practice?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Recurrent neural networks (RNNs) are a family of neural networks for processing sequential data.\",\n",
      "    \"Recurrent networks are specialized for processing sequences because they can scale to much longer sequences than networks without sequence-based specialization and can also handle sequences of variable length.\",\n",
      "    \"The advantage of parameter sharing in recurrent neural networks is that it allows the model to generalize across examples of different forms (such as sequence lengths) and share statistical strength across different sequence lengths and positions in time.\",\n",
      "    \"For example, if the task is to extract the year in which the narrator went to Nepal from two different sentences, 'I went to Nepal in 2009' and 'In 2009, I went to Nepal,' a recurrent neural network can recognize the year 2009 as the relevant information, regardless of its position in the sentence.\",\n",
      "    \"Unlike fully connected feedforward networks, recurrent neural networks share the same weights across several time steps.\",\n",
      "    \"Parameter sharing in recurrent neural networks refers to the use of the same weights across multiple time steps or positions in a sequence.\",\n",
      "    \"Parameter sharing in recurrent networks is different from convolutional networks because in recurrent networks, each member of the output is a function of the previous members of the output, while in convolutional networks, each member of the output is a function of a small number of neighboring members of the input.\",\n",
      "    \"In practice, recurrent neural networks usually operate on minibatches of sequences, each with a different sequence length. The time step index used in RNNs does not necessarily refer to the passage of time in the real world but can also refer to the position in the sequence. RNNs can also be applied to data involving time while having connections that go backwards in time as long as the entire sequence is observed before it is provided to the network.\"\n",
      "  ]\n",
      "}\n",
      "{\"questions\": [\"What is the purpose of unfolding a recursive or recurrent computation into a computational graph?\", \"What does equation 10.1 represent?\", \"How can a recurrent neural network use equation 10.1?\", \"What can the state of the system in equation 10.1 represent?\", \"How can a recurrent neural network be represented using a computational graph?\"], \"answers\": [\"The purpose of unfolding a recursive or recurrent computation into a computational graph is to formalize the structure of the computations and allow for parameter sharing across a deep network structure.\", \"Equation 10.1 represents a classical dynamical system, where the state of the system at time t is defined based on its definition at time t-1.\", \"A recurrent neural network can use equation 10.1 or a similar equation to determine the values of its hidden units.\", \"The state of the system in equation 10.1 represents the current state of the system and is updated based on the previous state.\", \"A recurrent neural network can be represented using a computational graph by unfolding the recurrent equations, resulting in a directed acyclic graph where each node represents the state at a specific time step and the function maps the state at one time step to the state at the next time step.\"]\n",
      "{'questions': [{'question': 'What are the advantages of using an unfolded computational graph in a recurrent neural network?'},\n",
      "  {'question': 'What does the black square in the circuit diagram indicate?'},\n",
      "  {'question': 'What is the purpose of the unfolding process in a recurrent neural network?'},\n",
      "  {'question': 'Why is it possible to use the same transition function for all time steps in an unfolded graph?'},\n",
      "  {'question': 'What are the uses of both the recurrent graph and the unfolded graph in a recurrent neural network?'}],\n",
      " 'answers': [{'answer': 'The advantages of using an unfolded computational graph in a recurrent neural network are: 1) Regardless of the sequence length, the learned model always has the same input size. 2) It is possible to use the same transition function with the same parameters at every time step.', 'context': '10.2  We can represent the unfolded recurrence after  t  steps with a function  − , . . . , x(2), x (1)) h( )t =g( )t (x( )t , x( 2) t − , x( 1) t − , x( )t ; )θ 1) t = (f h(  g( )t :  (10.6)  (10.7) ...'},\n",
      "  {'answer': 'The black square in the circuit diagram indicates that an interaction takes place with a delay of a single time step.', 'context': 'Throughout this chapter, we use a black square in a circuit diagram to indicate that an interaction takes place with a delay of a single time step, from the state at time t to the state at time t + 1.'},\n",
      "  {'answer': 'The purpose of the unfolding process in a recurrent neural network is to map a circuit as in the left side of the figure to a computational graph with repeated pieces as in the right side.', 'context': 'What we separate node of the computational graph, as in the right of ﬁgure call unfolding is the operation that maps a circuit as in the left side of the ﬁgure to a computational graph with repeated pieces as in the right side.'},\n",
      "  {'answer': 'It is possible to use the same transition function for all time steps in an unfolded graph because the unfolding process factorizes the function that takes the whole past sequence into repeated application of a function.', 'context': 'The unfolding process allows us to factorize g ( )t into repeated application of a function f. The unfolding process thus introduces two major advantages: ...  It is possible to use the same transition function f with the same parameters at every time step.'},\n",
      "  {'answer': 'The recurrent graph is succinct and provides a description of information flow, while the unfolded graph illustrates the computations to perform and helps to visualize the flow of information forward and backward in time.', 'context': 'Both the recurrent graph and the unrolled graph have their uses. The recurrent graph is succinct. The unfolded graph provides an explicit description of which computations to perform. The unfolded graph also helps to illustrate the idea of information ﬂow forward in time (computing outputs and losses) and backward in time (computing gradients) by explicitly showing the path along which this information ﬂows.'}]}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What are some examples of important design patterns for recurrent neural networks?\",\n",
      "    \"What is the universal property of the recurrent neural network?\",\n",
      "    \"What activation function is assumed for the hidden units?\",\n",
      "    \"How is the output represented?\",\n",
      "    \"What is the purpose of the softmax operation in the forward propagation?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Some examples of important design patterns for recurrent neural networks include recurrent networks that produce an output at each time step and have recurrent connections between hidden units, recurrent networks that produce an output at each time step and have recurrent connections only from the output at one time step to the hidden units at the next time step, and recurrent networks with recurrent connections between hidden units that read an entire sequence and then produce a single output.\",\n",
      "    \"The recurrent neural network is universal in the sense that any function computable by a Turing machine can be computed by such a recurrent network of a finite size. The output can be read from the RNN after a number of time steps that is asymptotically linear in the number of time steps used by the Turing machine and asymptotically linear in the length of the input.\",\n",
      "    \"The hyperbolic tangent activation function is assumed for the hidden units.\",\n",
      "    \"The output is represented as the unnormalized log probabilities of each possible value of the discrete variable.\",\n",
      "    \"The softmax operation is applied as a post-processing step to obtain a vector of normalized probabilities over the output. Its purpose is to convert the unnormalized log probabilities into a distribution of probabilities that sum up to 1.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': [{'question': 'What kind of network is shown in figure 10.4?', 'answers': ['An RNN with a feedback connection from the output to the hidden layer.']}, {'question': 'What makes the RNN in figure 10.4 less powerful compared to other RNNs?', 'answer... [(['The RNN in figure 10.4 can only send information about the past in its hidden representation, while other RNNs can transmit information from the past directly through hidden-to-hidden connections.'], 'This makes the RNN in figure 10.4 less powerful.')]}\n",
      "{'questions': \n",
      "  [\n",
      "    'What does it mean for a network to lack hidden-to-hidden recurrence?',\n",
      "    'Why are output units unlikely to capture the necessary information about the past history of the input?',\n",
      "    'What is the advantage of eliminating hidden-to-hidden recurrence in terms of training?',\n",
      "    'What is teacher forcing and when is it used?',\n",
      "    'How can teacher forcing be applied to models with hidden-to-hidden connections?'\n",
      "  ],\n",
      " 'answers': \n",
      "  [\n",
      "    'A network lacking hidden-to-hidden recurrence means that the network does not have connections between hidden units at different time steps.',\n",
      "    'Output units are unlikely to capture the necessary information about the past history of the input because they are explicitly trained to match the training set targets.',\n",
      "    'The advantage of eliminating hidden-to-hidden recurrence is that all time steps are decoupled, allowing for parallelization of training and isolation of gradient computation for each step.',\n",
      "    'Teacher forcing is a training technique where, during training, the model receives the ground truth output as input at the next time step. It is used to train models that have connections from their output to their hidden states at the next time step.',\n",
      "    'Teacher forcing can be applied to models with hidden-to-hidden connections as long as they have connections from the output at one time step to values computed in the next time step.'\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is one way to mitigate the problem of the network seeing different inputs during training and testing?\",\n",
      "    \"How can the network learn to take into account input conditions not seen during training?\",\n",
      "    \"What is the BPTT algorithm used for?\",\n",
      "    \"How do you compute the gradient recursively in a computational graph?\",\n",
      "    \"How do you compute the gradient for the final loss in the computational graph?\",\n",
      "    \"How do you back-propagate gradients through time?\",\n",
      "    \"How do you compute the gradients on the parameter nodes?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"One way to mitigate the problem is to train with both teacher-forced inputs and with free-running inputs.\",\n",
      "    \"The network can learn to take into account input conditions not seen during training by predicting the correct target a number of steps in the future through the unfolded recurrent output-to-input paths.\",\n",
      "    \"The BPTT algorithm is used to compute gradients through a recurrent neural network.\",\n",
      "    \"The gradient in a computational graph is computed recursively by starting with the nodes immediately preceding the final loss and working backwards, computing the gradient at each node based on the gradient computed at nodes that follow it in the graph.\",\n",
      "    \"The gradient for the final loss is computed using the equations provided in the text. It involves taking the derivative of the loss with respect to the outputs at each time step, assuming the outputs are used as the argument to the softmax function.\",\n",
      "    \"Gradients are back-propagated through time by iterating backwards in time, starting from the final time step and going down to the first time step. The gradient at each time step depends on the gradients at the next time step and the current output and hidden states.\",\n",
      "    \"The gradients on the parameter nodes are obtained by first computing the gradients on the internal nodes of the computational graph. Then, taking into account that the parameters are shared across many time steps, the contribution of each edge in the computational graph to the gradient is computed using the bprop method.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What notation is used to denote the contribution of the weights at time step t to the gradient?\",\n",
      "    \"What is the gradient on the remaining parameters given by?\",\n",
      "    \"Why is the gradient with respect to x( )t not computed for training?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The notation ∇ W ( )t is used to denote the contribution of the weights at time step t to the gradient.\",\n",
      "    \"The gradient on the remaining parameters is given by cL = ∇ bL = ∇ V L = ∇ W L = ∇ UL = ∇ t  i   diag i   diag t  ∂o( )t ∂c  ∂h( )t ∂b( )t  t   t   ∇o( )t L = ∇ o( )t L t  2 diag h( )t - 1    ∇o( )t L) h( )t  ( t  t  ∇h ( )t L = i = V o( )t ∂L ∂o( )t i  ∇ ∂L i  ∇W ( )t h( )t ∂h( )t i 1  - h( )t  2   ∇h( )t L) h( ( 1) t -  ∂L i  ∇U( )t h( )t ∂h( )t i 1  - h( )t  2   ∇h( )t L) x( )t  ∇h( )t L\",\n",
      "    \"The gradient with respect to x( )t is not computed for training because it does not have any parameters as ancestors in the computational graph defining the loss.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': ['What does the complete graph structure represent in the graphical model of an RNN?',\n",
      "  'How does the RNN parametrize long-term relationships between variables?',\n",
      "  'What remains computationally challenging in the graphical model of an RNN?'],\n",
      " 'answers': ['The complete graph structure represents direct dependencies between any pair of y values in the graphical model of an RNN.',\n",
      "  'The RNN parametrizes long-term relationships between variables efficiently by using recurrent applications of the same function and same parameters at each time step.',\n",
      "  'Even with the efficient parametrization of the graphical model, predicting missing values remains computationally challenging.']}\n",
      "{\n",
      "\"questions\": [\n",
      "\"What are the challenges faced when optimizing the parameters of recurrent networks?\",\n",
      "\"What is the assumption made in recurrent networks about the relationship between time steps?\",\n",
      "\"What are some methods to determine the length of a sequence in an RNN?\",\n",
      "\"What is the purpose of adding a special symbol corresponding to the end of a sequence in RNNs?\",\n",
      "\"What is the benefit of using an extra Bernoulli output in the RNN model to determine whether to continue or halt generation?\",\n",
      "\"How can the sequence length be predicted in an RNN model?\",\n",
      "\"What is the decomposition used in modeling sequences with RNNs?\",\n",
      "\"What is the difference between the joint distribution and the conditional distribution in RNNs?\",\n",
      "\"How can RNNs be used to represent a conditional distribution?\",\n",
      "\"What is the main operation that needs to be performed to draw samples from an RNN model?\"\n",
      "],\n",
      "\"answers\": [\n",
      "\"The challenges faced when optimizing the parameters of recurrent networks are due to the parameter sharing used in these networks. It relies on the assumption that the same parameters can be used for different time steps, which may not always hold true.\",\n",
      "\"The assumption made in recurrent networks is that the conditional probability distribution over the variables at time t+1 given the variables at time t is stationary, meaning that the relationship between the previous time step and the next time step does not depend on t.\",\n",
      "\"The length of a sequence in an RNN can be determined by adding a special symbol corresponding to the end of a sequence, introducing an extra Bernoulli output, or by predicting the integer τ itself.\",\n",
      "\"The purpose of adding a special symbol corresponding to the end of a sequence in RNNs is to indicate when the sampling process should stop. It helps determine the length of the sequence.\",\n",
      "\"Adding an extra Bernoulli output in the RNN model allows for a more general approach to determine whether to continue or halt generation. This approach can be applied to any RNN, not just RNNs that output a sequence of symbols.\",\n",
      "\"The sequence length can be predicted in an RNN model by adding an extra output that predicts the integer τ itself. The model can sample a value of τ and then sample τ steps worth of data.\",\n",
      "\"The decomposition used in modeling sequences with RNNs is given by the equation P(x(1), ..., x( )τ) = P(τ) * P(x(1), ..., x( )τ | τ).\",\n",
      "\"The joint distribution in RNNs represents a distribution over a sequence of random variables y. The conditional distribution represents a distribution over y given inputs x.\",\n",
      "\"RNNs can be used to represent a conditional distribution by extending the graphical model view to not only represent a joint distribution over the y variables but also a conditional distribution over y given x.\",\n",
      "\"The main operation that needs to be performed to draw samples from an RNN model is to sample from the conditional distribution at each time step.\"\n",
      "]\n",
      "}\n",
      "{'questions': [{'question': 'What are some ways to provide an extra input to an RNN?', 'context': 'y ω ) | with ω = θ. We can extend such a model to represent a distribution P ( y x ) by using the same P(y ω ) as before, but making ω a function of x. In ...'question': 'What restriction does the RNN have when representing a distribution over sequences?', 'context': 'sequence still has one restriction, which is that the length of both sequences must be the same. We describe how to remove this'}], 'answers': []}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the purpose of bidirectional recurrent neural networks?\",\n",
      "    \"What is an example of a task where bidirectional RNNs are useful?\",\n",
      "    \"How do bidirectional RNNs work?\",\n",
      "    \"How can bidirectional RNNs be extended to 2-dimensional input?\",\n",
      "    \"What advantage do RNNs have over convolutional networks when applied to images?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The purpose of bidirectional recurrent neural networks (RNNs) is to capture information from both the past and the future in order to make predictions or generate output sequences.\",\n",
      "    \"An example of a task where bidirectional RNNs are useful is speech recognition, where the interpretation of the current sound as a phoneme may depend on the next few phonemes or even the next few words.\",\n",
      "    \"Bidirectional RNNs combine two RNNs: one that moves forward through time from the start of the sequence, and another that moves backward through time from the end of the sequence. This allows the output units to compute representations that depend on both the past and the future.\",\n",
      "    \"Bidirectional RNNs can be extended to 2-dimensional input, such as images, by having RNNs moving in all four directions: up, down, left, and right. Each RNN would compute a representation at each point of the 2-D grid.\",\n",
      "    \"RNNs applied to images have the advantage of allowing long-range lateral interactions between features in the same feature map. This means that RNNs can capture both local and long-range dependencies in the image.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the purpose of Encoder-Decoder sequence-to-sequence architectures?\",\n",
      "    \"How does an RNN map an input sequence to a fixed-size vector?\",\n",
      "    \"What is the context in the context of RNNs?\",\n",
      "    \"What are some applications that require mapping input sequences to output sequences of different lengths?\",\n",
      "    \"What is the encoder-decoder or sequence-to-sequence architecture?\",\n",
      "    \"In the sequence-to-sequence architecture, how are the encoder and decoder trained?\",\n",
      "    \"What is the limitation of the encoder-decoder architecture when the context dimension is too small?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The purpose of Encoder-Decoder sequence-to-sequence architectures is to map an input sequence to an output sequence of possibly different lengths.\",\n",
      "    \"An RNN can map an input sequence to a fixed-size vector by using an encoder RNN to process the input sequence and emitting the context vector, which represents a semantic summary of the input sequence.\",\n",
      "    \"In the context of RNNs, the context refers to the input to the RNN, which is the sequence to be mapped to an output sequence.\",\n",
      "    \"Applications such as speech recognition, machine translation, and question answering require mapping input sequences to output sequences of different lengths.\",\n",
      "    \"The encoder-decoder or sequence-to-sequence architecture consists of an encoder RNN that reads the input sequence and emits a context vector, and a decoder RNN that generates the output sequence conditioned on the context vector.\",\n",
      "    \"In the sequence-to-sequence architecture, the encoder and decoder RNNs are trained jointly to maximize the average of log probability of the output sequences given the input sequences in the training set.\",\n",
      "    \"A limitation of the encoder-decoder architecture occurs when the context vector output by the encoder RNN has a dimension that is too small to properly summarize a long sequence.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the purpose of introducing depth in each operation of an RNN?\",\n",
      "    \"What does the experimental evidence suggest about adding depth to RNNs?\",\n",
      "    \"What did Pascanu et al. propose regarding the three blocks of operations in RNNs?\",\n",
      "    \"How can the path-lengthening effect of deep RNNs be mitigated?\",\n",
      "    \"What is the difference between a recurrent neural network (RNN) and a recursive neural network?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The purpose of introducing depth in each operation of an RNN is to perform the required mappings more effectively.\",\n",
      "    \"The experimental evidence suggests that adding depth to RNNs is beneficial for improving their performance.\",\n",
      "    \"Pascanu et al. proposed to have a separate MLP (possibly deep) for each of the three blocks of operations in RNNs.\",\n",
      "    \"The path-lengthening effect of deep RNNs can be mitigated by introducing skip connections in the hidden-to-hidden path.\",\n",
      "    \"A recurrent neural network (RNN) is designed for sequential data processing with a chain-like structure, while a recursive neural network is a generalization of RNNs with a tree-like structure that allows for variable-size sequences to be mapped to a fixed-size representation.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': ['What is the difference between recursive neural networks and recurrent neural networks?', 'What kind of computational graph is used in recursive neural networks?', 'What is the advantage of recursive networks over recurrent networks?', 'How can the tree structure in a recursive network be determined?', 'What is the challenge of learning long-term dependencies in recurrent networks?'], 'answers': ['Recursive neural networks have a deep tree structure, while recurrent neural networks have a chain-like structure.', 'Recursive neural networks use a deep tree computational graph.', 'Recursive networks have a reduced depth compared to recurrent networks, which helps with long-term dependencies.', 'The tree structure in a recursive network can be fixed to the structure of the parse tree of the input sentence or inferred by the learner.', 'The challenge in learning long-term dependencies in recurrent networks is the vanishing or exploding gradients, as well as the exponentially smaller weights given to long-term interactions compared to short-term ones.']}\n",
      "{\n",
      "\"questions\": [\n",
      "\t\"Why do recurrent networks result in extremely nonlinear behavior?\",\n",
      "\t\"What does the recurrence relation h(t) = W*h(t-1) represent?\",\n",
      "\t\"What happens to the eigenvalues in the recurrence relation as t increases?\",\n",
      "\t\"What problem is specific to recurrent networks?\",\n",
      "\t\"How can very deep feedforward networks avoid the vanishing and exploding gradient problem?\",\n",
      "\t\"What happens to the gradient of a long term interaction in a recurrent network?\",\n",
      "\t\"What is the main problem with gradient-based optimization for long-term dependencies in recurrent networks?\",\n",
      "\t\"What approaches have been proposed to reduce the difficulty of learning long-term dependencies in recurrent networks?\"\n",
      "],\n",
      "\"answers\": [\n",
      "\t\"Recurrent networks result in extremely nonlinear behavior due to the composition of the same function multiple times, which can lead to exponential decay or explosion of values.\",\n",
      "\t\"The recurrence relation h(t) = W*h(t-1) represents a simple recurrent neural network without a nonlinear activation function or input values.\",\n",
      "\t\"As t increases, the eigenvalues in the recurrence relation are raised to the power of t, causing eigenvalues with magnitude less than one to decay to zero and eigenvalues with magnitude greater than one to explode.\",\n",
      "\t\"The specific problem with recurrent networks is the vanishing and exploding gradient problem.\",\n",
      "\t\"Very deep feedforward networks can avoid the vanishing and exploding gradient problem by carefully choosing the scaling and variance of individual weights at each time step.\",\n",
      "\t\"The gradient of a long term interaction in a recurrent network has exponentially smaller magnitude than the gradient of a short term interaction.\",\n",
      "\t\"The main problem with gradient-based optimization for long-term dependencies in recurrent networks is that the signal about these dependencies tends to be hidden by small fluctuations arising from short-term dependencies, making it difficult to learn long-term dependencies.\",\n",
      "\t\"Approaches that have been proposed to reduce the difficulty of learning long-term dependencies in recurrent networks include using alternative activation functions, modifying the network architecture, and introducing attention mechanisms.\"\n",
      "]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is one of the main challenges in deep learning?\",\n",
      "    \"What is one proposed approach to avoiding the difficulty of learning recurrent weights in a network?\",\n",
      "    \"What are echo state networks (ESNs) and liquid state machines (LSMs)?\",\n",
      "    \"How are reservoir computing networks similar to kernel machines?\",\n",
      "    \"What is the training criterion used for reservoir computing networks?\",\n",
      "    \"How is the input and recurrent weights set in reservoir computing networks?\",\n",
      "    \"What is the effect of the spectral radius in a recurrent neural network?\",\n",
      "    \"How does back-propagation in a network with a fixed Jacobian matrix change as time propagates?\",\n",
      "    \"What happens when the eigenvalue of the Jacobian matrix is greater than 1?\",\n",
      "    \"What happens when the eigenvalue of the Jacobian matrix is less than 1?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The problem of learning long-term dependencies remains one of the main challenges in deep learning.\",\n",
      "    \"One proposed approach is to set the recurrent weights such that the recurrent hidden units capture the history of past inputs and learn only the output weights.\",\n",
      "    \"Echo state networks (ESNs) and liquid state machines (LSMs) are types of reservoir computing networks. ESNs use continuous-valued hidden units, while LSMs use spiking neurons with binary outputs.\",\n",
      "    \"Reservoir computing networks are similar to kernel machines in that they map an arbitrary length sequence into a fixed-length vector on which a linear predictor can be applied.\",\n",
      "    \"The training criterion for reservoir computing networks is often mean squared error, with linear regression from the hidden units to the output targets.\",\n",
      "    \"The input and recurrent weights in reservoir computing networks are set such that the dynamical system is near the edge of stability.\",\n",
      "    \"The spectral radius of a recurrent neural network affects the representation of rich sets of histories in the network state. Setting the eigenvalues of the Jacobian matrix near the edge of stability is important.\",\n",
      "    \"In back-propagation with a fixed Jacobian matrix, as time propagates, back-propagation starting from a gradient vector g and from g + δv diverge by δJ^n v after n steps. The deviation size becomes exponentially small when the eigenvalue of the Jacobian matrix is greater than 1, and exponentially large when it is less than 1.\",\n",
      "    \"When the eigenvalue of the Jacobian matrix is greater than 1, the deviation size becomes exponentially small.\",\n",
      "    \"When the eigenvalue of the Jacobian matrix is less than 1, the deviation size becomes exponentially large.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': [{'question': 'What is the purpose of using a nonlinearity in a recurrent network?', 'answer': 'The purpose of using a nonlinearity in a recurrent network is to prevent explosion resulting from a large spectral radius. The derivative of the nonlinearity will approach zero on many time steps, thus helping to prevent the explosion.'}, {'question': 'What is a contractive map in the context of recurrent networks?', 'answer': 'In the context of recurrent networks, a linear map W is considered contractive if it always shrinks the state h as measured by the L2 norm. When the spectral radius of W is less than one, the mapping from h(t) to h(t+1) is contractive, causing a small change to become smaller after each time step.'}, {'question': 'What information does the Jacobian matrix provide in the context of back-propagation?', 'answer': 'The Jacobian matrix tells us how a small change of the state vector h(t) propagates one step forward or how the gradient on h(t+1) propagates one step backward during back-propagation.'}, {'question': 'Why can the Jacobian matrix and W have complex-valued eigenvalues and eigenvectors?', 'answer': 'The Jacobian matrix and W can have complex-valued eigenvalues and eigenvectors because neither of them need to be symmetric. This means they can have imaginary components corresponding to potentially oscillatory behavior when applied iteratively.'}, {'question': 'How does the use of a squashing nonlinearity like tanh affect the recurrent dynamics?', 'answer': 'The use of a squashing nonlinearity like tanh can cause the recurrent dynamics to become bounded. It can prevent the dynamics from exploding by keeping the magnitudes of the basis coefficients from growing exponentially when the matrix is multiplied by the vector.'}, {'question': 'What is the strategy of echo state networks?', 'answer': 'The strategy of echo state networks is to fix the weights to have a spectral radius such as 1, where information is carried forward through time but does not explode due to the stabilizing effect of saturating nonlinearities like tanh.'}, {'question': 'What technique can be used to set the weights in echo state networks to learn long-term dependencies?', 'answer': 'The technique of using back-propagation through time (BPTT) can be used to set the hidden-to-hidden recurrent weights in echo state networks, helping to learn long-term dependencies. In this setting, an initial spectral radius of 1.2 combined with a sparse initialization scheme performs well.'}, {'question': 'How can multiple time scales be incorporated in a model to handle long-term dependencies?', 'answer': 'One way to handle long-term dependencies is to design a model that operates at multiple time scales. Some parts of the model can operate at fine-grained time scales to handle small details, while other parts operate at coarse time scales to transfer information from the distant past to the present more efficiently.'}], 'answers': [{'answer_start': 1562, 'text': 'The purpose of using a nonlinearity in a recurrent network is to prevent explosion resulting from a large spectral radius. The derivative of the nonlinearity will approach zero on many time steps, thus helping to prevent the explosion.'}, {'answer_start': 780, 'text': 'In the context of recurrent networks, a linear map W is considered contractive if it always shrinks the state h as measured by the L2 norm. When the spectral radius of W is less than one, the mapping from h(t) to h(t+1) is contractive, causing a small change to become smaller after each time step.'}, {'answer_start': 1243, 'text': 'The Jacobian matrix tells us how a small change of the state vector h(t) propagates one step forward or how the gradient on h(t+1) propagates one step backward during back-propagation.'}, {'answer_start': 1504, 'text': 'The Jacobian matrix and W can have complex-valued eigenvalues and eigenvectors because neither of them need to be symmetric. This means they can have imaginary components corresponding to potentially oscillatory behavior when applied iteratively.'}, {'answer_start': 1788, 'text': 'The use of a squashing nonlinearity like tanh can cause the recurrent dynamics to become bounded. It can prevent the dynamics from exploding by keeping the magnitudes of the basis coefficients from growing exponentially when the matrix is multiplied by the vector.'}, {'answer_start': 1875, 'text': 'The strategy of echo state networks is to fix the weights to have a spectral radius such as 1, where information is carried forward through time but does not explode due to the stabilizing effect of saturating nonlinearities like tanh.'}, {'answer_start': 2158, 'text': 'The technique of using back-propagation through time (BPTT) can be used to set the hidden-to-hidden recurrent weights in echo state networks, helping to learn long-term dependencies. In this setting, an initial spectral radius of 1.2 combined with a sparse initialization scheme performs well.'}, {'answer_start': 2477, 'text': 'One way to handle long-term dependencies is to design a model that operates at multiple time scales. Some parts of the model can operate at fine-grained time scales to handle small details, while other parts operate at coarse time scales to transfer information from the distant past to the present more efficiently.'}]}\n",
      "{\"questions\": [\"What are some possible ways to obtain coarse time scales in recurrent networks?\", \"What is the purpose of adding skip connections through time?\", \"How can gradients diminish exponentially as a function of time delay?\", \"What are leaky units and how do they behave?\", \"What is the difference between skip connections through time and linear self-connections?\", \"What are two strategies for setting the time constants used by leaky units?\", \"What is the idea behind organizing the state of the RNN at multiple time-scales?\"], \"answers\": [\"Some possible ways to obtain coarse time scales in recurrent networks are adding skip connections through time, using leaky units, and removing connections.\", \"The purpose of adding skip connections through time is to obtain coarse time scales by adding direct connections from variables in the distant past to variables in the present.\", \"Gradients can diminish exponentially as a function of time delay by introducing recurrent connections with a time-delay of d, where gradients now diminish exponentially as a function of τd rather than τ.\", \"Leaky units are units with linear self-connections and a weight near one. They behave similar to running averages, where a weight near one allows the running average to remember information about the past for a long time, and a weight near zero rapidly discards information about the past.\", \"Skip connections through time involve adding direct connections from variables in the distant past to variables in the present, while linear self-connections involve having units with self-connections and a weight near one.\", \"Two strategies for setting the time constants used by leaky units are fixing them to constant values or making them free parameters and learning them.\", \"The idea behind organizing the state of the RNN at multiple time-scales is to allow information to flow more easily through long distances at slower time scales. This involves actively removing length-one connections and replacing them with longer connections in order to operate on a longer time scale.\"]\n",
      "{'questions': \n",
      "[\"What are two options for making recurrent units operate at different time scales?\", \n",
      "\"What is the difference between leaky units and gated RNNs?\",\n",
      "\"What is the core contribution of the LSTM model?\",\n",
      "\"What are some applications of the LSTM model?\"],\n",
      " 'answers': \n",
      " [\"Two options for making recurrent units operate at different time scales are making the recurrent units leaky, with different groups of units associated with different fixed time scales, or having explicit and discrete updates taking place at different times, with a different frequency for different groups of units.\",\n",
      " \"Leaky units and gated RNNs are both based on the idea of creating paths through time with derivatives that neither vanish nor explode. However, leaky units use manually chosen constants or parameters to accomplish this, while gated RNNs generalize the concept to connection weights that may change at each time step.\",\n",
      " \"The core contribution of the LSTM model is the introduction of self-loops to produce paths where the gradient can flow for long durations. The weight on this self-loop is conditioned on the context, allowing the time scale of integration to be changed dynamically.\",\n",
      " \"Some applications of the LSTM model include unconstrained handwriting recognition, handwriting generation, image captioning, parsing, machine translation, and speech recognition.\"] }\n",
      "{'questions': [{'question': 'What are LSTM cells?', 'answers': ['LSTM cells are components of LSTM recurrent networks that have an internal recurrence and control the flow of information through a system of gating units.', 'LSTM cells are units in LSTM recurrent networks that have more parameters and a system of gating units to control the flow of information.', 'LSTM cells have the same inputs and outputs as ordinary recurrent networks but have more parameters and a system of gating units.']}, {'question': 'What is the most important component of LSTM cells?', 'answers': ['The most important component of LSTM cells is the state unit s( )t.', 'The most important component of LSTM cells is the state unit s( )t that has a linear self-loop.', 'The most important component of LSTM cells is the state unit s( )t that has a linear self-loop controlled by a forget gate unit.']}, {'question': 'How is the self-loop weight controlled?', 'answers': ['The self-loop weight is controlled by a forget gate unit f( )t.', 'The self-loop weight is controlled by a forget gate unit f( )t that sets the weight to a value between 0 and 1.', 'The self-loop weight is controlled by a forget gate unit f( )t using a sigmoid unit to obtain a value between 0 and 1.']}, {'question': 'How is the internal state of the LSTM cell updated?', 'answers': ['The internal state of the LSTM cell is updated using the self-loop weight f ( )t i.', 'The internal state of the LSTM cell is updated using the self-loop weight f ( )t i and the current input vector and hidden layer vector.', 'The internal state of the LSTM cell is updated using the self-loop weight f ( )t i and the current input vector, hidden layer vector, and biases, input weights, and recurrent weights.']}, {'question': 'What is the role of the output gate?', 'answers': ['The output gate controls the flow of information from the LSTM cell.', 'The output gate can shut off the output of the LSTM cell.', 'The output gate uses a sigmoid unit for gating.']}, {'question': 'What are GRUs?', 'answers': ['GRUs are gated recurrent units that are an alternative to LSTM.', 'GRUs are units in gated RNNs that control the forgetting factor and the decision to update the state unit.', 'GRUs are units in gated RNNs that have update and reset gates to control the state unit.']}, {'question': 'What are the main differences between GRUs and LSTM?', 'answers': ['The main difference is that GRUs have a single gating unit that controls both the forgetting factor and the decision to update the state unit.', 'GRUs have update and reset gates that are controlled by a single gating unit.', 'The key difference is that GRUs have a simpler architecture compared to LSTM.']}, {'question': 'What are the update equations for GRUs?', 'answers': ['The update equations for GRUs are h( )t i = u(  h( t −i ', 'The update equations for GRUs are h( )t i = u(  h( t −j', 'The update equations for GURs are h( )t i = u(  h( t u stands for \"update\" gate and r for \"reset\" gate.']}, {'question': 'How are the values of the update and reset gates defined?', 'answers': ['The values of the update and reset gates are defined based on the input and hidden layer vectors.', 'The values of the update and reset gates are defined using biases, input weights, and recurrent weights.', 'The values of the update and reset gates are defined using sigmoid units to obtain values between 0 and 1.']}, {'question': 'What are some alternatives to LSTM?', 'answers': ['GRUs (gated recurrent units) are alternative architectures to LSTM.', 'There are variants and alternatives to LSTM, such as GRUs.', 'Some alternatives to LSTM include GRUs and other gated RNNs.']}, {'question': 'What have LSTM networks been shown to be good at?', 'answers': ['LSTM networks have been shown to learn long-term dependencies more easily than simple recurrent architectures.', 'LSTM networks excel in learning long-term dependencies.', 'LSTM networks have superior performance in handling long-term dependencies.']}, {'question': 'What is the significance of the recent work on gated RNNs?', 'answers': ['The recent work on gated RNNs provides answers to questions about which parts of the LSTM architecture are necessary and explores successful alternatives.', 'The recent work on gated RNNs helps understand the necessity of different components in the LSTM architecture and suggests successful alternatives.', 'The recent work on gated RNNs addresses questions regarding the necessity of LSTM architecture components and presents successful alternatives.']}, {'question': 'What is the key difference between GRUs and LSTM?', 'answers': ['The key difference between GRUs and LSTM is that GRUs have a simpler architecture compared to LSTM.', 'The main difference is that GRUs have a single gating unit that controls both the forgetting factor and the decision to update the state unit.']}, {'question': 'What are some advantages of LSTM networks?', 'answers': ['LSTM networks have been shown to learn long-term dependencies more easily than simple recurrent architectures.', 'LSTM networks have superior performance in handling long-term dependencies.', 'LSTM networks excel in learning long-term dependencies and have achieved state-of-the-art performance in various tasks.']}], 'answers': []}\n",
      "{'questions': [{'question': 'What are the reset and update gates in the LSTM architecture used for?', 'answers': ['The reset and update gates in the LSTM architecture are used to control which parts of the state vector are ignored or copied for the computation of the next target state.']}, \n",
      "{'question': 'What are some possible variants of the LSTM architecture?', 'answers': ['Some possible variants of the LSTM architecture include sharing the reset gate output across multiple hidden units and combining global and local control using a global gate and a local gate.']}, \n",
      "{'question': 'According to investigations, which architectural variant of the LSTM and GRU performed the best in a wide range of tasks?', 'answers': ['No architectural variant was found to clearly beat both the LSTM and GRU in a wide range of tasks.']}, \n",
      "{'question': 'What is the key ingredient in the LSTM architecture according to Jozefowicz et al.?', 'answers': ['The key ingredient in the LSTM architecture according to Jozefowicz et al. is the forget gate.']}, \n",
      "{'question': 'What practice advocated by Gers et al. makes the LSTM as strong as the best explored architectural variants?', 'answers': ['Adding a bias of 1 to the LSTM forget gate is a practice advocated by Gers et al. that makes the LSTM as strong as the best explored architectural variants.']}, \n",
      "{'question': 'What are the problems that occur when optimizing RNNs over many time steps?', 'answers': ['The problems that occur when optimizing RNNs over many time steps include vanishing and exploding gradients.']}, \n",
      "{'question': 'What is the idea proposed by Martens and Sutskever regarding second derivatives in optimization algorithms?', 'answers': ['The idea proposed by Martens and Sutskever is that second derivatives may vanish at the same time as first derivatives vanish, which can affect second-order optimization algorithms.']}, \n",
      "{'question': 'What are some drawbacks of second-order optimization methods?', 'answers': ['Some drawbacks of second-order optimization methods include high computational cost, the need for a large minibatch, and a tendency to be attracted to saddle points.']}, \n",
      "{'question': 'What simpler methods did Sutskever et al. find to achieve similar results as second-order optimization methods?', 'answers': ['Sutskever et al. found that simpler methods such as Nesterov momentum with careful initialization could achieve similar results as second-order optimization methods.']}, \n",
      "{'question': 'What approach is often used to optimize LSTMs?', 'answers': ['The approach to optimize LSTMs often involves simply using stochastic gradient descent (SGD) with or without momentum.']}, \n",
      "{'question': 'What is clipping gradients used for in the context of RNN optimization?', 'answers': ['Clipping gradients is used to prevent parameter updates that are too large and could cause the objective function to increase.']}, \n",
      "{'question': 'Why is it important to choose a small enough update in gradient descent for RNN optimization?', 'answers': ['Choosing a small enough update in gradient descent is important to avoid traversing regions of the objective function with upward curvature, which could undo previous progress.']}, \n",
      "{'question': 'What does the landscape of the objective function look like in the context of strongly nonlinear functions computed by a recurrent net?', 'answers': ['The landscape of the objective function in the context of strongly nonlinear functions computed by a recurrent net consists of cliﬀs, which are wide and rather flat regions separated by tiny regions where the objective function changes quickly.']}]}\n",
      "\n",
      "Note: The line breaks and incomplete formulas in the text were not specified in order to generate meaningful questions and answers.\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the purpose of gradient clipping in recurrent networks?\",\n",
      "    \"How does gradient clipping prevent parameters from being propelled away from steep regions?\",\n",
      "    \"What are some methods for clipping the gradient in recurrent networks?\",\n",
      "    \"What is the advantage of clipping the norm of the gradient with a single scaling factor?\",\n",
      "    \"What happens when the gradient magnitude is above a threshold?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The purpose of gradient clipping in recurrent networks is to prevent parameters from being propelled away from steep regions and to avoid detrimental steps when the gradient explodes.\",\n",
      "    \"Gradient clipping restricts the step size so that it cannot be propelled away from steep regions near the solution, thus preventing parameters from moving too far.\",\n",
      "    \"There are two methods for clipping the gradient in recurrent networks: clipping the parameter gradient from a minibatch just before the parameter update, and clipping the norm of the gradient element-wise just before the parameter update.\",\n",
      "    \"Clipping the norm of the gradient with a single scaling factor guarantees that each step is still in the gradient direction.\",\n",
      "    \"When the gradient magnitude is above a threshold, a random step of size v can be taken, which will typically move away from the numerically unstable configuration.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': [{'question': 'What does gradient clipping help with?'},\n",
      "  {'question': 'How can long-term dependencies be captured in a recurrent architecture?'},\n",
      "  {'question': 'What is the purpose of the regularizer proposed by Pascanu et al.?'},\n",
      "  {'question': 'Why is gradient clipping important in maintaining RNN dynamics?'},\n",
      "  {'question': 'What is a key weakness of the approach discussed in the text?'},\n",
      "  {'question': 'What kind of knowledge do neural networks struggle to memorize?'},\n",
      "  {'question': 'According to Graves et al., why do neural networks struggle to memorize facts?'},\n",
      "  {'question': 'What did Graves hypothesize about the lack of explicit memory in neural networks?'}],\n",
      " 'answers': [{'answer': 'Gradient clipping helps to deal with exploding gradients.', 'context': '10.11.2 Regularizing to Encourage Information Flow  Gradient clipping helps to deal with exploding gradients, but it does not help with vanishing gradients.'},\n",
      "  {'answer': 'By creating paths in the computational graph with a product of gradients near 1.', 'context': 'To address vanishing gradients and better capture long-term dependencies, we discussed the idea of creating paths in the computational graph of the unfolded recurrent architecture along which the product of gradients associated with arcs is near 1.'},\n",
      "  {'answer': 'To encourage \"information flow\" and maintain magnitude of the gradient vector.', 'context': 'Another idea is to regularize or constrain the parameters so as to encourage “information ﬂow.” With this objective, Pascanu et al. ( ) propose the following regularizer: ( h ( )t L) ∂h( )t | ∇  ||∇h( )t L  | |   t ∂h( −  1)  |      Ω =  t   ...'},\n",
      "  {'answer': 'Gradient clipping is important to keep the RNN dynamics on the edge of explosive gradients.', 'context': 'Because it keeps the RNN dynamics on the edge of explosive gradients, the gradient clipping is particularly important. Without gradient clipping, gradient explosion prevents learning from succeeding.'},\n",
      "  {'answer': 'Not as effective as the LSTM for tasks where data is abundant, such as language modeling.', 'context': 'A key weakness of this approach is that it is not as eﬀective as the LSTM for tasks where data is abundant, such as language modeling.'},\n",
      "  {'answer': 'Explicit knowledge or facts.', 'context': 'Neural networks excel at storing implicit knowledge. However, they struggle to memorize facts.'},\n",
      "  {'answer': 'Neural networks lack an equivalent working memory system.', 'context': 'Graves hypothesized that this is because neural networks lack the equivalent of the working memory system that allows human beings to explicitly hold and manipulate pieces'},\n",
      "  {'answer': 'Neural networks lack an equivalent working memory system that allows explicit holding and manipulation of pieces of knowledge.', 'context': 'Graves hypothesized that this is because neural networks lack the equivalent of the working memory system that allows human beings to explicitly hold and manipulate pieces of knowledge.'}]}\n",
      "{'questions': [\n",
      "    'What is the purpose of memory networks?',\n",
      "    'What is the difference between memory cells in LSTMs/GRUs and memory cells in NTMs?',\n",
      "    'Why are vector-valued memory cells used in NTMs?',\n",
      "    'What is content-based addressing in NTMs?',\n",
      "    'What is location-based addressing in NTMs?'\n",
      "],\n",
      " 'answers': [\n",
      "    'The purpose of memory networks is to enable systems to store and retrieve specific facts and reason with them.',\n",
      "    'The difference is that memory networks output an internal state that chooses which cell to read from or write to, similar to how digital computers access specific memory addresses.',\n",
      "    'Vector-valued memory cells are used in NTMs to offset the increased cost of accessing memory cells and to enable content-based addressing.',\n",
      "    'Content-based addressing in NTMs allows for the retrieval of a complete vector-valued memory if there is a pattern that matches some of its elements.',\n",
      "    'Location-based addressing in NTMs does not consider the content of the memory and is based on specific memory locations.'\n",
      "]}\n",
      "{'questions': ['What is the advantage of location-based addressing in memory cells?',\n",
      "  'What is the role of explicit memory in a recurrent network?',\n",
      "  'Why can explicit memory models learn tasks that ordinary RNNs or LSTM RNNs cannot learn?',\n",
      "  'How can the memory addressing coefficients be interpreted?',\n",
      "  'What is the similarity between the attention mechanism and memory addressing?',\n",
      "  'What is the purpose of recurrent neural networks?',\n",
      "  'What is the focus of this discussion about deep learning?'],\n",
      " 'answers': ['The advantage of location-based addressing in memory cells is that it allows the information it contains to be propagated forward and the gradients to be propagated backward without vanishing or exploding.',\n",
      "  'Explicit memory in a recurrent network is coupled with a \"task neural network\" and allows for reading from or writing to specific memory addresses.',\n",
      "  'Explicit memory models can learn tasks that ordinary RNNs or LSTM RNNs cannot learn because information and gradients can be propagated for very long durations.',\n",
      "  'The memory addressing coefficients can be interpreted as probabilities and can be used for stochastic reading of just one memory cell.',\n",
      "  'The memory addressing mechanism is identical to the attention mechanism previously introduced in the context of machine translation.',\n",
      "  'The purpose of recurrent neural networks is to extend deep learning to sequential data.',\n",
      "  'The focus of this discussion about deep learning is on how to choose and use recurrent neural networks and apply them to real-world tasks.']}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What are convolutional networks?\",\n",
      "    \"What is the mathematical operation used in convolutional networks?\",\n",
      "    \"What is the purpose of using convolution in a neural network?\",\n",
      "    \"What is pooling in convolutional networks?\",\n",
      "    \"What are some variants of the convolution function used in practice?\",\n",
      "    \"How can convolution be applied to different kinds of data?\",\n",
      "    \"How can convolution be made more efficient?\",\n",
      "    \"What role do convolutional networks play in the history of deep learning?\",\n",
      "    \"What does this chapter not address regarding convolutional network architecture?\",\n",
      "    \"Why is it impractical to describe the best convolutional network architecture in print?\",\n",
      "    \"What are the building blocks of the best convolutional network architectures?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Convolutional networks, also known as convolutional neural networks or CNNs, are a specialized kind of neural network for processing data with a known, grid-like topology.\",\n",
      "    \"The mathematical operation used in convolutional networks is convolution.\",\n",
      "    \"Convolution is used in a neural network to replace general matrix multiplication in at least one of its layers.\",\n",
      "    \"Pooling is an operation used in convolutional networks to reduce the spatial dimensionality of the input, typically by taking the maximum or average value within each pooling region.\",\n",
      "    \"There are several variants of the convolution function used in practice for neural networks.\",\n",
      "    \"Convolution can be applied to data with different numbers of dimensions, such as 1D grid data (e.g., time-series) or 2D grid data (e.g., images).\",\n",
      "    \"Convolution can be made more efficient by using techniques such as sparse convolution, fast Fourier transform (FFT), or parallelization.\",\n",
      "    \"Convolutional networks are an example of neuroscientific principles influencing deep learning. They have been successful in practical applications and have played a major role in the history of deep learning.\",\n",
      "    \"This chapter does not address how to choose the architecture of a convolutional network.\",\n",
      "    \"It is impractical to describe the best convolutional network architecture in print because research into convolutional network architectures progresses rapidly and new best architectures are announced frequently.\",\n",
      "    \"The best convolutional network architectures consist of the building blocks described in this chapter.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': [{'question': 'What are the first and second arguments in convolutional network terminology?', 'category': 'Context', 'context': 'and may be used for other purposes besides taking weighted averages.  0  In convolutional network terminology, the ﬁrst argument (in this example, the function x) to the convolution is often referred to as the input and the second  331  CHAPTER 9. CONVOLUTIONAL NETWORKS  argument (in this example, the function w) as the kernel.'}, {'question': 'What is the output of a convolutional network referred to as?', 'category': 'Context', 'context': 'In convolutional network terminology, the output is sometimes referred to as the feature map.'}, {'question': 'How is discrete convolution defined?', 'category': 'Context', 'context': 'If we now assume that x and w are deﬁned only on integer t, we can deﬁne the discrete convolution:  ( ) = ( s t  x w t  )( ) =  ∗  ∞  a=   −∞  ( ) ( x a w t  ) a  −  (9.3)'}, {'question': 'What are tensors in the context of machine learning?', 'category': 'Context', 'context': 'In machine learning applications, the input is usually a multidimensional array of data and the kernel is usually a multidimensional array of parameters that are adapted by the learning algorithm. We will refer to these multidimensional arrays as tensors.'}, {'question': 'Why do neural network libraries call the cross-correlation function convolution?', 'category': 'Context', 'context': 'Many machine learning libraries implement cross-correlation but call it convolution. In this text we will follow this convention of calling both operations convolution, and specify whether we mean to ﬂip the kernel or not in contexts where kernel ﬂipping is relevant.'}, {'question': 'What is the difference between discrete convolution and cross-correlation?', 'category': 'Context', 'context': 'The difference between discrete convolution and cross-correlation is that cross-correlation does not flip the kernel, whereas convolution does flip the kernel.'}, {'question': 'What is the commutative property of convolution?', 'category': 'Context', 'context': 'The commutative property of convolution states that the order of the input and the kernel can be reversed without changing the result.'}, {'question': 'What is the main difference between convolution and cross-correlation in a neural network implementation?', 'category': 'Context', 'context': 'The main difference between convolution and cross-correlation in a neural network implementation is the flipping of the kernel. Convolution flips the kernel, while cross-correlation does not.'}, {'question': 'How is discrete convolution represented in matrix form?', 'category': 'Context', 'context': 'Discrete convolution can be viewed as multiplication by a matrix. However, the matrix has several entries constrained to be equal to other entries.'}], 'answers': [{'answer': 'The first argument is referred to as the input and the second argument is referred to as the kernel.', 'category': 'Context'}, {'answer': 'The output is referred to as the feature map.', 'category': 'Context'}, {'answer': '(f * g)(s) = ∑a=−∞^∞(x * a w)(s - a)', 'category': 'Context'}, {'answer': 'Tensors are multidimensional arrays of data and parameters in machine learning.', 'category': 'Context'}, {'answer': 'Neural network libraries call the cross-correlation function convolution for simplicity and convention.', 'category': 'Context'}, {'answer': 'Discrete convolution flips the kernel, while cross-correlation does not.', 'category': 'Context'}, {'answer': 'The commutative property of convolution means the order of inputs and kernel can be reversed without changing the result.', 'category': 'Context'}, {'answer': 'The main difference is that convolution flips the kernel, while cross-correlation does not.', 'category': 'Context'}, {'answer': 'Discrete convolution can be represented as multiplication by a matrix with constrained entries.', 'category': 'Context'}]}\n",
      "{\"questions\": [\"What are the three important ideas leveraged by convolution in machine learning systems?\", \"What is the purpose of making the kernel smaller than the input in convolutional networks?\", \"What does parameter sharing refer to in convolutional networks?\"], \"answers\": [\"The three important ideas leveraged by convolution in machine learning systems are sparse interactions, parameter sharing, and equivariant representations.\", \"Making the kernel smaller than the input in convolutional networks allows for sparse interactions or sparse connectivity, reducing the number of parameters and improving memory and computational efficiency.\", \"Parameter sharing in convolutional networks refers to using the same parameter for more than one function in the model, where each element of the weight matrix is used multiple times when computing the output of a layer.\"] }\n",
      "{'questions': [{'question': 'What is parameter sharing in a convolutional neural net?', 'answer': 'In a convolutional neural net, parameter sharing means that instead of learning a separate set of parameters for each position of the input, the same set of parameters is used at every position of the input.'}, {'question': 'What is the advantage of parameter sharing in convolutional neural nets?', 'answer': 'The advantage of parameter sharing in convolutional neural nets is that it significantly reduces the storage requirements of the model.'}, {'question': 'What is the effect of parameter sharing on the runtime of forward propagation?', 'answer': 'Parameter sharing does not affect the runtime of forward propagation, it is still O(k), where k is the number of parameters.'}, {'question': 'What is the concept of equivariance in convolution?', 'answer': 'In the case of convolution, equivariance means that if the input changes, the output changes in the same way.'}], 'answers': [{'answer_start': 139, 'text': 'In a convolutional neural net, parameter sharing means that instead of learning a separate set of parameters for each position of the input, the same set of parameters is used at every position of the input.'}, {'answer_start': 365, 'text': 'The advantage of parameter sharing in convolutional neural nets is that it significantly reduces the storage requirements of the model.'}, {'answer_start': 433, 'text': 'Parameter sharing does not affect the runtime of forward propagation, it is still O(k), where k is the number of parameters.'}, {'answer_start': 686, 'text': 'In the case of convolution, equivariance means that if the input changes, the output changes in the same way.'}]}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the purpose of the function g in image processing?\",\n",
      "    \"How does convolution represent a timeline when processing time series data?\",\n",
      "    \"What does convolution create in images?\",\n",
      "    \"Why is parameter sharing useful in convolutional networks?\",\n",
      "    \"What transformations is convolution not naturally equivariant to?\",\n",
      "    \"What are the three stages of a typical layer in a convolutional network?\",\n",
      "    \"What is the purpose of the pooling function in a convolutional network?\",\n",
      "    \"What is an advantage of using convolution over matrix multiplication for certain transformations?\",\n",
      "    \"How does the efficiency of convolution compare to matrix multiplication for edge detection?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The function g shifts every pixel of I one unit to the right.\",\n",
      "    \"Convolution produces a timeline that shows when different features appear in the input.\",\n",
      "    \"Convolution creates a 2-D map of where certain features appear in the input.\",\n",
      "    \"Parameter sharing is useful for detecting common features that appear throughout an image.\",\n",
      "    \"Convolution is not naturally equivariant to changes in the scale or rotation of an image.\",\n",
      "    \"The three stages of a typical layer in a convolutional network are: performing convolutions in parallel to produce linear activations, running each linear activation through a nonlinear activation function, and using a pooling function to modify the output.\",\n",
      "    \"The pooling function replaces the output at a certain location with a summary statistic of the nearby outputs.\",\n",
      "    \"Convolution is more efficient than matrix multiplication for representing certain transformations because it requires significantly fewer floating-point operations.\",\n",
      "    \"Convolution is approximately 60,000 times more computationally efficient than matrix multiplication for edge detection.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the purpose of convolution × in a convolutional neural network layer?\",\n",
      "    \"What is the difference between complex layer terminology and simple layer terminology in describing convolutional neural network layers?\",\n",
      "    \"What are some popular pooling functions?\",\n",
      "    \"What does it mean for a representation to be invariant to translation?\",\n",
      "    \"How does pooling improve the statistical efficiency of a network?\",\n",
      "    \"How does pooling reduce the computational efficiency of a network?\",\n",
      "    \"Why is pooling essential for handling inputs of varying size in many tasks?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Convolution × is an efficient way of describing transformations that apply the same linear transformation of a small, local region across the entire input.\",\n",
      "    \"Complex layer terminology views the convolutional net as a small number of relatively complex layers, while simple layer terminology views it as a larger number of simple layers.\",\n",
      "    \"Some popular pooling functions include the maximum output within a rectangular neighborhood, the average of a rectangular neighborhood, the L2 norm of a rectangular neighborhood, or a weighted average based on the distance from the central pixel.\",\n",
      "    \"Invariance to translation means that the values of most of the pooled outputs do not change when the input is translated by a small amount.\",\n",
      "    \"Pooling adds an infinitely strong prior that the function the layer learns must be invariant to small translations, which can greatly improve the statistical efficiency of the network.\",\n",
      "    \"Pooling can reduce the computational efficiency of a network by reducing the number of inputs the next layer needs to process.\",\n",
      "    \"Pooling is essential for handling inputs of varying size because it allows the input to the classification layer to have a fixed size.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': \n",
      "  ['What is the purpose of the final pooling layer in a convolutional network?', \n",
      "   'What does max pooling introduce invariance to?', \n",
      "   'How can a pooling unit become invariant to transformations of the input?', \n",
      "   'How does pooling with downsampling reduce the computational burden on the next layer?', \n",
      "   'What are some approaches to pooling in convolutional networks?'], \n",
      " 'answers': \n",
      "  ['The purpose of the final pooling layer is to always provide the same number of summary statistics regardless of the input size, ensuring consistent output for the classification layer.',\n",
      "   'Max pooling introduces invariance to the exact location of the maximum value in a neighborhood, making the pooling units only sensitive to the maximum value.',\n",
      "   'A pooling unit can become invariant to transformations of the input by pooling over multiple features learned with separate parameters, allowing it to respond similarly regardless of the specific detector unit activated.',\n",
      "   'Pooling with downsampling reduces the representation size by a factor of two, resulting in lower computational and statistical burden for the next layer.',\n",
      "   'Approaches to pooling in convolutional networks include dynamically pooling features together based on clustering algorithms, learning a single pooling structure applied to all images, and using theoretical guidance for choosing appropriate pooling methods.'] }\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the difference between a weak prior and a strong prior?\",\n",
      "    \"What does it mean for a prior to have high entropy?\",\n",
      "    \"What does it mean for a prior to have low entropy?\",\n",
      "    \"What is an infinitely strong prior?\",\n",
      "    \"How can we think of a convolutional net in terms of priors?\",\n",
      "    \"What does it mean for a unit in a convolutional net to be equivariant to translation?\",\n",
      "    \"What is the role of pooling in a convolutional net?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"A weak prior is a prior distribution with high entropy, allowing the data to move the parameters freely. A strong prior has low entropy and plays a more active role in determining the parameter values.\",\n",
      "    \"A prior with high entropy means that the probability density is spread out, allowing for more variability in the parameters.\",\n",
      "    \"A prior with low entropy means that the probability density is concentrated, allowing for less variability in the parameters.\",\n",
      "    \"An infinitely strong prior places zero probability on certain parameters, making those parameter values completely forbidden regardless of the data support.\",\n",
      "    \"A convolutional net can be thought of as having an infinitely strong prior probability distribution over its parameters, enforcing specific constraints on their values.\",\n",
      "    \"For a unit in a convolutional net to be equivariant to translation means that its function remains the same when the input is translated spatially.\",\n",
      "    \"Pooling in a convolutional net introduces an infinitely strong prior that each unit should be invariant to small translations, allowing for translation invariance in the output feature maps.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': [{'question': 'Why is using a connected net with an infinitely strong prior computationally wasteful?', 'answer': 'Using a connected net with an infinitely strong prior would be extremely computationally wasteful because it would require a large number of computations to process the input data.'}, {'question': 'How can thinking of a convolutional net as a fully connected net with an infinitely strong prior provide insights into how convolutional nets work?', 'answer': 'Thinking of a convolutional net as a fully connected net with an infinitely strong prior can provide insights into how convolutional nets work by understanding the assumptions made by convolution and pooling, and their impact on underfitting and preserving spatial information.'}, {'question': 'What can cause underfitting in convolutional nets?', 'answer': 'Convolution and pooling can cause underfitting in convolutional nets if the assumptions made by the prior are not reasonably accurate.'}, {'question': 'Why do some convolutional network architectures use pooling on some channels but not on others?', 'answer': 'Some convolutional network architectures use pooling on some channels but not on others in order to balance the need for highly invariant features with the need for features that will not underfit if the translation invariance prior is incorrect.'}, {'question': 'In which cases might the prior imposed by convolution be inappropriate?', 'answer': 'The prior imposed by convolution may be inappropriate when a task involves incorporating information from very distant locations in the input.'}, {'question': 'Why should convolutional models only be compared to other convolutional models in benchmarks?', 'answer': 'Convolutional models should only be compared to other convolutional models in benchmarks to evaluate their statistical learning performance, as models that do not use convolution may have different assumptions and abilities.'}, {'question': 'What is the difference between the standard discrete convolution operation and the convolution used in neural networks?', 'answer': 'The convolution used in neural networks consists of many applications of convolution in parallel and is designed to extract many kinds of features at many spatial locations, unlike the standard discrete convolution operation.'}, {'question': 'Why is the input to a convolutional network usually a grid of vector-valued observations?', 'answer': 'The input to a convolutional network is usually a grid of vector-valued observations, such as a color image with red, green, and blue intensities at each pixel, to capture multiple channels of information.'}, {'question': 'What is the format of the input and output of a convolutional network when working with images?', 'answer': 'When working with images, the input and output of a convolutional network are usually represented as 3-D tensors, with one index for the different channels and two indices for the spatial coordinates, although software implementations may use 4-D tensors for batching.'}, {'question': 'Why are the multi-channel operations in convolutional networks not guaranteed to be commutative?', 'answer': 'The multi-channel operations in convolutional networks are not guaranteed to be commutative because they depend on the number of input and output channels, and the ordering of these channels affects the computation.'}], 'answers': [{'question': 'Why is using a connected net with an infinitely strong prior computationally wasteful?', 'answer': 'Using a connected net with an infinitely strong prior would be extremely computationally wasteful because it would require a large number of computations to process the input data.'}, {'question': 'How can thinking of a convolutional net as a fully connected net with an infinitely strong prior provide insights into how convolutional nets work?', 'answer': 'Thinking of a convolutional net as a fully connected net with an infinitely strong prior can provide insights into how convolutional nets work by understanding the assumptions made by convolution and pooling, and their impact on underfitting and preserving spatial information.'}, {'question': 'What can cause underfitting in convolutional nets?', 'answer': 'Convolution and pooling can cause underfitting in convolutional nets if the assumptions made by the prior are not reasonably accurate.'}, {'question': 'Why do some convolutional network architectures use pooling on some channels but not on others?', 'answer': 'Some convolutional network architectures use pooling on some channels but not on others in order to balance the need for highly invariant features with the need for features that will not underfit if the translation invariance prior is incorrect.'}, {'question': 'In which cases might the prior imposed by convolution be inappropriate?', 'answer': 'The prior imposed by convolution may be inappropriate when a task involves incorporating information from very distant locations in the input.'}, {'question': 'Why should convolutional models only be compared to other convolutional models in benchmarks?', 'answer': 'Convolutional models should only be compared to other convolutional models in benchmarks to evaluate their statistical learning performance, as models that do not use convolution may have different assumptions and abilities.'}, {'question': 'What is the difference between the standard discrete convolution operation and the convolution used in neural networks?', 'answer': 'The convolution used in neural networks consists of many applications of convolution in parallel and is designed to extract many kinds of features at many spatial locations, unlike the standard discrete convolution operation.'}, {'question': 'Why is the input to a convolutional network usually a grid of vector-valued observations?', 'answer': 'The input to a convolutional network is usually a grid of vector-valued observations, such as a color image with red, green, and blue intensities at each pixel, to capture multiple channels of information.'}, {'question': 'What is the format of the input and output of a convolutional network when working with images?', 'answer': 'When working with images, the input and output of a convolutional network are usually represented as 3-D tensors, with one index for the different channels and two indices for the spatial coordinates, although software implementations may use 4-D tensors for batching.'}, {'question': 'Why are the multi-channel operations in convolutional networks not guaranteed to be commutative?', 'answer': 'The multi-channel operations in convolutional networks are not guaranteed to be commutative because they depend on the number of input and output channels, and the ordering of these channels affects the computation.'}]}\n",
      "{'questions': \n",
      "  [\n",
      "    {\n",
      "      'question': 'What is the purpose of downsampling in a convolutional network?',\n",
      "      'answers': ['The purpose of downsampling in a convolutional network is to reduce the computational cost and extract features less finely.']\n",
      "    },\n",
      "    {\n",
      "      'question': 'What is the stride in a downsampled convolution?',\n",
      "      'answers': ['The stride in a downsampled convolution is the number of pixels to be skipped in each direction in the output.']\n",
      "    },\n",
      "    {\n",
      "      'question': 'Why is zero padding important in convolutional network implementations?',\n",
      "      'answers': ['Zero padding is important in convolutional network implementations because it allows us to control the kernel width and the size of the output independently, preventing the shrinkage of the spatial extent of the network and limiting the expressive power of the network.']\n",
      "    },\n",
      "    {\n",
      "      'question': 'What are the three special cases of the zero-padding setting?',\n",
      "      'answers': ['The three special cases of the zero-padding setting are: valid convolution, same convolution, and full convolution.']\n",
      "    },\n",
      "    {\n",
      "      'question': 'What is the difference between valid convolution and same convolution?',\n",
      "      'answers': ['In valid convolution, no zero-padding is used, and the output size shrinks at each layer. In same convolution, just enough zero-padding is added to keep the size of the output equal to the size of the input.']\n",
      "    }\n",
      "  ],\n",
      " 'answers': \n",
      "  [\n",
      "    {'answer': 'The purpose of downsampling in a convolutional network is to reduce the computational cost and extract features less finely.'},\n",
      "    {'answer': 'The stride in a downsampled convolution is the number of pixels to be skipped in each direction in the output.'},\n",
      "    {'answer': 'Zero padding is important in convolutional network implementations because it allows us to control the kernel width and the size of the output independently, preventing the shrinkage of the spatial extent of the network and limiting the expressive power of the network.'},\n",
      "    {'answer': 'The three special cases of the zero-padding setting are: valid convolution, same convolution, and full convolution.'},\n",
      "    {'answer': 'In valid convolution, no zero-padding is used, and the output size shrinks at each layer. In same convolution, just enough zero-padding is added to keep the size of the output equal to the size of the input.'}\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the effect of using a stride greater than one in convolution?\",\n",
      "    \"Why is using downsampling with convolution computationally wasteful?\",\n",
      "    \"How can the rate of shrinking in a convolutional network be mitigated?\",\n",
      "    \"What is the purpose of zero padding in a convolutional network?\",\n",
      "    \"When would locally connected layers be more appropriate than convolution layers?\",\n",
      "    \"How can the connectivity in convolution or locally connected layers be further restricted?\",\n",
      "    \"What is the advantage of using tiled convolution?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Using a stride greater than one in convolution is mathematically equivalent to convolution with unit stride followed by downsampling.\",\n",
      "    \"Using downsampling with convolution is computationally wasteful because it computes many values that are then discarded.\",\n",
      "    \"The rate of shrinking in a convolutional network can be mitigated by using smaller kernels.\",\n",
      "    \"The purpose of zero padding in a convolutional network is to prevent the representation from shrinking with depth, allowing for an arbitrarily deep network.\",\n",
      "    \"Locally connected layers are more appropriate than convolution layers when each feature should be a function of a small part of space, but there is no need for the same feature to occur across all of space.\",\n",
      "    \"The connectivity in convolution or locally connected layers can be further restricted by constraining each output channel to be a function of only a subset of the input channels.\",\n",
      "    \"The advantage of using tiled convolution is that it offers a compromise between a convolutional layer and a locally connected layer, reducing the memory requirements while still allowing neighboring locations to have different filters.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is being compared in Figure 9.14?\",\n",
      "    \"What is the difference between a locally connected layer and a convolutional layer?\",\n",
      "    \"What does a fully connected layer resemble?\",\n",
      "    \"What is shown in Figure 9.15?\",\n",
      "    \"What is the difference between locally connected layers, tiled convolution, and standard convolution?\",\n",
      "    \"How can tiled convolution be algebraically defined?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Figure 9.14 compares local connections, convolution, and full connections.\",\n",
      "    \"The difference between a locally connected layer and a convolutional layer lies in how the parameters are shared. A locally connected layer has no parameter sharing, while a convolutional layer uses the same weights repeatedly across the entire input.\",\n",
      "    \"A fully connected layer resembles a locally connected layer in terms of each edge having its own parameter, but it does not have the restricted connectivity of a locally connected layer.\",\n",
      "    \"Figure 9.15 shows a convolutional network where the first two output channels are connected to only the first two input channels, and the second two output channels are connected to only the second two input channels.\",\n",
      "    \"The differences between locally connected layers, tiled convolution, and standard convolution lie in how they share parameters. Locally connected layers have no sharing, tiled convolution has different kernels for different locations in the output, and standard convolution applies the same kernel everywhere.\",\n",
      "    \"Tiled convolution can be algebraically defined by using a 6-D tensor, where output locations cycle through a set of t different choices of kernel stacks in each direction.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What are the operations necessary to implement a convolutional network?\", \n",
      "    \"What is the purpose of the transpose operation in a convolutional network?\", \n",
      "    \"What are the three operations needed to compute all the gradients in a convolutional network?\", \n",
      "    \"How is the backpropagation of error derivatives performed in a convolutional layer?\", \n",
      "    \"What is the role of the transpose convolution in a convolutional network?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The operations necessary to implement a convolutional network include convolution, backpropagation from output to weights, and backpropagation from output to inputs.\", \n",
      "    \"The transpose operation in a convolutional network is used to reconstruct the visible units from the hidden units.\", \n",
      "    \"The three operations needed to compute all the gradients in a convolutional network are convolution, backpropagation from output to weights, and backpropagation from output to inputs.\", \n",
      "    \"The backpropagation of error derivatives through a convolutional layer is performed by multiplying the transpose of the matrix defined by convolution.\", \n",
      "    \"The transpose convolution is necessary to construct convolutional versions of models like autoencoders, RBMs, and sparse coding.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the purpose of computing the gradient with respect to V?\",\n",
      "    \"How can we compute the gradient with respect to V?\",\n",
      "    \"What is the function h used for in autoencoder networks?\",\n",
      "    \"How can we train the autoencoder decoder?\",\n",
      "    \"How can we train the autoencoder encoder?\",\n",
      "    \"Why do we generally add a bias term in convolutional layers?\",\n",
      "    \"How can we share parameters among the biases in convolutional layers?\",\n",
      "    \"What is a structured output in convolutional networks?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The purpose of computing the gradient with respect to V is to back-propagate the error farther down.\",\n",
      "    \"The gradient with respect to V can be computed using the function (KG) i,j,k = h , , s ∂∂Vi,j,k ,(VK)J = ∑l,m s.t. smj1) += l(− × ∑n,p s.t. 1) += spk n(− × (9.12) (9.13) Kq,i,m,pGq,l,n.\",\n",
      "    \"In autoencoder networks, the function h is used to perform the transpose of the convolution operation in order to make the models convolutional.\",\n",
      "    \"To train the autoencoder decoder, we need to obtain the gradient with respect to K, which is given by g(HE, , s).\",\n",
      "    \"To train the autoencoder encoder, we need to obtain the gradient with respect to H, which is given by c(KE, , s).\",\n",
      "    \"A bias term is generally added in convolutional layers to apply a nonlinearity and account for differences in image statistics at different locations.\",\n",
      "    \"Parameters can be shared among the biases in convolutional layers by having one bias per channel of the output and sharing it across all locations within each convolution map.\",\n",
      "    \"A structured output in convolutional networks refers to a high-dimensional, structured object that is outputted by the network, such as a tensor that labels every pixel in an image or draws precise masks for individual objects.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the purpose of using pooling layers in convolutional networks?\",\n",
      "    \"How can convolutional networks be used for pixel-wise labeling of images?\",\n",
      "    \"What are some methods that can be used to further process the predictions obtained from pixel-wise labeling?\",\n",
      "    \"What types of data can be used with convolutional networks?\",\n",
      "    \"Why are convolutional networks suitable for processing inputs with varying spatial extents?\",\n",
      "    \"How does convolution relate to matrix multiplication?\",\n",
      "    \"What additional design steps are necessary when the network needs to produce a fixed-size output?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Pooling layers in convolutional networks are used to reduce the spatial dimensions of the network, either by using large stride pooling or emitting a lower-resolution grid of labels. Alternatively, pooling with unit stride can also be used.\",\n",
      "    \"One strategy for pixel-wise labeling of images is to produce an initial guess of the image labels and then refine this guess using the interactions between neighboring pixels. This can be achieved by using convolutions at each stage, sharing weights between the last layers of the deep net, which forms a recurrent network.\",\n",
      "    \"Methods such as graphical models can describe the probabilistic relationships between neighboring pixels and can be used to process the predictions obtained from pixel-wise labeling. The convolutional network can also be trained to maximize an approximation of the graphical model training objective.\",\n",
      "    \"Convolutional networks can handle data with multiple channels, where each channel represents the observation of a different quantity at some point in space or time. The data can have different dimensionalities and number of channels.\",\n",
      "    \"Convolutional networks are suitable for processing inputs with varying spatial extents because they can handle inputs of different sizes by applying the convolution operation a different number of times, and the output scales accordingly. This is in contrast to traditional neural networks based on matrix multiplication, which cannot handle inputs with varying spatial extents.\",\n",
      "    \"Convolution can be viewed as matrix multiplication, where the same convolution kernel induces a different size of a doubly block circulant matrix for each size of input.\",\n",
      "    \"When the network needs to produce a fixed-size output, additional design steps are necessary. For example, inserting a pooling layer with pooling regions that scale in size proportional to the input size can be used to obtain a fixed-size output. Other design choices depend on the specific requirements of the task.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What are some examples of data formats that can be used with convolutional networks?\",\n",
      "    \"Why does convolution not make sense for inputs that have variable size due to different kinds of observations?\",\n",
      "    \"How can convolution be made more efficient?\",\n",
      "    \"What is separable convolution?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Some examples of data formats that can be used with convolutional networks include 1-D audio waveform, 2-D audio data preprocessed with a Fourier transform, 3-D volumetric data like medical imaging scans, multi-channel skeleton animation data, color image data, and color video data.\",\n",
      "    \"Convolution does not make sense for inputs that have variable size due to different kinds of observations because the same weights cannot be convolved over features that correspond to different kinds of observations.\",\n",
      "    \"Convolution can be made more efficient by selecting an appropriate convolution algorithm. It can also be accelerated using parallel computation and exploiting resources effectively.\",\n",
      "    \"Separable convolution refers to a type of convolution where a d-dimensional kernel can be expressed as the outer product of d vectors, one vector per dimension. This allows for faster computation by performing one-dimensional convolutions instead of a single d-dimensional convolution.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': [{'question': 'What is an active area of research related to convolution?', 'answer': 'Devising faster ways of performing convolution or approximate convolution without harming the accuracy of the model.'}, {'question': 'Why is the output layer of a convolutional network usually relatively inexpensive?', 'answer': 'The output layer has a small number of features provided as input to this layer after passing through several layers of pooling.'}, {'question': 'What are the three basic strategies for obtaining convolution kernels without supervised training?', 'answer': 'The three basic strategies are: randomly initializing the kernels, designing them by hand to detect certain features, or learning the kernels with an unsupervised k-means clustering approach.'}, {'question': 'What does learning the features with an unsupervised criterion allow?', 'answer': 'Learning the features with an unsupervised criterion allows them to be determined separately from the classifier layer, and then the features can be extracted for the entire training set, essentially constructing a new training set for the last layer.'}, {'question': 'What is the advantage of using random filters in convolutional networks?', 'answer': 'Random filters can work surprisingly well in convolutional networks as they naturally become frequency selective and translation invariant.'}, {'question': 'What is an intermediate approach for learning the features in convolutional networks?', 'answer': 'An intermediate approach is to use greedy layer-wise pretraining, where each layer is trained in isolation and the features are extracted at each layer before training the next layer.'}, {'question': 'What is the advantage of training a model of a small patch in convolutional networks?', 'answer': 'Training a model of a small patch allows the parameters of the patch-based model to be used to define the kernels of a convolutional layer, enabling unsupervised learning without using convolution during the training process.'}], 'answers': [{'answer_start': 262, 'text': 'Devising faster ways of performing convolution or approximate convolution without harming the accuracy of the model.'}, {'answer_start': 882, 'text': 'The output layer has a small number of features provided as input to this layer after passing through several layers of pooling.'}, {'answer_start': 1015, 'text': 'The three basic strategies are: randomly initializing the kernels, designing them by hand to detect certain features, or learning the kernels with an unsupervised k-means clustering approach.'}, {'answer_start': 1165, 'text': 'Learning the features with an unsupervised criterion allows them to be determined separately from the classifier layer, and then the features can be extracted for the entire training set, essentially constructing a new training set for the last layer.'}, {'answer_start': 1520, 'text': 'Random filters can work surprisingly well in convolutional networks as they naturally become frequency selective and translation invariant.'}, {'answer_start': 1790, 'text': 'An intermediate approach is to use greedy layer-wise pretraining, where each layer is trained in isolation and the features are extracted at each layer before training the next layer.'}, {'answer_start': 2102, 'text': 'Training a model of a small patch allows the parameters of the patch-based model to be used to define the kernels of a convolutional layer, enabling unsupervised learning without using convolution during the training process.'}]}\n",
      "{'questions': [{'question': 'What were some of the benefits of unsupervised pretraining?'},\n",
      "              {'question': 'Who were the neurophysiologists that contributed to the understanding of the mammalian vision system?'},\n",
      "              {'question': 'What is the primary visual cortex called?'},\n",
      "              {'question': 'What are the three properties of V1 that convolutional networks are designed to capture?'}],\n",
      " 'answers': [{'answer': 'Unsupervised pretraining may offer some regularization relative to supervised training, or it may simply allow us to train much larger architectures due to the reduced computational cost of the learning rule.'},\n",
      "             {'answer': 'David Hubel and Torsten Wiesel'},\n",
      "             {'answer': 'V1 or the primary visual cortex'},\n",
      "             {'answer': '1. V1 is arranged in a spatial map. 2. V1 contains many simple cells. 3. V1 also contains many complex cells.'}]}\n",
      "{\n",
      "    \"questions\": [\n",
      "        \"What are some differences between convolutional networks and the mammalian vision system?\",\n",
      "        \"What is the basic strategy of detection and pooling in the visual system?\",\n",
      "        \"What are grandmother cells?\",\n",
      "        \"What is the inferotemporal cortex and how does it relate to convolutional networks?\",\n",
      "        \"How do convolutional networks compare to humans on object recognition tasks?\"\n",
      "    ],\n",
      "    \"answers\": [\n",
      "        \"Some differences between convolutional networks and the mammalian vision system include the low resolution of the human eye except for the fovea, the use of eye movements called saccades in humans to focus on salient parts of a scene, and the incorporation of attention mechanisms into deep learning models.\",\n",
      "        \"The basic strategy of the visual system is detection followed by pooling, which is repeatedly applied as we move deeper into the brain.\",\n",
      "        \"Grandmother cells are neurons that respond to a specific concept and are invariant to many transformations of the input. For example, a person may have a neuron that activates when seeing an image of their grandmother, regardless of various changes in the image.\",\n",
      "        \"The inferotemporal cortex (IT) is a brain area that receives information from V1, V2, and V4, and is similar to the last layer of features in a convolutional network. It can predict firing rates and perform similarly to humans on object recognition tasks.\",\n",
      "        \"Convolutional networks can predict IT firing rates and perform similarly to humans on object recognition tasks when only observing the first 100ms of feedforward activation.\"\n",
      "    ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What are some limitations of convolutional networks in comparison to the human visual system?\",\n",
      "    \"What are some additional functions of the human visual system beyond object recognition?\",\n",
      "    \"How is feedback from higher levels of the brain relevant to visual processing?\",\n",
      "    \"What is the difference between feedforward IT firing rates and convolutional network features?\",\n",
      "    \"What is the role of activation and pooling functions in the brain?\",\n",
      "    \"Describe the model of V1 that involves multiple quadratic filters for each neuron.\",\n",
      "    \"When did the use of back-propagation to train convolutional networks start?\",\n",
      "    \"What is the function of simple cells in a deep, nonlinear network?\",\n",
      "    \"What is the approach used to determine the weights of neurons in a biological neural network?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Some limitations of convolutional networks compared to the human visual system include the lack of integration with other senses, such as hearing, and the inability to understand entire scenes and process rich 3-D geometric information. Convolutional networks are purely visual and still in their infancy in terms of applying them to these problems.\",\n",
      "    \"The human visual system goes beyond object recognition and is able to understand entire scenes, including many objects and relationships between objects. It also processes rich 3-D geometric information necessary for our bodies to interface with the world.\",\n",
      "    \"Feedback from higher levels impacts brain areas like V1 and has been extensively explored in neural network models. However, it has not yet been proven to offer a significant improvement.\",\n",
      "    \"Feedforward IT firing rates capture similar information as convolutional network features, but the intermediate computations may be different. The brain likely uses different activation and pooling functions than those used in convolutional networks.\",\n",
      "    \"Activation and pooling functions in the brain are not well-characterized by a single linear filter response. A recent model of V1 suggests that it involves multiple quadratic filters for each neuron, indicating a continuum of behaviors ranging from simple to complex.\",\n",
      "    \"The model of V1 involves multiple quadratic filters for each neuron, indicating a continuum of behaviors ranging from simple to complex.\",\n",
      "    \"The use of back-propagation to train convolutional networks started with the development of modern convolutional networks in 1989.\",\n",
      "    \"Simple cells in a deep, nonlinear network play a role in analyzing the function of individual cells. Their responses are driven by a linear function, making them easier to analyze compared to other layers in the network.\",\n",
      "    \"To determine the weights of neurons in a biological neural network, an approach called reverse correlation is used. This involves putting an electrode in the neuron itself, displaying white noise images, and recording how each image causes the neuron to activate. A linear model can then be fitted to these responses to approximate the neuron's weights.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "\"questions\": [\n",
      "\"What is the purpose of reverse correlation in studying V1 cells?\",\n",
      "\"How is an image represented as a function of 2-D coordinates?\",\n",
      "\"What is the mathematical expression for the response of a simple cell to an image?\",\n",
      "\"What are the parameters that control the properties of the Gabor function?\",\n",
      "\"How does the Gaussian factor affect the response of a simple cell?\",\n",
      "\"How does the cosine factor affect the response of a simple cell?\",\n",
      "\"What is the cartoon view of a complex cell?\",\n",
      "\"When does a complex cell respond?\",\n",
      "\"What is the role of a quadrature pair in complex cells?\",\n",
      "\"What is the invariance property of a complex cell?\"\n",
      "],\n",
      "\"answers\": [\n",
      "\"Reverse correlation is used to study V1 cells and shows that most V1 cells have weights described by Gabor functions.\",\n",
      "\"An image can be represented as a function of 2-D coordinates, denoted as I(x, y).\",\n",
      "\"The response of a simple cell to an image is given by s = Σ Σ w(x, y) * I(x, y), where w(x, y) is a function of the location.\",\n",
      "\"The parameters that control the properties of the Gabor function are α, βx, βy, f, φ, x0, y0, and τ.\",\n",
      "\"The Gaussian factor in the Gabor function ensures that the simple cell responds only to values near the center of its receptive field.\",\n",
      "\"The cosine factor in the Gabor function controls how the simple cell responds to changing brightness along the x axis.\",\n",
      "\"A complex cell computes the L2 norm of the squared responses of two simple cells.\",\n",
      "\"A complex cell responds to high amplitude sinusoidal waves with a specific frequency and direction, regardless of the phase offset.\",\n",
      "\"A quadrature pair is formed by two simple cells with the same parameters except for a phase offset, and a complex cell responds to translations of the image in the direction defined by the phase offset.\",\n",
      "\"A complex cell is invariant to small translations of the image in a specific direction or to negating the image.\"\n",
      "]\n",
      "}\n",
      "{\n",
      "\"questions\": [\n",
      "\"What is the comparison made between machine learning models and V1 in the context of this passage?\",\n",
      "\"What kind of features do most deep learning algorithms learn in their first layer?\",\n",
      "\"What did Olshausen and Field's study in 1996 reveal about unsupervised learning algorithms?\",\n",
      "\"Why is it difficult to determine which learning algorithm is the 'right' model of the brain based on the features it learns?\",\n",
      "\"What is the role of convolutional networks in the history of deep learning?\",\n",
      "\"What commercial applications have convolutional networks been used for?\",\n",
      "\"When did the current interest in deep learning start?\",\n",
      "\"What was the impact of Krizhevsky et al.'s 2012 win in the ImageNet object recognition challenge?\"\n",
      "],\n",
      "\"answers\": [\n",
      "\"The comparison is between the features learned by machine learning models and those employed by V1, the primary visual cortex.\",\n",
      "\"Most deep learning algorithms learn features that detect edges or specific colors of edges in their first layer.\",\n",
      "\"Olshausen and Field's study in 1996 showed that the unsupervised learning algorithm, sparse coding, learns features with receptive fields similar to those of simple cells.\",\n",
      "\"It is difficult to determine the 'right' model of the brain based on the features a learning algorithm learns because many different algorithms can learn edge detectors and the features are part of the statistical structure of natural images.\",\n",
      "\"Convolutional networks have played an important role in the history of deep learning by applying insights obtained from studying the brain to machine learning applications. They were among the first successful deep models, solved important commercial applications, and still remain at the forefront of commercial applications of deep learning.\",\n",
      "\"Convolutional networks have been used for applications such as reading checks, OCR, and handwriting recognition.\",\n",
      "\"The current interest in deep learning started when Krizhevsky et al. won the ImageNet object recognition challenge in 2012.\",\n",
      "\"Krizhevsky et al.'s 2012 win in the ImageNet object recognition challenge had a significant impact on the intensity of commercial interest in deep learning.\"\n",
      "]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"Why were convolutional networks more computationally efficient than fully connected networks?\",\n",
      "    \"What may have been the primary barriers to the success of neural networks?\",\n",
      "    \"What is the significance of convolutional networks in the acceptance of neural networks?\",\n",
      "    \"What type of data is convolutional networks most successful in working with?\",\n",
      "    \"What type of neural networks are specialized for processing one-dimensional sequential data?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Convolutional networks were more computationally efficient than fully connected networks, making it easier to run multiple experiments and tune their implementation and hyperparameters.\",\n",
      "    \"The primary barriers to the success of neural networks may have been psychological, as practitioners did not expect neural networks to work, resulting in a lack of serious effort in utilizing them.\",\n",
      "    \"Convolutional networks carried the torch for the rest of deep learning and paved the way to the acceptance of neural networks in general.\",\n",
      "    \"Convolutional networks are most successful in working with data that has a clear grid-structured topology, particularly on a two-dimensional, image topology.\",\n",
      "    \"Recurrent neural networks are specialized for processing one-dimensional sequential data.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the focus of Chapter 8 in the context of deep learning?\",\n",
      "    \"Why is neural network training considered the most difficult optimization problem in deep learning?\",\n",
      "    \"How does optimization for training deep models differ from pure optimization?\",\n",
      "    \"What is the objective function defined in Equation 8.1?\",\n",
      "    \"What is the goal of a machine learning algorithm?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Chapter 8 focuses on optimization techniques for neural network training in deep learning.\",\n",
      "    \"Neural network training is considered the most difficult optimization problem in deep learning because it often requires a significant amount of time and computational resources.\",\n",
      "    \"Optimization for training deep models differs from pure optimization in that it involves optimizing a different cost function indirectly to improve a performance measure on a test set, rather than minimizing the cost function itself.\",\n",
      "    \"The objective function defined in Equation 8.1 is an average over the training set, where L is the per-example loss function and f(x; θ) is the predicted output.\",\n",
      "    \"The goal of a machine learning algorithm is to reduce the expected generalization risk.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the empirical risk minimization?\",\n",
      "    \"Why is empirical risk minimization prone to overfitting?\",\n",
      "    \"What are surrogate loss functions?\",\n",
      "    \"What is the difference between optimization in general and optimization for training algorithms?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Empirical risk minimization is the process of minimizing the expected loss on the training set to convert a machine learning problem into an optimization problem.\",\n",
      "    \"Empirical risk minimization is prone to overfitting because models with high capacity can simply memorize the training set.\",\n",
      "    \"Surrogate loss functions are used when the desired loss function cannot be efficiently optimized. They act as proxies for the desired loss function and have certain advantages.\",\n",
      "    \"The difference between optimization in general and optimization for training algorithms is that training algorithms do not usually halt at a local minimum. Instead, they minimize a surrogate loss function but halt when a convergence criterion based on the true underlying loss function is satisfied.\"\n",
      "  ]\n",
      "}\n",
      "{\"questions\": [\"What separates machine learning algorithms from general optimization algorithms?\", \"How are optimization algorithms for machine learning typically computed?\", \"What is the most commonly used property of the objective function used by optimization algorithms?\", \"Why is computing the expectation exactly expensive?\", \"What is the standard error of the mean?\", \"What is the benefit of using approximate estimates of the gradient in optimization algorithms?\", \"What is redundancy in the training set and how does it relate to gradient estimation?\", \"What is the difference between batch gradient descent and minibatch stochastic gradient descent?\", \"What are the common terms used to describe the size of a minibatch?\", \"What are stochastic or online methods in optimization algorithms for deep learning?\", \"What are minibatch or minibatch stochastic methods commonly referred to as in optimization algorithms for deep learning?\"], \"answers\": [\"The objective function in machine learning algorithms usually decomposes as a sum over the training examples.\", \"Optimization algorithms for machine learning typically compute each update to the parameters based on an expected value of the cost function estimated using only a subset of the terms of the full cost function.\", \"The most commonly used property of the objective function is the gradient.\", \"Computing the expectation exactly is expensive because it requires evaluating the model on every example in the entire dataset.\", \"The standard error of the mean is the true standard deviation of the value of the samples divided by the square root of the number of samples.\", \"Using approximate estimates of the gradient allows optimization algorithms to converge faster in terms of total computation.\", \"Redundancy in the training set refers to the presence of examples that make similar contributions to the gradient. Estimating the gradient using a small number of samples can be computationally more efficient in such cases.\", \"Batch gradient descent processes all of the training examples simultaneously in a large batch, while minibatch stochastic gradient descent uses only a subset of the training examples at a time.\", \"The term 'batch size' is commonly used to describe the size of a minibatch.\", \"Stochastic or online methods in optimization algorithms for deep learning use only a single example at a time, sometimes drawn from a stream of continually created examples.\", \"Minibatch or minibatch stochastic methods are commonly referred to as stochastic methods in optimization algorithms for deep learning.\"]}\n",
      "{'questions': [{'question': 'What is an example of a stochastic method?', 'answers': ['stochastic gradient descent']}, {'question': 'What factors drive the choice of minibatch sizes?', 'answers': ['larger batches providing more accurate estimate of the gradient, underutilization of multicore architectures by small batches, memory scaling with batch size, runtime improvement with power of 2 batch sizes, regularizing effect of small batches, and sensitivity to sampling error']}, {'question': 'What is the best batch size for generalization error?', 'answers': ['1']}, {'question': 'What kind of algorithms can handle smaller batch sizes?', 'answers': ['methods that compute updates based only on the gradient g']}, {'question': 'What kind of algorithms require larger batch sizes?', 'answers': ['second-order methods that use the Hessian matrix H and compute updates such as 1g']}, {'question': 'Why are larger batch sizes required for second-order methods?', 'answers': ['to minimize fluctuations in the estimates of H-1g due to pre-existing errors in g and poor condition number of H']}, {'question': 'Why is it crucial for minibatches to be selected randomly?', 'answers': ['to compute an unbiased estimate of the expected gradient and to ensure independence between subsequent gradient estimates']}, {'question': 'Why is shuffling examples necessary in some cases?', 'answers': ['to avoid bias in minibatches when the order of the dataset holds significance']}, {'question': 'When can it be impractical to sample examples uniformly at random?', 'answers': ['for very large datasets containing billions of examples in a data center']}], 'answers': [['stochastic gradient descent'], ['larger batches provide a more accurate estimate of the gradient, but with less than linear returns. Multicore architectures are usually underutilized by extremely small batches. This motivates using some absolute minimum batch size, below which there is no reduction in the time to process a minibatch. If all examples in the batch are to be processed in parallel (as is typically the case), then the amount of memory scales with the batch size. For many hardware setups this is the limiting factor in batch size. Some kinds of hardware achieve better runtime with specific sizes of arrays. Especially when using GPUs, it is common for power of 2 batch sizes to offer better runtime. Typical power of 2 batch sizes range from 32 to 256, with 16 sometimes being attempted for large models.']...\n",
      "{\"questions\": [\"What is the motivation for minibatch stochastic gradient descent?\", \"How does shuffling the dataset affect the algorithm?\", \"What is the advantage of using asynchronous parallel distributed approaches?\", \"What is the equivalence between minibatch stochastic gradient descent and online learning?\", \"How can an unbiased estimator of the exact gradient be obtained?\", \"Why is it usually best to make multiple passes through the training set in minibatch stochastic gradient descent?\"], \"answers\": [\"The motivation for minibatch stochastic gradient descent is that it follows the gradient of the true generalization error as long as no examples are repeated.\", \"Shuffling the dataset once and storing it in shuffled fashion does not have a significant detrimental effect on the algorithm, while failing to shuffle the examples can seriously reduce the effectiveness of the algorithm.\", \"The advantage of using asynchronous parallel distributed approaches is that multiple updates can be computed in parallel over different examples, increasing efficiency.\", \"The equivalence between minibatch stochastic gradient descent and online learning is that both involve drawing examples or minibatches from a stream of data, with every experience being a fair sample from the data generating distribution.\", \"An unbiased estimator of the exact gradient of the generalization error can be obtained by sampling a minibatch of examples from the data generating distribution and computing the gradient of the loss with respect to the parameters for that minibatch.\", \"It is usually best to make multiple passes through the training set in minibatch stochastic gradient descent because although only the first epoch follows the unbiased gradient of the generalization error, the additional epochs provide enough benefit due to decreased training error to offset the harm they cause by increasing the gap between training error and test error.\"] }\n",
      "{\n",
      "\"questions\": [\n",
      "  \"What are the challenges in neural network optimization?\",\n",
      "  \"What is ill-conditioning in optimization?\",\n",
      "  \"How does ill-conditioning manifest in neural network training?\",\n",
      "  \"What is the relationship between gradient norm and ill-conditioning?\",\n",
      "  \"Can Newton's method be directly applied to neural networks?\",\n",
      "  \"What is the issue of local minima in convex optimization?\"\n",
      "],\n",
      "\"answers\": [\n",
      "  \"The challenges in neural network optimization include ill-conditioning, local minima, and optimization in general.\",\n",
      "  \"Ill-conditioning in optimization refers to the problem of the Hessian matrix being poorly conditioned, which can cause optimization algorithms to get 'stuck' and increase the cost function even with small steps.\",\n",
      "  \"Ill-conditioning in neural network training can cause the gradient descent step to add a significant value to the cost function, making learning slow despite the presence of a strong gradient.\",\n",
      "  \"The gradient norm may not significantly decrease throughout learning while the gHg term grows by more than an order of magnitude, indicating the presence of ill-conditioning.\",\n",
      "  \"Newton's method is not directly applicable to neural networks and requires significant modifications before it can be used for optimization.\",\n",
      "  \"In convex optimization, local minima guarantee global minima, but the presence of a flat region at the bottom can complicate the search for the global minimum.\"\n",
      " ]\n",
      "}\n",
      "{\n",
      "\"questions\": [\n",
      "    \"What is the identifiability problem in models with latent variables?\",\n",
      "    \"What is weight space symmetry?\",\n",
      "    \"Why can non-identifiability lead to a large number of local minima in a neural network?\",\n",
      "    \"Are all local minima in a non-convex cost function problematic?\",\n",
      "    \"How can one test if local minima are the problem in neural network optimization?\"\n",
      "],\n",
      "\"answers\": [\n",
      "    \"The identifiability problem in models with latent variables refers to the fact that multiple arrangements of the latent variables can lead to equivalent models.\",\n",
      "    \"Weight space symmetry refers to the property of neural networks where exchanging latent variables with each other creates equivalent models. For example, swapping the incoming and outgoing weight vectors for certain units in a neural network leads to an equivalent model.\",\n",
      "    \"Non-identifiability in a neural network can result in a large number of local minima because different arrangements of the latent variables and scaling of weights can lead to equivalent models with different parameter settings.\",\n",
      "    \"No, not all local minima in a non-convex cost function are problematic. The non-identifiability of models with latent variables can result in multiple local minima that are equivalent in terms of their cost function value.\",\n",
      "    \"To test if local minima are the problem in neural network optimization, one can plot the norm of the gradient over time. If the norm of the gradient does not decrease significantly, local minima are not the issue. However, in high dimensional spaces, it can be challenging to establish if local minima are the cause of optimization difficulties.\"\n",
      "]\n",
      "}\n",
      "{\"questions\": [\"What is a saddle point in high-dimensional non-convex functions?\", \"What is the difference between a saddle point and a local minimum?\", \"Why are local minima more common in low-dimensional spaces compared to higher dimensional spaces?\", \"What is the intuition behind the ratio of saddle points to local minima growing exponentially with the dimensionality of a function?\", \"What happens to the likelihood of positive eigenvalues in regions of lower cost?\", \"Do shallow autoencoders with no nonlinearities have local minima with higher cost than the global minimum?\", \"What did Choromanska et al. show experimentally about loss functions in real neural networks?\", \"What is the implication of saddle points for first-order optimization algorithms that use only gradient information?\"], \"answers\": [\"In high-dimensional non-convex functions, a saddle point is a point with zero gradient where some points have greater cost and others have a lower cost compared to the saddle point.\",\n",
      "\"A local minimum is a point with zero gradient where all points around it have a higher cost. A saddle point, on the other hand, has points around it with both higher and lower costs.\",\n",
      "\"In low-dimensional spaces, local minima are common. In higher dimensional spaces, local minima are rare and saddle points are more common.\",\n",
      "\"The intuition behind the exponential growth of the ratio of saddle points to local minima with dimensionality is that in n-dimensional space, it becomes exponentially unlikely for all n coin tosses to result in heads, where each coin toss represents an eigenvalue's sign.\",\n",
      "\"In regions of lower cost, the likelihood of positive eigenvalues in the Hessian matrix becomes more likely.\",\n",
      "\"Shallow autoencoders with no nonlinearities have global minima and saddle points, but no local minima with higher cost than the global minimum.\",\n",
      "\"Choromanska et al. showed experimentally that real neural networks have loss functions that contain many high-cost saddle points.\",\n",
      "\"For first-order optimization algorithms that use only gradient information, the situation is unclear. The gradient can become very small near a saddle point, but gradient descent seems to be able to escape saddle points in many cases.\"] }\n",
      "{\"questions\": [\"What is the primary obstacle revealed by the cost function visualization?\", \"Why have second-order methods not succeeded in replacing gradient descent for neural network training?\", \"What are the different kinds of points with zero gradient?\", \"What problems do wide, flat regions with zero gradient pose for optimization algorithms?\", \"What is a cliff structure in neural networks?\", \"What happens in the gradient update step on an extremely steep cliff structure?\"], \"answers\": [\"The primary obstacle revealed by the cost function visualization is a saddle point of high cost near where the parameters are initialized.\", \"Second-order methods have not succeeded in replacing gradient descent for neural network training because of the proliferation of saddle points in high dimensional spaces.\", \"The different kinds of points with zero gradient are minima, saddle points, and maxima.\", \"Wide, flat regions with zero gradient pose major problems for all numerical optimization algorithms because in a general optimization problem, such a region could correspond to a high value of the objective function.\", \"A cliff structure in neural networks refers to extremely steep regions that result from the multiplication of several large weights together.\", \"On an extremely steep cliff structure, the gradient update step can move the parameters extremely far, usually jumping off of the cliff structure altogether.\"] }\n",
      "{'questions': [{'question': 'What does the gradient clipping heuristic do?', 'answer': 'When the traditional gradient descent algorithm proposes to make a very large step, the gradient clipping heuristic intervenes to reduce the step size to be small enough that it is less likely to go outside the region where the gradient indicates the direction of approximately steepest descent.'}, {'question': 'Why are cliﬀ structures most common in the cost functions for recurrent neural networks?', 'answer': 'Cliﬀ structures are most common in the cost functions for recurrent neural networks because such models involve a multiplication of many factors, with one factor for each time step, and long temporal sequences incur an extreme amount of multiplication.'}, {'question': 'What is the vanishing and exploding gradient problem?', 'answer': 'The vanishing and exploding gradient problem refers to the fact that gradients through a computational graph with repeated multiplication by a matrix can either vanish (making it difficult to know which direction the parameters should move to improve the cost function) or explode (making learning unstable).'}, {'question': 'When can feedforward networks largely avoid the vanishing and exploding gradient problem?', 'answer': 'Feedforward networks can largely avoid the vanishing and exploding gradient problem when the computational graph does not involve repeated multiplication by the same matrix at each step.'}, {'question': 'What is the main challenge of training recurrent networks?', 'answer': 'The main challenge of training recurrent networks is dealing with the vanishing and exploding gradient problem caused by the repeated multiplication by the same matrix at each time step.'}, {'question': 'What do most optimization algorithms assume about the gradient or Hessian matrix?', 'answer': 'Most optimization algorithms are designed with the assumption that we have access to the exact gradient or Hessian matrix.'}, {'question': 'What does sampling-based estimates refer to in terms of optimization in deep learning?', 'answer': 'Sampling-based estimates refer to the use of a minibatch of training examples to compute the gradient when the objective function is intractable.'}], 'answers': [{'start': 298, 'end': 435, 'text': 'When the traditional gradient descent algorithm proposes to make a very large step, the gradient clipping heuristic intervenes to reduce the step size to be small enough that it is less likely to go outside the region where the gradient indicates the direction of approximately steepest descent.'}, {'start': 267, 'end': 358, 'text': 'Cliﬀ structures are most common in the cost functions for recurrent neural networks because such models involve a multiplication of many factors, with one factor for each time step, and long temporal sequences incur an extreme amount of multiplication.'}, {'start': 553, 'end': 718,...\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What are some techniques for approximating the gradient of the intractable log-likelihood of a Boltzmann machine?\",\n",
      "    \"How can imperfections in the gradient estimate be accounted for in neural network optimization algorithms?\",\n",
      "    \"What problems can arise when there is poor correspondence between local and global structure in optimization?\",\n",
      "    \"According to Goodfellow et al., what contributes to the runtime of training in neural networks?\",\n",
      "    \"Why do neural networks often not arrive at a region of small gradient during training?\",\n",
      "    \"Give an example of a failure of local optimization in finding a good cost function value.\",\n",
      "    \"What is the main cause of difficulty in the given example of failure of local optimization?\",\n",
      "    \"What are some research directions focused on improving optimization in problems with difficult global structure?\",\n",
      "    \"What is the basis for gradient descent and other learning algorithms for training neural networks?\",\n",
      "    \"What are some existing research directions aimed at finding good initial points for problems with difficult global structure?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Contrastive divergence is a technique for approximating the gradient of the intractable log-likelihood of a Boltzmann machine.\",\n",
      "    \"Neural network optimization algorithms can account for imperfections in the gradient estimate by using surrogate loss functions that are easier to approximate.\",\n",
      "    \"Problems can arise when the direction that results in the most improvement locally does not point toward distant regions of much lower cost.\",\n",
      "    \"According to Goodfellow et al., the length of the trajectory needed to arrive at the solution contributes to the runtime of training in neural networks.\",\n",
      "    \"Neural networks often do not arrive at a region of small gradient because critical points, such as global minimum points, may not exist, and the loss function may asymptotically approach some value as the model becomes more confident.\",\n",
      "    \"An example of a failure of local optimization in finding a good cost function value is when the local surface does not point toward the global solution, even in the absence of saddle points or local minima.\",\n",
      "    \"The main cause of difficulty in the given example is being initialized on the wrong side of the 'mountain' and not being able to traverse it.\",\n",
      "    \"Some research directions focused on improving optimization in problems with difficult global structure involve finding good initial points instead of developing algorithms that use non-local moves.\",\n",
      "    \"Gradient descent and other learning algorithms for training neural networks are based on making small, local moves.\",\n",
      "    \"Some existing research directions aim to find good initial points for problems with difficult global structure, rather than developing algorithms that use non-local moves.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What are some issues that can arise in optimization algorithms for neural networks?\",\n",
      "    \"Why is it difficult to understand which problems make neural network optimization difficult?\",\n",
      "    \"What are the theoretical limits of optimization for neural networks?\",\n",
      "    \"What is stochastic gradient descent (SGD) and why is it commonly used in machine learning?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Some issues that can arise in optimization algorithms for neural networks include approximate objective function estimation, bias or variance in estimating the correct direction, poor conditioning or discontinuous gradients, lack of local information guidance, and paths that move away from solutions.\",\n",
      "    \"It is difficult to understand which problems make neural network optimization difficult because there are multiple factors that can contribute, such as flat regions, critical points, and unknown relevance of different problems. Research is actively being conducted in this area.\",\n",
      "    \"Theoretical results show that there are limits on the performance of optimization algorithms for neural networks. Some results apply to cases where neural network units output discrete values, while others show that finding a solution for a given network size can be intractable. However, in practice, larger networks can often find solutions easily. It is also difficult to analyze whether an optimization algorithm can achieve the goal of reducing the value sufficiently for good generalization error.\",\n",
      "    \"Stochastic gradient descent (SGD) is an optimization algorithm commonly used in machine learning, including deep learning. It involves following the gradient of randomly selected minibatches downhill. SGD and its variants are popular because they can accelerate the gradient descent algorithm and are effective for large datasets.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the purpose of Algorithm 8.1?\",\n",
      "    \"Why is it necessary to gradually decrease the learning rate in SGD?\",\n",
      "    \"What is the condition for convergence of SGD?\",\n",
      "    \"How is the learning rate decayed in practice?\",\n",
      "    \"How is the learning rate chosen?\",\n",
      "    \"What is the main advantage of SGD?\",\n",
      "    \"What is measured to study the convergence rate of an optimization algorithm?\",\n",
      "    \"What is the excess error in SGD?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The purpose of Algorithm 8.1 is to show how to follow an estimate of the gradient downhill in stochastic gradient descent (SGD).\",\n",
      "    \"It is necessary to gradually decrease the learning rate in SGD because the SGD gradient estimator introduces noise that does not vanish even at a minimum.\",\n",
      "    \"A sufficient condition for convergence of SGD is that the sum of the learning rates approaches infinity, and the squared sum of the learning rates approaches a finite value less than infinity.\",\n",
      "    \"In practice, the learning rate is commonly decayed linearly until iteration τ using the formula: learning_rate = (1 - α) * initial_learning_rate + α * learning_rate_τ, where α = k / τ. After iteration τ, the learning rate is kept constant.\",\n",
      "    \"The learning rate is usually chosen by monitoring learning curves that plot the objective function as a function of time. It is an iterative process and the optimal initial learning rate is typically higher than the best-performing learning rate after the first few iterations.\",\n",
      "    \"The main advantage of SGD is that its computation time per update does not grow with the number of training examples, allowing convergence even with very large datasets.\",\n",
      "    \"To study the convergence rate of an optimization algorithm, the excess error, which is the amount that the current cost function exceeds the minimum possible cost, is commonly measured.\",\n",
      "    \"The excess error in SGD refers to the amount by which the current cost function exceeds the minimum possible cost. In convex problems, the excess error is O(1) after k iterations, and in strongly convex problems, it is O(1/sqrt(k)).\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "\"questions\": [\n",
      "\"What is the purpose of the momentum algorithm in optimization?\",\n",
      "\"How does the momentum algorithm work?\",\n",
      "\"What role does the hyperparameter alpha play in the momentum algorithm?\",\n",
      "\"What is the update rule for the velocity in the momentum algorithm?\",\n",
      "\"What problems does momentum aim to solve?\",\n",
      "\"What is the difference between the step size in traditional gradient descent and the step size in the momentum algorithm?\"\n",
      "],\n",
      "\"answers\": [\n",
      "\"The purpose of the momentum algorithm in optimization is to accelerate learning, especially in the face of high curvature, small but consistent gradients, or noisy gradients.\",\n",
      "\"The momentum algorithm works by accumulating an exponentially decaying moving average of past gradients and continues to move in their direction.\",\n",
      "\"The hyperparameter alpha determines how quickly the contributions of previous gradients exponentially decay in the momentum algorithm.\",\n",
      "\"The update rule for the velocity in the momentum algorithm is given by: v <- alpha * v - learning_rate * gradient.\",\n",
      "\"Momentum aims to solve two problems: poor conditioning of the Hessian matrix and variance in the stochastic gradient.\",\n",
      "\"The step size in traditional gradient descent is simply the norm of the gradient multiplied by the learning rate, while the step size in the momentum algorithm depends on how large and how aligned a sequence of gradients are.\"\n",
      "]\n",
      "}\n",
      "{'questions': [{'question': 'What is the momentum hyperparameter used for in gradient descent?', 'answers': ['The momentum hyperparameter in gradient descent is used to control the size of each step in the direction of the gradient.']}], 'answers': [{'answer_start': 47, 'text': 'The momentum hyperparameter in gradient descent is used to control the size of each step in the direction of the gradient.'}]}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the purpose of using a lower power of velocity in turbulent drag?\",\n",
      "    \"How does viscous drag prevent motion when the gradient does not justify moving?\",\n",
      "    \"What is the difference between Nesterov momentum and standard momentum?\",\n",
      "    \"What is the rate of convergence of Nesterov momentum in the convex batch gradient case?\",\n",
      "    \"Does Nesterov momentum improve the rate of convergence in the stochastic gradient case?\",\n",
      "    \"Why is the choice of initialization important in training deep learning models?\",\n",
      "    \"What are some of the challenges in designing improved initialization strategies for neural networks?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Using a lower power of velocity in turbulent drag prevents the particle from moving away from its initial position forever.\",\n",
      "    \"Viscous drag is weak enough to allow motion if the gradient justifies moving, but strong enough to prevent motion if the gradient does not justify moving.\",\n",
      "    \"The difference between Nesterov momentum and standard momentum is where the gradient is evaluated. With Nesterov momentum, the gradient is evaluated after the current velocity is applied.\",\n",
      "    \"In the convex batch gradient case, Nesterov momentum improves the rate of convergence of the excess error from O(1/k) to O(1/k^2).\",\n",
      "    \"No, Nesterov momentum does not improve the rate of convergence in the stochastic gradient case.\",\n",
      "    \"The choice of initialization is important in training deep learning models because it can determine whether the algorithm converges, how quickly learning converges, and whether it converges to a point with high or low cost.\",\n",
      "    \"The challenges in designing improved initialization strategies for neural networks include the lack of understanding of neural network optimization and the need to achieve certain properties when the network is initialized.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What does the initial point need to accomplish from the perspective of optimization and generalization?\",\n",
      "    \"What is the only property known with certainty about the initial parameters?\",\n",
      "    \"Why is it important for hidden units with the same activation function to have different initial parameters?\",\n",
      "    \"Why is it necessary to initialize each unit to compute a different function from all the other units?\",\n",
      "    \"What is the goal of random initialization of the parameters?\",\n",
      "    \"How can we ensure that each unit computes a very different function from each other unit?\",\n",
      "    \"How are biases and extra parameters typically initialized in a model?\",\n",
      "    \"What is the common practice for initializing all the weights in a model?\",\n",
      "    \"What effect does the scale of the initial weight distribution have on the optimization procedure and network generalization?\",\n",
      "    \"What problems can arise from using initial weights that are too large?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The initial point needs to balance between optimization and generalization. Some initial points may be good for optimization but not for generalization.\",\n",
      "    \"The only known property is that the initial parameters need to break symmetry between different units.\",\n",
      "    \"If hidden units with the same activation function are connected to the same inputs and have the same initial parameters, a deterministic learning algorithm will update them in the same way, potentially losing important information.\",\n",
      "    \"Initializing each unit to compute a different function from all the other units helps prevent the loss of input patterns during forward propagation and gradient patterns during back-propagation.\",\n",
      "    \"The goal of random initialization is to have each unit compute a different function from the other units.\",\n",
      "    \"Gram-Schmidt orthogonalization can be used to ensure each unit computes a very different function from each other unit, but it incurs a noticeable computational cost. Random initialization avoids assigning the same function to multiple units.\",\n",
      "    \"Biases for each unit are set to heuristically chosen constants, and only the weights are initialized randomly. Extra parameters are also set to heuristically chosen constants.\",\n",
      "    \"All the weights in the model are initialized to values drawn randomly from a Gaussian or uniform distribution.\",\n",
      "    \"The choice of Gaussian or uniform distribution for weight initialization does not have a significant impact. However, the scale of the initial distribution affects the optimization procedure and the network's ability to generalize.\",\n",
      "    \"Larger initial weights help break symmetry and avoid redundant units, but they may also result in exploding values during forward or back-propagation. Additionally, extreme values from large weights can cause the activation function to saturate, leading to a complete loss of gradient.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': [{'question': 'What is the purpose of using an optimization algorithm like stochastic gradient descent?', 'answers': ['to make small incremental changes to the weights', 'to halt in areas near the initial parameters', 'to prevent overfitting']}, {'question': 'What does initializing the parameters to zero imply?', 'answers': ['A Gaussian prior with mean of zero is imposed on the parameters.', 'It is more likely that units do not interact with each other.']}], 'answers': [{'answer_start': 79, 'text': 'makes small incremental changes to the weights and tends to halt in areas that are nearer to the initial parameters'}, {'answer_start': 370, 'text': 'it makes sense to choose θ0 to be near 0.'}]}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the issue with scaling rules that set all initial weights to have the same standard deviation?\",\n",
      "    \"What is sparse initialization and how does it help?\",\n",
      "    \"What is a good rule of thumb for choosing the initial scales of weights?\",\n",
      "    \"Why is it important to coordinate the approach for setting biases with the approach for setting weights?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The issue with scaling rules that set all initial weights to have the same standard deviation is that every individual weight becomes extremely small when the layers become large.\",\n",
      "    \"Sparse initialization is an alternative initialization scheme where each unit is initialized with exactly k non-zero weights. It helps achieve more diversity among the units at initialization time without making the magnitude of individual weight elements shrink with the number of inputs.\",\n",
      "    \"A good rule of thumb for choosing the initial scales of weights is to look at the range or standard deviation of activations or gradients on a single minibatch of data. If the weights are too small, the range of activations across the minibatch will shrink as the activations propagate forward through the network.\",\n",
      "    \"It is important to coordinate the approach for setting biases with the approach for setting weights because setting biases to zero is compatible with most weight initialization schemes. However, in some cases, biases may need to be set to non-zero values to achieve the right marginal statistics of the output, especially for output units.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': [{'question': 'What is the purpose of initializing the biases in certain layers of a model?', 'answers': ['The purpose of initializing the biases in certain layers of a model is to match the marginal distribution over the input data.', 'The purpose of initializing the biases in certain layers of a model is to avoid causing too much saturation at initialization.']}, {'question': 'What is an example of setting the bias of a ReLU hidden unit to a non-zero value at initialization?', 'answers': ['An example of setting the bias of a ReLU hidden unit to a non-zero value at initialization is setting it to 0.1 instead of 0.']}, {'question': 'Why is this approach not recommended for use with random walk initialization?', 'answers': ['This approach is not recommended for use with random walk initialization because random walk initialization does not expect strong input from the biases.']}], 'answers': [{'answer_start': 77, 'answer_end': 157, 'text': 'The purpose of initializing the biases in certain layers of a model is to match the marginal distribution over the input data.'}, {'answer_start': 180, 'answer_end': 238, 'text': 'The purpose of initializing the biases in certain layers of a model is to avoid causing too much saturation at initialization.'}, {'answer_start': 290, 'answer_end': 320, 'text': 'An example of setting the bias of a ReLU hidden unit to a non-zero value at initialization is setting it to 0.1 instead of 0.'}, {'answer_start': 447, 'answer_end': 525, 'text': 'This approach is not recommended for use with random walk initialization because random walk initialization does not expect strong input from the biases.'}]}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the purpose of AdaGrad algorithm?\",\n",
      "    \"Why does AdaGrad perform well for some but not all deep learning models?\",\n",
      "    \"How does RMSProp modify AdaGrad?\",\n",
      "    \"What is the advantage of using RMSProp in a non-convex setting?\",\n",
      "    \"What is the concept of Adam algorithm?\",\n",
      "    \"What are the important distinctions of Adam compared to other algorithms?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The purpose of the AdaGrad algorithm is to adapt the learning rates of all model parameters individually by scaling them inversely proportional to the square root of the sum of their historical squared values. This allows for a more rapid decrease in the learning rate for parameters with larger partial derivatives of the loss.\",\n",
      "    \"AdaGrad performs well for some deep learning models because it allows for greater progress in more gently sloped directions of parameter space. However, it may result in a premature and excessive decrease in the effective learning rate for training deep neural network models.\",\n",
      "    \"RMSProp modifies AdaGrad by changing the gradient accumulation into an exponentially weighted moving average. This allows it to perform better in a non-convex setting, as it discards history from the extreme past and converges rapidly after finding a locally convex region.\",\n",
      "    \"The advantage of using RMSProp in a non-convex setting is that it can converge rapidly after finding a locally convex bowl, as if it were an instance of the AdaGrad algorithm initialized within that bowl. This is achieved by using an exponentially decaying average to discard excessive history from the extreme past.\",\n",
      "    \"Adam (derived from 'adaptive moments') is an adaptive learning rate optimization algorithm. It combines elements of RMSProp and momentum, incorporating momentum directly as an estimate of the first order moment of the gradient with exponential weighting.\",\n",
      "    \"The important distinctions of Adam compared to other algorithms are: (1) In Adam, momentum is incorporated directly as an estimate of the first order moment of the gradient, while RMSProp applies momentum to the rescaled gradients. (2) Adam includes bias correction terms to reduce the bias of moment estimates. (3) Adam uses adaptive learning rates for each parameter, allowing for individual scaling based on historical squared gradients.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the RMSProp algorithm used for?\",\n",
      "    \"What is the difference between RMSProp and Adam?\",\n",
      "    \"Which optimization algorithms are currently popular?\",\n",
      "    \"What factors influence the choice of optimization algorithm?\",\n",
      "    \"What is Newton's method and how is it applied to neural network training?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The RMSProp algorithm is used for optimizing deep models by adapting the learning rate for each model parameter.\",\n",
      "    \"Unlike Adam, the RMSProp algorithm lacks the correction factor and may have higher bias early in training.\",\n",
      "    \"The popular optimization algorithms currently in use include SGD, SGD with momentum, RMSProp, RMSProp with momentum, AdaDelta, and Adam.\",\n",
      "    \"The choice of which optimization algorithm to use depends largely on the user's familiarity with the algorithm and ease of hyperparameter tuning.\",\n",
      "    \"Newton's method is a second-order method that makes use of second derivatives to improve optimization. It is applied to neural network training by iteratively updating the parameters based on the gradient and the Hessian matrix of the objective function.\"\n",
      "  ]\n",
      "}\n",
      "{\"questions\": [\"What is the purpose of Newton's method in optimization?\", \"How does Newton's method update the parameters?\", \"Why is Newton's method not suitable for non-convex objective functions?\", \"What is the regularization strategy used in approximations to Newton's method?\", \"What happens to the updates in Newton's method when strong negative curvature is present?\"], \"answers\": [\"Newton's method is an optimization scheme based on using a second-order Taylor series expansion to approximate the objective function near a point and find the critical point of this function.\", \"Newton's method updates the parameters by computing the gradient and the Hessian, and then applying the update rule: theta_star = theta_0 - (H^-1) * gradient.\", \"Newton's method is not suitable for non-convex objective functions because the surface of the objective function typically has many problematic features, such as saddle points, which can cause updates to move in the wrong direction.\", \"The regularization strategy used in approximations to Newton's method is to add a regularization term, alpha * I, along the diagonal of the Hessian.\", \"When strong negative curvature is present, Newton's method may make smaller steps than gradient descent with a properly chosen learning rate, because the direction chosen by Newton's method converges to the standard gradient divided by the regularization factor, alpha.\"]}`\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the limitation of using Newton's method for training large neural networks?\",\n",
      "    \"What is the computational complexity of inverting a k x k matrix?\",\n",
      "    \"Why can only networks with a small number of parameters be practically trained via Newton's method?\",\n",
      "    \"What is the motivation behind the conjugate gradients method?\",\n",
      "    \"How does the method of conjugate gradients address the problem of zig-zag progress?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The application of Newton's method for training large neural networks is limited by the significant computational burden it imposes.\",\n",
      "    \"The computational complexity of inverting a k x k matrix is O(k^3).\",\n",
      "    \"Only networks with a small number of parameters can be practically trained via Newton's method because the inverse Hessian has to be computed at every training iteration.\",\n",
      "    \"The motivation behind the conjugate gradients method is to gain some of the advantages of Newton's method while avoiding the computational hurdles.\",\n",
      "    \"The method of conjugate gradients seeks to find a search direction that is conjugate to the previous line search direction, meaning it will not undo progress made in that direction.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "\"questions\": [\n",
      "  \"What are two popular methods for computing the βt?\",\n",
      "  \"What is the conjugate gradient method used for?\",\n",
      "  \"What modifications are made to the conjugate gradients algorithm for training neural networks?\",\n",
      "  \"What is the BFGS algorithm and how does it approximate Newton's update?\"\n",
      "],\n",
      "\"answers\": [\n",
      "  \"Two popular methods for computing the βt are the Fletcher-Reeves method and the Polak-Ribière method.\",\n",
      "  \"The conjugate gradient method ensures that the gradient along the previous direction does not increase in magnitude, allowing us to stay at the minimum along the previous directions.\",\n",
      "  \"For training neural networks, the nonlinear conjugate gradients algorithm includes occasional resets and line searches along the unaltered gradient. It is often beneficial to initialize the optimization with a few iterations of stochastic gradient descent.\",\n",
      "  \"The Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm is an optimization method that approximates Newton's update without the computational burden. It uses a matrix, Mt, to iteratively refine the approximation of the inverse Hessian, H^-1.\"\n",
      "]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What determines the direction of descent in the BFGS algorithm?\",\n",
      "    \"How does the BFGS algorithm differ from the method of conjugate gradients?\",\n",
      "    \"What is the disadvantage of using the BFGS algorithm in modern deep learning models?\",\n",
      "    \"What is the L-BFGS algorithm?\",\n",
      "    \"Why is batch normalization important in optimizing deep neural networks?\",\n",
      "    \"What problem can arise when updating all layers simultaneously in a deep neural network?\",\n",
      "    \"What is the first-order Taylor series approximation used in the example provided?\",\n",
      "    \"What is an example of a second-order term that arises from the update in the example?\",\n",
      "    \"What is the purpose of the L-BFGS algorithm?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The direction of descent in the BFGS algorithm is determined by the inverse Hessian approximation multiplied by the gradient.\",\n",
      "    \"Unlike the method of conjugate gradients, the success of the BFGS algorithm is not heavily dependent on finding the true minimum along the line in the line search.\",\n",
      "    \"The disadvantage of using the BFGS algorithm in modern deep learning models is that it requires a large amount of memory to store the inverse Hessian matrix.\",\n",
      "    \"The L-BFGS algorithm is a variant of the BFGS algorithm that avoids storing the complete inverse Hessian approximation.\",\n",
      "    \"Batch normalization is important in optimizing deep neural networks because it helps to address the challenge of training very deep models by adapting the reparametrization of the network.\",\n",
      "    \"When updating all layers simultaneously in a deep neural network, unexpected results can occur because the updates are computed under the assumption that the other layers remain constant.\",\n",
      "    \"The first-order Taylor series approximation used in the example predicts that the value of the output will decrease by the learning rate multiplied by the gradient.\",\n",
      "    \"An example of a second-order term that arises from the update is the square of the learning rate multiplied by the product of the gradients of the weights.\",\n",
      "    \"The L-BFGS algorithm is used to decrease the memory costs of the BFGS algorithm by avoiding storing the complete inverse Hessian approximation.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': ['What does batch normalization do in deep networks?', 'How is batch normalization applied to the layers in a network?', 'What is the benefit of back-propagating through the mean and standard deviation operations in batch normalization?', 'What is the advantage of using running averages for µ and σ at test time?', 'How does batch normalization make learning in a model with multiple layers simpler?'], 'answers': ['Batch normalization provides a way to reparametrize deep networks in order to reduce the problem of coordinating updates across many layers.', 'Batch normalization can be applied to any input or hidden layer in a network by replacing the activations of the layer with the normalized activations.', 'Back-propagating through the mean and standard deviation operations ensures that the gradient will not propose operations that increase the standard deviation or mean of the activations.', 'Using running averages for µ and σ at test time allows the model to be evaluated on a single example without depending on an entire minibatch.', 'Batch normalization makes learning in a model with multiple layers simpler by reparametrizing the model to have units that are always standardized, avoiding the need for penalties or renormalization steps.']}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the purpose of batch normalization in a neural network?\",\n",
      "    \"What does batch normalization normalize?\",\n",
      "    \"Why are the lower layers in a deep neural network with batch normalization often useless?\",\n",
      "    \"What are the learned parameters in batch normalization used for?\",\n",
      "    \"Should batch normalization be applied to the input or the transformed value in a neural network layer?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The purpose of batch normalization in a neural network is to standardize the mean and variance of each unit in order to stabilize learning.\",\n",
      "    \"Batch normalization normalizes the mean and standard deviation of each unit in a neural network.\",\n",
      "    \"The lower layers in a deep neural network with batch normalization are often useless because the normalization of mean and variance removes their harmful and beneficial effects.\",\n",
      "    \"The learned parameters in batch normalization, γ and β, are used to introduce a new parametrization that allows the mean and standard deviation of a unit to be set to any arbitrary value, thus preserving the expressive power of the network.\",\n",
      "    \"Batch normalization should be applied to the transformed value, XW, in a neural network layer.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': [\n",
      "  'What is coordinate descent?',\n",
      "  'When does coordinate descent make the most sense?',\n",
      "  'What is the cost function J used for?',\n",
      "  'Why is coordinate descent not a good strategy for some problems?',\n",
      "  'What is Polyak averaging?',\n",
      "  'How is Polyak averaging typically used in non-convex problems?'\n",
      "], 'answers': [\n",
      "  'Coordinate descent is a method of minimizing a function by optimizing one coordinate at a time.',\n",
      "  'Coordinate descent makes the most sense when the variables in the optimization problem can be separated into groups that play isolated roles or when optimizing with respect to one group of variables is more efficient.',\n",
      "  'The cost function J is used for a learning problem called sparse coding, where the goal is to find a weight matrix W that can decode activation values H to reconstruct the training set X.',\n",
      "  'Coordinate descent is not a good strategy when the value of one variable strongly influences the optimal value of another variable.',\n",
      "  'Polyak averaging is a method of averaging together several points in the trajectory through parameter space visited by an optimization algorithm.',\n",
      "  'In non-convex problems, Polyak averaging is typically used with an exponentially decaying running average to only include points from the recent past.'\n",
      "]}\n",
      "{'questions': [{'question': 'What is pretraining in deep learning?', 'answer': 'Pretraining is the process of training a simpler model to solve a task before training a more complex model to perform the desired task.'}, {'question': 'What are the benefits of greedy supervised pretraining?', 'answer': 'Greedy supervised pretraining can provide better guidance to the optimization process, help with optimization, and improve the quality of the solution found.'}, {'question': 'How does greedy supervised pretraining work?', 'answer': 'Greedy supervised pretraining involves training a neural network in stages, with each stage consisting of a supervised learning task involving only a subset of the layers in the final network. Each added hidden layer is pretrained as part of the process before jointly training the network.'}], 'answers': [{'text': 'Pretraining is the process of training a simpler model to solve a task before training a more complex model to perform the desired task.', 'answer_start': 61}, {'text': 'Greedy supervised pretraining can provide better guidance to the optimization process, help with optimization, and improve the quality of the solution found.', 'answer_start': 355}, {'text': 'Greedy supervised pretraining involves training a neural network in stages, with each stage consisting of a supervised learning task involving only a subset of the layers in the final network. Each added hidden layer is pretrained as part of the process before jointly training the network.', 'answer_start': 573}]}\n",
      "{\"questions\": [\"What is the approach related to supervised pretraining?\", \"How does the FitNets approach work?\", \"What is the importance of designing models in optimization?\", \"What are some examples of model innovations mentioned in the text?\"], \"answers\": [\"The approach related to supervised pretraining is to pretrain a deep convolutional net with 8 layers of weights on a set of tasks and then initialize another network with the first k layers of the first net and train them jointly with a different set of tasks.\", \"The FitNets approach involves training a network with low depth and great width to become a teacher for a deeper and thinner student network. The student network is trained not only to predict the output for the original task but also to predict the value of the middle layer of the teacher network, which helps in simplifying the optimization problem.\", \"Designing models to be easier to optimize is crucial in improving optimization. It is more important to choose a model family that is easy to optimize than to rely on a powerful optimization algorithm.\", \"Some examples of model innovations mentioned in the text are LSTM, rectified linear units, and maxout units. These models use more linear functions compared to older models like deep networks based on sigmoidal units, making optimization easier.\"]}`\n",
      "{'questions': [{'question': 'What is the purpose of adding auxiliary heads in neural networks?', 'answers': ['The purpose of adding auxiliary heads in neural networks is to ensure that the lower layers receive a large gradient.']}, {'question': 'What are continuation methods?', 'answers': ['Continuation methods are a family of strategies that can make optimization easier by choosing initial points to ensure that local optimization spends most of its time in well-behaved regions of space.']}, {'question': 'What is the main goal of continuation methods?', 'answers': ['The main goal of continuation methods is to reach a global minimum despite the presence of many local minima.']}, {'question': 'What is the relationship between continuation methods and simulated annealing?', 'answers': ['Continuation methods are closely related to simulated annealing, which adds noise to the parameters.']}], 'answers': [{'answer_start': [238], 'text': 'The purpose of adding auxiliary heads in neural networks is to ensure that the lower layers receive a large gradient.'}, {'answer_start': [334], 'text': 'Continuation methods are a family of strategies that can make optimization easier by choosing initial points to ensure that local optimization spends most of its time in well-behaved regions of space.'}, {'answer_start': [430], 'text': 'The main goal of continuation methods is to reach a global minimum despite the presence of many local minima.'}, {'answer_start': [566], 'text': 'Continuation methods are closely related to simulated annealing, which adds noise to the parameters.'}]}\n",
      "{'questions': ['What is the intuition behind blurring the cost function in continuation methods?',\n",
      "  'What are the three ways in which continuation methods can fail?',\n",
      "  'What is curriculum learning or shaping?',\n",
      "  'What is the significance of curriculum learning in neural network optimization?',\n",
      "  'What is the contribution of Zaremba and Sutskever in curriculum learning?'],\n",
      " 'answers': ['The intuition behind blurring the cost function in continuation methods is that some non-convex functions become approximately convex when blurred. Blurring preserves enough information about the location of a global minimum and allows finding the global minimum by solving progressively less blurred versions of the problem.',\n",
      "  'Continuation methods can fail in three ways. Firstly, it may require a large number of incremental cost functions, resulting in a high overall cost. Secondly, the function may not become convex even after blurring. Lastly, the minimum of the blurred function may track to a local minimum rather than a global minimum of the original cost function.',\n",
      "  'Curriculum learning or shaping is an approach where the learning process starts with simpler concepts and progresses to more complex concepts. It involves assigning larger coefficients to simpler examples or sampling them more frequently to make earlier objectives easier.',\n",
      "  'Curriculum learning is significant in neural network optimization as it eliminates flat regions, decreases variance in gradient estimates, improves conditioning of the Hessian matrix, and improves the correspondence between local update directions and progress towards a global solution.',\n",
      "  'Zaremba and Sutskever made a significant contribution to curriculum learning by finding that stochastic curriculum learning resulted in much better results in training recurrent neural networks to capture long-term dependencies.']\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the ulum context described in the text?\",\n",
      "    \"What was observed when using a deterministic curriculum?\",\n",
      "    \"What does the text mention about the optimization methods discussed in this chapter?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The ulum context is a random mix of easy and difficult examples presented to the learner, with the proportion of difficult examples gradually increasing.\",\n",
      "    \"No improvement over the baseline was observed when using a deterministic curriculum.\",\n",
      "    \"The text mentions that the optimization methods discussed in this chapter are often directly applicable to specialized architectures with little or no modification.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': ['What is regularization in machine learning?', 'What are some strategies used in regularization?', 'How does regularization work in the context of deep learning?', 'What is the goal of regularization?', 'Why is it difficult to know if a model family includes the true data generating process?'], 'answers': ['Regularization in machine learning refers to strategies that are designed to reduce the test error by sacrificing some training error. It is intended to improve the algorithm's performance on new inputs.', 'Some regularization strategies include adding constraints on parameter values, incorporating extra terms in the objective function, encoding prior knowledge in the constraints and penalties, and using ensemble methods.', 'In the context of deep learning, regularization strategies typically focus on regularizing estimators. The goal is to trade increased bias for reduced variance, effectively moving the model from an overfitting regime to a regime where variance dominates the estimation error.', 'The goal of regularization is to improve generalization performance by finding a balance between bias and variance. The aim is to reduce variance significantly without overly increasing the bias.', 'It is difficult to know if a model family includes the true data generating process because the true process is often complex and outside the model family. In deep learning, algorithms are commonly applied to domains with complex generation processes, making it challenging to accurately capture the true process.'] }\n",
      "{'questions': [\n",
      "  'What is the purpose of regularization in deep learning models?',\n",
      "  'What is the regularized objective function denoted as?',\n",
      "  'What does setting α to 0 in the regularized objective function result in?',\n",
      "  'Why are biases typically left unregularized in neural networks?',\n",
      "  'What is L2 parameter regularization also known as?',\n",
      "  'What does weight decay regularization do to the weights?',\n",
      "  'How can the behavior of weight decay regularization be studied?',\n",
      "  'What is the formula for the regularized objective function?',\n",
      "  'What is the formula for the parameter gradient in weight decay regularization?'\n",
      " ],\n",
      " 'answers': [\n",
      "  'The purpose of regularization in deep learning models is to control the complexity of the model and minimize generalization error.',\n",
      "  'The regularized objective function is denoted as ˜J.',\n",
      "  'Setting α to 0 in the regularized objective function results in no regularization.',\n",
      "  \"Biases are typically left unregularized in neural networks because they require less data to fit accurately than the weights and regularizing them can introduce a significant amount of underfitting.\",\n",
      "  'L2 parameter regularization is also known as weight decay, ridge regression, or Tikhonov regularization.',\n",
      "  'Weight decay regularization drives the weights closer to the origin.',\n",
      "  'The behavior of weight decay regularization can be studied by analyzing the gradient of the regularized objective function.',\n",
      "  'The regularized objective function is given by ˜J(θ) = J(θ) + αΩ(θ).',\n",
      "  'The parameter gradient in weight decay regularization is given by ∇w˜J(θ) = ∇wJ(θ) + αw.'\n",
      " ]\n",
      "}\n",
      "{\"questions\": [\n",
      "    \"What does the weight decay term do to the learning rule?\",\n",
      "    \"What is the quadratic approximation used in the analysis?\",\n",
      "    \"How is the gradient of the regularized version of J used to find the minimum?\",\n",
      "    \"What happens to the regularized solution as alpha grows?\",\n",
      "    \"What is the effect of weight decay on the components of w∗?\",\n",
      "    \"What does the diagram in Figure 7.1 illustrate?\"\n",
      "],\n",
      "\"answers\": [\n",
      "    \"The weight decay term multiplicatively shrinks the weight vector by a constant factor before performing the usual gradient update.\",\n",
      "    \"The quadratic approximation used is given by ˆJ(θ) = J(w∗) + 1/2(w-w∗)ᵀH(w-w∗), where H is the Hessian matrix of J with respect to w evaluated at w∗.\",\n",
      "    \"The minimum of the regularized version of ˆJ occurs where its gradient, ˜w=∇ˆJ(w∗) + αw∗, is equal to zero.\",\n",
      "    \"As alpha grows, the regularized solution ˜w approaches w∗.\",\n",
      "    \"The components of w∗ that are aligned with the eigenvectors of the Hessian matrix are rescaled by a factor of the corresponding eigenvalues plus alpha.\",\n",
      "    \"The diagram in Figure 7.1 illustrates the effect of L2 (or weight decay) regularization on the value of the optimal w. It shows how the regularizer and the objective function reach an equilibrium at the point ˜w, where the regularizer pulls one component close to zero and has a small effect on the other component.\"\n",
      "]}\n",
      "{'questions': [{'question': 'What is the objective function for linear regression?', 'context': 'How do these effects relate to machine learning in particular? We can find out by studying linear regression, a model for which the true cost function is quadratic and therefore amenable to the same kind of analysis we have used so far. Applying the analysis again, we will be able to obtain a special case of the same results, but with the solution now phrased in terms of the training data. For linear regression, the cost function is Xw y ( - \\\\({Xw y}\\\\) ) - When we add L2 regularization, the objective function changes to Xw y ( - ) \\\\({Xw y}\\\\) + \\\\({1} \\over {2} \\\\) αw \\\\({w}\\\\)', 'answers': ['Xw y ( - ) \\\\({Xw y}\\\\) + \\\\({1} \\over {2} \\\\) αw \\\\({w}\\\\)']}, {'question': 'What does L2 regularization do to the learning algorithm in linear regression?', 'context': 'We can see that L2 regularization causes the learning algorithm to “perceive” the input X as having higher variance, which makes it shrink the weights on features whose covariance with the output target is low compared to this added variance.', 'answers': ['L2 regularization causes the learning algorithm to \"perceive\" the input X as having higher variance and shrink the weights on features with low covariance with the output target.']}, {'question': 'What is L1 regularization?', 'context': 'While L 2 weight decay is the most common form of weight decay, there are other ways to penalize the size of the model parameters. Another option is to use L1 regularization.  Formally, L1 regularization on the model parameter w  is defined as: θ Ω( ) = ||  ||  , wi| |  i', 'answers': ['L1 regularization is defined as the sum of absolute values of the individual parameters wi.']}, {'question': 'How is the effect of L1 regularization different from that of L2 regularization?', 'context': 'By inspecting equation, we can see immediately that the effect of L 1 regularization is quite different from that of L2 regularization. Specifically, we can see that the regularization contribution to the gradient no longer scales linearly with each wi; instead it is a constant factor with a sign equal to sign(wi).', 'answers': ['The effect of L1 regularization is that the regularization contribution to the gradient is a constant factor with a sign equal to sign(wi), unlike L2 regularization that scales linearly with each wi.']}], 'answers': ['Xw y ( - ) \\\\({Xw y}\\\\) + \\\\({1} \\over {2} \\\\) αw \\\\({w}\\\\)', 'L2 regularization causes the learning algorithm to \"perceive\" the input X as having higher variance and shrink the weights on features with low covariance with the output target.', 'L1 regularization is defined as the sum of absolute values of the individual parameters wi.', 'The effect of L1 regularization is that the regularization contribution to the gradient is a constant factor with a sign equal to sign(wi), unlike L2 regularization that scales linearly with each wi.']}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the gradient given by in the context of the Taylor series?\",\n",
      "    \"Under what assumption is the Hessian matrix diagonal?\",\n",
      "    \"What are the two possible outcomes when w∗i > 0?\",\n",
      "    \"How does L1 regularization differ from L2 regularization in terms of sparsity?\",\n",
      "    \"What is the LASSO model?\",\n",
      "    \"What is the equivalent prior term for L1 regularization in Bayesian inference?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The gradient in the context of the Taylor series is given by w ∇ J (∗)\",\n",
      "    \"The Hessian matrix is assumed to be diagonal when there is no correlation between the input features.\",\n",
      "    \"The two possible outcomes when w∗i > 0 are: 1) wi = 0 if w∗i ≤ α Hi,i, and 2) wi = sign(w∗i) * (|w∗i| - α Hi,i) otherwise.\",\n",
      "    \"L1 regularization results in sparsity, where some parameters have an optimal value of zero. L2 regularization does not cause sparsity.\",\n",
      "    \"The LASSO model integrates an L1 penalty with a linear model and a least squares cost function, causing a subset of the weights to become zero.\",\n",
      "    \"The equivalent prior term for L1 regularization in Bayesian inference is the log-prior term that is maximized by MAP Bayesian inference when the prior is an isotropic Laplace distribution.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': [{'question': 'What is the purpose of the Karush–Kuhn–Tucker (KKT) multiplier in the generalized Lagrange function?', 'answers': ['The purpose of the Karush–Kuhn–Tucker (KKT) multiplier in the generalized Lagrange function is to construct penalties for each constraint.']}, {'question': 'How can we constrain Ω(θ) to be less than a certain constant?', 'answers': ['We can construct a generalized Lagrange function with a penalty term equal to the product of a coeﬃcient (KKT multiplier) and a function representing whether the constraint is satisﬁed.']}, {'question': 'What is the solution to the constrained problem?', 'answers': ['The solution to the constrained problem is given by θ∗ = arg min θ max α,α ≥ )θ, α.']}, {'question': 'How is the effect of the constraint on the function observed?', 'answers': ['By fixing the optimal value of α and viewing the problem as a function of Ω(θ), the effect of the constraint can be observed.']}, {'question': 'What does a parameter norm penalty impose as a constraint?', 'answers': ['A parameter norm penalty imposes a constraint on the L2 norm of the weights.']}], 'answers': [{'answer_start': 338, 'text': 'The purpose of the Karush–Kuhn–Tucker (KKT) multiplier in the generalized Lagrange function is to construct penalties for each constraint.'}, {'answer_start': 449, 'text': 'We can construct a generalized Lagrange function with a penalty term equal to the product of a coeﬃcient (KKT multiplier) and a function representing whether the constraint is satisﬁed.'}, {'answer_start': 545, 'text': 'The solution to the constrained problem is given by θ∗ = arg min θ max α,α ≥ )θ, α.'}, {'answer_start': 629, 'text': 'By fixing the optimal value of α and viewing the problem as a function of Ω(θ), the effect of the constraint can be observed.'}, {'answer_start': 702, 'text': 'A parameter norm penalty imposes a constraint on the L2 norm of the weights.'}]}\n",
      "{\n",
      "\"questions\": [\n",
      "\"What is the purpose of constraining the norm of each column of the weight matrix in a neural net layer?\",\n",
      "\"What are some linear models in machine learning that require regularization?\",\n",
      "\"When would a matrix XX be singular? How is this problem addressed?\",\n",
      "\"Why is regularization important for underdetermined problems in machine learning?\",\n",
      "\"What is the Moore-Penrose pseudoinverse used for in linear algebra?\"\n",
      "],\n",
      "\"answers\": [\n",
      "\"The purpose of constraining the norm of each column of the weight matrix in a neural net layer is to prevent any one hidden unit from having very large weights and to maintain stability in the parameter space.\",\n",
      "\"Linear models such as linear regression and PCA in machine learning require regularization.\",\n",
      "\"A matrix XX is singular when the data generating distribution has no variance in some direction or when there are fewer examples (rows of X) than input features (columns of X). This problem is addressed by using regularization techniques that involve inverting the matrix I+α instead.\",\n",
      "\"Regularization is important for underdetermined problems in machine learning because it guarantees the convergence of iterative methods and prevents numerical overflow. It helps in stabilizing the problem and finding a solution.\",\n",
      "\"The Moore-Penrose pseudoinverse is used in linear algebra for solving underdetermined linear equations and stabilizing underdetermined problems using regularization. It is equivalent to performing linear regression with a shrinking regularization coefficient.\"\n",
      "]\n",
      "}\n",
      "{'questions': [\n",
      "  'What is the main task facing a classifier?',\n",
      "  'What is dataset augmentation?',\n",
      "  'What operations can be applied to training images in object recognition problem?',\n",
      "  'Why are horizontal flips and 180° rotations not appropriate for augmenting datasets in optical character recognition tasks?',\n",
      "  'What is out-of-plane rotation and why is it difficult to perform?',\n",
      "  'What is an example of applying dataset augmentation in speech recognition tasks?',\n",
      "  'How can neural networks be made robust to noise?',\n",
      "  'What is the purpose of input noise injection in unsupervised learning algorithms?',\n",
      "  'What is dropout regularization strategy?',\n",
      "  'Why is it important to take the effect of dataset augmentation into account when comparing machine learning benchmark results?',\n",
      "  'What should be ensured in controlled experiments when comparing machine learning algorithms?',\n",
      "  'What is a possible reason for improved performance in algorithm B when combined with numerous synthetic transformations?'\n",
      "],\n",
      "'answers': [\n",
      "  'The main task facing a classifier is to be invariant to a wide variety of transformations and summarize the input with a single category identity.',\n",
      "  'Dataset augmentation is a technique of generating new fake data by transforming the inputs in the training set.',\n",
      "  'Operations such as translating the training images, rotating the image, or scaling the image can be applied to training images in object recognition problem.',\n",
      "  'Horizontal flips and 180° rotations are not appropriate for augmenting datasets in optical character recognition tasks because they change the correct class and make it difficult to recognize the difference between similar characters such as \"b\" and \"d\", or \"6\" and \"9\".',\n",
      "  'Out-of-plane rotation refers to rotation in three-dimensional space which cannot be implemented as a simple geometric operation on the input pixels.',\n",
      "  'Injecting noise in the input to a neural network is an example of applying dataset augmentation in speech recognition tasks.',\n",
      "  'Neural networks can be made robust to noise by training them with random noise applied to their inputs.',\n",
      "  'Input noise injection is a technique in unsupervised learning algorithms where random noise is applied to the hidden units, which helps in dataset augmentation at multiple levels of abstraction.',\n",
      "  'Dropout is a powerful regularization strategy where new inputs are constructed by multiplying by noise.',\n",
      "  'It is important to take the effect of dataset augmentation into account when comparing machine learning benchmark results because hand-designed dataset augmentation schemes can dramatically reduce the generalization error of a machine learning technique.',\n",
      "  'In controlled experiments when comparing machine learning algorithms, it should be ensured that both algorithms were evaluated using the same hand-designed dataset augmentation schemes.',\n",
      "  'A possible reason for improved performance in algorithm B when combined with numerous synthetic transformations is that the synthetic transformations caused the improved performance rather than the use of machine learning algorithm B.'\n",
      "]}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What are some examples of operations that are generally applicable in machine learning algorithms?\",\n",
      "    \"What is the importance of noise applied to the hidden units?\",\n",
      "    \"How can noise be used to regularize models?\",\n",
      "    \"How can noise applied to the weights be interpreted?\",\n",
      "    \"What is the objective function for minimizing the squared error of the output of a network with injected weight noise?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Examples of operations that are generally applicable in machine learning algorithms include adding Gaussian noise to the input.\",\n",
      "    \"Noise applied to the hidden units is important for regularization as it can be more powerful than simply shrinking the parameters.\",\n",
      "    \"Noise can be used to regularize models by injecting it into the inputs, hidden units, or weights.\",\n",
      "    \"Noise applied to the weights can be interpreted as a stochastic implementation of Bayesian inference over the weights or as a form of regularization that encourages stability of the function to be learned.\",\n",
      "    \"The objective function for minimizing the squared error of the output of a network with injected weight noise is ∼JW = Ep(x W, y)[(ˆyW(x) - y)²].\"\n",
      "  ]\n",
      "}\n",
      "{'questions': [{'question': 'What is the purpose of label smoothing?', 'answer': 'The purpose of label smoothing is to prevent the pursuit of hard probabilities without discouraging correct classification.'}, {'question': 'What does semi-supervised learning in deep learning refer to?', 'answer': 'In deep learning, semi-supervised learning refers to learning a representation where examples from the same class have similar representations.'}, {'question': 'What is multi-task learning?', 'answer': 'Multi-task learning is a way to improve generalization by pooling examples arising from several tasks and sharing part of the model across those tasks.'}], 'answers': []}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What are the two kinds of parts in the model?\",\n",
      "    \"What are task-specific parameters?\",\n",
      "    \"What are generic parameters?\",\n",
      "    \"What is the assumption in multi-task learning?\",\n",
      "    \"What is the purpose of shared parameters in multi-task learning?\",\n",
      "    \"What is early stopping in deep learning?\",\n",
      "    \"How can a model with better validation set error be obtained?\",\n",
      "    \"When does the training algorithm terminate?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The two kinds of parts in the model are task-specific parameters and generic parameters.\",\n",
      "    \"Task-specific parameters are the upper layers of the neural network that only benefit from the examples of their task to achieve good generalization.\",\n",
      "    \"Generic parameters are shared across all the tasks and benefit from the pooled data of all the tasks.\",\n",
      "    \"The assumption in multi-task learning is that there exists a common pool of factors that explain the variations in the input, while each task is associated with a subset of these factors.\",\n",
      "    \"Shared parameters in multi-task learning improve generalization and generalization error bounds by utilizing statistical strength from the increased number of examples for the shared parameters compared to single-task models.\",\n",
      "    \"Early stopping in deep learning refers to the observation that training error decreases steadily over time, but validation set error begins to rise again. It involves returning to the parameter setting at the point with the lowest validation set error.\",\n",
      "    \"A model with better validation set error can be obtained by returning to the parameter setting at the point with the lowest validation set error.\",\n",
      "    \"The training algorithm terminates when no parameters have improved over the threshold.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is early stopping?\",\n",
      "    \"Why is early stopping popular in deep learning?\",\n",
      "    \"How is early stopping related to hyperparameter selection?\",\n",
      "    \"What is the cost of using early stopping?\",\n",
      "    \"How does early stopping compare to weight decay as a regularization strategy?\",\n",
      "    \"Can early stopping be used with other regularization strategies?\",\n",
      "    \"What requirement does early stopping have in terms of training data?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Early stopping is a regularization strategy in deep learning that involves stopping the training process when the validation error starts to worsen.\",\n",
      "    \"Early stopping is popular in deep learning due to its effectiveness and simplicity as a regularization strategy.\",\n",
      "    \"Early stopping can be seen as a hyperparameter selection algorithm, where the number of training steps is treated as a hyperparameter.\",\n",
      "    \"The cost of using early stopping includes the need to periodically evaluate the validation set error during training and the storage of the best parameters. However, these costs are generally considered negligible.\",\n",
      "    \"Early stopping is a less intrusive form of regularization compared to weight decay, as it requires minimal changes to the training procedure and does not risk trapping the network in bad local minima.\",\n",
      "    \"Early stopping can be used alone or in conjunction with other regularization strategies.\",\n",
      "    \"Early stopping requires a validation set, which means that some training data is not used to train the model.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What are the two strategies discussed for the second training procedure?\",\n",
      "    \"What are the subtleties associated with retraining for the same number of passes through the dataset?\",\n",
      "    \"How does the second strategy for using all the data differ from the first strategy?\",\n",
      "    \"What is the advantage of using early stopping as a regularization strategy?\",\n",
      "    \"How does early stopping restrict the optimization procedure according to Bishop (1995a)?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The two strategies discussed for the second training procedure are reinitializing the model and retraining on all the data or continuing training using the parameters obtained from the first round.\",\n",
      "    \"The subtleties associated with retraining for the same number of passes through the dataset include the fact that each pass through the dataset in the second round of training will require more parameter updates due to the larger training set.\",\n",
      "    \"The second strategy for using all the data involves continuing training with the parameters obtained from the first round, without any guide on when to stop in terms of a number of steps.\",\n",
      "    \"The advantage of using early stopping as a regularization strategy is that it reduces the computational cost of the training procedure, as it limits the number of training iterations and provides regularization without the need for penalty terms or their gradients.\",\n",
      "    \"According to Bishop (1995a), early stopping restricts the optimization procedure to a relatively small volume of parameter space around the initial parameter value. The product of the number of iterations and the learning rate is viewed as a measure of effective capacity.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': [{'question': 'What is the behavior of early stopping compared to L2 regularization?', 'category': 'summary'}, {'question': 'What is the quadratic approximation used to model the cost function?', 'category': 'summary'}, {'question': 'What is the ...a setting with linear weights?', 'category': 'summary'}, {'question': 'Under what assumptions are L2 regularization and early stopping shown to be equivalent?', 'category': 'summary'}, {'question': 'What role does the number of training iterations play?', 'category': 'summary'}], 'answers': []}\n",
      "{'questions': \n",
      " [{'question': 'What is the advantage of early stopping over weight decay?', 'answers': ['Early stopping automatically determines the correct amount of regularization while weight decay requires many training experiments with different values of its hyperparameter.']}, \n",
      " {'question': 'What is parameter tying and parameter sharing in deep learning?', 'answers': ['Parameter tying and parameter sharing refer to expressing prior knowledge about suitable values of model parameters, where certain parameters should be close to one another.', 'Parameter sharing is a regularization method that forces sets of parameters to be equal, reducing the memory footprint of the model.', 'Parameter tying is a method used to ensure that parameters in different models are close to each other.']},\n",
      " {'question': 'What is the purpose of using parameter norm penalty or constraints in regularization?','answers': ['The purpose is to regularize parameters such that they are close to each other or force sets of parameters to be equal.']},\n",
      " {'question': 'What is a common application of parameter sharing in deep learning?', 'answers': ['Parameter sharing is extensively used in convolutional neural networks (CNNs) applied to computer vision.']}, \n",
      " {'question': 'What statistical property of natural images do convolutional neural networks (CNNs) take into account?', 'answers': ['CNNs take into account the statistical property of translation invariance in natural images.']}, \n",
      " {'question': 'What does the L2-regularized objective correspond to?', 'answers': ['The L2-regularized objective corresponds to a minimum point in space where a trajectory of length τ ends.']}, \n",
      " {'question': 'What is the main advantage of parameter sharing over parameter norm penalty in deep learning?', 'answers': ['Parameter sharing only requires storing a subset of the parameters in memory, leading to a significant reduction in the memory footprint of the model.']}],\n",
      " 'answers': []}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the purpose of parameter sharing in CNNs?\",\n",
      "    \"How does weight decay act as a penalty on the model parameters?\",\n",
      "    \"What does representational sparsity mean?\",\n",
      "    \"What is the difference between parameter sparsity and representational sparsity?\",\n",
      "    \"What is norm penalty regularization of representations?\",\n",
      "    \"What does an L1 penalty on the elements of the representation induce?\",\n",
      "    \"What are some examples of penalties that result in a sparse representation?\",\n",
      "    \"How can representational sparsity be achieved with a hard constraint on activation values?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The purpose of parameter sharing in CNNs is to reduce the number of unique model parameters and increase network sizes without requiring more training data.\",\n",
      "    \"Weight decay acts as a penalty on the model parameters by directly placing a penalty on them.\",\n",
      "    \"Representational sparsity refers to a representation where many of the elements are zero or close to zero.\",\n",
      "    \"Parameter sparsity refers to a sparse parametrization where many of the parameters become zero or close to zero, while representational sparsity refers to a representation where many of the elements are zero or close to zero.\",\n",
      "    \"Norm penalty regularization of representations is performed by adding a norm penalty on the representation to the loss function.\",\n",
      "    \"An L1 penalty on the elements of the representation induces representational sparsity.\",\n",
      "    \"Examples of penalties that result in a sparse representation include penalties derived from a Student-t prior, KL divergence penalties, and penalties based on regularizing the average activation across examples.\",\n",
      "    \"Representational sparsity can be achieved with a hard constraint on activation values by solving a constrained optimization problem where the number of non-zero entries in the representation is limited.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is bagging?\",\n",
      "    \"How does model averaging work?\",\n",
      "    \"What is the expected squared error of the ensemble predictor?\",\n",
      "    \"How does the ensemble perform when errors are perfectly uncorrelated?\",\n",
      "    \"How are different ensemble methods constructed?\",\n",
      "    \"What is the purpose of bagging?\",\n",
      "    \"Describe how bagging works with an example.\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Bagging is a technique for reducing generalization error by combining several models.\",\n",
      "    \"Model averaging works because different models usually do not make all the same errors on the test set.\",\n",
      "    \"The expected squared error of the ensemble predictor is 1/k^2 * (k * v + 1 - k * (1/k * sum(η^2_i) + 1/k * sum(η_i * η_j))) where η_i and η_j are the errors made by each model.\",\n",
      "    \"When errors are perfectly uncorrelated, the expected squared error of the ensemble decreases linearly with the ensemble size.\",\n",
      "    \"Different ensemble methods construct the ensemble of models in different ways.\",\n",
      "    \"The purpose of bagging is to reuse the same kind of model, training algorithm, and objective function several times.\",\n",
      "    \"Bagging involves constructing k different datasets by sampling with replacement from the original dataset, training each model on its corresponding dataset, and then averaging their output to create a robust ensemble.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': ['What is the power of model averaging?', 'Why is model averaging discouraged in benchmark comparisons?', 'What is boosting and how is it used with neural networks?', 'What is dropout and how does it work?', 'How does dropout approximate the process of bagging?', 'What is the purpose of the binary mask in dropout?', 'Is the probability of sampling a mask value fixed or variable during training?'], 'answers': ['Model averaging is an extremely powerful and reliable method for reducing generalization error.', 'Model averaging is discouraged in benchmark comparisons because it can substantially increase computation and memory requirements.', 'Boosting is a technique that constructs ensembles with higher capacity than individual models. It has been applied in neural networks by incrementally adding hidden units to the network.', 'Dropout is a method of regularizing a broad family of models and can be thought of as a computationally inexpensive version of bagging for large neural network ensembles.', 'Dropout approximates the process of bagging by training an ensemble consisting of all sub-networks that can be formed by removing non-output units from a base network.', 'The binary mask in dropout is used to randomly select units (input and hidden units) to be included or excluded during training. It helps in regularization by preventing units from relying too heavily on each other.', 'The probability of sampling a mask value is fixed before training begins and is not dependent on the current model parameters or input example.'] }\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the purpose of dropout training?\",\n",
      "    \"How does dropout training differ from bagging training?\",\n",
      "    \"What is the relationship between dropout training and parameter sharing?\",\n",
      "    \"What is the difference between dropout training and bagging training in terms of the number of models trained?\",\n",
      "    \"How does dropout training handle the training set for each sub-network?\",\n",
      "    \"What is the process of inference in bagged ensemble models?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The purpose of dropout training is to minimize the cost of a model defined by parameters and a mask vector, through the inclusion/exclusion of units.\",\n",
      "    \"Unlike bagging training, dropout training involves models that share parameters and inherit different subsets of parameters from the parent neural network.\",\n",
      "    \"Dropout training utilizes parameter sharing to represent an exponential number of models with a tractable amount of memory.\",\n",
      "    \"In bagging training, each model is trained to convergence on its respective training set, while in dropout training, only a tiny fraction of possible sub-networks are trained for a single step, with the remaining sub-networks arriving at good parameter settings through parameter sharing.\",\n",
      "    \"For each sub-network, dropout training samples a subset of the original training set with replacement.\",\n",
      "    \"Inference in bagged ensemble models refers to the process of accumulating votes from all the ensemble members to make a prediction.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': [{'question': 'What does bagging assume about the model that is outputting a probability distribution?', 'context': 'Now, we assume that the model’s role is to output a probability distribution.'}, {'question': 'In bagging, how is the pred...teristics of the weight scaling inference rule?'}], 'answers': [{'answer': 'The bagging algorithm assumes that the model's role is to output a probability distribution.', 'is_impossible': False}, {'answer': 'The ensemble prediction is the geometric mean.', 'is_impossible': False}, {'answer': 'No, the geometric mean of multiple probability distributions is not guaranteed to be a probability distribution.', 'is_impossible': False}, {'answer': 'It performs very well empirically.', 'is_impossible': False}]}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the weight scaling rule?\",\n",
      "    \"Is the weight scaling rule exact in all settings?\",\n",
      "    \"What is the advantage of dropout regularization?\",\n",
      "    \"What type of models does dropout work well with?\",\n",
      "    \"Does using dropout increase the size of the model?\",\n",
      "    \"What is the cost of using dropout in a complete system?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The weight scaling rule is a formula used in regularization for deep learning.\",\n",
      "    \"No, the weight scaling rule is only exact in certain settings and is an approximation in deep models with nonlinearities.\",\n",
      "    \"The advantage of dropout regularization is that it is computationally cheap and can be applied to nearly any model that uses a distributed representation and can be trained with stochastic gradient descent.\",\n",
      "    \"Dropout works well with models such as feedforward neural networks, probabilistic models (such as restricted Boltzmann machines), and recurrent neural networks.\",\n",
      "    \"Yes, using dropout requires increasing the size of the model to offset its reduction in effective capacity.\",\n",
      "    \"Using dropout in a complete system can be significant, as it requires a larger model and many more iterations of the training algorithm.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      " \"questions\": [\n",
      "   \"What is the purpose of regularization in deep learning?\",\n",
      "   \"When does regularization confer little reduction in generalization error?\",\n",
      "   \"What is the advantage of unsupervised feature learning over dropout?\",\n",
      "   \"What is the relationship between dropout and L2 weight decay?\",\n",
      "   \"What is fast dropout and how does it affect convergence time?\",\n",
      "   \"What is dropout boosting and how does it differ from traditional dropout?\",\n",
      "   \"What is the main idea behind DropConnect?\",\n",
      "   \"What is the most widely used implicit ensemble method?\",\n",
      "   \"How does dropout implement a form of bagging with parameter sharing?\",\n",
      "   \"What should be considered when choosing modification families for dropout?\"\n",
      " ],\n",
      " \"answers\": [\n",
      "   \"The purpose of regularization in deep learning is to prevent overfitting and improve generalization performance.\",\n",
      "   \"Regularization confers little reduction in generalization error when working with large datasets or when additional unlabeled data is available.\",\n",
      "   \"Unsupervised feature learning can be more effective than dropout when there is additional unlabeled data available.\",\n",
      "   \"Dropout is equivalent to L2 weight decay when applied to linear regression, but for deep models, dropout is not equivalent to weight decay.\",\n",
      "   \"Fast dropout is an approximation of dropout that reduces the stochasticity in the computation of the gradient, leading to faster convergence time.\",\n",
      "   \"Dropout boosting is a method that uses the same mask noise as traditional dropout but lacks its regularizing effect. It trains the entire ensemble to maximize the log-likelihood on the training set.\",\n",
      "   \"DropConnect is a special case of dropout where each product between a single scalar weight and a single hidden unit state is considered a unit that can be dropped.\",\n",
      "   \"The most widely used implicit ensemble method is dropout.\",\n",
      "   \"By training a network with stochastic behavior and averaging predictions over multiple stochastic decisions, dropout implements a form of bagging with parameter sharing.\",\n",
      "   \"When choosing modification families for dropout, it is important to use random modifications that neural networks can learn to resist and model families that allow for fast approximation.\"\n",
      " ]\n",
      "}\n",
      "{\"questions\": [\"What is the purpose of dropout in deep learning?\", \"How does dropout affect the performance of hidden units?\", \"What is the advantage of using multiplicative noise in dropout?\", \"What is the purpose of batch normalization in deep learning?\", \"Why is it important to probe the level of understanding of a neural network?\"], \"answers\": [\"The purpose of dropout in deep learning is to train an ensemble of models that share hidden units and perform efficient, approximate bagging.\", \"Dropout affects the performance of hidden units by regularizing them to be good features in many different contexts.\", \"The advantage of using multiplicative noise in dropout is that it prevents hidden units from simply becoming very large to make the noise insignificant by comparison.\", \"The purpose of batch normalization in deep learning is to improve optimization, but it can also have a regularizing effect and make dropout unnecessary.\", \"It is important to probe the level of understanding of a neural network to assess if it has obtained a true human-level understanding of the tasks it is trained on.\"]\n",
      "{'questions': [{'question': 'What are adversarial examples?', 'answers': ['Adversarial examples are inputs that are intentionally constructed to cause a neural network to make highly different predictions compared to the original input.']}], 'answers': [{'answer_start': 170, 'text': 'Adversarial examples are inputs that are intentionally constructed to cause a neural network to make highly different predictions compared to the original input.'}]}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the tangent distance algorithm?\",\n",
      "    \"How does the tangent distance algorithm calculate distances between points?\",\n",
      "    \"What is the tangent prop algorithm?\",\n",
      "    \"How does the tangent prop algorithm achieve local invariance?\",\n",
      "    \"What is the relationship between tangent propagation and dataset augmentation?\",\n",
      "    \"What are the drawbacks of the analytical approach used in tangent propagation?\",\n",
      "    \"Why does dataset augmentation work well with rectified linear units?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The tangent distance algorithm is a non-parametric nearest-neighbor algorithm that assumes the data lies near a low-dimensional manifold.\",\n",
      "    \"The tangent distance algorithm approximates the distance between points by measuring the distance between the tangent planes of the manifolds to which the points belong.\",\n",
      "    \"The tangent prop algorithm is a neural network classifier trained with an extra penalty to achieve local invariance to known factors of variation.\",\n",
      "    \"The tangent prop algorithm achieves local invariance by requiring the directional derivative of the neural network output to be small in the directions of the known manifold tangent vectors.\",\n",
      "    \"Both tangent propagation and dataset augmentation involve encoding prior knowledge of the task by specifying transformations that should not alter the network output. However, dataset augmentation explicitly trains the network on distinct inputs created by applying transformations, while tangent propagation analytically regularizes the model to resist perturbations in the specified transformation directions.\",\n",
      "    \"The analytical approach used in tangent propagation only regularizes the model to resist infinitesimal perturbations. Dataset augmentation confers resistance to larger perturbations. Additionally, the approach poses difficulties for models based on rectified linear units, as they cannot shrink their derivatives by saturating at a high value with large weights.\",\n",
      "    \"Dataset augmentation works well with rectified linear units because different subsets of rectified units can activate for different transformed versions of each original input.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What are some regularization techniques for neural networks mentioned in the text?\",\n",
      "    \"What is the main idea behind tangent propagation?\",\n",
      "    \"How does the manifold tangent classiﬁer estimate tangent vectors?\",\n",
      "    \"What are the two steps in the algorithm proposed with the manifold tangent classiﬁer?\",\n",
      "    \"What are the central themes of machine learning mentioned in the text?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Regularization techniques mentioned in the text include double backprop, adversarial training, and dataset augmentation using manually specified transformations.\",\n",
      "    \"The main idea behind tangent propagation is that the model should be invariant to certain specified directions of change in the input.\",\n",
      "    \"The manifold tangent classiﬁer estimates tangent vectors by training an autoencoder to fit the training data.\",\n",
      "    \"The algorithm proposed with the manifold tangent classiﬁer involves using an autoencoder for unsupervised learning to learn the manifold structure, and then using these tangent vectors to regularize a neural net classifier.\",\n",
      "    \"The central themes of machine learning mentioned in the text are regularization and optimization.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What are deep feedforward networks often called?\",\n",
      "    \"What is the goal of a feedforward network?\",\n",
      "    \"What is the difference between feedforward networks and recurrent neural networks?\",\n",
      "    \"What are feedforward networks used for?\",\n",
      "    \"How are feedforward neural networks represented?\",\n",
      "    \"What is the final layer of a feedforward network called?\",\n",
      "    \"What does the training data specify in a feedforward network?\",\n",
      "    \"Why are the layers between the input and output layers called hidden layers?\",\n",
      "    \"Why are feedforward networks called neural?\",\n",
      "    \"What is the inspiration for using many layers of vector-valued representation in neural networks?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Deep feedforward networks are often called feedforward neural networks or multilayer perceptrons (MLPs).\",\n",
      "    \"The goal of a feedforward network is to approximate some function f * by learning the value of the parameters θ that result in the best function approximation.\",\n",
      "    \"Feedforward neural networks do not have feedback connections, while recurrent neural networks do.\",\n",
      "    \"Feedforward networks are used in many important commercial applications, including object recognition and natural language processing.\",\n",
      "    \"Feedforward neural networks are represented by composing together many different functions, forming a directed acyclic graph.\",\n",
      "    \"The final layer of a feedforward network is called the output layer.\",\n",
      "    \"The training data specifies what the output layer must do at each point x, but does not specify the behavior of the hidden layers.\",\n",
      "    \"The layers between the input and output layers are called hidden layers because the training data does not show the desired output for each of these layers.\",\n",
      "    \"Feedforward networks are called neural because they are loosely inspired by neuroscience and each hidden layer can be interpreted as playing a role analogous to a neuron.\",\n",
      "    \"The inspiration for using many layers of vector-valued representation in neural networks comes from neuroscience and the functions that biological neurons compute.\"\n",
      "  ]\n",
      "}\n",
      "{\"questions\": [\"What are the limitations of linear models?\", \"How can linear models be extended to represent nonlinear functions?\", \"What is the strategy of deep learning?\", \"How does deep learning improve upon manual feature engineering?\", \"What is the general principle of improving models by learning features?\", \"What other types of models can benefit from the principle of learning features?\", \"What design decisions are necessary for training a feedforward network?\"], \"answers\": [\"Linear models have the limitation of being limited to representing linear functions and cannot understand the interaction between input variables.\", \"Linear models can be extended to represent nonlinear functions by applying a nonlinear transformation to the input, either through a transformation function or by using the kernel trick.\", \"The strategy of deep learning is to learn the feature representation, rather than manually engineering it. It involves using a model to learn the representation from a broad class of functions.\", \"Deep learning improves upon manual feature engineering by allowing the model to learn the optimal representation for the task, instead of relying on human-designed features. This reduces the need for extensive domain expertise and transfer between different domains.\", \"The general principle of improving models is by learning features, which involves finding an optimal representation from a set of features that capture the necessary information for the task. This principle applies to all types of models in deep learning.\", \"Other types of models, such as models for learning stochastic mappings, learning functions with feedback, and learning probability distributions, can benefit from the principle of learning features.\", \"The design decisions necessary for training a feedforward network include choosing the optimizer, the cost function, and the form of the output units.\"]\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the XOR function?\",\n",
      "    \"What is the goal of the learning algorithm in this example?\",\n",
      "    \"Why is the mean squared error (MSE) loss function used in this example?\",\n",
      "    \"What is the form of the linear model used in this example?\",\n",
      "    \"Why is a linear model unable to represent the XOR function?\",\n",
      "    \"What is the architecture of the feedforward network used in this example?\",\n",
      "    \"Why must we use a nonlinear function to describe the features in the feedforward network?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The XOR function returns 1 when exactly one of the binary values is equal to 1, otherwise it returns 0.\",\n",
      "    \"The goal of the learning algorithm is to adapt the parameters of the model to make it as similar as possible to the target XOR function.\",\n",
      "    \"The mean squared error (MSE) loss function is used to simplify the math in this example, although it may not be appropriate for modeling binary data in practical applications.\",\n",
      "    \"The linear model used in this example is defined as f(x; θ) = x * w + b, where θ consists of the weights w and the bias b.\",\n",
      "    \"A linear model is unable to represent the XOR function because it cannot capture the nonlinear relationship between the input values and the output.\",\n",
      "    \"The feedforward network used in this example has one hidden layer with two hidden units. The hidden layer computes the values of the hidden units, which are then used as input for the output layer.\",\n",
      "    \"A nonlinear function must be used to describe the features in the feedforward network because if a linear function was used, the whole network would remain a linear function of its input. By using a nonlinear function, the network can learn a different feature space where a linear model can represent the solution.\"\n",
      "  ]\n",
      "}\n",
      "{\"questions\": [\"What is the purpose of an activation function in a neural network?\", \"What is the default activation function recommended for use with most feedforward neural networks?\", \"How can the XOR problem be solved using a feedforward neural network?\"], \"answers\": [\"The purpose of an activation function in a neural network is to introduce nonlinearity into the model, allowing it to learn and represent complex relationships in the data.\", \"The default activation function recommended for use with most feedforward neural networks is the rectified linear unit or ReLU.\", \"The XOR problem can be solved using a feedforward neural network by learning a representation in a transformed space where a linear model can solve the problem. The transformed space is obtained by applying a nonlinear activation function to the output of a linear transformation.\"] }\n",
      "{'questions': [{'question': 'What principle can be applied to build complicated systems from minimal components?', 'answers': ['The common principle throughout computer science is that we can build complicated systems from minimal components']}, {'question': 'What is the purpose of the first step in the neural network?', 'answers': ['The purpose of the first step in the neural network is to multiply the input matrix by the first layer’s weight matrix']}, {'question': 'What transformation is applied to compute the value of h for each example?', 'answers': ['The rectified linear transformation is applied to compute the value of h for each example']}, {'question': 'Is a linear model able to implement a function that rises and then drops?', 'answers': ['No, a linear model cannot implement a function that rises and then drops']}, {'question': 'How does the neural network solve the XOR problem?', 'answers': ['The neural network obtains the correct answer for every example in the batch']}, {'question': 'What type of optimization algorithm can find parameters that produce very little error?', 'answers': ['A gradient-based optimization algorithm can find parameters that produce very little error']}, {'question': 'Why are neural networks usually trained with gradient-based optimizers?', 'answers': ['Neural networks are usually trained with gradient-based optimizers because the nonlinearity of neural networks causes most interesting loss functions to become non-convex']}, {'question': 'What is the importance of initializing all weights in feedforward neural networks?', 'answers': ['It is important to initialize all weights to small random values in feedforward neural networks']}, {'question': 'What type of optimization algorithm is sensitive to the values of the initial parameters?', 'answers': ['Stochastic gradient descent applied to non-convex loss functions is sensitive to the values of the initial parameters']}, {'question': 'What should the biases be initialized to in feedforward neural networks?', 'answers': ['The biases may be initialized to zero or to small positive values in feedforward neural networks']}, {'question': 'What is the purpose of the iterative gradient-based optimization algorithms?', 'answers': ['The purpose of the iterative gradient-based optimization algorithms is to train feedforward networks and almost all other deep learning models']}, {'question': 'What type of optimization converges starting from any initial parameters?', 'answers': ['Convex optimization converges starting from any initial parameters']}, {'question': 'What type of optimization algorithm is used to train logistic regression or SVMs?', 'answers': ['Convex optimization algorithms with global convergence guarantees are used to train logistic regression or SVMs']}], 'answers': [{'answer_start': 17, 'text': 'The common principle throughout computer science is that we can build complicated systems from minimal components.'}, {'answer_start': 286, 'text': 'The purpose of the first step in the neural network is to multiply the input matrix by the first layer’s weight matrix.'}, {'answer_start': 748, 'text': 'The rectified linear transformation is applied to compute the value of h for each example.'}, {'answer_start': 1612, 'text': 'No, a linear model cannot implement a function that rises and then drops.'}, {'answer_start': 1779, 'text': 'The neural network obtains the correct answer for every example in the batch.'}, {'answer_start': 1861, 'text': 'A gradient-based optimization algorithm can find parameters that produce very little error.'}, {'answer_start': 2321, 'text': 'Neural networks are usually trained with gradient-based optimizers because the nonlinearity of neural networks causes most interesting loss functions to become non-convex.'}, {'answer_start': 3140, 'text': 'It is important to initialize all weights to small random values in feedforward neural networks.'}, {'answer_start': 3203, 'text': 'Stochastic gradient descent applied to non-convex loss functions is sensitive to the values of the initial parameters.'}, {'answer_start': 3420, 'text': 'The biases may be initialized to zero or to small positive values in feedforward neural networks.'}, {'answer_start': 3695, 'text': 'The purpose of the iterative gradient-based optimization algorithms is to train feedforward networks and almost all other deep learning models.'}, {'answer_start': 4128, 'text': 'Convex optimization converges starting from any initial parameters.'}, {'answer_start': 4217, 'text': 'Convex optimization algorithms with global convergence guarantees are used to train logistic regression or SVMs.'}]}\n",
      "{\"questions\": [\"What is the purpose of the training algorithm?\", \"What is the back-propagation algorithm used for?\", \"What are the design considerations for deep neural networks?\", \"What are some common cost functions for neural networks?\", \"How are neural networks trained using maximum likelihood?\", \"What advantage does deriving the cost function from maximum likelihood have?\"], \"answers\": [\"The purpose of the training algorithm is to use the gradient to descend the cost function.\", \"The back-propagation algorithm is used to obtain the gradient for a neural network.\", \"The design considerations for deep neural networks include choosing a cost function and representing the output of the model.\", \"Some common cost functions for neural networks include cross-entropy between the training data and the model's predictions, and specialized loss functions for predicting statistics of the output.\", \"Neural networks are trained using maximum likelihood, where the cost function is the negative log-likelihood or the cross-entropy between the training data and the model distribution.\", \"Deriving the cost function from maximum likelihood removes any terms that do not depend on the model parameters and simplifies the cost function.\"]\n",
      "{'questions': \n",
      "  ['Why is it important for the gradient of the cost function to be large and predictable?',\n",
      "   'What can happen when activation functions saturate?',\n",
      "   'What is the purpose of the log function in the negative log-likelihood cost function?',\n",
      "   'Why does the cross-entropy cost function usually not have a minimum value?',\n",
      "   'What are some regularization techniques used to prevent the cross-entropy from approaching negative infinity?',\n",
      "   'What is a functional in the context of a cost function?',\n",
      "   'What mathematical tool may be used to solve optimization problems with respect to a function?',\n",
      "   'What statistic can be learned when we want to predict the mean of y given x?',\n",
      "   'What is the result of minimizing the mean squared error cost function?',\n",
      "   'What result is derived using calculus of variations from minimizing the mean absolute error cost function?'],\n",
      "\n",
      "  'answers': \n",
      "  ['The gradient of the cost function must be large and predictable to serve as a good guide for the learning algorithm.',\n",
      "   When activation functions saturate, the gradient becomes very small, undermining the objective of having a large and predictable gradient.\n",
      "   The log function in the negative log-likelihood cost function undoes the exponential function to avoid saturation when the argument is very negative.\n",
      "   The cross-entropy cost function usually does not have a minimum value because most models used in practice are parametrized in a way that they cannot represent a probability of zero or one.\n",
      "   Regularization techniques described in the chapter can modify the learning problem to prevent the cross-entropy from approaching negative infinity.\n",
      "   In the context of a cost function, a functional is a mapping from functions to real numbers.\n",
      "   Calculus of variations is a mathematical tool that can be used to solve optimization problems with respect to a function.\n",
      "   When wanting to predict the mean of y given x, the statistic to learn is the expected value of y given x.\n",
      "   Minimizing the mean squared error cost function gives a function that predicts the mean of y for each value of x.\n",
      "   Minimizing the mean absolute error cost function yields a function. (The complete derivation and result are not provided in the given text.)']}\n",
      "{\n",
      "    \"questions\": [\n",
      "        \"What is the issue with using mean squared error and mean absolute error with gradient-based optimization?\",\n",
      "        \"Why is the cross-entropy cost function more popular than mean squared error or mean absolute error?\",\n",
      "        \"What determines the choice of cost function in a neural network?\",\n",
      "        \"Can output units also be used as hidden units in a neural network?\",\n",
      "        \"What is the role of the output layer in a feedforward network?\",\n",
      "        \"What is the purpose of linear output units in a neural network?\",\n",
      "        \"What distribution is often modeled using linear output units?\",\n",
      "        \"What is the advantage of using linear units in gradient-based optimization?\",\n",
      "        \"Why is it difficult to satisfy constraints on the covariance matrix with a linear output layer?\",\n",
      "        \"What type of output units are used for predicting binary variables?\",\n",
      "        \"What constraint does the output of a neural net need to satisfy when modeling a Bernoulli distribution?\",\n",
      "        \"Why is using a linear thresholding approach problematic for training a Bernoulli distribution model?\",\n",
      "        \"What approach is better for training a Bernoulli distribution model with gradient descent?\",\n",
      "        \"What type of output units are used in this approach?\",\n",
      "        \"What does the sigmoid output units combined with maximum likelihood ensure?\",\n",
      "        \"What is the purpose of using sigmoid output units in a Bernoulli distribution model?\"\n",
      "    ],\n",
      "    \"answers\": [\n",
      "        \"Mean squared error and mean absolute error often lead to poor results in gradient-based optimization because output units that saturate produce very small gradients when combined with these cost functions.\",\n",
      "        \"The cross-entropy cost function is more popular than mean squared error or mean absolute error because it overcomes the issue of small gradients produced by saturating output units and is not limited to estimating an entire distribution.\",\n",
      "        \"The choice of cost function is tightly coupled with the choice of output unit in a neural network.\",\n",
      "        \"Yes, output units can also be used as hidden units in a neural network.\",\n",
      "        \"The role of the output layer in a feedforward network is to provide additional transformation from the hidden features to complete the task that the network must perform.\",\n",
      "        \"Linear output units in a neural network are used to produce the mean of a conditional Gaussian distribution.\",\n",
      "        \"Linear output units are often used to model a conditional Gaussian distribution.\",\n",
      "        \"Using linear units in gradient-based optimization is advantageous because they do not saturate and pose little difficulty for optimization algorithms.\",\n",
      "        \"It is difficult to satisfy constraints on the covariance matrix with a linear output layer because the covariance must be a positive definite matrix for all inputs.\",\n",
      "        \"For predicting binary variables, sigmoid units are used as output units.\",\n",
      "        \"The output of a neural net needs to satisfy the constraint of lying in the interval [0, 1] when modeling a Bernoulli distribution.\",\n",
      "        \"Using a linear thresholding approach for training a Bernoulli distribution model is problematic because it leads to gradients of 0 whenever the model's output strays outside the unit interval, making it difficult for the learning algorithm to improve the parameters.\",\n",
      "        \"A better approach for training a Bernoulli distribution model with gradient descent is to use sigmoid output units combined with maximum likelihood.\",\n",
      "        \"Sigmoid output units are used in the approach of training a Bernoulli distribution model with gradient descent.\",\n",
      "        \"Sigmoid output units combined with maximum likelihood ensure that there is always a strong gradient whenever the model has the wrong answer, facilitating effective training with gradient descent.\",\n",
      "        \"The purpose of using sigmoid output units in a Bernoulli distribution model is to ensure a strong gradient is present whenever the model predicts the wrong answer, improving the training process.\"\n",
      "    ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the definition of the sigmoid output unit?\",\n",
      "    \"How is the sigmoid activation function used in the output unit?\",\n",
      "    \"How is a probability distribution over y defined using the sigmoid output?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The sigmoid output unit is defined as ^y = σ(w^h + b)\",\n",
      "    \"The sigmoid activation function is used to convert the linear output z into a probability\",\n",
      "    \"A probability distribution over y is defined by exponentiating the unnormalized log probabilities and normalizing them using a sigmoidal transformation of z\"\n",
      "  ]\n",
      "}\n",
      "{'questions': [{'question': 'What is the softmax function used for?', 'answers': ['The softmax function is most often used as the output of a classifier to represent the probability distribution over different classes.', 'The softmax function can also be used inside a model to choose between different options for some internal variable.']}], 'answers': ['The softmax function is most often used as the output of a classifier to represent the probability distribution over different classes.', 'The softmax function can also be used inside a model to choose between different options for some internal variable.']}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What can cause difficulties for learning if the softmax function saturates?\",\n",
      "    \"How can we derive a numerically stable variant of the softmax function?\",\n",
      "    \"What is the purpose of overparametrizing the distribution in the softmax function?\",\n",
      "    \"What does the softmax function do from a neuroscientific point of view?\",\n",
      "    \"What is the difference between the softmax function and the arg max function?\",\n",
      "    \"Why is the softmax function called 'softmax'?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"If the softmax function saturates, it can cause difficulties for learning if the loss function is not designed to compensate for it.\",\n",
      "    \"A numerically stable variant of the softmax can be derived by using the formula softmax(z) = softmax(z - max(z)). This allows for evaluation of the softmax function with small numerical errors even with extremely large or extremely negative input values (z).\",\n",
      "    \"Overparametrizing the distribution in the softmax function is not necessary, as only one parameter is needed to ensure that the outputs sum to 1. However, it is simpler to implement the overparametrized version.\",\n",
      "    \"From a neuroscientific point of view, the softmax function can be seen as a way to create competition between the participating units. It causes an increase in the value of one unit to correspond to a decrease in the value of others, similar to lateral inhibition between neurons in the cortex.\",\n",
      "    \"The softmax function is more closely related to the arg max function than the max function. The arg max function, represented as a one-hot vector, is not continuous or differentiable. The softmax function provides a softened version of the arg max.\",\n",
      "    \"The softmax function is called 'softmax' because it is a softened version of the arg max function. The term 'soft' refers to the fact that the softmax function is continuous and differentiable, unlike the arg max function.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "\"questions\": [\n",
      "\"What is the principle of maximum likelihood when designing a cost function for an output layer?\",\n",
      "\"What can the neural network represent?\",\n",
      "\"What is the loss function used to interpret the outputs of the neural network?\",\n",
      "\"How can we learn the variance of a conditional Gaussian distribution with the neural network?\",\n",
      "\"What is a heteroscedastic model?\",\n",
      "\"What is the typical way to formulate the Gaussian distribution when predicting variance?\",\n",
      "\"Why is using precision instead of variance preferred in the formulation of the Gaussian distribution?\",\n",
      "\"What condition needs to be enforced on the output of the model when using a diagonal precision matrix?\",\n",
      "\"How can we ensure the covariance matrix of the Gaussian is positive definite?\"\n",
      "],\n",
      "\"answers\": [\n",
      "\"The principle of maximum likelihood suggests using log(p(y|x;θ)) as the cost function for designing an output layer.\",\n",
      "\"The neural network represents a function f(x;θ) where the outputs of this function provide parameters for a distribution over y.\",\n",
      "\"The loss function interprets the outputs as the negative log-likelihood of the distribution p(y|ω(x)).\",\n",
      "\"We can learn the variance of a conditional Gaussian distribution by including the variance as one of the properties of the function f(x;θ) controlled by ω.\",\n",
      "\"A heteroscedastic model is a model where the amount of variance in y can vary for different values of x.\",\n",
      "\"The typical way to formulate the Gaussian distribution when predicting variance is to use precision instead of variance.\",\n",
      "\"Using precision instead of variance is preferred because it allows for well-behaved gradient operations in the log-likelihood calculation.\",\n",
      "\"When using a diagonal precision matrix, the only condition that needs to be enforced on the output of the model is positivity.\",\n",
      "\"To ensure the covariance matrix of the Gaussian is positive definite, we need to ensure the precision matrix is positive definite. This can be achieved by using a diagonal matrix or a scalar multiplied by the diagonal matrix.\"\n",
      "]\n",
      "}\n",
      "{'questions': ['What is the purpose of parametrization in a full and conditional covariance matrix?',\n",
      "               'Why are Gaussian mixtures often used in multimodal regression?',\n",
      "               'What are the three outputs required for a neural network with Gaussian mixture as output?',\n",
      "               \"Why is gradient-based optimization of conditional Gaussian mixtures unreliable? \"\n",
      "              ],\n",
      " 'answers': ['The purpose of parametrization in a full and conditional covariance matrix is to guarantee positive-definiteness of the predicted covariance matrix.',\n",
      "             'Gaussian mixtures are often used in multimodal regression because they can represent outputs with several different peaks in the target space for the same input value.',\n",
      "             'The three outputs required for a neural network with Gaussian mixture as output are: p(c=i|x) (the mixture components), µ( )i(x) (the means), and Σ( )i(x) (the covariances).',\n",
      "             'Gradient-based optimization of conditional Gaussian mixtures is unreliable due to numerical instability caused by divisions by small variances, resulting in large gradients.']\n",
      "}\n",
      "{'questions': [{'question': 'What is the purpose of using a mixture density network?', 'context': 'Context: strategy gives a way for the network to represent multiple output modes and to control the variance of its output, which is crucial for obtaining a high degree of quality in these real-valued domains.'}, {'question': 'What type of output can a neural network generate?', 'context': 'In general, we may wish to continue to model larger vectors y containing more variables, and to impose richer and richer structures on these output variables.'}, {'question': 'What does the figure 6.4 represent?', 'context': 'y  x  |  Figure 6.4: Samples drawn from a neural network with a mixture density output layer. The input x is sampled from a uniform distribution and the output y is sampled from ).'}, {'question': 'What is the advantage of using rectified linear units as hidden units in neural networks?', 'context': 'Rectiﬁed linear units are an excellent default choice of hidden unit.'}, {'question': 'How do we decide which type of hidden unit to use in a neural network?', 'context': 'The design process consists of trial and error, intuiting that a kind of hidden unit may work well, and then training a network with that kind of hidden unit and evaluating its performance on a validation set.'}], 'answers': [{'text': 'to represent multiple output modes and control the variance of its output', 'context': 'strategy gives a way for the network to represent multiple output modes and to control the variance of its output, which is crucial for obtaining a high degree of quality in these real-valued domains.'}, {'text': 'a sequence of characters', 'context': 'For example, we may wish for our neural network to output a sequence of characters'}, {'text': 'Samples drawn from a neural network with a mixture density output layer', 'context': 'Figure 6.4: Samples drawn from a neural network with a mixture density output layer.'}, {'text': 'They are an excellent default choice of hidden unit', 'context': 'Rectiﬁed linear units are an excellent default choice of hidden unit.'}, {'text': 'Through trial and error and evaluating the performance on a validation set', 'context': 'The design process consists of trial and error, intuiting that a kind of hidden unit may work well, and then training a network with that kind of hidden unit and evaluating its performance on a validation set.'}]}\n",
      "{\"questions\": [\"What is the activation function used in rectified linear units?\", \"Why are rectified linear units easy to optimize?\", \"What is the purpose of setting all elements of b to a small positive value in the initialization of the parameters?\", \"What are some generalizations of rectified linear units?\", \"How do maxout units differ from rectified linear units?\"], \"answers\": [\"The activation function used in rectified linear units is max(0, z).\", \"Rectified linear units are easy to optimize because they are similar to linear units and their derivatives remain large and consistent.\", \"Setting all elements of b to a small positive value ensures that rectified linear units will be initially active for most inputs in the training set and allow the derivatives to pass through.\", \"Some generalizations of rectified linear units include leaky ReLU, parametric ReLU or PReLU, and absolute value rectification.\", \"Maxout units are a further generalization of rectified linear units where the input is divided into groups and the maximum element of each group is outputted.\"] }\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the purpose of a maxout unit?\",\n",
      "    \"What are the benefits of using maxout units?\",\n",
      "    \"Why do maxout units typically require more regularization than rectified linear units?\",\n",
      "    \"What is catastrophic forgetting?\",\n",
      "    \"Why are rectified linear units and their generalizations based on linear behavior?\",\n",
      "    \"What are the problems with using sigmoidal activation functions in neural networks?\",\n",
      "    \"Which activation function typically performs better than the logistic sigmoid?\",\n",
      "    \"When are sigmoidal activation functions still applicable in neural networks?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The purpose of a maxout unit is to learn a piecewise linear function with multiple directions in the input space.\",\n",
      "    \"The benefits of using maxout units include the ability to learn the activation function itself, resistance to catastrophic forgetting, and potential reduction in the number of parameters needed.\",\n",
      "    \"Maxout units typically require more regularization than rectified linear units because each maxout unit is parametrized by multiple weight vectors.\",\n",
      "    \"Catastrophic forgetting is a phenomenon in which neural networks forget how to perform tasks that they were trained on in the past. Maxout units with redundancy can help resist catastrophic forgetting.\",\n",
      "    \"Rectified linear units and their generalizations are based on linear behavior because models with linear behavior are easier to optimize.\",\n",
      "    \"The problems with using sigmoidal activation functions in neural networks include their widespread saturation, which makes gradient-based learning difficult.\",\n",
      "    \"The hyperbolic tangent activation function typically performs better than the logistic sigmoid.\",\n",
      "    \"Sigmoidal activation functions are still applicable in neural networks as output units when an appropriate cost function can undo the saturation of the sigmoid in the output layer.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': [{'question': 'What is the advantage of using the tanh activation function in a deep neural network?', 'category': 'summary'}, {'question': 'Why are sigmoidal activation functions more commonly used in recurrent networks and probabilistic models?', 'category': 'summary'}, {'question': 'What are some examples of other hidden unit types?', 'category': 'summary'}, {'question': 'How can linear hidden units reduce the number of parameters in a network?', 'category': 'summary'}, {'question': 'What is the purpose of softmax units?', 'category': 'summary'}, {'question': 'What are some challenges in optimizing radial basis function units?', 'category': 'summary'}, {'question': 'What is the softplus activation function?', 'category': 'summary'}], 'answers': [{'answer': 'The advantage of using the tanh activation function in a deep neural network is that it is similar to the identity function near 0, making training the network easier.', 'category': 'summary'}, {'answer': 'Sigmoidal activation functions are more commonly used in recurrent networks and probabilistic models due to their suitability for meeting specific requirements and despite the drawbacks of saturation.', 'category': 'summary'}, {'answer': 'Some examples of other hidden unit types include linear units, softmax units, radial basis function units, and softplus units.', 'category': 'summary'}, {'answer': 'Linear hidden units can reduce the number of parameters in a network by factoring the weight matrix of a layer and constraining the linear transformation to be low-rank.', 'category': 'summary'}, {'answer': 'Softmax units are used as outputs in neural networks and can also be used as hidden units. They represent probability distributions over discrete variables.', 'category': 'summary'}, {'answer': 'Radial basis function units become more active as input approaches a template and can be difficult to optimize due to saturation.', 'category': 'summary'}, {'answer': 'The softplus activation function is a smooth version of the rectifier activation function, defined as g(a) = log(1 + exp(a)).', 'category': 'summary'}]}\n",
      "{'questions': [{'question': 'What is the main focus of the text?', 'context': '6.4 Architecture Design  Another key design consideration for neural networks is determining the architecture. The word architecture refers to the overall structure of the network: how many units it should have and how...'}, {'question': 'What is the purpose of the softplus function in neural networks?', 'context': 'The softplus demonstrates that the performance of hidden unit types can be very counterintuitive—one might expect it to have an advantage over the rectiﬁer due to being diﬀerentiable everywhere or due to saturating less completely, but empirically it does not.'}, {'question': 'What is the universal approximation theorem?', 'context': 'A linear model, mapping from features to ...'}], 'answers': [{'question': 'What is the main focus of the text?', 'answer': 'The main focus of the text is the architecture design for neural networks.'}, {'question': 'What is the purpose of the softplus function in neural networks?', 'answer': 'The purpo...\n",
      "{'questions': ['What is the universal approximation theorem?', 'What are two reasons why learning can fail in a feedforward network?', 'What does the \"no free lunch\" theorem state?', 'When using deeper models, what are two benefits compared to using a single-layer network?', 'What types of functions can be efficiently approximated by architectures with greater depth?', 'What was the first type of model for which results on approximation were proved?', 'What type of activation functions do modern neural networks commonly use?', 'What did Montufar et al. show about rectifier networks?', 'How does a network with absolute value rectification create mirror images?', 'What is the main takeaway from the text?'], 'answers': ['The universal approximation theorem states that there exists a feedforward network that can approximate any function.', 'Learning can fail in a feedforward network due to the optimization algorithm not finding the desired parameters or due to overfitting.', 'The \"no free lunch\" theorem states that there is no one algorithm that is universally superior for machine learning.', 'Using deeper models can reduce the number of units required to represent a function and can reduce generalization error.', 'Families of functions that can be efficiently approximated require architecture with depth greater than a certain value, whereas shallower models require exponentially more hidden units.', 'The first type of model for which results on approximation were proved was circuits of logic gates.', 'Modern neural networks commonly use rectified linear units as activation functions.', 'Montufar et al. showed that rectifier networks can require an exponential number of hidden units with a shallow network.', 'A network with absolute value rectification creates mirror images of the function computed on top of a hidden unit by folding the input space.', 'The main takeaway from the text is that feedforward networks with a single layer can represent any function, but deeper models can reduce the number of units required and improve generalization.']}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the main theorem stated in this text?\",\n",
      "    \"What belief does choosing a deep model encode?\",\n",
      "    \"What has empirical evidence shown about deep architectures?\",\n",
      "    \"What are some other architectural considerations in neural networks?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The main theorem stated in the text is that the number of linear regions carved out by a deep rectifier network is exponential in the depth.\",\n",
      "    \"Choosing a deep model encodes a belief that the function to be learned involves composition of several simpler functions, or that it is a computer program consisting of multiple steps.\",\n",
      "    \"Empirical evidence has shown that greater depth in neural networks tends to result in better generalization for a wide variety of tasks.\",\n",
      "    \"Some other architectural considerations in neural networks include specialized architectures for specific tasks, such as convolutional networks for computer vision, and recurrent neural networks for sequence processing.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the purpose of skip connections in deep feedforward networks?\",\n",
      "    \"What does increasing the depth of a neural network result in, according to empirical results?\",\n",
      "    \"What are some strategies for reducing the number of connections in neural networks?\",\n",
      "    \"What are convolutional networks particularly effective for?\",\n",
      "    \"What is the back-propagation algorithm used for?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Skip connections in deep feedforward networks make it easier for the gradient to flow from output layers to layers nearer the input.\",\n",
      "    \"Empirical results show that increasing the depth of a neural network tends to improve its generalization ability, as demonstrated in the context of transcribing multi-digit numbers from photographs of addresses.\",\n",
      "    \"Some strategies for reducing the number of connections in neural networks include using specialized patterns of sparse connections, such as in convolutional networks, and connecting each unit in the input layer to only a small subset of units in the output layer.\",\n",
      "    \"Convolutional networks are particularly effective for computer vision problems.\",\n",
      "    \"The back-propagation algorithm allows the information from the cost to flow backwards through the network in order to compute the gradient.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the back-propagation method used for in neural networks?\",\n",
      "    \"What does back-propagation compute?\",\n",
      "    \"Is back-propagation specific to multi-layer neural networks?\",\n",
      "    \"What is the gradient most often required in learning algorithms?\",\n",
      "    \"What can the back-propagation algorithm be used for besides computing the gradient of the cost function?\",\n",
      "    \"What is a computational graph in the context of neural networks?\",\n",
      "    \"What is an operation in a computational graph?\",\n",
      "    \"What does the chain rule of calculus state? \"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The back-propagation method is used to compute the gradient in neural networks.\",\n",
      "    \"Back-propagation computes the gradient.\",\n",
      "    \"No, back-propagation is not specific to multi-layer neural networks. It can compute derivatives of any function.\",\n",
      "    \"The gradient of the cost function with respect to the parameters is most often required.\",\n",
      "    \"The back-propagation algorithm can be used to compute derivatives in other machine learning tasks or to analyze the learned model.\",\n",
      "    \"A computational graph is a formalization of computation as a graph, where each node represents a variable and each edge represents an operation.\",\n",
      "    \"An operation is a simple function of one or more variables in a computational graph.\",\n",
      "    \"The chain rule of calculus states that the derivative of a composition of functions is equal to the product of the derivatives of the individual functions.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': [{'question': 'What is the purpose of the back-propagation algorithm?', 'category': 'Objective'}, {'question': 'What does the chain rule allow us to do in the back-propagation algorithm?', 'category': 'Functionality'}, {'question': 'Why can a naive implementation of the chain rule be infeasible for complicated graphs?', 'category': 'Explanation'}, {'question': 'What is the difference between back-propagation with vectors and back-propagation with tensors?', 'category': 'Comparison'}, {'question': 'What does the equation Xz represent?', 'category': 'Definition'}, {'question': 'What is the key consideration in evaluating the gradient expression in a computer?', 'category': 'Explanation'}], 'answers': [{'answer': 'The back-propagation algorithm computes the gradients of a scalar with respect to any node in the computational graph.', 'category': 'Objective'}, {'answer': 'The chain rule allows us to compute the gradient of a scalar with respect to any node in the computational graph by recursively applying the chain rule.', 'category': 'Functionality'}, {'answer': 'A naive implementation of the chain rule can be infeasible for complicated graphs because there can be exponentially many wasted computations.', 'category': 'Explanation'}, {'answer': 'The difference between back-propagation with vectors and back-propagation with tensors is how the numbers are arranged in a grid to form a tensor.', 'category': 'Comparison'}, {'answer': 'The equation Xz represents the gradient of a value z with respect to a tensor X.', 'category': 'Definition'}, {'answer': 'The key consideration in evaluating the gradient expression in a computer is whether to store repeated subexpressions or to recompute them multiple times.', 'category': 'Explanation'}]}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the purpose of back-propagation in computational graphs?\",\n",
      "    \"How is the forward propagation computation performed in a computational graph?\",\n",
      "    \"What is the relationship between the forward and backward graphs in back-propagation?\",\n",
      "    \"How does the amount of computation required for back-propagation scale with the number of edges in the graph?\",\n",
      "    \"What is the main advantage of the back-propagation algorithm in terms of reducing common subexpressions?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The purpose of back-propagation in computational graphs is to compute the gradient of a scalar quantity with respect to the input nodes.\",\n",
      "    \"In forward propagation computation, each node in the graph computes a numerical value by applying a function to the set of arguments comprising the values of previous nodes.\",\n",
      "    \"The backward graph in back-propagation proceeds in the reverse order of computation in the forward graph and computes the derivative associated with each forward graph node.\",\n",
      "    \"The amount of computation required for back-propagation scales linearly with the number of edges in the graph.\",\n",
      "    \"The main advantage of the back-propagation algorithm is that it reduces the number of common subexpressions, resulting in fewer computations.\"\n",
      "  ]\n",
      "}\n",
      "{\"questions\": [\"What is the computational cost of the back-propagation algorithm?\", \"What is the purpose of the grad_table data structure?\", \"How does back-propagation algorithm avoid repeated subexpressions?\", \"What is the alternative approach to storing subexpressions in the back-propagation algorithm?\", \"What are the modern software implementations based on?\"], \n",
      "\"answers\": [\"The computational cost of the back-propagation algorithm is proportional to the number of edges in the graph.\", \"The grad_table data structure stores the derivatives that have been computed.\", \"Back-propagation algorithm avoids repeated subexpressions by storing and reusing intermediate values computed during the forward propagation.\", \"The alternative approach to storing subexpressions in the back-propagation algorithm is to recompute them each time they are needed.\", \"Modern software implementations are based on the generalized form of back-propagation that can accommodate any computational graph.\"] }\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the purpose of the loss function in a deep neural network?\",\n",
      "    \"What does the regularizer do in the computation of the total cost?\",\n",
      "    \"How are the gradients of the cost function computed with respect to the parameters?\",\n",
      "    \"What are symbol-to-symbol derivatives?\",\n",
      "    \"How can the gradients on weights and biases be used?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The purpose of the loss function in a deep neural network is to quantify the difference between the predicted output and the target output.\",\n",
      "    \"The regularizer in the computation of the total cost is added to the loss to incorporate a penalty for complex models, helping prevent overfitting.\",\n",
      "    \"The gradients of the cost function with respect to the parameters (weights and biases) are computed using the backpropagation algorithm. Starting from the output layer and going backwards, the algorithm computes the gradients on the activations for each layer, which are then used to compute the gradients on weights and biases.\",\n",
      "    \"Symbol-to-symbol derivatives refer to the approach of computing derivatives without accessing specific numeric values. Instead, a computational graph is constructed to describe the computation of derivatives symbolically.\",\n",
      "    \"The gradients on weights and biases can be used for stochastic gradient updates or other gradient-based optimization methods to update the model parameters.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': ['What is the approach taken by Theano and TensorFlow?', 'What is the advantage of the approach that adds additional nodes to the computational graph?', 'How are higher-order derivatives computed?', 'What is the key difference between the symbol-to-symbol approach and the symbol-to-number approach?', 'How does the back-propagation algorithm compute the gradient of a variable with respect to one of its ancestors in the graph?', 'What does the bprop operation associated with each operation do?'], 'answers': ['The approach taken by Theano and TensorFlow is to take a computational graph and add additional nodes to the graph that provide a symbolic description of the desired derivatives.', 'The advantage of the approach that adds additional nodes to the computational graph is that the derivatives are described in the same language as the original expression.', 'Higher-order derivatives are computed by running back-propagation again, differentiating the derivatives in order to obtain higher derivatives.', 'The key difference between the symbol-to-symbol approach and the symbol-to-number approach is that the symbol-to-number approach does not expose the graph.', 'The back-propagation algorithm computes the gradient of a variable with respect to one of its ancestors in the graph by multiplying the current gradient by the Jacobian of the operation that produced the variable.', 'The bprop operation associated with each operation can compute a Jacobian-vector product.'] }\n",
      "{\"questions\": [\"What is the purpose of the back-propagation algorithm?\", \"What is the role of the op.bprop method in the back-propagation algorithm?\", \"How are repeated subexpressions handled in the back-propagation algorithm?\", \"What is the computational cost of the back-propagation algorithm?\"], \"answers\": [\"The purpose of the back-propagation algorithm is to compute the gradients of variables in a computational graph.\", \"The op.bprop method is responsible for implementing the backward propagation rules for each operation in the graph.\", \"In order to avoid computing the same subexpression in the chain rule multiple times, the back-propagation algorithm prunes the graph to contain only nodes that are ancestors of the target variable and descendants of the variable to be differentiated.\", \"The computational cost of the back-propagation algorithm is analyzed in terms of the number of operations executed in the graph, which is never more than O(n^2).\"]}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the back-propagation algorithm?\",\n",
      "    \"What is the purpose of the back-propagation algorithm?\",\n",
      "    \"What is the computational cost of back-propagation?\",\n",
      "    \"How does back-propagation avoid recomputation of common subexpressions?\",\n",
      "    \"What is the example used to explain the back-propagation algorithm in this text?\",\n",
      "    \"How is the cost function defined in the example?\",\n",
      "    \"What does the weight decay term in the total cost represent?\",\n",
      "    \"What does the computational graph illustrate?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The back-propagation algorithm is an algorithm used to train neural networks by computing the gradient of the cost function with respect to the network's parameters.\",\n",
      "    \"The purpose of the back-propagation algorithm is to compute the gradient of the cost function in order to update the network's parameters and minimize the cost.\",\n",
      "    \"The computational cost of back-propagation is O(n) for most neural network cost functions, which is much more efficient than the naive approach that would require exponentially many nodes.\",\n",
      "    \"Back-propagation avoids recomputation of common subexpressions by using a table-filling dynamic programming strategy, where intermediate results are stored in a table and filled in order to avoid repeating computations.\",\n",
      "    \"The example used to explain the back-propagation algorithm is training a multilayer perceptron with a single hidden layer using minibatch stochastic gradient descent.\",\n",
      "    \"The cost function in the example consists of the cross-entropy between the targets and the predicted probability distribution, as well as a weight decay term with coefficient lambda.\",\n",
      "    \"The weight decay term in the total cost represents a regularization term that penalizes large weights in the network, helping to prevent overfitting.\",\n",
      "    \"The computational graph illustrates the flow of computations and dependencies in the neural network, showing how the inputs are transformed through layers of hidden features to produce predictions and compute the cost.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the benefit of the back-propagation algorithm?\",\n",
      "    \"What are the two different paths to compute the gradients on the weights?\",\n",
      "    \"What is the computational cost dominated by in the MLP?\",\n",
      "    \"What is the memory cost of the algorithm?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The benefit of the back-propagation algorithm is that it can automatically generate gradients that would be tedious to derive manually.\",\n",
      "    \"The two different paths to compute the gradients on the weights are through the cross-entropy cost and through the weight decay cost.\",\n",
      "    \"The computational cost in the MLP is dominated by the cost of matrix multiplication.\",\n",
      "    \"The memory cost of the algorithm is O(mnh), where m is the number of examples in the minibatch and nh is the number of hidden units.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': \n",
      "[\"What are some technicalities that make real-world differentiation more complicated?\", \n",
      "\"What does the deep learning community use to compute derivatives?\", \n",
      "\"What is reverse mode accumulation?\", \n",
      "\"What problem arises when finding the optimal sequence of operations to compute the gradient?\", \n",
      "\"What approach can be taken to simplify the computational graph for back-propagation?\", \n",
      "\"What is the naive implementation of back-propagation?\"\n",
      "],\n",
      "'answers': \n",
      "[\"Some technicalities that make real-world differentiation more complicated include handling various data types, tracking undefined gradients, and dealing with other subtleties.\",\n",
      "\"The deep learning community has developed its own cultural attitudes concerning how to perform differentiation, using techniques such as automatic differentiation.\",\n",
      "\"Reverse mode accumulation is a broader class of techniques for automatic differentiation that includes the back-propagation algorithm.\",\n",
      "\"Finding the optimal sequence of operations to compute the gradient is an NP-complete problem, which means it may require simplifying algebraic expressions into their least expensive form.\",\n",
      "\"The computational graph constructed by back-propagation can be simplified by using heuristics based on known simplification patterns.\",\n",
      "\"The naive implementation of back-propagation computes k gradients for each scalar internal node in the original forward graph, instead of a single gradient.\"] }\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is forward mode accumulation in automatic differentiation?\",\n",
      "    \"How does forward mode computation avoid storing the values and gradients for the whole graph?\",\n",
      "    \"What is the relationship between forward mode and backward mode in automatic differentiation?\",\n",
      "    \"Why is it cheaper to run multiplications left-to-right in forward mode?\",\n",
      "    \"What is the drawback of using specialized libraries for computational graphs in deep learning?\",\n",
      "    \"Why is back-propagation a practical method for computing gradients in deep learning?\",\n",
      "    \"What are Krylov methods in the context of computing derivatives?\",\n",
      "    \"How is the Hessian matrix typically handled in deep learning applications?\",\n",
      "    \"What technique is used to compute the product between the Hessian matrix and an arbitrary vector?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Forward mode accumulation is another form of automatic differentiation that is sometimes preferable to use over backward mode. It helps obtain real-time computation of gradients in recurrent networks and avoids the need to store the values and gradients for the whole graph.\",\n",
      "    \"Forward mode computation avoids storing the values and gradients for the whole graph by trading off computational efficiency for memory. It starts multiplying from the left and involves a series of matrix-matrix products.\",\n",
      "    \"The relationship between forward mode and backward mode in automatic differentiation is analogous to left-multiplying versus right-multiplying a sequence of matrices. Backward mode corresponds to starting the multiplications from the end and going backwards, while forward mode corresponds to starting the multiplications from the left and going forwards.\",\n",
      "    \"If the matrix A has fewer rows than D has columns, it is cheaper to run the multiplications left-to-right in forward mode.\",\n",
      "    \"The drawback of using specialized libraries for computational graphs in deep learning is that it requires the library developer to define the bprop methods for every operation and limits the user to only those operations that have been defined.\",\n",
      "    \"Back-propagation is a practical method for computing gradients in deep learning because it allows customized back-propagation rules to be developed for each operation, enabling the developer to improve speed or stability in non-obvious ways that an automatic procedure may not replicate.\",\n",
      "    \"Krylov methods are iterative techniques used in deep learning for performing operations like approximately inverting a matrix or finding approximations to its eigenvectors or eigenvalues, without using any operation other than matrix-vector products.\",\n",
      "    \"In deep learning applications, instead of explicitly computing a single second derivative, the focus is on properties of the Hessian matrix, which is typically too large to represent. Krylov methods are used as an alternative.\",\n",
      "    \"The technique used to compute the product between the Hessian matrix and an arbitrary vector is to compute the gradient of the product of the function f and the vector v, denoted as Hv, using the formula: Hv = ∇(∇xf)(v).\"\n",
      "  ]\n",
      "}\n",
      "{\"questions\": [\"What is the context of the given text?\", \"What does the outer gradient expression do?\", \"Why is it important to specify that the automatic differentiation software should not differentiate through the graph that produced 'v'?\", \"What were the earliest models of machine learning based on?\", \"What led to a backlash against the entire neural network approach?\", \"What are some of the core ideas behind modern feedforward networks?\"], \"answers\": [\"The context of the given text is about deep feedforward networks and their historical development.\", \"The outer gradient expression takes the gradient of a function of the inner gradient expression.\", \"It is important to specify that the automatic differentiation software should not differentiate through the graph that produced 'v' to prevent unnecessary differentiation.\", \"The earliest models of machine learning were based on linear models.\", \"The inability of linear models to learn the XOR function led to a backlash against the entire neural network approach.\", \"Some of the core ideas behind modern feedforward networks include the back-propagation algorithm and gradient descent approaches.\"]}.\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What were some algorithmic changes that improved the performance of neural networks?\",\n",
      "    \"Why were cross-entropy losses used in place of mean squared error?\",\n",
      "    \"What were the major changes made to hidden units in feedforward networks?\",\n",
      "    \"Why were rectified linear units avoided in the early 2000s?\",\n",
      "    \"Why did the use of rectified linear units begin to change around 2009?\",\n",
      "    \"What were some properties of biological neurons that influenced the development of rectified linear units?\",\n",
      "    \"What was the perception of feedforward networks from 2006-2012?\",\n",
      "    \"How is gradient-based learning in feedforward networks viewed today?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The algorithmic changes that improved the performance of neural networks were the replacement of mean squared error with cross-entropy family of loss functions and the replacement of sigmoid hidden units with rectified linear units.\",\n",
      "    \"Cross-entropy losses were used in place of mean squared error because they greatly improved the performance of models with sigmoid and softmax outputs, which had issues with saturation and slow learning when using mean squared error.\",\n",
      "    \"The major changes made to hidden units in feedforward networks were the replacement of sigmoid hidden units with piecewise linear hidden units, such as rectified linear units.\",\n",
      "    \"Rectified linear units were avoided in the early 2000s due to a belief that activation functions with non-differentiable points should be avoided.\",\n",
      "    \"The use of rectified linear units began to change around 2009 when it was observed that using rectifying non-linearities was the most important factor in improving the performance of a recognition system.\",\n",
      "    \"Rectified linear units were influenced by biological neurons as they aimed to capture properties such as complete inactivity for some inputs, proportionality of output to input for some inputs, and sparse activations.\",\n",
      "    \"From 2006-2012, feedforward networks were believed to not perform well unless assisted by other models, such as probabilistic models.\",\n",
      "    \"Today, gradient-based learning in feedforward networks is viewed as a powerful technology that can be used to develop probabilistic models and is no longer seen as an unreliable technology that requires support from other techniques.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': ['What is the current trend in using unsupervised learning and supervised learning?', 'What is the potential of feedforward networks?', 'What does the chapter primarily focus on?', 'What is the future expectation for feedforward networks?', 'What will subsequent chapters discuss?'], 'answers': ['It is more common to use supervised learning to support unsupervised learning.', 'Feedforward networks have unfulfilled potential.', 'The chapter primarily focuses on the neural network family of models.', 'In the future, feedforward networks are expected to be applied to many more tasks and their performance will be improved further.', 'Subsequent chapters will discuss how to regularize and train the neural network models.']}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the purpose of chapter 5 in the book?\",\n",
      "    \"What is the difference between deep learning and machine learning?\",\n",
      "    \"What are hyperparameters in machine learning?\",\n",
      "    \"What are the two central approaches to statistics?\",\n",
      "    \"What are the categories of machine learning algorithms?\",\n",
      "    \"What is stochastic gradient descent?\",\n",
      "    \"What are some factors that have limited traditional machine learning?\",\n",
      "    \"What is the definition of a learning algorithm according to Mitchell?\",\n",
      "    \"What is the task in machine learning?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The purpose of chapter 5 is to provide a brief course in the most important general principles of machine learning.\",\n",
      "    \"Deep learning is a specific kind of machine learning that requires a solid understanding of the basic principles of machine learning.\",\n",
      "    \"Hyperparameters in machine learning are settings external to the learning algorithm itself that need to be determined.\",\n",
      "    \"The two central approaches to statistics are frequentist estimators and Bayesian inference.\",\n",
      "    \"Machine learning algorithms can be divided into the categories of supervised learning and unsupervised learning.\",\n",
      "    \"Stochastic gradient descent is an optimization algorithm commonly used in deep learning algorithms.\",\n",
      "    \"Factors such as the ability to generalize have limited traditional machine learning.\",\n",
      "    \"According to Mitchell, a learning algorithm is an algorithm that learns from experience to improve its performance at tasks.\",\n",
      "    \"The task in machine learning refers to what the machine learning system should process in order to perform a specific task.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': [{'question': 'What is the purpose of machine learning?', 'answers': ['The purpose of machine learning is to process information from objects or events.']}]}\n",
      "{'questions': \n",
      "  [\n",
      "    'What is one example of a task that uses deep learning to process address numbers?',\n",
      "    'Which major companies use deep learning for speech recognition?',\n",
      "    'What is an example of a machine translation task?',\n",
      "    'What are some examples of structured output tasks?',\n",
      "    'What is one example of an anomaly detection task?',\n",
      "    'How can machine learning be used for synthesis and sampling?'\n",
      "  ],\n",
      " 'answers': \n",
      "  [\n",
      "    'One example of a task that uses deep learning to process address numbers is Google Street View.',\n",
      "    'Major companies including Microsoft, IBM, and Google use deep learning for speech recognition.',\n",
      "    'An example of a machine translation task is translating from English to French.',\n",
      "    'Some examples of structured output tasks include parsing natural language sentences and pixel-wise segmentation of images.',\n",
      "    'One example of an anomaly detection task is credit card fraud detection.',\n",
      "    'Machine learning can be used for synthesis and sampling in applications such as generating textures for video games or synthesizing spoken audio from written sentences.'\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What are the three types of tasks mentioned in the text?\",\n",
      "    \"What is the purpose of density estimation?\",\n",
      "    \"How is accuracy measured in tasks like classification?\",\n",
      "    \"Why is it important to evaluate a machine learning algorithm on data it has not seen before?\",\n",
      "    \"What is the common approach to measuring performance in density estimation tasks?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The three types of tasks mentioned in the text are imputation of missing values, denoising, and density estimation or probability mass function estimation.\",\n",
      "    \"The purpose of density estimation is to learn a function that captures the structure of the data and represents it as a probability density function or probability mass function.\",\n",
      "    \"Accuracy in tasks like classification is measured by the proportion of examples for which the model produces the correct output.\",\n",
      "    \"It is important to evaluate a machine learning algorithm on data it has not seen before because it determines how well it will work in the real world.\",\n",
      "    \"The common approach to measuring performance in density estimation tasks is to report the average log-probability the model assigns to some examples.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What are the challenges in choosing a performance measure for a system?\",\n",
      "    \"What are some design choices when measuring the accuracy of a transcription system?\",\n",
      "    \"Why is measuring the actual probability value assigned to a specific point in space intractable in some probabilistic models?\",\n",
      "    \"What is the difference between unsupervised learning and supervised learning?\",\n",
      "    \"How does the chain rule of probability relate to machine learning?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The challenges in choosing a performance measure for a system include deciding what should be measured and making design choices that depend on the application.\",\n",
      "    \"When performing a transcription task, one can measure the accuracy of the system at transcribing entire sequences or use a more fine-grained performance measure that gives partial credit.\",\n",
      "    \"Measuring the actual probability value assigned to a specific point in space is intractable in many probabilistic models because these models represent probability distributions only implicitly.\",\n",
      "    \"Unsupervised learning involves learning useful properties of the structure of a dataset, while supervised learning involves learning to predict a target value or vector based on a dataset with associated labels.\",\n",
      "    \"The chain rule of probability allows us to compute the joint probability distribution of a vector by multiplying the conditional probabilities of each element given its predecessors. This relation is used in machine learning to estimate the probability distribution and make predictions.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the purpose of the decomposition described in equation (5.1)?\",\n",
      "    \"How does equation (5.2) relate supervised and unsupervised learning?\",\n",
      "    \"What are some examples of tasks categorized as supervised learning?\",\n",
      "    \"What is semi-supervised learning?\",\n",
      "    \"What is multi-instance learning?\",\n",
      "    \"What is the difference between supervised and unsupervised learning?\",\n",
      "    \"What is a design matrix?\",\n",
      "    \"When is it not possible to describe a dataset as a design matrix?\",\n",
      "    \"What is the role of labels in supervised learning?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The purpose of the decomposition described in equation (5.1) is to model the probability distribution p(x) by splitting it into n supervised learning problems.\",\n",
      "    \"Equation (5.2) relates supervised and unsupervised learning by using traditional supervised learning technologies to learn the joint distribution p(x, y) and infer the conditional probability p(y | x).\",\n",
      "    \"Examples of tasks categorized as supervised learning include regression, classification, and structured output problems.\",\n",
      "    \"Semi-supervised learning is a learning paradigm where some examples include a supervision target, but others do not.\",\n",
      "    \"Multi-instance learning is a learning paradigm where an entire collection of examples is labeled as containing or not containing an example of a class, but the individual members of the collection are not labeled.\",\n",
      "    \"Supervised learning and unsupervised learning categorize the things done with machine learning algorithms, with supervised learning focusing on tasks like regression and classification, while unsupervised learning is associated with density estimation and support of other tasks.\",\n",
      "    \"A design matrix is a matrix that describes a dataset, with each row representing a different example and each column representing a different feature.\",\n",
      "    \"It is not possible to describe a dataset as a design matrix when the examples in the dataset cannot be represented as vectors of the same size. For example, if a collection of photographs has varying widths and heights, the vector sizes will differ.\",\n",
      "    \"Labels in supervised learning provide target information for each example in the dataset, allowing the learning algorithm to associate the features with the corresponding labels and learn a mapping between them.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': [{'question': 'What is the example given in this text?', 'answer': 'The example given in this text is linear regression.'}, {'question': 'What is the goal of linear regression?', 'answer': 'The goal of linear regression is to predict the value...tion is to minimize the mean squared error on the training set, MSEtrain.'}, {'question': 'What is the formula for calculating the mean squared error on the test set?', 'answer': 'The formula for calculating the mean squared error on the test set is MSEtest...\n",
      "{'questions': [{'question': 'What is the central challenge in machine learning?', 'answer': 'The central challenge in machine learning is achieving good performance on previously unseen inputs, also known as generalization.'}, {'question': 'How is the generalization error defined?', 'answer': 'The generalization error is the expected value of the error on a new input, taken across different possible inputs drawn from the distribution of inputs encountered in practice.'}, {'question': 'How is the generalization error estimated?', 'answer': 'The generalization error of a machine learning model is typically estimated by measuring its performance on a separate test set of examples that were not used during training.'}, {'question': 'What does linear regression aim to minimize?', 'answer': 'Linear regression aims to minimize the mean squared error on the training set.'}, {'question': 'What is the purpose of the normal equations?', 'answer': 'The normal equations provide a solution for finding the optimal weight vector in linear regression.'}, {'question': 'What is the role of the bias parameter in linear regression?', 'answer': 'The bias parameter (intercept term) in linear regression allows the model to make predictions that do not need to pass through the origin, making it an aﬃne function.'}], 'answers': [{'answer_start': 40, 'answer_end': 142}, {'answer_start': 559, 'answer_end': 722}, {'answer_start': 733, 'answer_end': 896}, {'answer_start': 416, 'answer_end': 478}, {'answer_start': 183, 'answer_end': 308}, {'answer_start': 834, 'answer_end': 993}]}\n",
      "{'questions': [{'question': 'What assumptions are made in statistical learning theory?', 'answer': 'The assumptions made in statistical learning theory are the i.i.d. assumptions. These assumptions state that the examples in each dataset are independent from each other, and that the train set and test set are identically distributed, drawn from the same probability distribution as each other.'}, {'question': 'What is the data generating distribution?', 'answer': 'The data generating distribution, denoted pdata, is the shared underlying distribution used to generate every train and test example.'}, {'question': 'How are training error and test error related?', 'answer': 'The expected training error of a randomly selected model is equal to the expected test error of that model.'}, {'question': 'What is the difference between underfitting and overfitting?', 'answer': 'Underfitting occurs when the model is not able to obtain a sufficiently low error value on the training set. Overfitting occurs when the gap between the training error and test error is too large.'}, {'question': 'How can the capacity of a learning algorithm be controlled?', 'answer': 'The capacity of a learning algorithm can be controlled by choosing its hypothesis space, which is the set of functions that the learning algorithm is allowed to select as being the solution.'}, {'question': 'Give an example of increasing a model\\'s capacity.', 'answer': 'An example of increasing a model\\'s capacity is by generalizing linear regression to include polynomials, rather than just linear functions, in its hypothesis space.'}], 'answers': [{'answer_start': 280, 'text': 'The assumptions made in statistical learning theory are the i.i.d. assumptions.'}, {'answer_start': 604, 'text': 'The data generating distribution, denoted pdata, is the shared underlying distribution used to generate every train and test example.'}, {'answer_start': 730, 'text': 'The expected training error of a randomly selected model is equal to the expected test error of that model.'}, {'answer_start': 1011, 'text': 'Underfitting occurs when the model is not able to obtain a sufficiently low error value on the training set. Overfitting occurs when the gap between the training error and test error is too large.'}, {'answer_start': 1263, 'text': 'The capacity of a learning algorithm can be controlled by choosing its hypothesis space, which is the set of functions that the learning algorithm is allowed to select as being the solution.'}, {'answer_start': 1464, 'text': 'An example of increasing a model\\'s capacity is by generalizing linear regression to include polynomials, rather than just linear functions, in its hypothesis space.'}]}\n",
      "{'questions': [{'question': 'What is the principle of Occam’s razor?', 'answers': ['The principle of Occam’s razor states that among competing hypotheses that explain known observations equally well, one should choose the “simplest” one.']}], 'answers': [{'text': 'The principle of Occam’s razor states that among competing hypotheses that explain known observations equally well, one should choose the “simplest” one.'}]}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the Vapnik-Chervonenkis (VC) dimension?\",\n",
      "    \"Why are the bounds provided by statistical learning theory rarely used in practice for deep learning algorithms?\",\n",
      "    \"Why is determining the capacity of a deep learning model difficult?\",\n",
      "    \"What is the relationship between model capacity and error?\",\n",
      "    \"What are non-parametric models and how are they different from parametric models?\",\n",
      "    \"What is an example of a practical non-parametric model?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The Vapnik-Chervonenkis (VC) dimension measures the capacity of a binary classifier by defining the largest possible value of m for which there exists a training set of m different x points that the classifier can label arbitrarily.\",\n",
      "    \"The bounds provided by statistical learning theory are rarely used in practice for deep learning algorithms because they are often loose, and it is difficult to determine the capacity of deep learning algorithms. Additionally, there is little theoretical understanding of the non-convex optimization problems involved in deep learning.\",\n",
      "    \"Determining the capacity of a deep learning model is difficult because the effective capacity is limited by the capabilities of the optimization algorithm, and there is little theoretical understanding of the non-convex optimization problems involved in deep learning.\",\n",
      "    \"Typically, training error decreases as model capacity increases until it asymptotes to the minimum possible error value. However, generalization error has a U-shaped curve as a function of model capacity, indicating that there is an optimal capacity point where the gap between training and generalization error is minimized.\",\n",
      "    \"Non-parametric models have no fixed size limitation for the parameter vector and their complexity can be a function of the training set size. In contrast, parametric models like linear regression have a fixed-length vector of weights that describe the function being learned.\",\n",
      "    \"An example of a practical non-parametric model is nearest neighbor regression. Instead of using a fixed-length vector of weights, this model stores the training set and classifies a test point by finding the nearest entry in the training set and returning the associated regression target.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': [{'question': 'What is the no free lunch theorem for machine learning?', 'context': 'The no free lunch theorem for machine learning (Wolpert 1996 ) states that, averaged over all possible data generating distributions, every classiﬁcation algorithm has the same error rate when classifying previously unobserved points.'}, {'question': 'What is the ideal model in machine learning?', 'context': 'The ideal model is an oracle that simply knows the true probability distribution that generates the data.'}, {'question': 'How does the training and generalization error vary with the size of the training set?', 'context': 'Training and generalization error vary as the size of the training set varies. Expected generalization error can never increase as the number of training examples increases.'}, {'question': 'What is the purpose of machine learning algorithms?', 'context': 'Machine learning algorithms aim to find probabilistic rules that are correct for most members of the set they concern.'}], 'answers': [{'answer': 'The no free lunch theorem for machine learning states that, averaged over all possible data generating distributions, every classification algorithm has the same error rate when classifying previously unobserved points.', 'context': 'The no free lunch theorem for machine learning (Wolpert 1996 ) states that, averaged over all possible data generating distributions, every classiﬁcation algorithm has the same error rate when classifying previously unobserved points.'}, {'answer': 'The ideal model is an oracle that simply knows the true probability distribution that generates the data.', 'context': 'The ideal model is an oracle that simply knows the true probability distribution that generates the data.'}, {'answer': 'Expected generalization error can never increase as the number of training examples increases.', 'context': 'Training and generalization error vary as the size of the training set varies. Expected generalization error can never increase as the number of training examples increases.'}, {'answer': 'Machine learning algorithms aim to find probabilistic rules that are correct for most members of the set they concern.', 'context': 'Machine learning algorithms aim to find probabilistic rules that are correct for most members of the set they concern.'}]}\n",
      "{'questions': \n",
      "  [\n",
      "    {'question': 'What happens to the test error as the dataset size increases?', 'answer': 'The test error decreases.'},\n",
      "    {'question': 'Why does the training error fall below the Bayes error?', 'answer': 'Due to the ability of the training algorithm to memorize specific instances of the training set.'},\n",
      "    {'question': 'What happens to the training error of a fixed-capacity model as the training set size increases?', 'answer': 'It must rise to at least the Bayes error.'},\n",
      "    {'question': 'What is the goal of machine learning research?', 'answer': 'To understand the kinds of data generating distributions in the real world and design machine learning algorithms that perform well on them.'},\n",
      "    {'question': 'What does the no free lunch theorem imply about machine learning algorithms?', 'answer': 'They must be designed to perform well on a specific task.'},\n",
      "    {'question': 'How can we control the performance of machine learning algorithms?', 'answer': 'By choosing the kind of functions they are allowed to draw solutions from and controlling the amount of these functions.'},\n",
      "    {'question': 'What is weight decay?', 'answer': 'Weight decay is a technique in machine learning that expresses a preference for weights to have smaller squared L2 norm.'},\n",
      "    {'question': 'What does the criterion J(w) express in linear regression with weight decay?', 'answer': 'A preference for the weights to have smaller squared L2 norm.'}\n",
      "  ],\n",
      " 'answers':\n",
      "  [\n",
      "    {'answer': 'The test error decreases, because fewer incorrect hypotheses are consistent with the training data.'},\n",
      "    {'answer': 'The ability of the training algorithm to memorize specific instances of the training set.'},\n",
      "    {'answer': 'It must rise to at least the Bayes error.'},\n",
      "    {'answer': 'To understand what kinds of distributions are relevant to the real world and what kinds of machine learning algorithms perform well on them.'},\n",
      "    {'answer': 'They must be designed to perform well on a specific task.'},\n",
      "    {'answer': 'By choosing the kind of functions they are allowed to draw solutions from and controlling the amount of these functions.'},\n",
      "    {'answer': 'Weight decay is a technique in machine learning that expresses a preference for weights to have smaller squared L2 norm.'},\n",
      "    {'answer': 'A preference for the weights to have smaller squared L2 norm.'}\n",
      "  ]\n",
      " }\n",
      "{'questions': [{'question': 'What does λ = 0 imply in terms of weights?', 'answers': ['When λ = 0, we impose no preference, and larger λ forces the weights to become smaller.']}], 'answers': [{'answer_start': 20, 'text': 'When λ = 0, we impose no preference, and larger λ forces the weights to become smaller.'}]}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"Why is it not appropriate to learn hyperparameters on the training set?\",\n",
      "    \"What is the purpose of a validation set?\",\n",
      "    \"How is the validation set constructed?\",\n",
      "    \"What is the typical split of training and validation data?\",\n",
      "    \"How does the use of a test set help estimate the generalization error?\",\n",
      "    \"Why can evaluations using the same test set become optimistic?\",\n",
      "    \"What is the problem with using a small test set?\",\n",
      "    \"What is cross-validation?\",\n",
      "    \"How does cross-validation work?\",\n",
      "    \"What is the purpose of k-fold cross-validation?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"It is not appropriate to learn hyperparameters on the training set because it would always result in choosing the maximum possible model capacity, leading to overfitting.\",\n",
      "    \"The purpose of a validation set is to estimate the generalization error during or after training and update the hyperparameters accordingly.\",\n",
      "    \"The validation set is constructed by splitting the training data into two disjoint subsets, with one subset used for parameter learning and the other subset used for guiding the selection of hyperparameters.\",\n",
      "    \"Typically, around 80% of the training data is used for training and 20% is used for validation.\",\n",
      "    \"The test set is used to estimate the generalization error after hyperparameter optimization is complete.\",\n",
      "    \"Evaluations using the same test set can become optimistic over time because different algorithms attempt to beat the reported state-of-the-art performance, leading to biased evaluations.\",\n",
      "    \"The problem with using a small test set is that it results in statistical uncertainty around the estimated average test error, making it difficult to claim one algorithm works better than another.\",\n",
      "    \"Cross-validation is a procedure used when the dataset is too small to have a fixed training and test set. It allows for the estimation of the mean test error using all of the examples in the dataset.\",\n",
      "    \"In k-fold cross-validation, the dataset is divided into k non-overlapping subsets. On each trial, one of the subsets is used as the test set and the rest of the data is used as the training set. The test error is then averaged across the k trials.\",\n",
      "    \"The purpose of k-fold cross-validation is to estimate the test error by using all of the examples in the dataset, at the cost of increased computational cost.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': [{'question': 'What is the purpose of point estimation?', 'answer': 'Point estimation is the attempt to provide the single “best” prediction of some quantity of interest.'}, {'question': 'What does a point estimator do?', 'answer': 'A point ...tion is a function whose output is close to the true underlying θ that generated the training data.'}, {'question': 'What is the frequentist perspective on statistics?', 'answer': 'The frequentist perspective assumes that the true parameter value θ is fixed.'}], 'answers': [{'answer_start': 1277, 'text': 'Point estimation is the attempt to provide the single “best” prediction of some quantity of interest.'}, {'answer_start': 1784, 'text': 'A point estimator is any function of the data.'}, {'answer...\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the definition of bias of an estimator?\", \n",
      "    \"What does it mean for an estimator to be unbiased?\", \n",
      "    \"What is an asymptotically unbiased estimator?\", \n",
      "    \"What is the estimator for the θ parameter of a Bernoulli distribution?\", \n",
      "    \"Is the estimator for the θ parameter of a Bernoulli distribution biased?\", \n",
      "    \"What is the estimator for the mean of a Gaussian distribution?\", \n",
      "    \"Is the estimator for the mean of a Gaussian distribution biased?\", \n",
      "    \"What are the two estimators of the variance parameter of a Gaussian distribution?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The bias of an estimator is defined as the difference between the expected value of the estimator and the true underlying value.\",\n",
      "    \"An estimator is unbiased if its bias is equal to zero, that is, its expected value is equal to the true underlying value.\",\n",
      "    \"An estimator is asymptotically unbiased if its bias approaches zero as the number of samples approaches infinity.\",\n",
      "    \"The estimator for the θ parameter of a Bernoulli distribution is the mean of the training samples.\",\n",
      "    \"No, the estimator for the θ parameter of a Bernoulli distribution is unbiased.\",\n",
      "    \"The estimator for the mean of a Gaussian distribution is the sample mean.\",\n",
      "    \"Yes, the estimator for the mean of a Gaussian distribution is unbiased.\",\n",
      "    \"The two estimators of the variance parameter of a Gaussian distribution are the sample variance and the unbiased sample variance.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the variance of an estimator?\",\n",
      "    \"What is the standard error of an estimator?\",\n",
      "    \"Why is it desirable for an estimator to have low variance?\",\n",
      "    \"How is the standard error of the mean useful in machine learning experiments?\",\n",
      "    \"What is the 95% confidence interval used for?\",\n",
      "    \"How can you determine if algorithm A is better than algorithm B in machine learning experiments?\",\n",
      "    \"What is the Bernoulli distribution?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The variance of an estimator is the measure of how much we would expect the estimate to vary as we independently resample the dataset from the underlying data generating process.\",\n",
      "    \"The standard error of an estimator is the square root of its variance.\",\n",
      "    \"It is desirable for an estimator to have low variance because it means that we would expect the estimate to have less variability as we resample the dataset, leading to more consistent results.\",\n",
      "    \"The standard error of the mean is useful in machine learning experiments for estimating the generalization error. It helps in computing the probability that the true expectation falls within a chosen interval.\",\n",
      "    \"The 95% confidence interval is used to determine the range within which the true expectation is likely to fall with 95% confidence. It is centered on the mean estimate and is computed using the standard error.\",\n",
      "    \"In machine learning experiments, algorithm A is considered better than algorithm B if the upper bound of the 95% confidence interval for the error of A is less than the lower bound of the 95% confidence interval for the error of B.\",\n",
      "    \"The Bernoulli distribution is a discrete probability distribution that models the outcomes of a random variable that can take on one of two possible values, usually represented as 0 and 1.\"\n",
      "  ]\n",
      "}\n",
      "{\"questions\": [{\"question\": \"What are bias and variance in the context of estimator?\", \"answer\": \"Bias measures the expected deviation from the true value of the function or parameter, while variance provides a measure of the deviation from the expected estimator value that any particular sampling of the data is likely to cause.\"}, {\"question\": \"How can we choose between two estimators with more bias and more variance?\", \"answer\": \"The most common way to choose between two estimators with more bias and variance is to use cross-validation or compare the mean squared error (MSE) of the estimates.\"}, {\"question\": \"How is the mean squared error (MSE) calculated?\", \"answer\": \"The mean squared error (MSE) is calculated as the sum of the squared bias and the variance of the estimator.\"}, {\"question\": \"What are desirable estimators in terms of MSE?\", \"answer\": \"Desirable estimators are those with small MSE, which means they manage to keep both their bias and variance somewhat in check.\"}, {\"question\": \"What is the relationship between bias, variance, and capacity?\", \"answer\": \"In machine learning, increasing capacity tends to increase variance and decrease bias, resulting in a U-shaped curve for generalization error as a function of capacity.\"}, {\"question\": \"What does consistency mean in the context of estimators?\", \"answer\": \"Consistency means that as the amount of training data grows, the estimator's point estimates converge to the true value of the corresponding parameters.\"}], \"answers\": [{\"included_sentences\": [1]}, {\"included_sentences\": [2]}, {\"included_sentences\": [5]}, {\"included_sentences\": [8]}, {\"included_sentences\": [12]}, {\"included_sentences\": [16]}]}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the maximum likelihood principle?\",\n",
      "    \"How is the maximum likelihood estimator defined?\",\n",
      "    \"Why is taking the logarithm of the likelihood convenient?\",\n",
      "    \"How can the criterion of maximum likelihood be expressed as an expectation?\",\n",
      "    \"What is the KL divergence and how is it related to maximum likelihood?\",\n",
      "    \"How is maximum likelihood related to minimizing cross-entropy?\",\n",
      "    \"What is the difference between maximizing likelihood and minimizing KL divergence?\",\n",
      "    \"What is the negative log-likelihood (NLL) and how is it related to maximum likelihood?\",\n",
      "    \"What is the conditional log-likelihood?\",\n",
      "    \"How can the maximum likelihood estimator be generalized to the case of regression problems?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The maximum likelihood principle is a common principle used to derive specific functions that are good estimators for different models. It is based on finding the parameter values that maximize the likelihood of the observed data.\",\n",
      "    \"The maximum likelihood estimator for θ is defined as θML = arg max θ ∏ pmodel(x(i); θ) set of m examples X drawn independently from the true data generating distribution pdata(x).\",\n",
      "    \"Taking the logarithm of the likelihood transforms the product over many probabilities into a sum, which is more convenient for optimization. It does not change the arg max of the likelihood.\",\n",
      "    \"By rescaling the cost function and dividing by the number of examples, the maximum likelihood criterion can be expressed as an expectation with respect to the empirical distribution defined by the training data.\",\n",
      "    \"The KL (Kullback-Leibler) divergence measures the dissimilarity between two distributions. In maximum likelihood, minimizing the KL divergence is equivalent to minimizing the cross-entropy between the empirical distribution and the model distribution.\",\n",
      "    \"Maximum likelihood can be interpreted as minimizing the dissimilarity between the empirical distribution and the model distribution, where the dissimilarity is measured by the KL divergence. Cross-entropy is a form of loss that measures the similarity between two distributions, and minimizing it is equivalent to minimizing the KL divergence.\",\n",
      "    \"Maximizing likelihood and minimizing KL divergence refer to the same optimization problem, but they have different objective functions. Maximizing likelihood is equivalent to minimizing the negative log-likelihood, while minimizing KL divergence has a known minimum value of zero.\",\n",
      "    \"The negative log-likelihood (NLL) is the negative logarithm of the likelihood function. In maximum likelihood, minimizing the NLL is equivalent to maximizing the likelihood.\",\n",
      "    \"The conditional log-likelihood is a variation of the likelihood function that considers only the probability of the observed outputs given the inputs, rather than the joint probability of the inputs and outputs.\",\n",
      "    \"The maximum likelihood estimator can be generalized to regression problems by considering the conditional log-likelihood, which measures the likelihood of the observed outputs given the inputs and the model parameters.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': \n",
      "   ['What is the conditional maximum likelihood estimator?',\n",
      "    'How can the conditional maximum likelihood estimator be decomposed?',\n",
      "    'What is an example of linear regression as maximum likelihood?',\n",
      "    'What is the goal of the learning algorithm in linear regression?',\n",
      "    'How does maximizing the log-likelihood relate to minimizing the mean squared error?',\n",
      "    'What are the properties of maximum likelihood estimation?'],\n",
      " 'answers':\n",
      "   ['The conditional maximum likelihood estimator is used to estimate a conditional probability P(y | x ; θ) in order to predict y given x.',\n",
      "    'The conditional maximum likelihood estimator can be decomposed into the sum of log likelihoods of individual training examples.',\n",
      "    'Linear regression can be justified as a maximum likelihood procedure.',\n",
      "    'The goal of the learning algorithm in linear regression is to fit the conditional distribution p(y | x) to different y values that are all compatible with x.',\n",
      "    'Maximizing the log-likelihood yields the same estimate of the parameters as minimizing the mean squared error in linear regression.',\n",
      "    'The maximum likelihood estimator is consistent, meaning it converges to the true value of the parameter as the number of training examples approaches infinity.'] }\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the measure used to determine how close the estimated parameter values are to the true parameter values?\",\n",
      "    \"What does the Cramér-Rao lower bound show?\",\n",
      "    \"What is maximum likelihood estimation?\",\n",
      "    \"How can regularization strategies be used in machine learning?\",\n",
      "    \"What is the difference between frequentist statistics and Bayesian statistics?\",\n",
      "    \"What is the prior probability distribution in Bayesian statistics?\",\n",
      "    \"What is Bayes' rule?\",\n",
      "    \"How does Bayesian estimation differ from maximum likelihood estimation?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The measure used is the expected mean squared error.\",\n",
      "    \"The Cramér-Rao lower bound shows that no consistent estimator has a lower mean squared error than the maximum likelihood estimator.\",\n",
      "    \"Maximum likelihood estimation is considered the preferred estimator to use for machine learning due to its consistency and efficiency.\",\n",
      "    \"Regularization strategies such as weight decay can be used to obtain a biased version of maximum likelihood that has less variance when training data is limited.\",\n",
      "    \"Frequentist statistics consider the true parameter value as fixed but unknown, while Bayesian statistics treat the true parameter as a random variable.\",\n",
      "    \"The prior probability distribution represents the knowledge or uncertainty about the true parameter value before observing any data.\",\n",
      "    \"Bayes' rule combines the likelihood of the data with the prior probability distribution to calculate the posterior probability distribution.\",\n",
      "    \"Unlike maximum likelihood estimation, Bayesian estimation makes predictions using a full distribution over the parameters instead of a point estimate.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the Bayesian approach to dealing with uncertainty in a point estimate?\",\n",
      "    \"What is the criticism of the Bayesian approach regarding the prior distribution?\",\n",
      "    \"How does the Bayesian approach to estimation differ from the maximum likelihood approach?\",\n",
      "    \"In Bayesian linear regression, what is the prediction equation?\",\n",
      "    \"What is the prior distribution commonly used for real-valued parameters in Bayesian linear regression?\",\n",
      "    \"How is the posterior distribution over model parameters determined in Bayesian linear regression?\",\n",
      "    \"How is the posterior distribution in Bayesian linear regression expressed?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The Bayesian approach incorporates uncertainty in a point estimate by integrating over it.\",\n",
      "    \"Critics argue that the prior in Bayesian methods introduces subjective human judgment to the predictions.\",\n",
      "    \"The Bayesian approach incorporates the prior distribution, which shifts probability mass density towards preferred regions of the parameter space.\",\n",
      "    \"In Bayesian linear regression, the prediction is parametrized by the vector w and the equation is ˆy = wx.\",\n",
      "    \"A Gaussian distribution is commonly used as the prior distribution for real-valued parameters in Bayesian linear regression.\",\n",
      "    \"The posterior distribution over model parameters in Bayesian linear regression is determined by multiplying the likelihood function and the prior distribution.\",\n",
      "    \"The posterior distribution in Bayesian linear regression is expressed as a Gaussian distribution.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': [{'question': 'What is the purpose of Equation 3.23?', 'context': 'Context: rms that do not include the parameter vector w have been omitted; they are implied by the fact that the distribution must be normalized to integrate to .1 Equation  shows how ... 'answer_start': 775}, {'text': 'Regularization terms that are not the logarithm of a probability distribution and those that depend on the data are not allowed in Bayesian inference.', 'answer_start': 896}, {'text': 'A mixture of Gaussians can be used as a more complicated penalty term in MAP Bayesian inference.', 'answer_start': 942}], 'title': 'Machine Learning Basics'}\n",
      "{\"questions\": [\"What does supervised learning involve?\", \"What does linear regression correspond to?\", \"How can logistic regression be used for classification?\", \"How is the optimal weight in logistic regression found?\", \"What is the support vector machine (SVM) model?\", \"What is the kernel trick in SVM?\"], \"answers\": [\"Supervised learning involves estimating the output (y) based on input (x) using a training set.\", \"Linear regression corresponds to the family (p(y|x, θ) = θ'x + ε).\", \"Logistic regression can be used for classification by using the logistic sigmoid function to interpret the output of the linear function as a probability.\", \"The optimal weights in logistic regression are found by maximizing the log-likelihood, which can be done by minimizing the negative log-likelihood using gradient descent.\", \"The support vector machine (SVM) model is a linear function (w'x + b) that predicts the class identity without providing probabilities.\", \"The kernel trick in SVM allows the replacement of dot products with kernel evaluations, which enables the use of nonlinear functions for predictions.\"] }\n",
      "{\n",
      "\"questions\": [\n",
      "\"What is the purpose of the kernel trick in machine learning?\",\n",
      "\"What are the advantages of using the Gaussian kernel in kernel-based methods?\",\n",
      "\"What is the role of support vectors in support vector machines?\",\n",
      "\"What is a drawback of kernel machines in terms of computational cost?\",\n",
      "\"What are some limitations of generic kernels in kernel machines?\"\n",
      "],\n",
      "\"answers\": [\n",
      "\"The purpose of the kernel trick is to allow us to learn nonlinear models as a function of x using convex optimization techniques that are guaranteed to converge efficiently.\",\n",
      "\"The Gaussian kernel is commonly used because it can perform template matching, where training examples become templates for classes. It assigns a large weight to associated training labels based on the similarity of examples.\",\n",
      "\"Support vectors play a crucial role in support vector machines. They are the training examples that have non-zero αi values and are used to evaluate the decision function. These examples are selected to mitigate the linear computational cost.\",\n",
      "\"A drawback of kernel machines is that evaluating the decision function has a linear cost in the number of training examples. However, this can be mitigated by learning an α vector with mostly zeros and only evaluating the kernel function for support vectors.\",\n",
      "\"Generic kernels in kernel machines struggle to generalize well, leading to performance limitations. This is explained further in the text.\"\n",
      "]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the purpose of the k-nearest neighbors algorithm?\",\n",
      "    \"How does the k-nearest neighbors algorithm work at test time?\",\n",
      "    \"How does the k-nearest neighbors algorithm handle classification?\",\n",
      "    \"What does it mean for the k-nearest neighbors algorithm to have high capacity?\",\n",
      "    \"What is a weakness of the k-nearest neighbors algorithm?\",\n",
      "    \"What is the problem when using k-nearest neighbors on a regression task with multiple features?\",\n",
      "    \"What is the difference between internal nodes and leaf nodes in a decision tree?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The purpose of the k-nearest neighbors algorithm is to perform classification or regression.\",\n",
      "    \"At test time, the k-nearest neighbors algorithm finds the k nearest neighbors to a new test input and returns the average of the corresponding output values in the training set.\",\n",
      "    \"In classification, the k-nearest neighbors algorithm can average over one-hot code vectors to obtain a probability distribution over classes.\",\n",
      "    \"Having high capacity means that the k-nearest neighbors algorithm can achieve high accuracy given a large training set.\",\n",
      "    \"A weakness of the k-nearest neighbors algorithm is that it cannot learn which features are more discriminative than others.\",\n",
      "    \"In a regression task with multiple features, the k-nearest neighbors algorithm may be unable to detect a simple pattern if the relevant feature is overshadowed by other features.\",\n",
      "    \"Internal nodes in a decision tree are used to categorize examples and divide the input space, while leaf nodes represent distinct regions and their parameters define the function within that region.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': \n",
      "[\n",
      "  'What is the purpose of decision trees?',\n",
      "  'Why do decision trees struggle with some problems?',\n",
      "  'What are some limitations of nearest neighbor predictors and decision trees?',\n",
      "  'What is the difference between supervised and unsupervised learning algorithms?',\n",
      "  'What are some common tasks in unsupervised learning?',\n",
      "  'What are low-dimensional representations in unsupervised learning?',\n",
      "  'What are sparse representations in unsupervised learning?',\n",
      "  'What are independent representations in unsupervised learning?'\n",
      "],\n",
      " 'answers': \n",
      "[\n",
      "  'The purpose of decision trees is to map input regions to specific outputs by breaking the space into sub-regions.',\n",
      "  'Decision trees struggle with problems that have non-axis-aligned decision boundaries.',\n",
      "  'Some limitations of nearest neighbor predictors and decision trees include their computational resource constraints.',\n",
      "  'Supervised learning algorithms involve a supervision signal, while unsupervised algorithms do not.',\n",
      "  'Common tasks in unsupervised learning include density estimation, drawing samples from a distribution, denoising data, finding manifolds, and clustering.',\n",
      "  'Low-dimensional representations compress information about the data in a smaller representation.',\n",
      "  'Sparse representations embed the dataset into a representation with mostly zeroes for most inputs.',\n",
      "  'Independent representations aim to disentangle sources of variation so that dimensions of the representation are statistically independent.'\n",
      "]\n",
      "}\n",
      "{'questions': [{'question': 'What is one way to reduce the size of a representation?', 'answers': ['To find and remove redundancies.']}, {'question': 'What are the three criteria for a simple representation?', 'answers': ['Lower dimensionality, no linear...nds a representation whose elements have no linear correlation with each other.']}, {'question': 'What is the purpose of PCA in deep learning?', 'answers': ['To learn a lower-dimensional representation of data.']}], 'answers': [{'answer_start': 308, 'text': 'To find and remove redundancies.'}, {'answer_start': 494, 'text': 'Lower dimensionality, no linear correlation between elements, and no nonlinear relationships between variables.'}, {'answer_start': 534, 'text': 'To learn a lower-dimensional representation of data.'}]}\n",
      "{'questions': [{'question': 'What is the purpose of PCA?', 'answers': ['The purpose of PCA is to transform data into a representation where the elements are mutually uncorrelated.']}, {'question': 'What does k-means clustering do?', 'answers': ['k-means ... 'The k-means algorithm works by initializing different centroids to different values and alternating between assigning training examples to clusters and updating the centroids until convergence.']}, {'question': 'Why is the clustering problem ill-posed?', 'answers': ['The clustering problem is ill-posed because there is no single criterion that measures how well a clustering of the data corresponds to the real world.']}], 'answers': [{'answer_start': 0, 'answer_end': 85}, {'answer_start': 650, 'answer_end': 699}, {'answer_start': 912, 'answer_end': 984}]}\n",
      "{\"questions\": [\"What is the difference between stochastic gradient descent and gradient descent?\", \"Why is large training set more computationally expensive?\", \"How is the gradient estimate formed in stochastic gradient descent?\", \"What is the learning rate in stochastic gradient descent?\", \"What was the perception of gradient descent in the past?\"], \"answers\": [\"Stochastic gradient descent is an extension of the gradient descent algorithm introduced in section 4.3.\", \"Large training sets are more computationally expensive because the computational cost of computing the gradient for each example is O(m).\", \"The estimate of the gradient in stochastic gradient descent is formed using a minibatch of examples drawn uniformly from the training set.\", \"The learning rate in stochastic gradient descent is denoted by  𝜂 (eta).\", \"In the past, gradient descent was regarded as slow or unreliable, especially when applied to non-convex optimization problems.\"] }\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What does stochastic gradient descent optimize?\",\n",
      "    \"What is the cost per SGD update dependent on?\",\n",
      "    \"What happens to the number of updates required to reach convergence as the training set size increases?\",\n",
      "    \"What is the asymptotic cost of training a model with SGD as the training set size approaches infinity?\",\n",
      "    \"Why was deep learning initially interesting in academia?\",\n",
      "    \"What is the recipe for constructing a learning algorithm?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Stochastic gradient descent optimizes the cost function.\",\n",
      "    \"The cost per SGD update does not depend on the training set size m.\",\n",
      "    \"The number of updates required to reach convergence usually increases with training set size.\",\n",
      "    \"The asymptotic cost of training a model with SGD is O(1/m).\",\n",
      "    \"Deep learning was initially interesting in academia because it was able to generalize to new examples better than competing algorithms when trained on medium-sized datasets.\",\n",
      "    \"The recipe for constructing a learning algorithm involves combining a dataset, a cost function, an optimization procedure, and a model.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': [{'question': 'What is the curse of dimensionality?', 'answers': ['The curse of dimensionality refers to the difficulty that arises in many machine learning problems when the number of dimensions in the data is high. It is particularly concerning because the number of possible distinct configurations of a set of variables increases exponentially as the number of variables increases.']}], 'answers': [{'answer_start': 600, 'text': 'The curse of dimensionality refers to the difficulty that arises in many machine learning problems when the number of dimensions in the data is high. It is particularly concerning because the number of possible distinct configurations of a set of variables increases exponentially as the number of variables increases.'}]}\n",
      "{'questions': [{'question': 'What do traditional machine learning algorithms assume about cells with no training examples associated with them?', 'answers': ['Traditional machine learning algorithms assume that the output at a new point should be approximately the same as the output at the nearest training point for cells with no training examples associated with them.']}, {'question': 'What is the smoothness prior or local constancy prior?', 'answers': ['The smoothness prior or local constancy prior is an implicit \"prior\" that states that the function learned should not change very much within a small region.']}, {'question': 'Why is the smoothness prior alone insufficient for sophisticated tasks?', 'answers': ['The smoothness prior alone is insufficient for sophisticated tasks because it fails to scale to the statistical challenges involved in solving AI-level tasks.']}, {'question': 'What is the approach used by the k-nearest neighbors family of learning algorithms?', 'answers': ['The k-nearest neighbors family of learning algorithms uses the local constancy approach, where predictors are constant over each region containing all the points with the same set of k nearest neighbors in the training set.']}, {'question': 'What class of kernels perform template matching by measuring similarity between points?', 'answers': ['Local kernels, a class of kernels, perform template matching by measuring similarity between points.']}, {'question': 'What factor determines the size of a local kernel?', 'answers': ['The size of a local kernel is determined by the distance between points.']}], 'answers': ['Traditional machine learning algorithms assume that the output at a new point should be approximately the same as the output at the nearest training point for cells with no training examples associated with them.', 'The smoothness prior or local constancy prior is an implicit \"prior\" that states that the function learned should not change very much within a small region.', 'The smoothness prior alone is insufficient for sophisticated tasks because it fails to scale to the statistical challenges involved in solving AI-level tasks.', 'The k-nearest neighbors family of learning algorithms uses the local constancy approach, where predictors are constant over each region containing all the points with the same set of k nearest neighbors in the training set.', 'Local kernels, a class of kernels, perform template matching by measuring similarity between points.', 'The size of a local kernel is determined by the distance between points.']}\n",
      "{\"questions\": [\n",
      "  \"What are the limitations of local template matching?\",\n",
      "  \"What is the disadvantage of decision trees in smoothness-based learning?\",\n",
      "  \"How many training examples are generally needed to fit a tree accurately?\",\n",
      "  \"What is the relationship between the number of regions in input space and the number of examples needed?\",\n",
      "  \"How does the smoothness assumption affect learning algorithms?\",\n",
      "  \"What happens when the function to be learned is complicated in high dimensions?\",\n",
      "  \"Is it possible to generalize well in a complicated function with few examples?\"\n",
      "],\n",
      "\"answers\": [\n",
      "  \"Local template matching fails in cases where deep models succeed.\",\n",
      "  \"Decision trees break the input space into as many regions as there are leaves.\",\n",
      "  \"If a tree with at least n leaves is required to represent the target function accurately, then at least n training examples are needed.\",\n",
      "  \"All of these methods require O(k) examples to distinguish O(k) regions in input space.\",\n",
      "  \"The smoothness assumption helps learning algorithms observe high points on most peaks and low points on most valleys of the true underlying function.\",\n",
      "  \"In high dimensions, even a smooth function can change smoothly but in a different way along each dimension.\",\n",
      "  \"Generalizing well in a complicated function with few examples can be challenging.\"\n",
      "]}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the key insight in representing a complicated function efficiently?\",\n",
      "    \"What are some assumptions made by deep learning algorithms?\",\n",
      "    \"How do deep learning algorithms counter the curse of dimensionality?\",\n",
      "    \"What is a manifold in the context of machine learning?\",\n",
      "    \"How do manifold learning algorithms surmount the obstacle of learning functions across all of Rn?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The key insight is that a very large number of regions can be defined with a small number of examples by introducing dependencies between the regions via additional assumptions about the underlying data generating distribution.\",\n",
      "    \"Deep learning algorithms make implicit or explicit assumptions about the composition of factors or features at multiple levels in a hierarchy.\",\n",
      "    \"Deep learning algorithms use deep and distributed representations which confer exponential advantages to counter the exponential challenges posed by the curse of dimensionality.\",\n",
      "    \"In the context of machine learning, a manifold is a connected region of points with a neighborhood around each point. It can be loosely used to designate a connected set of points that can be approximated well by considering a small number of dimensions embedded in a higher-dimensional space.\",\n",
      "    \"Manifold learning algorithms overcome the obstacle of learning functions across all of Rn by allowing the dimensionality of the manifold to vary from one point to another, which often happens when a manifold intersects itself.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What assumption is made in manifold learning?\",\n",
      "    \"Why is the assumption that the data lies along a low-dimensional manifold useful in AI tasks?\",\n",
      "    \"What is the first observation in favor of the manifold hypothesis?\",\n",
      "    \"What is the second argument in favor of the manifold hypothesis?\",\n",
      "    \"What experiments support the manifold hypothesis?\",\n",
      "    \"Why is it natural to represent data in terms of coordinates on the manifold?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The assumption made in manifold learning is that the data lies along a low-dimensional manifold.\",\n",
      "    \"The assumption that the data lies along a low-dimensional manifold is useful in AI tasks involving processing images, sounds, or text because it is approximately correct for these domains.\",\n",
      "    \"The first observation in favor of the manifold hypothesis is that the probability distribution over real-life images, text strings, and sounds is highly concentrated, while uniformly sampled points resemble noise rather than structured inputs.\",\n",
      "    \"The second argument in favor of the manifold hypothesis is that there are neighborhoods and transformations that connect similar examples, allowing traversal of the manifold. For example, in the case of images, we can imagine transformations like dimming or brightening lights, moving or rotating objects, and altering colors on object surfaces.\",\n",
      "    \"Experiments by Cayton, Narayanan et al., Mitter, Schölkopf, Brand, Belkin and Niyogi, Donoho and Grimes, and Weinberger and Saul support the manifold hypothesis for a large class of datasets in AI.\",\n",
      "    \"When the data lies on a low-dimensional manifold, it is more natural for machine learning algorithms to represent the data in terms of coordinates on the manifold rather than in terms of coordinates in Rn.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': [{'question': 'What is the main purpose of extracting manifold coordinates?', 'context': 'Context:  1-D manifolds embedded in 3-D space. We give directions to speciﬁc addresses in terms of address numbers along these 1-D roads, not in terms of coordinates in 3-D space. Extracting these manifold coordinates is challenging, but holds the promise to improve many machine learning algorithms. This general principle is applied in many contexts. Figure shows the manifold structure of a dataset consisting of faces. By the end of this book, we will have developed the methods necessary to learn such a manifold structure. In ﬁgure , we will see how a machine learning algorithm can successfully accomplish this goal.  20.6  5.13  This concludes part , which has provided the basic concepts in mathematics and machine learning which are employed throughout the remaining parts of the book. You are now prepared to embark upon your study of deep learning.  I  164  \\n\\nCHAPTER 5. MACHINE LEARNING BASICS  Gong et al. 2000 ) Figure 5.13: Training examples from the QMUL Multiview Face Dataset ( for which the subjects were asked to move in such a way as to cover the two-dimensional manifold corresponding to two angles of rotation. We would like learning algorithms to be illustrates such a able to discover and disentangle such manifold coordinates. Figure feat.  20.6  ,  165  '}, {'question': 'In what context is the principle of extracting manifold coordinates applied?', 'context': 'Context:  1-D manifolds embedded in 3-D space. We give directions to speciﬁc addresses in terms of address numbers along these 1-D roads, not in terms of coordinates in 3-D space. Extracting these manifold coordinates is challenging, but holds the promise to improve many machine learning algorithms. This general principle is applied in many contexts. Figure shows the manifold structure of a dataset consisting of faces. By the end of this book, we will have developed the methods necessary to learn such a manifold structure. In ﬁgure , we will see how a machine learning algorithm can successfully accomplish this goal.  20.6  5.13  This concludes part , which has provided the basic concepts in mathematics and machine learning which are employed throughout the remaining parts of the book. You are now prepared to embark upon your study of deep learning.  I  164  \\n\\nCHAPTER 5. MACHINE LEARNING BASICS  Gong et al. 2000 ) Figure 5.13: Training examples from the QMUL Multiview Face Dataset ( for which the subjects were asked to move in such a way as to cover the two-dimensional manifold corresponding to two angles of rotation. We would like learning algorithms to be illustrates such a able to discover and disentangle such manifold coordinates. Figure feat.  20.6  ,  165  '}, {'question': 'What does the figure in the context show?', 'context': 'Context:  1-D manifolds embedded in 3-D space. We give directions to speciﬁc addresses in terms of address numbers along these 1-D roads, not in terms of coordinates in 3-D space. Extracting these manifold coordinates is challenging, but holds the promise to improve many machine learning algorithms. This general principle is applied in many contexts. Figure shows the manifold structure of a dataset consisting of faces. By the end of this book, we will have developed the methods necessary to learn such a manifold structure. In ﬁgure , we will see how a machine learning algorithm can successfully accomplish this goal.  20.6  5.13  This concludes part , which has provided the basic concepts in mathematics and machine learning which are employed throughout the remaining parts of the book. You are now prepared to embark upon your study of deep learning.  I  164  \\n\\nCHAPTER 5. MACHINE LEARNING BASICS  Gong et al. 2000 ) Figure 5.13: Training examples from the QMUL Multiview Face Dataset ( for which the subjects were asked to move in such a way as to cover the two-dimensional manifold corresponding to two angles of rotation. We would like learning algorithms to be illustrates such a able to discover and disentangle such manifold coordinates. Figure feat.  20.6  ,  165  '}, {'question': 'What is the goal of the machine learning algorithm mentioned in the context?', 'context': 'Context:  1-D manifolds embedded in 3-D space. We give directions to speciﬁc addresses in terms of address numbers along these 1-D roads, not in terms of coordinates in 3-D space. Extracting these manifold coordinates is challenging, but holds the promise to improve many machine learning algorithms. This general principle is applied in many contexts. Figure shows the manifold structure of a dataset consisting of faces. By the end of this book, we will have developed the methods necessary to learn such a manifold structure. In ﬁgure , we will see how a machine learning algorithm can successfully accomplish this goal.  20.6  5.13  This concludes part , which has provided the basic concepts in mathematics and machine learning which are employed throughout the remaining parts of the book. You are now prepared to embark upon your study of deep learning.  I  164  \\n\\nCHAPTER 5. MACHINE LEARNING BASICS  Gong et al. 2000 ) Figure 5.13: Training examples from the QMUL Multiview Face Dataset ( for which the subjects were asked to move in such a way as to cover the two-dimensional manifold corresponding to two angles of rotation. We would like learning algorithms to be illustrates such a able to discover and disentangle such manifold coordinates. Figure feat.  20.6  ,  165  '}, {'question': 'What is the conclusion of the part mentioned in the context?', 'context': 'Context:  1-D manifolds embedded in 3-D space. We give directions to speciﬁc addresses in terms of address numbers along these 1-D roads, not in terms of coordinates in 3-D space. Extracting these manifold coordinates is challenging, but holds the promise to improve many machine learning algorithms. This general principle is applied in many contexts. Figure shows the manifold structure of a dataset consisting of faces. By the end of this book, we will have developed the methods necessary to learn such a manifold structure. In ﬁgure , we will see how a machine learning algorithm can successfully accomplish this goal.  20.6  5.13  This concludes part , which has provided the basic concepts in mathematics and machine learning which are employed throughout the remaining parts of the book. You are now prepared to embark upon your study of deep learning.  I  164  \\n\\nCHAPTER 5. MACHINE LEARNING BASICS  Gong et al. 2000 ) Figure 5.13: Training examples from the QMUL Multiview Face Dataset ( for which the subjects were asked to move in such a way as to cover the two-dimensional manifold corresponding to two angles of rotation. We would like learning algorithms to be illustrates such a able to discover and disentangle such manifold coordinates. Figure feat.  20.6  ,  165  '}, {'question': 'What is the purpose of the dataset mentioned in the context?', 'context': 'Context:  1-D manifolds embedded in 3-D space. We give directions to speciﬁc addresses in terms of address numbers along these 1-D roads, not in terms of coordinates in 3-D space. Extracting these manifold coordinates is challenging, but holds the promise to improve many machine learning algorithms. This general principle is applied in many contexts. Figure shows the manifold structure of a dataset consisting of faces. By the end of this book, we will have developed the methods necessary to learn such a manifold structure. In ﬁgure , we will see how a machine learning algorithm can successfully accomplish this goal.  20.6  5.13  This concludes part , which has provided the basic concepts in mathematics and machine learning which are employed throughout the remaining parts of the book. You are now prepared to embark upon your study of deep learning.  I  164  \\n\\nCHAPTER 5. MACHINE LEARNING BASICS  Gong et al. 2000 ) Figure 5.13: Training examples from the QMUL Multiview Face Dataset ( for which the subjects were asked to move in such a way as to cover the two-dimensional manifold corresponding to two angles of rotation. We would like learning algorithms to be illustrates such a able to discover and disentangle such manifold coordinates. Figure feat.  20.6  ,  165  '}, {'question': 'What are the basic concepts provided in the part mentioned in the context?', 'context': 'Context:  1-D manifolds embedded in 3-D space. We give directions to speciﬁc addresses in terms of address numbers along these 1-D roads, not in terms of coordinates in 3-D space. Extracting these manifold coordinates is challenging, but holds the promise to improve many machine learning algorithms. This general principle is applied in many contexts. Figure shows the manifold structure of a dataset consisting of faces. By the end of this book, we will have developed the methods necessary to learn such a manifold structure. In ﬁgure , we will see how a machine learning algorithm can successfully accomplish this goal.  20.6  5.13  This concludes part , which has provided the basic concepts in mathematics and machine learning which are employed throughout the remaining parts of the book. You are now prepared to embark upon your study of deep learning.  I  164  \\n\\nCHAPTER 5. MACHINE LEARNING BASICS  Gong et al. 2000 ) Figure 5.13: Training examples from the QMUL Multiview Face Dataset ( for which the subjects were asked to move in such a way as to cover the two-dimensional manifold corresponding to two angles of rotation. We would like learning algorithms to be illustrates such a able to discover and disentangle such manifold coordinates. Figure feat.  20.6  ,  165  '}, {'question': 'What is the purpose of the methods mentioned in the context?', 'context': 'Context:  1-D manifolds embedded in 3-D space. We give directions to speciﬁc addresses in terms of address numbers along these 1-D roads, not in terms of coordinates in 3-D space. Extracting these manifold coordinates is challenging, but holds the promise to improve many machine learning algorithms. This general principle is applied in many contexts. Figure shows the manifold structure of a dataset consisting of faces. By the end of this book, we will have developed the methods necessary to learn such a manifold structure. In ﬁgure , we will see how a machine learning algorithm can successfully accomplish this goal.  20.6  5.13  This concludes part , which has provided the basic concepts in mathematics and machine learning which are employed throughout the remaining parts of the book. You are now prepared to embark upon your study of deep learning.  I  164  \\n\\nCHAPTER 5. MACHINE LEARNING BASICS  Gong et al. 2000 ) Figure 5.13: Training examples from the QMUL Multiview Face Dataset ( for which the subjects were asked to move in such a way as to cover the two-dimensional manifold corresponding to two angles of rotation. We would like learning algorithms to be illustrates such a able to discover and disentangle such manifold coordinates. Figure feat.  20.6  ,  165  '}] ,'answers': [{'answer': 'To improve many machine learning algorithms', 'question': 'What is the main purpose of extracting manifold coordinates?'}, {'answer': 'In many contexts', 'question': 'In what context is the principle of extracting manifold coordinates applied?'}, {'answer': 'The manifold structure of a dataset consisting of faces', 'question': 'What does the figure in the context show?'}, {'answer': 'Discover and disentangle manifold coordinates', 'question': 'What is the goal of the machine learning algorithm mentioned in the context?'}, {'answer': 'The basic concepts in mathematics and machine learning', 'question': 'What is the conclusion of the part mentioned in the context?'}, {'answer': 'Covering two angles of rotation', 'question': 'What is the purpose of the dataset mentioned in the context?'}, {'answer': 'The methods necessary to learn such a manifold structure', 'question': 'What is the purpose of the methods mentioned in the context?'}]}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What are some common operations that require numerical computation in machine learning algorithms?\",\n",
      "    \"What is rounding error and why is it problematic?\",\n",
      "    \"What is underflow and why is it damaging?\",\n",
      "    \"What is overflow and why is it damaging?\",\n",
      "    \"Why do we need to stabilize the softmax function against underflow and overflow?\",\n",
      "    \"How can underflow and overflow be resolved when evaluating the softmax function?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Some common operations that require numerical computation in machine learning algorithms include optimization, solving systems of linear equations, and evaluating mathematical functions.\",\n",
      "    \"Rounding error refers to the approximation error incurred when representing real numbers in a finite number of bit patterns. It can be problematic because it accumulates across many operations and can cause algorithms to fail if not managed properly.\",\n",
      "    \"Underflow occurs when numbers near zero are rounded to zero. It is damaging because many functions behave differently when their argument is zero rather than a small positive number. Division by zero or taking the logarithm of zero can lead to undefined results.\",\n",
      "    \"Overflow occurs when numbers with large magnitude are approximated and usually result in infinite or not-a-number values. It is damaging because it can lead to undefined results and affect the stability of numerical calculations.\",\n",
      "    \"The softmax function needs to be stabilized against underflow and overflow because its denominator can become zero or undefined when the input vector has large magnitudes. This can lead to division by zero or undefined results in the softmax calculation.\",\n",
      "    \"Underflow and overflow when evaluating the softmax function can be resolved by subtracting the maximum value from the input vector before applying the softmax function. This ensures that the exponential terms do not result in underflow or overflow, making the calculation numerically stable.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': [{'question': 'What is the function of low-level libraries in deep learning algorithms?', 'answer': 'Low-level libraries in deep learning algorithms provide stable implementations for readers of the book.'}, {'question': 'What is an example of a software package that automatically detects and stabilizes numerically unstable expressions in deep learning?', 'answer': 'Theano is an example of a software package that automatically detects and stabilizes numerically unstable expressions in the context of deep learning.'}, {'question': 'What does conditioning refer to in scientific computation?', 'answer': 'Conditioning refers to how rapidly a function changes with respect to small changes in its inputs.'}, {'question': 'What is the condition number of a matrix?', 'answer': 'The condition number of a matrix is the ratio of the magnitude of the largest and smallest eigenvalue of the matrix.'}, {'question': 'What does poorly conditioned matrices amplify?', 'answer': 'Poorly conditioned matrices amplify pre-existing errors when multiplied by the true matrix inverse.'}, {'question': 'What is optimization in the context of deep learning algorithms?', 'answer': 'Optimization refers to the task of either minimizing or maximizing the objective function or criterion in deep learning algorithms.'}, {'question': 'What is the derivative of a function?', 'answer': 'The derivative of a function gives the slope of the function at a given point and specifies how to scale a small change in the input to obtain the corresponding change in the output.'}], 'answers': [{'answer_start': 127, 'text': 'provide stable implementations'}, {'answer_start': 187, 'text': 'Theano'}, {'answer_start': 355, 'text': 'how rapidly a function changes with respect to small changes in its inputs'}, {'answer_start': 524, 'text': 'the ratio of the magnitude of the largest and smallest eigenvalue'}, {'answer_start': 709, 'text': 'pre-existing errors when multiplied by the true matrix inverse.'}, {'answer_start': 859, 'text': 'either minimizing or maximizing the objective function or criterion in deep learning algorithms.'}, {'answer_start': 992, 'text': 'The derivative of a function gives the slope of the function at a given point and specifies how to scale a small change in the input to obtain the corresponding change in the output.'}]}\n",
      "{'questions': [{'question': 'What is a local minimum?', 'answers': ['A local minimum is a point where f(x) is lower than at all neighboring points.']}], 'answers': {'answer_start': [67], 'text': ['A local minimum is a point where f (x) is lower than at all neighboring points,']}}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What are some popular approaches to choosing the learning rate?\",\n",
      "    \"What is line search?\",\n",
      "    \"What does steepest descent convergence depend on?\",\n",
      "    \"How can gradient descent be generalized to discrete spaces?\",\n",
      "    \"What is a Jacobian matrix?\",\n",
      "    \"What does the second derivative of a function measure?\",\n",
      "    \"What does the second derivative tell us about curvature?\",\n",
      "    \"What is the Hessian matrix and how is it defined?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Some popular approaches to choosing the learning rate are setting it to a small constant or solving for the step size that makes the directional derivative vanish.\",\n",
      "    \"Line search is a strategy where different values of the learning rate are evaluated and the one resulting in the smallest objective function value is chosen.\",\n",
      "    \"Steepest descent convergence depends on every element of the gradient being zero (or very close to zero).\",\n",
      "    \"Gradient descent can be generalized to discrete spaces by ascending an objective function of discrete parameters, also known as hill climbing.\",\n",
      "    \"A Jacobian matrix is a matrix containing all the partial derivatives of a function whose input and output are both vectors.\",\n",
      "    \"The second derivative of a function measures how the first derivative will change as the input varies.\",\n",
      "    \"The second derivative determines the curvature of a function. If the second derivative is negative, the function curves downward and the cost function will decrease by more than the gradient alone would predict. If the second derivative is positive, the function curves upward and the cost function can decrease by less than the gradient predicts.\",\n",
      "    \"The Hessian matrix is a matrix containing all the second derivatives of a function with multiple input dimensions. It is defined as the Jacobian of the gradient.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': [{'question': 'What property of the Hessian matrix allows for its decomposition into eigenvalues and eigenvectors?', 'answer': 'symmetry'}, {'question': 'What does the second derivative in a specific direction represented by a unit vector tell us?', 'answer': 'the second derivative in that direction is given by the corresponding eigenvalue'}, {'question': 'How is the maximum and minimum second derivative determined?', 'answer': 'The maximum eigenvalue determines the maximum second derivative and the minimum eigenvalue determines the minimum second derivative'}, {'question': 'What does the second-order Taylor series approximation to a function tell us?', 'answer': 'the expected improvement due to the slope of the function and the correction we must apply to account for the curvature of the function'}, {'question': 'What is the optimal step size for a positive value of gHg?', 'answer': 'gg divided by gHg'}, {'question': 'How can the second derivative be used to determine the type of critical point?', 'answer': 'When f (x) > 0, the critical point is a local minimum. When f (x) < 0, the critical point is a local maximum. Otherwise, the type is inconclusive'}], 'answers': [{'answer_start': 87, 'text': 'symmetric'}, {'answer_start': 143, 'text': 'the second derivative in that direction is given by the corresponding eigenvalue'}, {'answer_start': 435, 'text': 'The maximum eigenvalue determines the maximum second derivative and the minimum eigenvalue determines the minimum second derivative'}, {'answer_start': 524, 'text': 'the expected improvement due to the slope of the function and the correction we must apply to account for the curvature of the function'}, {'answer_start': 816, 'text': 'gg divided by gHg'}, {'answer_start': 993, 'text': 'When f (x) > 0, the critical point is a local minimum. When f (x) < 0, the critical point is a local maximum. Otherwise, the type is inconclusive'}]}\n",
      "{'questions': [{'question': 'What does it mean when the Hessian matrix is positive definite?', 'context': 'itive), the point is a local minimum.'}, {'question': 'How can we determine if a point is a local maximum in multiple dimensions?', 'context': 'Likew...x has a poor condition number?', 'context': 'When the Hessian has a poor condition number, gradient descent performs poorly.'}, {'question': 'What is the issue with gradient descent when the Hessian matrix has a poor condition number?', 'context': 'It also m'}], 'answers': [{'answer': 'The point is a local minimum.', 'question': 'What does it mean when the Hessian matrix is positive definite?'}, {'answer': 'It can be a local maximum on one cross section and a local minimum on another cross section.', 'qu\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is Newton's method based on?\",\n",
      "    \"When is Newton's method most effective?\",\n",
      "    \"What is a first-order optimization algorithm?\",\n",
      "    \"What property does Lipschitz continuity guarantee?\",\n",
      "    \"What guarantees do convex optimization algorithms provide?\",\n",
      "    \"What is the importance of convex optimization in deep learning?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Newton's method is based on using a second-order Taylor series expansion to approximate a function near some point.\",\n",
      "    \"Newton's method is most effective when the nearby critical point is a minimum.\",\n",
      "    \"A first-order optimization algorithm is an optimization algorithm that uses only the gradient.\",\n",
      "    \"Lipschitz continuity guarantees that a small change in the input will result in a small change in the output.\",\n",
      "    \"Convex optimization algorithms provide more guarantees by making strong restrictions and are only applicable to convex functions.\",\n",
      "    \"The importance of convex optimization in deep learning is greatly diminished, as most problems in deep learning are difficult to express in terms of convex optimization.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is constrained optimization?\",\n",
      "    \"How are feasible points defined in constrained optimization?\",\n",
      "    \"What is one approach for constrained optimization?\",\n",
      "    \"How can the efficiency of the approach be improved?\",\n",
      "    \"What is the Karush-Kuhn-Tucker (KKT) approach to constrained optimization?\",\n",
      "    \"How is the generalized Lagrangian defined in the KKT approach?\",\n",
      "    \"How can a constrained minimization problem be solved using unconstrained optimization?\",\n",
      "    \"What properties guarantee that the optimal solution of a constrained problem is within the feasible points?\",\n",
      "    \"What is the formulation for constrained maximization?\",\n",
      "    \"How are active constraints defined in the KKT approach?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Constrained optimization is finding the maximal or minimal value of a function while considering constraints on the input values.\",\n",
      "    \"Feasible points in constrained optimization are the points that lie within the set of constraints.\",\n",
      "    \"One approach for constrained optimization is modifying gradient descent to take the constraints into account.\",\n",
      "    \"The efficiency of the approach can be improved by projecting the gradient into the tangent space of the feasible region.\",\n",
      "    \"The Karush-Kuhn-Tucker (KKT) approach is a general solution to constrained optimization.\",\n",
      "    \"The generalized Lagrangian is defined as the sum of the objective function, the products of Lagrange multipliers and equality constraints, and the products of Lagrange multipliers and inequality constraints.\",\n",
      "    \"A constrained minimization problem can be solved using unconstrained optimization of the generalized Lagrangian.\",\n",
      "    \"The properties guarantee that the optimal solution of a constrained problem is within the feasible points, and only feasible points can be optimal.\",\n",
      "    \"The formulation for constrained maximization involves minimizing the negative of the objective function while considering the Lagrange multipliers and inequality constraints.\",\n",
      "    \"Active constraints in the KKT approach are the constraints that are satisfied with equality at the optimal solution.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What are Karush-Kuhn-Tucker (KKT) conditions?\",\n",
      "    \"What does it mean for a constraint to be inactive?\",\n",
      "    \"What is the purpose of the Lagrangian in optimization?\",\n",
      "    \"How can the gradient of the Lagrangian be used in optimization?\",\n",
      "    \"How can the Moore-Penrose pseudoinverse be used to solve the least squares problem?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Karush-Kuhn-Tucker (KKT) conditions are necessary conditions for a point to be optimal in constrained optimization problems.\",\n",
      "    \"An inactive constraint does not affect the solution to the problem and can be removed without changing the local optimality of the solution.\",\n",
      "    \"The Lagrangian is introduced in optimization to handle constraints by combining the objective function and constraints into a single function.\",\n",
      "    \"The gradient of the Lagrangian can be used to find the stationary points in optimization by setting it to zero and solving the resulting equations.\",\n",
      "    \"The Moore-Penrose pseudoinverse can be used to find the smallest-norm solution to the unconstrained least squares problem.\"\n",
      "  ]\n",
      "}\n",
      "{\"questions\": [\"What happens when we increase λ in the Lagrangian?\", \"Why do we solve the linear equation for x?\", \"How do we determine if x has the correct norm?\", \"What is the purpose of the mathematical preliminaries in developing machine learning algorithms?\", \"What is the next step after the mathematical preliminaries?\", \"Is the text a transcription of slides in the PDF file format?\"], \n",
      "\"answers\": [\"When we increase λ in the Lagrangian, the coefficient on the xx penalty increases, resulting in a solution with smaller norm for x.\", \"We solve the linear equation for x to obtain a solution with the correct norm.\", \"We determine if x has the correct norm by checking if the derivative on λ is 0.\", \"The purpose of the mathematical preliminaries is to provide the necessary foundation for building and analyzing machine learning algorithms.\", \"The next step after the mathematical preliminaries is to build and analyze some full-fledged learning systems.\", \"The given context does not provide information about the format of the text transcription.\"] }\n",
      "{'questions': ['What is the field of study discussed in the text?', 'What did early AI systems focus on solving?', 'What approach does the book propose to solve intuitive problems?', 'What is one challenge in artificial intelligence?', 'What is the knowledge base approach to AI?'], 'answers': ['Artificial intelligence', 'Problems that are intellectually difficult for humans but easy for computers', 'To allow computers to learn from experience and understand the world in terms of a hierarchy of concepts', 'Getting informal knowledge into a computer', 'Hard-coding knowledge about the world in formal languages']}\n",
      "{'questions': \n",
      "['What is the name of the capability in AI systems that allows them to acquire their own knowledge?',\n",
      "'What is the purpose of machine learning algorithms?',\n",
      "'What is a feature in the context of machine learning?',\n",
      "'Why is the choice of representation important in machine learning algorithms?',\n",
      "'What is an example of a task where it is difficult to know what features should be extracted?'],\n",
      " 'answers': \n",
      "['The capability in AI systems that allows them to acquire their own knowledge is called machine learning.',\n",
      "'The purpose of machine learning algorithms is to tackle problems involving knowledge of the real world and make subjective decisions.',\n",
      "'A feature in the context of machine learning is a piece of information included in the representation of the data, such as the presence or absence of a uterine scar in the case of recommending cesarean delivery.',\n",
      "'The choice of representation is important in machine learning algorithms because it can have a significant effect on their performance. Operations such as searching a collection of data can proceed exponentially faster if the collection is structured and indexed intelligently. Different representations can yield different results.',\n",
      "'An example of a task where it is difficult to know what features should be extracted is detecting cars in photographs. While one might want to use the presence of a wheel as a feature, it is difficult to describe exactly what a wheel looks like in terms of pixel values due to various factors like shadows, glare, and obstructions.'] }\n",
      "{\"questions\": [\"What is the purpose of representation learning in machine learning?\", \"How do learned representations compare to hand-designed representations?\", \"What is an autoencoder?\", \"What are factors of variation in the context of representation learning?\", \"What is the role of deep learning in representation learning?\", \"What is a multilayer perceptron?\"], \"answers\": [\"The purpose of representation learning is to discover both the mapping from representation to output and the representation itself.\", \"Learned representations often perform better than hand-designed representations and allow for rapid adaptation to new tasks with minimal human intervention.\", \"An autoencoder is a learning algorithm that consists of an encoder function and a decoder function. The encoder converts input data into a different representation, while the decoder converts the new representation back into the original format.\", \"Factors of variation refer to separate sources of influence that explain observed data. They can be unobserved objects or forces, constructs in the human mind, or concepts that help make sense of data variability.\", \"Deep learning enables the creation of representations expressed in terms of simpler representations, allowing the computer to build complex concepts from basic ones.\", \"A multilayer perceptron is a mathematical function that maps a set of input values to output values. It is a type of deep learning model.\"]\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is one perspective on deep learning?\",\n",
      "    \"What does each layer of the representation in deep learning represent?\",\n",
      "    \"What is the advantage of networks with greater depth?\",\n",
      "    \"How does deep learning break down the complicated mapping from raw sensory input data?\",\n",
      "    \"Why are the hidden layers in deep learning called 'hidden'?\",\n",
      "    \"What do the images in the figure represent?\",\n",
      "    \"What is the purpose of state information in the representation of deep learning?\",\n",
      "    \"What are the two main ways of measuring the depth of a model?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"One perspective on deep learning is that it involves learning the right representation of the data.\",\n",
      "    \"Each layer of the representation in deep learning represents a new set of instructions or features extracted from the input.\",\n",
      "    \"Networks with greater depth can execute more instructions in sequence, providing more power.\",\n",
      "    \"Deep learning breaks the complicated mapping from raw sensory input data by breaking it into a series of nested simple mappings, each described by a different layer of the model.\",\n",
      "    \"The hidden layers in deep learning are called 'hidden' because their values are not given in the data; instead, the model determines which concepts are useful for explaining relationships in the observed data.\",\n",
      "    \"The images in the figure represent visualizations of the features represented by each hidden unit in the deep learning model.\",\n",
      "    \"State information in the representation of deep learning helps the model organize its processing, similar to a counter or pointer in a traditional computer program.\",\n",
      "    \"The two main ways of measuring the depth of a model are based on the number of sequential instructions that must be executed to evaluate the architecture, and the definition of what constitutes a possible computational step.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "\"questions\": [\n",
      "\"1. What is the depth of a model in the context of flowcharts?\",\n",
      "\"2. What is another approach to measuring the depth of a model?\",\n",
      "\"3. Can the depth of a model vary depending on the choice of language?\",\n",
      "\"4. How is the depth of a model in deep probabilistic models defined?\",\n",
      "\"5. Why can the depth of the flowchart of computations be much deeper than the graph of concepts?\",\n",
      "\"6. Can the depth of an architecture have a single correct value?\",\n",
      "\"7. What is deep learning?\",\n",
      "\"8. Who are the main target audiences of this book on deep learning?\",\n",
      "\"9. What is the relationship between deep learning, representation learning, machine learning, and AI?\"\n",
      "],\n",
      "\"answers\": [\n",
      "\"1. The depth of a model in the context of flowcharts refers to the longest path through the flowchart that describes how to compute each output given the inputs.\",\n",
      "\"2. Another approach to measuring the depth of a model is to consider the depth of the graph describing how concepts are related to each other.\",\n",
      "\"3. Yes, the depth of a model can vary depending on the choice of language used to represent the model.\",\n",
      "\"4. In deep probabilistic models, the depth of the model is determined by the depth of the graph describing how concepts are related to each other.\",\n",
      "\"5. The depth of the flowchart of computations can be much deeper than the graph of concepts because the system's understanding of simpler concepts can be refined given information about more complex concepts.\",\n",
      "\"6. The depth of an architecture does not have a single correct value.\",\n",
      "\"7. Deep learning is an approach to AI and a type of machine learning that involves a greater amount of composition of learned functions or learned concepts than traditional machine learning.\",\n",
      "\"8. The main target audiences of this book on deep learning are university students (undergraduate or graduate) learning about machine learning, and software engineers who want to rapidly acquire a machine learning and statistics background.\",\n",
      "\"9. Deep learning is a kind of representation learning, which is a kind of machine learning. Machine learning is used for many, but not all, approaches to AI.\"\n",
      "]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What are the different parts of an AI system?\",\n",
      "    \"What are some of the disciplines that AI is applied to?\",\n",
      "    \"How is the book organized?\",\n",
      "    \"What background knowledge is assumed for the readers?\",\n",
      "    \"What are some historical trends in deep learning?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The different parts of an AI system include input, simple features, features, hand-designed features, hand-designed program, additional layers of more abstract features, mapping from features, output, rule-based systems, classic machine learning, and deep learning.\",\n",
      "    \"AI is applied to many disciplines including computer vision, speech and audio processing, natural language processing, robotics, bioinformatics and chemistry, video games, search engines, online advertising, and finance.\",\n",
      "    \"The book is organized into three parts. Part I introduces basic mathematical tools and machine learning concepts. Part II describes the most established deep learning algorithms and modern practices. Part III discusses more speculative ideas that are believed to be important for future research in deep learning.\",\n",
      "    \"The readers are assumed to have a computer science background and familiarity with programming, computational performance issues, complexity theory, introductory level calculus, and some of the terminology of graph theory.\",\n",
      "    \"Some historical trends in deep learning include: deep learning has had a long and rich history, deep learning has become more useful as the amount of available training data has increased, deep learning models have grown in size over time, and deep learning has solved increasingly complicated applications with increasing accuracy over time.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What are the three waves of development of deep learning?\",\n",
      "    \"What is the neural perspective on deep learning?\",\n",
      "    \"What is the motivation behind the neural perspective on deep learning?\",\n",
      "    \"What is the modern term 'deep learning' based on?\",\n",
      "    \"What were the earliest predecessors of modern deep learning?\",\n",
      "    \"What was the first wave of neural networks research called?\",\n",
      "    \"What is the McCulloch-Pitts Neuron?\",\n",
      "    \"What were the two categories of inputs the McCulloch-Pitts Neuron could recognize?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The three waves of development of deep learning are deep learning known as cybernetics in the 1940s–1960s, deep learning known as connectionism in the 1980s–1990s, and the current resurgence under the name deep learning beginning in 2006.\",\n",
      "    \"The neural perspective on deep learning is that deep learning models are engineered systems inspired by the biological brain.\",\n",
      "    \"The motivation behind the neural perspective on deep learning is that the brain provides proof that intelligent behavior is possible and reverse engineering the brain's computational principles can lead to building intelligence. Additionally, understanding the brain and principles behind human intelligence is also a scientific interest.\",\n",
      "    \"The modern term 'deep learning' is based on the principle of learning multiple levels of composition, which can be applied in machine learning frameworks that are not necessarily neurally inspired.\",\n",
      "    \"The earliest predecessors of modern deep learning were simple linear models motivated from a neuroscientific perspective.\",\n",
      "    \"The first wave of neural networks research was called cybernetics.\",\n",
      "    \"The McCulloch-Pitts Neuron was an early model of brain function.\",\n",
      "    \"The McCulloch-Pitts Neuron could recognize two categories of inputs by testing whether the output of the model is positive or negative.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': [{'question': 'What were the first models that could learn the weights defining the categories?', 'answers': ['perceptron and ADALINE']}, {'question': 'What is the training algorithm used to adapt the weights of the ADALINE?', 'answers': ['a spec...ased on?', 'answers': ['linear models']}, {'question': 'What is the main reason for the diminished role of neuroscience in deep learning research today?', 'answers': ['lack of information about the brain']}, {'question': 'What is the Neocognitron inspired by?', 'answers': ['the structure of the mammalian visual system']}], 'answers': [{'id': 0, 'text': 'perceptron and ADALINE'}, {'id': 1, 'text': 'a special case of an algorithm called stochastic gradient descent'}, {'id': 2, 'text': 'linear models'}...\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What inspires modern deep learning?\",\n",
      "    \"What is computational neuroscience primarily concerned with?\",\n",
      "    \"What is connectionism in the context of cognitive science?\",\n",
      "    \"What is the concept of distributed representation?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Modern deep learning draws inspiration from many fields, especially applied math fundamentals like linear algebra, probability, information theory, and numerical optimization.\",\n",
      "    \"Computational neuroscience is primarily concerned with building more accurate models of how the brain actually works.\",\n",
      "    \"Connectionism in the context of cognitive science refers to the idea that a large number of simple computational units can achieve intelligent behavior when networked together.\",\n",
      "    \"The concept of distributed representation is the idea that each input to a system should be represented by many features, and each feature should be involved in the representation of many possible inputs.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the concept of distributed representation?\",\n",
      "    \"How did the connectionist movement use back-propagation?\",\n",
      "    \"What did researchers in the 1990s accomplish in sequence modeling?\",\n",
      "    \"What caused a decline in the popularity of neural networks until 2007?\",\n",
      "    \"What led to a breakthrough in neural networks research in 2007?\",\n",
      "    \"What is the significance of the term 'deep learning'?\",\n",
      "    \"What was the focus of deep learning research during the third wave?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The concept of distributed representation refers to the representation of objects based on features obtained from multiple categories and not just one specific category.\",\n",
      "    \"The connectionist movement successfully used back-propagation to train deep neural networks with internal representations. Back-propagation algorithm is the dominant approach to training deep models.\",\n",
      "    \"In the 1990s, researchers identified some fundamental difficulties in modeling long sequences and introduced the long short-term memory (LSTM) network to resolve these difficulties. The LSTM is widely used for many sequence modeling tasks.\",\n",
      "    \"The decline in the popularity of neural networks until 2007 can be attributed to the unrealistic claims made by ventures based on neural networks and the advances made by other fields of machine learning, particularly kernel machines and graphical models.\",\n",
      "    \"The breakthrough in neural networks research in 2007 was the successful training of deep belief networks using a strategy called greedy layer-wise pre-training. This strategy was popularized by Geoﬀrey Hinton.\",\n",
      "    \"The term 'deep learning' was popularized during the third wave of neural networks research to emphasize the ability to train deeper neural networks than before and highlight the theoretical importance of depth.\",\n",
      "    \"During the third wave of deep learning research, the focus shifted dramatically. It began with a focus on improving generalization on test examples and outperforming other AI systems. However, the specific focus changed over time and was not mentioned in the text.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': \n",
      "\n",
      "1. When were the first experiments with artificial neural networks conducted?\n",
      "2. What is the main reason for the recent recognition of deep learning as a crucial technology?\n",
      "3. How does the increasing digitization of society impact the availability of training data for machine learning?\n",
      "4. What is one of the main insights of connectionism in relation to neural networks?\n",
      "5. How have the size of neural networks and the number of connections per neuron evolved over time?\n",
      "6. What is the rough rule of thumb for the number of labeled examples required for acceptable performance of a supervised deep learning algorithm?\n",
      "7. What research area focuses on working successfully with smaller datasets?\n",
      "8. When is it projected that artificial neural networks will have the same number of neurons as the human brain?\n",
      "9. Why might biological neural networks have even larger numbers of neurons than artificial neural networks?\n",
      "10. What are the implications of larger networks in terms of task complexity and accuracy? \n",
      "\n",
      "}, \n",
      "\n",
      "{'answers':\n",
      "\n",
      "1. The first experiments with artificial neural networks were conducted in the 1950s.\n",
      "2. The recent recognition of deep learning as a crucial technology is due to the ability of deep models to leverage large labeled datasets.\n",
      "3. The increasing digitization of society has led to more activities being recorded on computers, which has resulted in larger datasets appropriate for machine learning applications.\n",
      "4. One of the main insights of connectionism is that animals become intelligent when many of their neurons work together.\n",
      "5. The size of neural networks and the number of connections per neuron have grown significantly over time, with artificial neural networks doubling in size roughly every 2.4 years.\n",
      "6. A rough rule of thumb is that a supervised deep learning algorithm will generally achieve acceptable performance with around 5,000 labeled examples per category.\n",
      "7. The research area that focuses on working successfully with smaller datasets is unsupervised or semi-supervised learning, particularly leveraging large quantities of unlabeled examples.\n",
      "8. It is projected that artificial neural networks will have the same number of neurons as the human brain until at least the 2050s.\n",
      "9. Biological neural networks may have even larger numbers of neurons than artificial neural networks because biological neurons may represent more complicated functions.\n",
      "10. Larger networks are able to achieve higher accuracy on more complex tasks.\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the trend in model size over time?\",\n",
      "    \"What is the significance of larger datasets in deep learning?\",\n",
      "    \"What is the MNIST dataset used for?\",\n",
      "    \"What has been an important trend in the history of deep learning?\",\n",
      "    \"How has deep learning improved since the 1980s?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The trend in model size over time has been an increase due to the availability of faster CPUs.\",\n",
      "    \"Larger datasets have allowed for more advanced deep learning techniques and have changed what is possible with deep learning.\",\n",
      "    \"The MNIST dataset is used for handwriting digit recognition.\",\n",
      "    \"The increasing availability of faster network connectivity and better software infrastructure for distributed computing is an important trend in the history of deep learning.\",\n",
      "    \"Deep learning has consistently improved in its ability to provide accurate recognition or prediction since the 1980s.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "\"questions\": [\n",
      "\"What is the largest contest in object recognition called?\",\n",
      "\"What was the top-5 error rate in the ImageNet Large Scale Visual Recognition Challenge in 2012?\",\n",
      "\"What is the latest top-5 error rate in the ImageNet Large Scale Visual Recognition Challenge?\",\n",
      "\"What impact did deep learning have on speech recognition?\",\n",
      "\"What are some other areas where deep learning has had successes?\"\n",
      "],\n",
      "\"answers\": [\n",
      "\"The largest contest in object recognition is the ImageNet Large Scale Visual Recognition Challenge (ILSVRC).\",\n",
      "\"The top-5 error rate in the ImageNet Large Scale Visual Recognition Challenge in 2012 was 15.3%.\",\n",
      "\"The latest top-5 error rate in the ImageNet Large Scale Visual Recognition Challenge is 3.6%.\",\n",
      "\"Deep learning resulted in a sudden drop of error rates in speech recognition.\",\n",
      "\"Deep learning has also had successes in pedestrian detection, image segmentation, and traﬃc sign classiﬁcation.\"\n",
      "]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What are some applications of deep learning?\",\n",
      "    \"Which companies use deep learning?\",\n",
      "    \"How has deep learning contributed to other sciences?\",\n",
      "    \"What factors have contributed to the growth of deep learning?\",\n",
      "    \"What are some challenges and opportunities for deep learning in the future?\",\n",
      "    \"What is the growth rate of artificial neural networks since the introduction of hidden units?\",\n",
      "    \"What are some early developments in neural networks?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Some applications of deep learning include image recognition, speech recognition, natural language processing, machine translation, and reinforcement learning.\",\n",
      "    \"Companies such as Google, Microsoft, Facebook, IBM, Baidu, Apple, Adobe, Netflix, NVIDIA, and NEC use deep learning.\",\n",
      "    \"Deep learning has contributed to other sciences by providing tools for processing massive amounts of data and making predictions. It has been used for predicting molecular interactions, designing new drugs, searching for subatomic particles, and analyzing microscope images.\",\n",
      "    \"The growth of deep learning has been influenced by more powerful computers, larger datasets, and techniques for training deeper networks.\",\n",
      "    \"The years ahead present challenges and opportunities for improving deep learning further and exploring new frontiers.\",\n",
      "    \"Artificial neural networks have doubled in size roughly every 2.4 years since the introduction of hidden units.\",\n",
      "    \"Some early developments in neural networks include the perceptron, adaptive linear element, neocognitron, early back-propagation network, and recurrent neural network for speech recognition.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the purpose of the mentioned deep networks?\",\n",
      "    \"What is shown in Figure 1.12?\",\n",
      "    \"What is the significance of winning the ImageNet challenge?\",\n",
      "    \"What is the trend of error rates in the ImageNet challenge over the years?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The purpose of the mentioned deep networks is speech recognition, belief network modeling, image classification, and unsupervised learning.\",\n",
      "    \"Figure 1.12 shows the consistent winning streak of deep networks in the ImageNet Large Scale Visual Recognition Challenge.\",\n",
      "    \"Winning the ImageNet challenge is significant as it reflects the ability of deep networks to outperform other methods in image classification tasks.\",\n",
      "    \"The trend of error rates in the ImageNet challenge has been consistently decreasing over the years, indicating improving performance of deep networks.\"\n",
      "  ]\n",
      "}\n",
      "{\"questions\": [\"What is linear algebra?\", \"Why is linear algebra important for understanding deep learning?\", \"What are the types of mathematical objects studied in linear algebra?\", \"What is a scalar? Give an example.\", \"What is a vector? How are the elements of a vector identified?\", \"What is a matrix? How are the elements of a matrix identified?\"], \"answers\": [\"Linear algebra is a branch of mathematics widely used in science and engineering.\", \"Linear algebra is important for understanding deep learning because many machine learning algorithms, especially deep learning algorithms, rely on linear algebra.\", \"The types of mathematical objects studied in linear algebra are scalars, vectors, matrices, and tensors.\", \"A scalar is a single number. For example, the slope of a line or the number of units can be scalar values.\", \"A vector is an array of numbers where each number is identified by its index. The elements of a vector are identified using subscripts. For example, x1 represents the first element, x2 represents the second element, and so on.\", \"A matrix is a 2-D array of numbers where each element is identified by two indices. Matrices are usually denoted by upper-case letters. The elements of a matrix are identified using subscripts with separating commas. For example, A1,1 represents the upper left entry of the matrix.\"] }\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What does the transpose of a matrix represent?\",\n",
      "    \"How do you denote the transpose of a matrix A?\",\n",
      "    \"What is the definition of the transpose of a matrix A?\",\n",
      "    \"How do you represent an element at coordinates (i, j, k) of a tensor A?\",\n",
      "    \"What is the transpose of a vector?\",\n",
      "    \"What is the element-wise product of matrices A and B?\",\n",
      "    \"What is the dot product between two vectors x and y?\",\n",
      "    \"What properties do matrix products have?\",\n",
      "    \"Is matrix multiplication commutative?\",\n",
      "    \"What is the transpose of a matrix product AB?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The transpose of a matrix is a mirror image of the matrix across its main diagonal.\",\n",
      "    \"The transpose of a matrix A is denoted as A\\u1fbf.\",\n",
      "    \"The transpose of a matrix A is defined as (A\\u1fbf)\\u1dd8,\\u1dd6 = A\\u1dd7,\\u1dd8.\",\n",
      "    \"An element at coordinates (i, j, k) of a tensor A is represented as Ai,j,k.\",\n",
      "    \"The transpose of a vector is a matrix with only one row.\",\n",
      "    \"The element-wise product of matrices A and B is denoted as A \\u2297 B.\",\n",
      "    \"The dot product between two vectors x and y is represented as x\\u1fbf y.\",\n",
      "    \"Matrix products are distributive and associative, but matrix multiplication is not commutative.\",\n",
      "    \"No, matrix multiplication is not commutative.\",\n",
      "    \"The transpose of a matrix product AB is denoted as (AB)\\u1fbf = B\\u1fbfA\\u1fbf.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': \n",
      "   [\n",
      "      {\n",
      "         'question': 'What does the matrix inversion tool allow us to do?',\n",
      "         'answers': ['Matrix inversion allows us to analytically solve equations of the form Ax = b.']\n",
      "      },\n",
      "      {\n",
      "         'question': 'What is an identity matrix?',\n",
      "         'answers': ['An identity matrix is a matrix that does not change any vector when we multiply that vector by the matrix. It preserves n-dimensional vectors.']\n",
      "      },\n",
      "      {\n",
      "         'question': 'How is the identity matrix denoted?',\n",
      "         'answers': ['The identity matrix that preserves n-dimensional vectors is denoted as In.']\n",
      "      },\n",
      "      {\n",
      "         'question': 'How can the equation Ax = b be solved using matrix inversion?',\n",
      "         'answers': ['The equation can be solved by multiplying both sides by the inverse of matrix A, resulting in x = A^(-1)b.']\n",
      "      },\n",
      "      {\n",
      "         'question': 'Are there any limitations to using matrix inversion in practice?',\n",
      "         'answers': ['Yes, matrix inversion should not be used in practice for most software applications because it can only be represented with limited precision on a digital computer.']\n",
      "      },\n",
      "      {\n",
      "         'question': 'What does it mean for a system of equations to have one solution, no solution, or infinitely many solutions?',\n",
      "         'answers': ['A system of equations has one solution if there is exactly one solution for every value of b. It has no solution if there are values of b for which there is no solution, and it has infinitely many solutions if there are values of b for which there are infinitely many solutions.']\n",
      "      },\n",
      "      {\n",
      "         'question': 'What is the interpretation of the columns of matrix A in the context of solving equations?',\n",
      "         'answers': ['The columns of matrix A can be interpreted as specifying different directions we can travel from the origin. xi specifies how far to move in the direction of column i.']\n",
      "      },\n",
      "      {\n",
      "         'question': 'What is the formal definition of a linear combination?',\n",
      "         'answers': ['A linear combination of a set of vectors is given by multiplying each vector by a scalar coefficient and adding the results.']\n",
      "      }\n",
      "   ],\n",
      " 'answers': []}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the span of a set of vectors?\",\n",
      "    \"What is the column space of a matrix?\",\n",
      "    \"What condition should be met for the system Ax=b to have a solution for all values of b?\",\n",
      "    \"What is linear dependence?\",\n",
      "    \"What condition must be met for a matrix to have an inverse?\",\n",
      "    \"What is a norm?\",\n",
      "    \"What is the L2 norm?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The span of a set of vectors is the set of all points obtainable by linear combination of the original vectors.\",\n",
      "    \"The column space of a matrix is the set of all points that can be obtained by linear combination of the columns of the matrix.\",\n",
      "    \"For the system Ax=b to have a solution for all values of b in Rm, the column space of A must be all of Rm.\",\n",
      "    \"Linear dependence is when a vector in a set is a linear combination of the other vectors in the set.\",\n",
      "    \"For a matrix to have an inverse, it must be square and have linearly independent columns.\",\n",
      "    \"A norm is a function that measures the size of a vector.\",\n",
      "    \"The L2 norm is the Euclidean norm, which is the Euclidean distance from the origin to the point identified by the vector x.\"\n",
      "  ]\n",
      "}\n",
      "{\"questions\": [\"What is the squared L2 norm?\", \"Why is the squared L2 norm more convenient to work with mathematically and computationally?\", \"When is the L1 norm commonly used in machine learning?\", \"What is the L1 norm often used as a substitute for?\", \"What is the L∞ norm also known as?\", \"What is the Frobenius norm?\", \"What is a diagonal matrix?\", \"Why are diagonal matrices of interest?\", \"What is a symmetric matrix?\"], \"answers\": [\"The squared L2 norm is the square of the L2 norm of a vector, denoted as ||x||2^2 = x^T * x.\", \"The squared L2 norm is more convenient to work with mathematically and computationally because the derivatives of the squared L2 norm depend only on the corresponding element of the vector, whereas the derivatives of the L2 norm depend on the entire vector.\", \"The L1 norm is commonly used in machine learning when the difference between zero and nonzero elements is very important.\", \"The L1 norm is often used as a substitute for the number of nonzero entries in a vector.\", \"The L∞ norm is also known as the max norm.\", \"The Frobenius norm of a matrix is analogous to the L2 norm of a vector and is calculated as the square root of the sum of the squares of all the elements in the matrix.\", \"A diagonal matrix consists mostly of zeros and has non-zero entries only along the main diagonal.\", \"Diagonal matrices are of interest because multiplying by a diagonal matrix is computationally efficient, and inverting a square diagonal matrix is also efficient.\", \"A symmetric matrix is any matrix that is equal to its own transpose, denoted as A = A^T.\"]}\n",
      "{'questions': \n",
      "  ['What is an example of a function that does not depend on the order of its arguments?',\n",
      "   'What is the definition of a unit vector?',\n",
      "   'How are two vectors defined as orthogonal to each other?',\n",
      "   'What is an orthonormal matrix?',\n",
      "   'Why are orthogonal matrices of interest?',\n",
      "   'What is an eigenvector of a matrix?',\n",
      "   'What is an eigenvalue of a matrix?',\n",
      "   'What is the eigendecomposition of a matrix?'],\n",
      " 'answers': \n",
      "  ['Distance functions are an example of a function that does not depend on the order of its arguments.',\n",
      "   'A unit vector is a vector with unit norm, meaning its length is equal to 1.',\n",
      "   'Two vectors are orthogonal to each other if their dot product is equal to 0.',\n",
      "   'An orthonormal matrix is a square matrix whose rows and columns are mutually orthogonal and have unit norm.',\n",
      "   'Orthogonal matrices are of interest because their inverse is cheap to compute.',\n",
      "   'An eigenvector of a matrix is a non-zero vector that is only scaled by the matrix.',\n",
      "   'An eigenvalue of a matrix is the scalar value associated with an eigenvector.',\n",
      "   'The eigendecomposition of a matrix involves decomposing the matrix into a set of eigenvectors and eigenvalues.'] }\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the purpose of decomposing a matrix into eigenvalues and eigenvectors?\",\n",
      "    \"Can every matrix be decomposed into eigenvalues and eigenvectors?\",\n",
      "    \"What class of matrices can be decomposed using only real-valued eigenvectors and eigenvalues?\",\n",
      "    \"What does it mean if two or more eigenvectors share the same eigenvalue?\",\n",
      "    \"What useful facts about a matrix does the eigendecomposition provide?\",\n",
      "    \"What is a positive definite matrix?\",\n",
      "    \"Why is the singular value decomposition (SVD) more generally applicable than the eigendecomposition?\",\n",
      "    \"What is the purpose of the singular value decomposition (SVD)?\",\n",
      "    \"What are the special structures of the matrices U, V, and D in the SVD?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The purpose of decomposing a matrix into eigenvalues and eigenvectors is to analyze certain properties of the matrix and understand its behavior. It is analogous to decomposing an integer into its prime factors to understand its behavior.\",\n",
      "    \"Not every matrix can be decomposed into eigenvalues and eigenvectors. In some cases, the decomposition may involve complex numbers instead of real numbers.\",\n",
      "    \"Every real symmetric matrix can be decomposed into a expression using real-valued eigenvectors and eigenvalues.\",\n",
      "    \"If two or more eigenvectors share the same eigenvalue, any set of orthogonal vectors lying in their span are also eigenvectors with that eigenvalue.\",\n",
      "    \"The eigendecomposition of a matrix provides useful facts such as determining if the matrix is singular (any eigenvalue is zero), optimizing quadratic expressions, finding maximum and minimum eigenvalues, and classifying the matrix as positive definite, positive semidefinite, negative definite, or negative semidefinite.\",\n",
      "    \"A positive definite matrix is a matrix whose eigenvalues are all positive.\",\n",
      "    \"The singular value decomposition (SVD) is more generally applicable than the eigendecomposition because every real matrix has an SVD, while the same is not true for the eigenvalue decomposition. The SVD can be used even for non-square matrices.\",\n",
      "    \"The purpose of the singular value decomposition (SVD) is to factorize a matrix into singular vectors and singular values. It helps in discovering information about the matrix similar to eigendecomposition.\",\n",
      "    \"The matrices U and V in the SVD are orthogonal matrices, while the matrix D is a diagonal matrix.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What are the left-singular vectors of matrix A?\",\n",
      "    \"What are the right-singular vectors of matrix A?\",\n",
      "    \"What are the non-zero singular values of matrix A?\",\n",
      "    \"What is the Moore-Penrose pseudoinverse of matrix A?\",\n",
      "    \"How is the pseudoinverse of a diagonal matrix obtained?\",\n",
      "    \"What does the trace operator give?\",\n",
      "    \"What is the determinant of a square matrix?\",\n",
      "    \"What does the determinant of a matrix represent?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The left-singular vectors of matrix A are the eigenvectors of AA^T.\",\n",
      "    \"The right-singular vectors of matrix A are the eigenvectors of A^TA.\",\n",
      "    \"The non-zero singular values of matrix A are the square roots of the eigenvalues of A^TA.\",\n",
      "    \"The Moore-Penrose pseudoinverse of matrix A is obtained using the formula A^+ = VD^+U^T, where U, D, and V are the singular value decomposition of matrix A.\",\n",
      "    \"The pseudoinverse of a diagonal matrix D is obtained by taking the reciprocal of its non-zero elements and then taking the transpose of the resulting matrix.\",\n",
      "    \"The trace operator gives the sum of all the diagonal entries of a matrix.\",\n",
      "    \"The determinant of a square matrix is a function that maps the matrix to a real scalar and is equal to the product of all the eigenvalues of the matrix.\",\n",
      "    \"The determinant of a matrix represents how much multiplication by the matrix expands or contracts space. If the determinant is 0, space is contracted completely along at least one dimension, losing all volume. If the determinant is 1, the transformation preserves volume.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the purpose of lossy compression in principal components analysis (PCA)?\",\n",
      "    \"How can we encode the points in PCA?\",\n",
      "    \"What is the decoding function in PCA?\",\n",
      "    \"What constraint does PCA apply to the columns of the decoding matrix?\",\n",
      "    \"How is the optimal code point determined in PCA?\",\n",
      "    \"What is the equation for the L2 norm in PCA?\",\n",
      "    \"How is the optimization problem of finding the optimal code point solved?\",\n",
      "    \"What are the encoder and decoder functions in PCA?\",\n",
      "    \"How is the PCA reconstruction operation defined?\",\n",
      "    \"How do we choose the encoding matrix in PCA?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The purpose of lossy compression in PCA is to store the points in a way that requires less memory but may lose some precision.\",\n",
      "    \"The points can be encoded by representing them in a lower-dimensional space.\",\n",
      "    \"The decoding function in PCA uses matrix multiplication to map the code back into the original space.\",\n",
      "    \"PCA constrains the columns of the decoding matrix to be orthogonal to each other.\",\n",
      "    \"The optimal code point in PCA is determined by minimizing the distance between the input point and its reconstruction using the L2 norm.\",\n",
      "    \"The equation for the L2 norm in PCA is ||x - g(c)||^2.\",\n",
      "    \"The optimization problem of finding the optimal code point in PCA is solved using vector calculus.\",\n",
      "    \"The encoder function in PCA is f(x) = Dx, where D is the encoding matrix.\",\n",
      "    \"The decoder function in PCA is g(c) = DD^T x, where D is the decoding matrix.\",\n",
      "    \"The PCA reconstruction operation is defined as r(x) = g(f(x)) = DD^T x.\",\n",
      "    \"The encoding matrix in PCA is chosen based on minimizing the L2 distance between inputs and reconstructions.\"\n",
      "  ]\n",
      "}\n",
      "{'questions': [{'question': 'What is the goal when minimizing the Frobenius norm of the matrix of errors?', 'category': 'step'}, {'question': 'What is the formulation of the optimization problem when l=1?', 'category': 'step'}, {'question': 'How can we simplif...a-problems', 'answer': 'The formulation of the optimization problem when l=1 is to find the vector d that minimizes the Frobenius norm of the matrix of errors, subject to the constraint that the vector d has a squared norm of 1.'}, {'answer_start': 494, 'tex\n",
      "{\n",
      "\"questions\": [\n",
      "    \"What is probability theory used for in artificial intelligence?\",\n",
      "    \"How does probability theory help in analyzing proposed AI systems?\",\n",
      "    \"What is information theory used for?\",\n",
      "    \"Why does machine learning make heavy use of probability theory?\",\n",
      "    \"What are the three possible sources of uncertainty?\"\n",
      "],\n",
      "\"answers\": [\n",
      "    \"Probability theory is used in artificial intelligence to represent uncertain statements, quantify uncertainty, and derive new uncertain statements.\",\n",
      "    \"Probability theory helps in analyzing proposed AI systems by providing laws of probability that guide reasoning and by using probability and statistics to analyze behavior.\",\n",
      "    \"Information theory is used to quantify the amount of uncertainty in a probability distribution.\",\n",
      "    \"Machine learning makes heavy use of probability theory because it deals with uncertain and stochastic quantities, which can arise from inherent stochasticity, incomplete observability, and incomplete modeling.\",\n",
      "    \"The three possible sources of uncertainty are inherent stochasticity in the system being modeled, incomplete observability, and incomplete modeling.\"\n",
      "]\n",
      "}\n",
      "{\"questions\": [\"What is the purpose of discretization in the context of the robot's prediction?\", \"Why is it more practical to use a simple but uncertain rule?\", \"How does probability theory help in representing and reasoning about uncertainty in AI applications?\", \"What is the difference between frequentist probability and Bayesian probability?\", \"How is probability theory related to logic?\", \"What is a random variable?\"], \"answers\": [\"The purpose of discretization in the context of the robot's prediction is to make the robot uncertain about the precise position of objects.\", \"It is more practical to use a simple but uncertain rule because it is cheaper to develop and broadly useful compared to a complex but certain rule.\", \"Probability theory helps in representing and reasoning about uncertainty in AI applications by providing tools to quantify degrees of belief and compute probabilities based on evidence.\", \"Frequentist probability is related to the rates at which events occur, while Bayesian probability represents qualitative levels of certainty.\", \"Probability theory can be seen as the extension of logic to deal with uncertainty by providing formal rules for determining the likelihood of propositions being true.\", \"A random variable is a variable that can take on different values randomly.\"] }\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is a random variable?\",\n",
      "    \"What is the difference between a discrete random variable and a continuous random variable?\",\n",
      "    \"How can a probability distribution be described?\",\n",
      "    \"What is a probability mass function?\",\n",
      "    \"How is the probability of a specific state denoted in a probability mass function?\",\n",
      "    \"What is a joint probability distribution?\",\n",
      "    \"What properties must a probability mass function satisfy?\",\n",
      "    \"How can a uniform distribution be placed on a discrete random variable?\",\n",
      "    \"What is a probability density function?\",\n",
      "    \"What properties must a probability density function satisfy?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"A random variable is a description of the possible states that can occur.\",\n",
      "    \"A discrete random variable has a finite or countably infinite number of states, while a continuous random variable is associated with a real value.\",\n",
      "    \"A probability distribution describes how likely a random variable or set of random variables is to take on each of its possible states.\",\n",
      "    \"A probability mass function (PMF) is a way to describe a probability distribution over discrete variables.\",\n",
      "    \"The probability of a specific state in a probability mass function is denoted as P(x).\",\n",
      "    \"A joint probability distribution is a probability distribution over multiple variables.\",\n",
      "    \"A probability mass function must satisfy the following properties: the domain must be the set of all possible states, the probability of any state must be greater than or equal to 0, the sum of all probabilities must be 1, and no state can have a greater chance of occurring than 1.\",\n",
      "    \"A uniform distribution can be placed on a discrete random variable by setting the probability mass function to 1/k for all states.\",\n",
      "    \"A probability density function (PDF) is used to describe a probability distribution for continuous random variables.\",\n",
      "    \"A probability density function must satisfy the following properties: the domain must be the set of all possible states, the density at any state must be greater than or equal to 0, and the integral of the density function over all states must be 1.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What does the integral of the density function represent?\",\n",
      "    \"How can we ensure that there is no probability mass outside an interval?\",\n",
      "    \"What is the formula for calculating marginal probability for discrete variables?\",\n",
      "    \"How is the conditional probability computed?\",\n",
      "    \"What is the chain rule of conditional probabilities?\",\n",
      "    \"When are two random variables considered independent?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The integral of the density function represents the actual probability mass of a set of points.\",\n",
      "    \"To ensure that there is no probability mass outside an interval, we set the density function to 0 for all values outside the interval.\",\n",
      "    \"The formula for calculating marginal probability for discrete variables is P(x) = ∑ P(x, y) for all values of y.\",\n",
      "    \"The conditional probability is computed using the formula P(y = y | x = x) = P(x = x, y = y) / P(x = x).\",\n",
      "    \"The chain rule of conditional probabilities states that any joint probability distribution can be decomposed into conditional distributions over one variable. The formula is P(x1, ..., xn) = P(x1) * P(x2 | x1) * ... * P(xn | x1, ..., xn-1).\",\n",
      "    \"Two random variables x and y are considered independent if their probability distribution can be expressed as the product of two factors, one involving only x and one involving only y.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the expectation or expected value of a function f(x) with respect to a probability distribution P(x)?\",\n",
      "    \"How is the expectation computed for discrete variables?\",\n",
      "    \"How is the expectation computed for continuous variables?\",\n",
      "    \"What does the variance measure?\",\n",
      "    \"What does the covariance measure?\",\n",
      "    \"What does a high absolute value of covariance mean?\",\n",
      "    \"What is the relationship between covariance and dependence?\",\n",
      "    \"Give an example of two dependent variables with zero covariance.\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The expectation or expected value of a function f(x) with respect to a probability distribution P(x) is the average or mean value that f takes on when x is drawn from the distribution.\",\n",
      "    \"For discrete variables, the expectation is computed by summing the product of each value of x and its probability: Ex[f(x)] = summation(x * P(x) * f(x)).\",\n",
      "    \"For continuous variables, the expectation is computed by integrating the product of each value of x and its probability: Ex[f(x)] = integral(x * p(x) * f(x) dx).\",\n",
      "    \"The variance gives a measure of how much the values of a function of a random variable x vary as we sample different values of x from its probability distribution.\",\n",
      "    \"The covariance gives a measure of how much two values are linearly related to each other, as well as the scale of these variables.\",\n",
      "    \"A high absolute value of covariance means that the values change very much and are both far from their respective means at the same time.\",\n",
      "    \"Covariance and dependence are related but distinct concepts. Two variables that are independent have zero covariance, but two variables with non-zero covariance can still be dependent. Independence is a stronger requirement than zero covariance.\",\n",
      "    \"For example, if we sample a real number x uniformly from the interval [1, 1], and then sample a random variable y such that y = s*x, where s is chosen to be 1 or -1 with equal probability, x and y would be dependent but have zero covariance.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the Bernoulli Distribution?\",\n",
      "    \"How is the variance of a random variable defined?\",\n",
      "    \"What is the multinoulli distribution?\",\n",
      "    \"Why are the Bernoulli and multinoulli distributions sufficient to describe any distribution over their domain?\",\n",
      "    \"What is the Gaussian distribution?\",\n",
      "    \"What parameters control the Gaussian distribution?\",\n",
      "    \"Why is the normal distribution a good default choice when there is no prior knowledge about the form of a distribution over real numbers?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The Bernoulli Distribution is a distribution over a single binary random variable that gives the probability of the random variable being equal to 1. It is controlled by a parameter φ.\",\n",
      "    \"The variance of a random variable is defined as the diagonal elements of the covariance matrix. For a variable xi, Cov(xi, xi) gives the variance.\",\n",
      "    \"The multinoulli distribution is a distribution over a single discrete variable with k different states, where k is finite.\",\n",
      "    \"The Bernoulli and multinoulli distributions are sufficient to describe any distribution over their domain because they are able to model discrete variables for which it is feasible to enumerate all states.\",\n",
      "    \"The Gaussian distribution, also known as the normal distribution, is the most commonly used distribution over real numbers.\",\n",
      "    \"The Gaussian distribution is controlled by two parameters: mean (µ) and standard deviation (σ).\",\n",
      "    \"The normal distribution is a good default choice when there is no prior knowledge about the form of a distribution over real numbers because it exhibits a classic 'bell curve' shape and is a sensible choice for many applications.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "\"questions\": [\n",
      "\"What does the central limit theorem show?\",\n",
      "\"What is the significance of the normal distribution?\",\n",
      "\"What is the multivariate normal distribution?\",\n",
      "\"What is the exponential distribution used for?\",\n",
      "\"What is the Laplace distribution used for?\",\n",
      "\"What is the Dirac distribution used for?\",\n",
      "\"What is an empirical distribution?\",\n",
      "\"Why is the Dirac delta function used in empirical distributions?\",\n",
      "\"What is the difference between the Dirac delta distribution and the empirical distribution?\"\n",
      "],\n",
      "\"answers\": [\n",
      "\"The central limit theorem shows that the sum of many independent random variables is approximately normally distributed.\",\n",
      "\"The normal distribution can model many complicated systems that can be decomposed into parts with more structured behavior.\",\n",
      "\"The multivariate normal distribution is a generalization of the normal distribution to multiple dimensions.\",\n",
      "\"The exponential distribution is used to have a sharp point at x=0 in a probability distribution.\",\n",
      "\"The Laplace distribution is used to place a sharp peak of probability mass at an arbitrary point.\",\n",
      "\"The Dirac distribution is used to specify that all of the mass in a probability distribution clusters around a single point.\",\n",
      "\"An empirical distribution is a distribution that is based on a given dataset or collection of samples.\",\n",
      "\"The Dirac delta function is used in empirical distributions over continuous variables to define in infinitely narrow and high peak of probability mass at a specific point.\",\n",
      "\"The Dirac delta distribution is a mathematical object defined using the Dirac delta function, while the empirical distribution is a distribution based on a dataset.\"\n",
      "]\n",
      "}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the empirical distribution?\",\n",
      "    \"What is a mixture distribution?\",\n",
      "    \"What is a latent variable?\",\n",
      "    \"What is a Gaussian mixture model?\",\n",
      "    \"What is the logistic sigmoid function used for?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The empirical distribution is formed from a dataset of training examples and specifies the distribution that we sample from when training a model on this dataset.\",\n",
      "    \"A mixture distribution combines several component distributions and the choice of which component distribution generates the sample is determined by sampling a component identity from a multinoulli distribution.\",\n",
      "    \"A latent variable is a random variable that cannot be observed directly. In the context of the mixture model, the component identity variable is an example of a latent variable.\",\n",
      "    \"A Gaussian mixture model is a type of mixture model where the components are Gaussians. It consists of separately parametrized means and covariances for each component, as well as prior probabilities given to each component.\",\n",
      "    \"The logistic sigmoid function is commonly used to produce the parameter of a Bernoulli distribution in deep learning models. It is defined as the function 1 / (1 + exp(-x)).\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "\"questions\": [\n",
      "\"What is the sigmoid function?\",\n",
      "\"What is the range of values for the phi parameter in the sigmoid function?\",\n",
      "\"What is the softplus function used for?\",\n",
      "\"What are the properties of the softplus function?\",\n",
      "\"What is Bayes' rule used for?\"\n",
      "],\n",
      "\"answers\": [\n",
      "\"The sigmoid function is a mathematical function that maps any real number to a value between 0 and 1.\",\n",
      "\"The range of values for the phi parameter in the sigmoid function is (0,1).\",\n",
      "\"The softplus function is used for producing the beta or sigma parameter of a normal distribution. It also arises commonly when manipulating expressions involving sigmoids.\",\n",
      "\"Some properties of the softplus function are: sigma(x) = exp(x)/(1 + exp(x)), x(σ(x)) = (1 - σ(x))σ(x), σ(-x) = 1 - σ(x), log(1 + exp(x)) = σ(x), log(exp(x) - 1) = x\",\n",
      "\"Bayes' rule is used to compute the conditional probability of an event based on prior knowledge of related events. It allows us to update our beliefs or knowledge about an event given new evidence.\"\n",
      "]\n",
      "}\n",
      "{'questions': [{'question': 'What is the purpose of measure theory?', 'answer': 'Provide a characterization of the set of sets that we can compute the probability of without encountering paradoxes.'}, {'question': 'What does it mean for a set to hav...h 'almost everywhere'?'}, {'question': 'What does information theory quantify?', 'answer': 'How much information is present in a signal.'}], 'answers': [{'answer': 'Provide a characterization of the set of sets that we can compute the probability of without encountering paradoxes.'}, {'answer': 'A set of measure zero occupies no volume in the space being measured.'}, {'answer': 'The exceptions occupy a negligible amount of space and can be safely ignored for many applications.'}, {'answer': 'How much information is present in a signal.'}]}\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the basic intuition behind information theory?\",\n",
      "    \"How is self-information defined in information theory?\",\n",
      "    \"What is the Shannon entropy used to quantify?\",\n",
      "    \"How is the KL divergence used to measure the difference between two distributions?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The basic intuition behind information theory is that learning about an unlikely event is more informative than learning about a likely event.\",\n",
      "    \"Self-information of an event x is defined as -log(P(x)) in nats.\",\n",
      "    \"The Shannon entropy quantifies the amount of uncertainty in an entire probability distribution. It represents the expected amount of information in an event drawn from that distribution.\",\n",
      "    \"The KL divergence measures the extra amount of information needed to send a message containing symbols drawn from distribution P when using a code designed for distribution Q.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "\"questions\": [\n",
      "  \"What is the KL divergence and how is it conceptualized?\",\n",
      "  \"Why is the KL divergence not a true distance measure?\",\n",
      "  \"What is Shannon entropy? How does it relate to the probability distribution?\",\n",
      "  \"What is cross-entropy and how is it related to KL divergence?\",\n",
      "  \"What is the significance of the asymmetry in KL divergence?\",\n",
      "  \"How can the KL divergence be minimized?\",\n",
      "  \"What is the effect of minimizing DKL(pq)?\",\n",
      "  \"What is the effect of minimizing DKL(qp)?\"\n",
      "],\n",
      "\"answers\": [\n",
      "  \"The KL divergence measures the difference between two probability distributions and is often conceptualized as measuring some sort of distance between them.\",\n",
      "  \"The KL divergence is not a true distance measure because it is not symmetric. DKL(PQ) is not equal to DKL(QP).\",\n",
      "  \"Shannon entropy is a measure of the uncertainty or randomness in a probability distribution. When the distribution is close to deterministic, the entropy is low, and when it is close to uniform, the entropy is high.\",\n",
      "  \"Cross-entropy is similar to KL divergence but lacks a certain term. Minimizing the cross-entropy is equivalent to minimizing the KL divergence.\",\n",
      "  \"The asymmetry in KL divergence means that there are important consequences in choosing the direction to use. It depends on whether the application requires a high probability where the true distribution has high probability or low probability where the true distribution has low probability.\",\n",
      "  \"The KL divergence can be minimized by choosing the distribution q that is closest to the distribution p.\",\n",
      "  \"Minimizing DKL(pq) results in selecting a q that has high probability where p has high probability, potentially blurring multiple modes together. Minimizing DKL(qp) results in selecting a q that has low probability where p has low probability, potentially emphasizing a single mode.\",\n",
      "  \"Minimizing DKL(qp) can result in choosing a single mode to avoid putting probability mass in low-probability areas between modes of p.\"\n",
      "  ]\n",
      "}\n",
      "{\"questions\": [\"What is a structured probabilistic model?\", \"What are the two main kinds of structured probabilistic models?\", \"What is the difference between directed and undirected models?\", \"What is a clique in an undirected model?\", \"How is the probability of a configuration of random variables calculated in an undirected model?\"], \"answers\": [\"A structured probabilistic model is a way of representing a probability distribution by splitting it into several factors that are multiplied together.\", \"The two main kinds of structured probabilistic models are directed and undirected models.\", \"Directed models use graphs with directed edges and represent factorizations into conditional probability distributions. Undirected models use graphs with undirected edges and represent factorizations into a set of functions.\", \"In an undirected model, a clique is a set of nodes that are all connected to each other.\", \"In an undirected model, the probability of a configuration of random variables is calculated by taking the product of all the factors associated with the cliques and dividing it by a normalizing constant.\"]\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What are factorizations used for in probability distributions?\",\n",
      "    \"Are directed and undirected graphical models mutually exclusive?\",\n",
      "    \"Is being directed or undirected a property of a probability distribution?\",\n",
      "    \"What does the undirected graphical model in Figure 3.8 represent?\",\n",
      "    \"How are a and c related in the undirected graphical model of Figure 3.8?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Factorizations are used as a language to describe probability distributions.\",\n",
      "    \"No, directed and undirected graphical models are not mutually exclusive.\",\n",
      "    \"No, being directed or undirected is a property of a particular description of a probability distribution.\",\n",
      "    \"The undirected graphical model in Figure 3.8 represents a probability distribution that can be factored as (a b c d e) = 1/Z * φ(1)(a b c) * φ(2)(b d) * φ(3)(c e).\",\n",
      "    \"In the undirected graphical model of Figure 3.8, a and c interact directly.\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from openai import OpenAI\n",
    "from math import ceil\n",
    "\n",
    "\n",
    "api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "client = OpenAI(api_key=api_key)\n",
    "responses = []\n",
    "\n",
    "for t in transcriptions:\n",
    "    context = t\n",
    "    i = 0\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(ceil(len(t)/4097)):\n",
    "        chunks.append(t[i*4097:i*4097+4097])\n",
    "\n",
    "    for c in chunks:\n",
    "        context = c\n",
    "        question = \"The text above is the result of the transcription of slides in the PDF file format. Remove chapter numbers, slides numbers and fix the text where there are line-breaks or incomplete formulas . Once you do that generate meaningful questions on the text and the respective answers. Plese reply in the JSON format {'questions':<questions generated>,'answers':<answers generated>}. DO NOT write anything else than the requested JSON and remember to write the full elaborated content and not just one part.\"\n",
    "        #question = \"The text above is the result of the transcription of slides in the PDF file format. Remove chapter names and slides numbers and rephrase the sentences. Once you do that generate 3 meaningful questions based on the new text and the respective answers. As for the reply, follow the following template FOR EACH pair of question and the respective answer: '[INST] <question> [/INST] <answer>'  and so on, let's call this template a 'block'.  NEVER use newlines other than separating blocks and NEVER write anything that is not formatted as the proposed template. DO NOT write anything else than the requested blocks and make sure everything is formatted correctly.\"\n",
    "        # response = openai.Completion.create(\n",
    "        # engine=\"gpt-3.5-turbo\",\n",
    "        prompt=f\"\\nContext: {context}\\nQuestion: {question}\"\n",
    "        # )\n",
    "        # answer = response.choices[0].text.strip()\n",
    "        # print(answer)\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        print(response.choices[0].message.content)\n",
    "        responses.append(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "231"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] What are recurrent neural networks? [/INST] Recurrent neural networks (RNNs) are a family of neural networks for processing sequential data. </s>\n",
      "<s> [INST] Why are recurrent networks specialized for processing sequences? [/INST] Recurrent networks are specialized for processing sequences because they can scale to much longer sequences than networks without sequence-based specialization and can also handle sequences of variable length. </s>\n",
      "<s> [INST] What is the advantage of parameter sharing in recurrent neural networks? [/INST] The advantage of parameter sharing in recurrent neural networks is that it allows the model to generalize across examples of different forms (such as sequence lengths) and share statistical strength across different sequence lengths and positions in time. </s>\n",
      "<s> [INST] Give an example of how recurrent neural networks can recognize information at different positions within a sequence. [/INST] For example, if the task is to extract the year in which the narrator went to Nepal from two different sentences, 'I went to Nepal in 2009' and 'In 2009, I went to Nepal,' a recurrent neural network can recognize the year 2009 as the relevant information, regardless of its position in the sentence. </s>\n",
      "<s> [INST] How do recurrent neural networks differ from fully connected feedforward networks? [/INST] Unlike fully connected feedforward networks, recurrent neural networks share the same weights across several time steps. </s>\n",
      "<s> [INST] What is the concept of parameter sharing in recurrent neural networks? [/INST] Parameter sharing in recurrent neural networks refers to the use of the same weights across multiple time steps or positions in a sequence. </s>\n",
      "<s> [INST] What is the difference between the parameter sharing in recurrent networks and convolutional networks? [/INST] Parameter sharing in recurrent networks is different from convolutional networks because in recurrent networks, each member of the output is a function of the previous members of the output, while in convolutional networks, each member of the output is a function of a small number of neighboring members of the input. </s>\n",
      "<s> [INST] How do recurrent neural networks handle sequence length and time in practice? [/INST] In practice, recurrent neural networks usually operate on minibatches of sequences, each with a different sequence length. The time step index used in RNNs does not necessarily refer to the passage of time in the real world but can also refer to the position in the sequence. RNNs can also be applied to data involving time while having connections that go backwards in time as long as the entire sequence is observed before it is provided to the network. </s>\n",
      "<s> [INST] What are some examples of important design patterns for recurrent neural networks? [/INST] Some examples of important design patterns for recurrent neural networks include recurrent networks that produce an output at each time step and have recurrent connections between hidden units, recurrent networks that produce an output at each time step and have recurrent connections only from the output at one time step to the hidden units at the next time step, and recurrent networks with recurrent connections between hidden units that read an entire sequence and then produce a single output. </s>\n",
      "<s> [INST] What is the universal property of the recurrent neural network? [/INST] The recurrent neural network is universal in the sense that any function computable by a Turing machine can be computed by such a recurrent network of a finite size. The output can be read from the RNN after a number of time steps that is asymptotically linear in the number of time steps used by the Turing machine and asymptotically linear in the length of the input. </s>\n",
      "<s> [INST] What activation function is assumed for the hidden units? [/INST] The hyperbolic tangent activation function is assumed for the hidden units. </s>\n",
      "<s> [INST] How is the output represented? [/INST] The output is represented as the unnormalized log probabilities of each possible value of the discrete variable. </s>\n",
      "<s> [INST] What is the purpose of the softmax operation in the forward propagation? [/INST] The softmax operation is applied as a post-processing step to obtain a vector of normalized probabilities over the output. Its purpose is to convert the unnormalized log probabilities into a distribution of probabilities that sum up to 1. </s>\n",
      "<s> [INST] What is one way to mitigate the problem of the network seeing different inputs during training and testing? [/INST] One way to mitigate the problem is to train with both teacher-forced inputs and with free-running inputs. </s>\n",
      "<s> [INST] How can the network learn to take into account input conditions not seen during training? [/INST] The network can learn to take into account input conditions not seen during training by predicting the correct target a number of steps in the future through the unfolded recurrent output-to-input paths. </s>\n",
      "<s> [INST] What is the BPTT algorithm used for? [/INST] The BPTT algorithm is used to compute gradients through a recurrent neural network. </s>\n",
      "<s> [INST] How do you compute the gradient recursively in a computational graph? [/INST] The gradient in a computational graph is computed recursively by starting with the nodes immediately preceding the final loss and working backwards, computing the gradient at each node based on the gradient computed at nodes that follow it in the graph. </s>\n",
      "<s> [INST] How do you compute the gradient for the final loss in the computational graph? [/INST] The gradient for the final loss is computed using the equations provided in the text. It involves taking the derivative of the loss with respect to the outputs at each time step, assuming the outputs are used as the argument to the softmax function. </s>\n",
      "<s> [INST] How do you back-propagate gradients through time? [/INST] Gradients are back-propagated through time by iterating backwards in time, starting from the final time step and going down to the first time step. The gradient at each time step depends on the gradients at the next time step and the current output and hidden states. </s>\n",
      "<s> [INST] How do you compute the gradients on the parameter nodes? [/INST] The gradients on the parameter nodes are obtained by first computing the gradients on the internal nodes of the computational graph. Then, taking into account that the parameters are shared across many time steps, the contribution of each edge in the computational graph to the gradient is computed using the bprop method. </s>\n",
      "<s> [INST] What notation is used to denote the contribution of the weights at time step t to the gradient? [/INST] The notation ∇ W ( )t is used to denote the contribution of the weights at time step t to the gradient. </s>\n",
      "<s> [INST] What is the gradient on the remaining parameters given by? [/INST] The gradient on the remaining parameters is given by cL = ∇ bL = ∇ V L = ∇ W L = ∇ UL = ∇ t  i   diag i   diag t  ∂o( )t ∂c  ∂h( )t ∂b( )t  t   t   ∇o( )t L = ∇ o( )t L t  2 diag h( )t - 1    ∇o( )t L) h( )t  ( t  t  ∇h ( )t L = i = V o( )t ∂L ∂o( )t i  ∇ ∂L i  ∇W ( )t h( )t ∂h( )t i 1  - h( )t  2   ∇h( )t L) h( ( 1) t -  ∂L i  ∇U( )t h( )t ∂h( )t i 1  - h( )t  2   ∇h( )t L) x( )t  ∇h( )t L </s>\n",
      "<s> [INST] Why is the gradient with respect to x( )t not computed for training? [/INST] The gradient with respect to x( )t is not computed for training because it does not have any parameters as ancestors in the computational graph defining the loss. </s>\n",
      "<s> [INST] What are the challenges faced when optimizing the parameters of recurrent networks? [/INST] The challenges faced when optimizing the parameters of recurrent networks are due to the parameter sharing used in these networks. It relies on the assumption that the same parameters can be used for different time steps, which may not always hold true. </s>\n",
      "<s> [INST] What is the assumption made in recurrent networks about the relationship between time steps? [/INST] The assumption made in recurrent networks is that the conditional probability distribution over the variables at time t+1 given the variables at time t is stationary, meaning that the relationship between the previous time step and the next time step does not depend on t. </s>\n",
      "<s> [INST] What are some methods to determine the length of a sequence in an RNN? [/INST] The length of a sequence in an RNN can be determined by adding a special symbol corresponding to the end of a sequence, introducing an extra Bernoulli output, or by predicting the integer τ itself. </s>\n",
      "<s> [INST] What is the purpose of adding a special symbol corresponding to the end of a sequence in RNNs? [/INST] The purpose of adding a special symbol corresponding to the end of a sequence in RNNs is to indicate when the sampling process should stop. It helps determine the length of the sequence. </s>\n",
      "<s> [INST] What is the benefit of using an extra Bernoulli output in the RNN model to determine whether to continue or halt generation? [/INST] Adding an extra Bernoulli output in the RNN model allows for a more general approach to determine whether to continue or halt generation. This approach can be applied to any RNN, not just RNNs that output a sequence of symbols. </s>\n",
      "<s> [INST] How can the sequence length be predicted in an RNN model? [/INST] The sequence length can be predicted in an RNN model by adding an extra output that predicts the integer τ itself. The model can sample a value of τ and then sample τ steps worth of data. </s>\n",
      "<s> [INST] What is the decomposition used in modeling sequences with RNNs? [/INST] The decomposition used in modeling sequences with RNNs is given by the equation P(x(1), ..., x( )τ) = P(τ) * P(x(1), ..., x( )τ | τ). </s>\n",
      "<s> [INST] What is the difference between the joint distribution and the conditional distribution in RNNs? [/INST] The joint distribution in RNNs represents a distribution over a sequence of random variables y. The conditional distribution represents a distribution over y given inputs x. </s>\n",
      "<s> [INST] How can RNNs be used to represent a conditional distribution? [/INST] RNNs can be used to represent a conditional distribution by extending the graphical model view to not only represent a joint distribution over the y variables but also a conditional distribution over y given x. </s>\n",
      "<s> [INST] What is the main operation that needs to be performed to draw samples from an RNN model? [/INST] The main operation that needs to be performed to draw samples from an RNN model is to sample from the conditional distribution at each time step. </s>\n",
      "<s> [INST] What is the purpose of bidirectional recurrent neural networks? [/INST] The purpose of bidirectional recurrent neural networks (RNNs) is to capture information from both the past and the future in order to make predictions or generate output sequences. </s>\n",
      "<s> [INST] What is an example of a task where bidirectional RNNs are useful? [/INST] An example of a task where bidirectional RNNs are useful is speech recognition, where the interpretation of the current sound as a phoneme may depend on the next few phonemes or even the next few words. </s>\n",
      "<s> [INST] How do bidirectional RNNs work? [/INST] Bidirectional RNNs combine two RNNs: one that moves forward through time from the start of the sequence, and another that moves backward through time from the end of the sequence. This allows the output units to compute representations that depend on both the past and the future. </s>\n",
      "<s> [INST] How can bidirectional RNNs be extended to 2-dimensional input? [/INST] Bidirectional RNNs can be extended to 2-dimensional input, such as images, by having RNNs moving in all four directions: up, down, left, and right. Each RNN would compute a representation at each point of the 2-D grid. </s>\n",
      "<s> [INST] What advantage do RNNs have over convolutional networks when applied to images? [/INST] RNNs applied to images have the advantage of allowing long-range lateral interactions between features in the same feature map. This means that RNNs can capture both local and long-range dependencies in the image. </s>\n",
      "<s> [INST] What is the purpose of Encoder-Decoder sequence-to-sequence architectures? [/INST] The purpose of Encoder-Decoder sequence-to-sequence architectures is to map an input sequence to an output sequence of possibly different lengths. </s>\n",
      "<s> [INST] How does an RNN map an input sequence to a fixed-size vector? [/INST] An RNN can map an input sequence to a fixed-size vector by using an encoder RNN to process the input sequence and emitting the context vector, which represents a semantic summary of the input sequence. </s>\n",
      "<s> [INST] What is the context in the context of RNNs? [/INST] In the context of RNNs, the context refers to the input to the RNN, which is the sequence to be mapped to an output sequence. </s>\n",
      "<s> [INST] What are some applications that require mapping input sequences to output sequences of different lengths? [/INST] Applications such as speech recognition, machine translation, and question answering require mapping input sequences to output sequences of different lengths. </s>\n",
      "<s> [INST] What is the encoder-decoder or sequence-to-sequence architecture? [/INST] The encoder-decoder or sequence-to-sequence architecture consists of an encoder RNN that reads the input sequence and emits a context vector, and a decoder RNN that generates the output sequence conditioned on the context vector. </s>\n",
      "<s> [INST] In the sequence-to-sequence architecture, how are the encoder and decoder trained? [/INST] In the sequence-to-sequence architecture, the encoder and decoder RNNs are trained jointly to maximize the average of log probability of the output sequences given the input sequences in the training set. </s>\n",
      "<s> [INST] What is the limitation of the encoder-decoder architecture when the context dimension is too small? [/INST] A limitation of the encoder-decoder architecture occurs when the context vector output by the encoder RNN has a dimension that is too small to properly summarize a long sequence. </s>\n",
      "<s> [INST] What is the purpose of introducing depth in each operation of an RNN? [/INST] The purpose of introducing depth in each operation of an RNN is to perform the required mappings more effectively. </s>\n",
      "<s> [INST] What does the experimental evidence suggest about adding depth to RNNs? [/INST] The experimental evidence suggests that adding depth to RNNs is beneficial for improving their performance. </s>\n",
      "<s> [INST] What did Pascanu et al. propose regarding the three blocks of operations in RNNs? [/INST] Pascanu et al. proposed to have a separate MLP (possibly deep) for each of the three blocks of operations in RNNs. </s>\n",
      "<s> [INST] How can the path-lengthening effect of deep RNNs be mitigated? [/INST] The path-lengthening effect of deep RNNs can be mitigated by introducing skip connections in the hidden-to-hidden path. </s>\n",
      "<s> [INST] What is the difference between a recurrent neural network (RNN) and a recursive neural network? [/INST] A recurrent neural network (RNN) is designed for sequential data processing with a chain-like structure, while a recursive neural network is a generalization of RNNs with a tree-like structure that allows for variable-size sequences to be mapped to a fixed-size representation. </s>\n",
      "<s> [INST] Why do recurrent networks result in extremely nonlinear behavior? [/INST] Recurrent networks result in extremely nonlinear behavior due to the composition of the same function multiple times, which can lead to exponential decay or explosion of values. </s>\n",
      "<s> [INST] What does the recurrence relation h(t) = W*h(t-1) represent? [/INST] The recurrence relation h(t) = W*h(t-1) represents a simple recurrent neural network without a nonlinear activation function or input values. </s>\n",
      "<s> [INST] What happens to the eigenvalues in the recurrence relation as t increases? [/INST] As t increases, the eigenvalues in the recurrence relation are raised to the power of t, causing eigenvalues with magnitude less than one to decay to zero and eigenvalues with magnitude greater than one to explode. </s>\n",
      "<s> [INST] What problem is specific to recurrent networks? [/INST] The specific problem with recurrent networks is the vanishing and exploding gradient problem. </s>\n",
      "<s> [INST] How can very deep feedforward networks avoid the vanishing and exploding gradient problem? [/INST] Very deep feedforward networks can avoid the vanishing and exploding gradient problem by carefully choosing the scaling and variance of individual weights at each time step. </s>\n",
      "<s> [INST] What happens to the gradient of a long term interaction in a recurrent network? [/INST] The gradient of a long term interaction in a recurrent network has exponentially smaller magnitude than the gradient of a short term interaction. </s>\n",
      "<s> [INST] What is the main problem with gradient-based optimization for long-term dependencies in recurrent networks? [/INST] The main problem with gradient-based optimization for long-term dependencies in recurrent networks is that the signal about these dependencies tends to be hidden by small fluctuations arising from short-term dependencies, making it difficult to learn long-term dependencies. </s>\n",
      "<s> [INST] What approaches have been proposed to reduce the difficulty of learning long-term dependencies in recurrent networks? [/INST] Approaches that have been proposed to reduce the difficulty of learning long-term dependencies in recurrent networks include using alternative activation functions, modifying the network architecture, and introducing attention mechanisms. </s>\n",
      "<s> [INST] What is one of the main challenges in deep learning? [/INST] The problem of learning long-term dependencies remains one of the main challenges in deep learning. </s>\n",
      "<s> [INST] What is one proposed approach to avoiding the difficulty of learning recurrent weights in a network? [/INST] One proposed approach is to set the recurrent weights such that the recurrent hidden units capture the history of past inputs and learn only the output weights. </s>\n",
      "<s> [INST] What are echo state networks (ESNs) and liquid state machines (LSMs)? [/INST] Echo state networks (ESNs) and liquid state machines (LSMs) are types of reservoir computing networks. ESNs use continuous-valued hidden units, while LSMs use spiking neurons with binary outputs. </s>\n",
      "<s> [INST] How are reservoir computing networks similar to kernel machines? [/INST] Reservoir computing networks are similar to kernel machines in that they map an arbitrary length sequence into a fixed-length vector on which a linear predictor can be applied. </s>\n",
      "<s> [INST] What is the training criterion used for reservoir computing networks? [/INST] The training criterion for reservoir computing networks is often mean squared error, with linear regression from the hidden units to the output targets. </s>\n",
      "<s> [INST] How is the input and recurrent weights set in reservoir computing networks? [/INST] The input and recurrent weights in reservoir computing networks are set such that the dynamical system is near the edge of stability. </s>\n",
      "<s> [INST] What is the effect of the spectral radius in a recurrent neural network? [/INST] The spectral radius of a recurrent neural network affects the representation of rich sets of histories in the network state. Setting the eigenvalues of the Jacobian matrix near the edge of stability is important. </s>\n",
      "<s> [INST] How does back-propagation in a network with a fixed Jacobian matrix change as time propagates? [/INST] In back-propagation with a fixed Jacobian matrix, as time propagates, back-propagation starting from a gradient vector g and from g + δv diverge by δJ^n v after n steps. The deviation size becomes exponentially small when the eigenvalue of the Jacobian matrix is greater than 1, and exponentially large when it is less than 1. </s>\n",
      "<s> [INST] What happens when the eigenvalue of the Jacobian matrix is greater than 1? [/INST] When the eigenvalue of the Jacobian matrix is greater than 1, the deviation size becomes exponentially small. </s>\n",
      "<s> [INST] What happens when the eigenvalue of the Jacobian matrix is less than 1? [/INST] When the eigenvalue of the Jacobian matrix is less than 1, the deviation size becomes exponentially large. </s>\n",
      "<s> [INST] What is the purpose of gradient clipping in recurrent networks? [/INST] The purpose of gradient clipping in recurrent networks is to prevent parameters from being propelled away from steep regions and to avoid detrimental steps when the gradient explodes. </s>\n",
      "<s> [INST] How does gradient clipping prevent parameters from being propelled away from steep regions? [/INST] Gradient clipping restricts the step size so that it cannot be propelled away from steep regions near the solution, thus preventing parameters from moving too far. </s>\n",
      "<s> [INST] What are some methods for clipping the gradient in recurrent networks? [/INST] There are two methods for clipping the gradient in recurrent networks: clipping the parameter gradient from a minibatch just before the parameter update, and clipping the norm of the gradient element-wise just before the parameter update. </s>\n",
      "<s> [INST] What is the advantage of clipping the norm of the gradient with a single scaling factor? [/INST] Clipping the norm of the gradient with a single scaling factor guarantees that each step is still in the gradient direction. </s>\n",
      "<s> [INST] What happens when the gradient magnitude is above a threshold? [/INST] When the gradient magnitude is above a threshold, a random step of size v can be taken, which will typically move away from the numerically unstable configuration. </s>\n",
      "<s> [INST] What are convolutional networks? [/INST] Convolutional networks, also known as convolutional neural networks or CNNs, are a specialized kind of neural network for processing data with a known, grid-like topology. </s>\n",
      "<s> [INST] What is the mathematical operation used in convolutional networks? [/INST] The mathematical operation used in convolutional networks is convolution. </s>\n",
      "<s> [INST] What is the purpose of using convolution in a neural network? [/INST] Convolution is used in a neural network to replace general matrix multiplication in at least one of its layers. </s>\n",
      "<s> [INST] What is pooling in convolutional networks? [/INST] Pooling is an operation used in convolutional networks to reduce the spatial dimensionality of the input, typically by taking the maximum or average value within each pooling region. </s>\n",
      "<s> [INST] What are some variants of the convolution function used in practice? [/INST] There are several variants of the convolution function used in practice for neural networks. </s>\n",
      "<s> [INST] How can convolution be applied to different kinds of data? [/INST] Convolution can be applied to data with different numbers of dimensions, such as 1D grid data (e.g., time-series) or 2D grid data (e.g., images). </s>\n",
      "<s> [INST] How can convolution be made more efficient? [/INST] Convolution can be made more efficient by using techniques such as sparse convolution, fast Fourier transform (FFT), or parallelization. </s>\n",
      "<s> [INST] What role do convolutional networks play in the history of deep learning? [/INST] Convolutional networks are an example of neuroscientific principles influencing deep learning. They have been successful in practical applications and have played a major role in the history of deep learning. </s>\n",
      "<s> [INST] What does this chapter not address regarding convolutional network architecture? [/INST] This chapter does not address how to choose the architecture of a convolutional network. </s>\n",
      "<s> [INST] Why is it impractical to describe the best convolutional network architecture in print? [/INST] It is impractical to describe the best convolutional network architecture in print because research into convolutional network architectures progresses rapidly and new best architectures are announced frequently. </s>\n",
      "<s> [INST] What are the building blocks of the best convolutional network architectures? [/INST] The best convolutional network architectures consist of the building blocks described in this chapter. </s>\n",
      "<s> [INST] What are the three important ideas leveraged by convolution in machine learning systems? [/INST] The three important ideas leveraged by convolution in machine learning systems are sparse interactions, parameter sharing, and equivariant representations. </s>\n",
      "<s> [INST] What is the purpose of making the kernel smaller than the input in convolutional networks? [/INST] Making the kernel smaller than the input in convolutional networks allows for sparse interactions or sparse connectivity, reducing the number of parameters and improving memory and computational efficiency. </s>\n",
      "<s> [INST] What does parameter sharing refer to in convolutional networks? [/INST] Parameter sharing in convolutional networks refers to using the same parameter for more than one function in the model, where each element of the weight matrix is used multiple times when computing the output of a layer. </s>\n",
      "<s> [INST] What is the purpose of the function g in image processing? [/INST] The function g shifts every pixel of I one unit to the right. </s>\n",
      "<s> [INST] How does convolution represent a timeline when processing time series data? [/INST] Convolution produces a timeline that shows when different features appear in the input. </s>\n",
      "<s> [INST] What does convolution create in images? [/INST] Convolution creates a 2-D map of where certain features appear in the input. </s>\n",
      "<s> [INST] Why is parameter sharing useful in convolutional networks? [/INST] Parameter sharing is useful for detecting common features that appear throughout an image. </s>\n",
      "<s> [INST] What transformations is convolution not naturally equivariant to? [/INST] Convolution is not naturally equivariant to changes in the scale or rotation of an image. </s>\n",
      "<s> [INST] What are the three stages of a typical layer in a convolutional network? [/INST] The three stages of a typical layer in a convolutional network are: performing convolutions in parallel to produce linear activations, running each linear activation through a nonlinear activation function, and using a pooling function to modify the output. </s>\n",
      "<s> [INST] What is the purpose of the pooling function in a convolutional network? [/INST] The pooling function replaces the output at a certain location with a summary statistic of the nearby outputs. </s>\n",
      "<s> [INST] What is an advantage of using convolution over matrix multiplication for certain transformations? [/INST] Convolution is more efficient than matrix multiplication for representing certain transformations because it requires significantly fewer floating-point operations. </s>\n",
      "<s> [INST] How does the efficiency of convolution compare to matrix multiplication for edge detection? [/INST] Convolution is approximately 60,000 times more computationally efficient than matrix multiplication for edge detection. </s>\n",
      "<s> [INST] What is the purpose of convolution × in a convolutional neural network layer? [/INST] Convolution × is an efficient way of describing transformations that apply the same linear transformation of a small, local region across the entire input. </s>\n",
      "<s> [INST] What is the difference between complex layer terminology and simple layer terminology in describing convolutional neural network layers? [/INST] Complex layer terminology views the convolutional net as a small number of relatively complex layers, while simple layer terminology views it as a larger number of simple layers. </s>\n",
      "<s> [INST] What are some popular pooling functions? [/INST] Some popular pooling functions include the maximum output within a rectangular neighborhood, the average of a rectangular neighborhood, the L2 norm of a rectangular neighborhood, or a weighted average based on the distance from the central pixel. </s>\n",
      "<s> [INST] What does it mean for a representation to be invariant to translation? [/INST] Invariance to translation means that the values of most of the pooled outputs do not change when the input is translated by a small amount. </s>\n",
      "<s> [INST] How does pooling improve the statistical efficiency of a network? [/INST] Pooling adds an infinitely strong prior that the function the layer learns must be invariant to small translations, which can greatly improve the statistical efficiency of the network. </s>\n",
      "<s> [INST] How does pooling reduce the computational efficiency of a network? [/INST] Pooling can reduce the computational efficiency of a network by reducing the number of inputs the next layer needs to process. </s>\n",
      "<s> [INST] Why is pooling essential for handling inputs of varying size in many tasks? [/INST] Pooling is essential for handling inputs of varying size because it allows the input to the classification layer to have a fixed size. </s>\n",
      "<s> [INST] What is the difference between a weak prior and a strong prior? [/INST] A weak prior is a prior distribution with high entropy, allowing the data to move the parameters freely. A strong prior has low entropy and plays a more active role in determining the parameter values. </s>\n",
      "<s> [INST] What does it mean for a prior to have high entropy? [/INST] A prior with high entropy means that the probability density is spread out, allowing for more variability in the parameters. </s>\n",
      "<s> [INST] What does it mean for a prior to have low entropy? [/INST] A prior with low entropy means that the probability density is concentrated, allowing for less variability in the parameters. </s>\n",
      "<s> [INST] What is an infinitely strong prior? [/INST] An infinitely strong prior places zero probability on certain parameters, making those parameter values completely forbidden regardless of the data support. </s>\n",
      "<s> [INST] How can we think of a convolutional net in terms of priors? [/INST] A convolutional net can be thought of as having an infinitely strong prior probability distribution over its parameters, enforcing specific constraints on their values. </s>\n",
      "<s> [INST] What does it mean for a unit in a convolutional net to be equivariant to translation? [/INST] For a unit in a convolutional net to be equivariant to translation means that its function remains the same when the input is translated spatially. </s>\n",
      "<s> [INST] What is the role of pooling in a convolutional net? [/INST] Pooling in a convolutional net introduces an infinitely strong prior that each unit should be invariant to small translations, allowing for translation invariance in the output feature maps. </s>\n",
      "<s> [INST] What is the effect of using a stride greater than one in convolution? [/INST] Using a stride greater than one in convolution is mathematically equivalent to convolution with unit stride followed by downsampling. </s>\n",
      "<s> [INST] Why is using downsampling with convolution computationally wasteful? [/INST] Using downsampling with convolution is computationally wasteful because it computes many values that are then discarded. </s>\n",
      "<s> [INST] How can the rate of shrinking in a convolutional network be mitigated? [/INST] The rate of shrinking in a convolutional network can be mitigated by using smaller kernels. </s>\n",
      "<s> [INST] What is the purpose of zero padding in a convolutional network? [/INST] The purpose of zero padding in a convolutional network is to prevent the representation from shrinking with depth, allowing for an arbitrarily deep network. </s>\n",
      "<s> [INST] When would locally connected layers be more appropriate than convolution layers? [/INST] Locally connected layers are more appropriate than convolution layers when each feature should be a function of a small part of space, but there is no need for the same feature to occur across all of space. </s>\n",
      "<s> [INST] How can the connectivity in convolution or locally connected layers be further restricted? [/INST] The connectivity in convolution or locally connected layers can be further restricted by constraining each output channel to be a function of only a subset of the input channels. </s>\n",
      "<s> [INST] What is the advantage of using tiled convolution? [/INST] The advantage of using tiled convolution is that it offers a compromise between a convolutional layer and a locally connected layer, reducing the memory requirements while still allowing neighboring locations to have different filters. </s>\n",
      "<s> [INST] What is being compared in Figure 9.14? [/INST] Figure 9.14 compares local connections, convolution, and full connections. </s>\n",
      "<s> [INST] What is the difference between a locally connected layer and a convolutional layer? [/INST] The difference between a locally connected layer and a convolutional layer lies in how the parameters are shared. A locally connected layer has no parameter sharing, while a convolutional layer uses the same weights repeatedly across the entire input. </s>\n",
      "<s> [INST] What does a fully connected layer resemble? [/INST] A fully connected layer resembles a locally connected layer in terms of each edge having its own parameter, but it does not have the restricted connectivity of a locally connected layer. </s>\n",
      "<s> [INST] What is shown in Figure 9.15? [/INST] Figure 9.15 shows a convolutional network where the first two output channels are connected to only the first two input channels, and the second two output channels are connected to only the second two input channels. </s>\n",
      "<s> [INST] What is the difference between locally connected layers, tiled convolution, and standard convolution? [/INST] The differences between locally connected layers, tiled convolution, and standard convolution lie in how they share parameters. Locally connected layers have no sharing, tiled convolution has different kernels for different locations in the output, and standard convolution applies the same kernel everywhere. </s>\n",
      "<s> [INST] How can tiled convolution be algebraically defined? [/INST] Tiled convolution can be algebraically defined by using a 6-D tensor, where output locations cycle through a set of t different choices of kernel stacks in each direction. </s>\n",
      "<s> [INST] What are the operations necessary to implement a convolutional network? [/INST] The operations necessary to implement a convolutional network include convolution, backpropagation from output to weights, and backpropagation from output to inputs. </s>\n",
      "<s> [INST] What is the purpose of the transpose operation in a convolutional network? [/INST] The transpose operation in a convolutional network is used to reconstruct the visible units from the hidden units. </s>\n",
      "<s> [INST] What are the three operations needed to compute all the gradients in a convolutional network? [/INST] The three operations needed to compute all the gradients in a convolutional network are convolution, backpropagation from output to weights, and backpropagation from output to inputs. </s>\n",
      "<s> [INST] How is the backpropagation of error derivatives performed in a convolutional layer? [/INST] The backpropagation of error derivatives through a convolutional layer is performed by multiplying the transpose of the matrix defined by convolution. </s>\n",
      "<s> [INST] What is the role of the transpose convolution in a convolutional network? [/INST] The transpose convolution is necessary to construct convolutional versions of models like autoencoders, RBMs, and sparse coding. </s>\n",
      "<s> [INST] What is the purpose of computing the gradient with respect to V? [/INST] The purpose of computing the gradient with respect to V is to back-propagate the error farther down. </s>\n",
      "<s> [INST] How can we compute the gradient with respect to V? [/INST] The gradient with respect to V can be computed using the function (KG) i,j,k = h , , s ∂∂Vi,j,k ,(VK)J = ∑l,m s.t. smj1) += l(− × ∑n,p s.t. 1) += spk n(− × (9.12) (9.13) Kq,i,m,pGq,l,n. </s>\n",
      "<s> [INST] What is the function h used for in autoencoder networks? [/INST] In autoencoder networks, the function h is used to perform the transpose of the convolution operation in order to make the models convolutional. </s>\n",
      "<s> [INST] How can we train the autoencoder decoder? [/INST] To train the autoencoder decoder, we need to obtain the gradient with respect to K, which is given by g(HE, , s). </s>\n",
      "<s> [INST] How can we train the autoencoder encoder? [/INST] To train the autoencoder encoder, we need to obtain the gradient with respect to H, which is given by c(KE, , s). </s>\n",
      "<s> [INST] Why do we generally add a bias term in convolutional layers? [/INST] A bias term is generally added in convolutional layers to apply a nonlinearity and account for differences in image statistics at different locations. </s>\n",
      "<s> [INST] How can we share parameters among the biases in convolutional layers? [/INST] Parameters can be shared among the biases in convolutional layers by having one bias per channel of the output and sharing it across all locations within each convolution map. </s>\n",
      "<s> [INST] What is a structured output in convolutional networks? [/INST] A structured output in convolutional networks refers to a high-dimensional, structured object that is outputted by the network, such as a tensor that labels every pixel in an image or draws precise masks for individual objects. </s>\n",
      "<s> [INST] What is the purpose of using pooling layers in convolutional networks? [/INST] Pooling layers in convolutional networks are used to reduce the spatial dimensions of the network, either by using large stride pooling or emitting a lower-resolution grid of labels. Alternatively, pooling with unit stride can also be used. </s>\n",
      "<s> [INST] How can convolutional networks be used for pixel-wise labeling of images? [/INST] One strategy for pixel-wise labeling of images is to produce an initial guess of the image labels and then refine this guess using the interactions between neighboring pixels. This can be achieved by using convolutions at each stage, sharing weights between the last layers of the deep net, which forms a recurrent network. </s>\n",
      "<s> [INST] What are some methods that can be used to further process the predictions obtained from pixel-wise labeling? [/INST] Methods such as graphical models can describe the probabilistic relationships between neighboring pixels and can be used to process the predictions obtained from pixel-wise labeling. The convolutional network can also be trained to maximize an approximation of the graphical model training objective. </s>\n",
      "<s> [INST] What types of data can be used with convolutional networks? [/INST] Convolutional networks can handle data with multiple channels, where each channel represents the observation of a different quantity at some point in space or time. The data can have different dimensionalities and number of channels. </s>\n",
      "<s> [INST] Why are convolutional networks suitable for processing inputs with varying spatial extents? [/INST] Convolutional networks are suitable for processing inputs with varying spatial extents because they can handle inputs of different sizes by applying the convolution operation a different number of times, and the output scales accordingly. This is in contrast to traditional neural networks based on matrix multiplication, which cannot handle inputs with varying spatial extents. </s>\n",
      "<s> [INST] How does convolution relate to matrix multiplication? [/INST] Convolution can be viewed as matrix multiplication, where the same convolution kernel induces a different size of a doubly block circulant matrix for each size of input. </s>\n",
      "<s> [INST] What additional design steps are necessary when the network needs to produce a fixed-size output? [/INST] When the network needs to produce a fixed-size output, additional design steps are necessary. For example, inserting a pooling layer with pooling regions that scale in size proportional to the input size can be used to obtain a fixed-size output. Other design choices depend on the specific requirements of the task. </s>\n",
      "<s> [INST] What are some examples of data formats that can be used with convolutional networks? [/INST] Some examples of data formats that can be used with convolutional networks include 1-D audio waveform, 2-D audio data preprocessed with a Fourier transform, 3-D volumetric data like medical imaging scans, multi-channel skeleton animation data, color image data, and color video data. </s>\n",
      "<s> [INST] Why does convolution not make sense for inputs that have variable size due to different kinds of observations? [/INST] Convolution does not make sense for inputs that have variable size due to different kinds of observations because the same weights cannot be convolved over features that correspond to different kinds of observations. </s>\n",
      "<s> [INST] How can convolution be made more efficient? [/INST] Convolution can be made more efficient by selecting an appropriate convolution algorithm. It can also be accelerated using parallel computation and exploiting resources effectively. </s>\n",
      "<s> [INST] What is separable convolution? [/INST] Separable convolution refers to a type of convolution where a d-dimensional kernel can be expressed as the outer product of d vectors, one vector per dimension. This allows for faster computation by performing one-dimensional convolutions instead of a single d-dimensional convolution. </s>\n",
      "<s> [INST] What are some differences between convolutional networks and the mammalian vision system? [/INST] Some differences between convolutional networks and the mammalian vision system include the low resolution of the human eye except for the fovea, the use of eye movements called saccades in humans to focus on salient parts of a scene, and the incorporation of attention mechanisms into deep learning models. </s>\n",
      "<s> [INST] What is the basic strategy of detection and pooling in the visual system? [/INST] The basic strategy of the visual system is detection followed by pooling, which is repeatedly applied as we move deeper into the brain. </s>\n",
      "<s> [INST] What are grandmother cells? [/INST] Grandmother cells are neurons that respond to a specific concept and are invariant to many transformations of the input. For example, a person may have a neuron that activates when seeing an image of their grandmother, regardless of various changes in the image. </s>\n",
      "<s> [INST] What is the inferotemporal cortex and how does it relate to convolutional networks? [/INST] The inferotemporal cortex (IT) is a brain area that receives information from V1, V2, and V4, and is similar to the last layer of features in a convolutional network. It can predict firing rates and perform similarly to humans on object recognition tasks. </s>\n",
      "<s> [INST] How do convolutional networks compare to humans on object recognition tasks? [/INST] Convolutional networks can predict IT firing rates and perform similarly to humans on object recognition tasks when only observing the first 100ms of feedforward activation. </s>\n",
      "<s> [INST] What are some limitations of convolutional networks in comparison to the human visual system? [/INST] Some limitations of convolutional networks compared to the human visual system include the lack of integration with other senses, such as hearing, and the inability to understand entire scenes and process rich 3-D geometric information. Convolutional networks are purely visual and still in their infancy in terms of applying them to these problems. </s>\n",
      "<s> [INST] What are some additional functions of the human visual system beyond object recognition? [/INST] The human visual system goes beyond object recognition and is able to understand entire scenes, including many objects and relationships between objects. It also processes rich 3-D geometric information necessary for our bodies to interface with the world. </s>\n",
      "<s> [INST] How is feedback from higher levels of the brain relevant to visual processing? [/INST] Feedback from higher levels impacts brain areas like V1 and has been extensively explored in neural network models. However, it has not yet been proven to offer a significant improvement. </s>\n",
      "<s> [INST] What is the difference between feedforward IT firing rates and convolutional network features? [/INST] Feedforward IT firing rates capture similar information as convolutional network features, but the intermediate computations may be different. The brain likely uses different activation and pooling functions than those used in convolutional networks. </s>\n",
      "<s> [INST] What is the role of activation and pooling functions in the brain? [/INST] Activation and pooling functions in the brain are not well-characterized by a single linear filter response. A recent model of V1 suggests that it involves multiple quadratic filters for each neuron, indicating a continuum of behaviors ranging from simple to complex. </s>\n",
      "<s> [INST] Describe the model of V1 that involves multiple quadratic filters for each neuron. [/INST] The model of V1 involves multiple quadratic filters for each neuron, indicating a continuum of behaviors ranging from simple to complex. </s>\n",
      "<s> [INST] When did the use of back-propagation to train convolutional networks start? [/INST] The use of back-propagation to train convolutional networks started with the development of modern convolutional networks in 1989. </s>\n",
      "<s> [INST] What is the function of simple cells in a deep, nonlinear network? [/INST] Simple cells in a deep, nonlinear network play a role in analyzing the function of individual cells. Their responses are driven by a linear function, making them easier to analyze compared to other layers in the network. </s>\n",
      "<s> [INST] What is the approach used to determine the weights of neurons in a biological neural network? [/INST] To determine the weights of neurons in a biological neural network, an approach called reverse correlation is used. This involves putting an electrode in the neuron itself, displaying white noise images, and recording how each image causes the neuron to activate. A linear model can then be fitted to these responses to approximate the neuron's weights. </s>\n",
      "<s> [INST] What is the purpose of reverse correlation in studying V1 cells? [/INST] Reverse correlation is used to study V1 cells and shows that most V1 cells have weights described by Gabor functions. </s>\n",
      "<s> [INST] How is an image represented as a function of 2-D coordinates? [/INST] An image can be represented as a function of 2-D coordinates, denoted as I(x, y). </s>\n",
      "<s> [INST] What is the mathematical expression for the response of a simple cell to an image? [/INST] The response of a simple cell to an image is given by s = Σ Σ w(x, y) * I(x, y), where w(x, y) is a function of the location. </s>\n",
      "<s> [INST] What are the parameters that control the properties of the Gabor function? [/INST] The parameters that control the properties of the Gabor function are α, βx, βy, f, φ, x0, y0, and τ. </s>\n",
      "<s> [INST] How does the Gaussian factor affect the response of a simple cell? [/INST] The Gaussian factor in the Gabor function ensures that the simple cell responds only to values near the center of its receptive field. </s>\n",
      "<s> [INST] How does the cosine factor affect the response of a simple cell? [/INST] The cosine factor in the Gabor function controls how the simple cell responds to changing brightness along the x axis. </s>\n",
      "<s> [INST] What is the cartoon view of a complex cell? [/INST] A complex cell computes the L2 norm of the squared responses of two simple cells. </s>\n",
      "<s> [INST] When does a complex cell respond? [/INST] A complex cell responds to high amplitude sinusoidal waves with a specific frequency and direction, regardless of the phase offset. </s>\n",
      "<s> [INST] What is the role of a quadrature pair in complex cells? [/INST] A quadrature pair is formed by two simple cells with the same parameters except for a phase offset, and a complex cell responds to translations of the image in the direction defined by the phase offset. </s>\n",
      "<s> [INST] What is the invariance property of a complex cell? [/INST] A complex cell is invariant to small translations of the image in a specific direction or to negating the image. </s>\n",
      "<s> [INST] What is the comparison made between machine learning models and V1 in the context of this passage? [/INST] The comparison is between the features learned by machine learning models and those employed by V1, the primary visual cortex. </s>\n",
      "<s> [INST] What kind of features do most deep learning algorithms learn in their first layer? [/INST] Most deep learning algorithms learn features that detect edges or specific colors of edges in their first layer. </s>\n",
      "<s> [INST] What did Olshausen and Field's study in 1996 reveal about unsupervised learning algorithms? [/INST] Olshausen and Field's study in 1996 showed that the unsupervised learning algorithm, sparse coding, learns features with receptive fields similar to those of simple cells. </s>\n",
      "<s> [INST] Why is it difficult to determine which learning algorithm is the 'right' model of the brain based on the features it learns? [/INST] It is difficult to determine the 'right' model of the brain based on the features a learning algorithm learns because many different algorithms can learn edge detectors and the features are part of the statistical structure of natural images. </s>\n",
      "<s> [INST] What is the role of convolutional networks in the history of deep learning? [/INST] Convolutional networks have played an important role in the history of deep learning by applying insights obtained from studying the brain to machine learning applications. They were among the first successful deep models, solved important commercial applications, and still remain at the forefront of commercial applications of deep learning. </s>\n",
      "<s> [INST] What commercial applications have convolutional networks been used for? [/INST] Convolutional networks have been used for applications such as reading checks, OCR, and handwriting recognition. </s>\n",
      "<s> [INST] When did the current interest in deep learning start? [/INST] The current interest in deep learning started when Krizhevsky et al. won the ImageNet object recognition challenge in 2012. </s>\n",
      "<s> [INST] What was the impact of Krizhevsky et al.'s 2012 win in the ImageNet object recognition challenge? [/INST] Krizhevsky et al.'s 2012 win in the ImageNet object recognition challenge had a significant impact on the intensity of commercial interest in deep learning. </s>\n",
      "<s> [INST] Why were convolutional networks more computationally efficient than fully connected networks? [/INST] Convolutional networks were more computationally efficient than fully connected networks, making it easier to run multiple experiments and tune their implementation and hyperparameters. </s>\n",
      "<s> [INST] What may have been the primary barriers to the success of neural networks? [/INST] The primary barriers to the success of neural networks may have been psychological, as practitioners did not expect neural networks to work, resulting in a lack of serious effort in utilizing them. </s>\n",
      "<s> [INST] What is the significance of convolutional networks in the acceptance of neural networks? [/INST] Convolutional networks carried the torch for the rest of deep learning and paved the way to the acceptance of neural networks in general. </s>\n",
      "<s> [INST] What type of data is convolutional networks most successful in working with? [/INST] Convolutional networks are most successful in working with data that has a clear grid-structured topology, particularly on a two-dimensional, image topology. </s>\n",
      "<s> [INST] What type of neural networks are specialized for processing one-dimensional sequential data? [/INST] Recurrent neural networks are specialized for processing one-dimensional sequential data. </s>\n",
      "<s> [INST] What is the focus of Chapter 8 in the context of deep learning? [/INST] Chapter 8 focuses on optimization techniques for neural network training in deep learning. </s>\n",
      "<s> [INST] Why is neural network training considered the most difficult optimization problem in deep learning? [/INST] Neural network training is considered the most difficult optimization problem in deep learning because it often requires a significant amount of time and computational resources. </s>\n",
      "<s> [INST] How does optimization for training deep models differ from pure optimization? [/INST] Optimization for training deep models differs from pure optimization in that it involves optimizing a different cost function indirectly to improve a performance measure on a test set, rather than minimizing the cost function itself. </s>\n",
      "<s> [INST] What is the objective function defined in Equation 8.1? [/INST] The objective function defined in Equation 8.1 is an average over the training set, where L is the per-example loss function and f(x; θ) is the predicted output. </s>\n",
      "<s> [INST] What is the goal of a machine learning algorithm? [/INST] The goal of a machine learning algorithm is to reduce the expected generalization risk. </s>\n",
      "<s> [INST] What is the empirical risk minimization? [/INST] Empirical risk minimization is the process of minimizing the expected loss on the training set to convert a machine learning problem into an optimization problem. </s>\n",
      "<s> [INST] Why is empirical risk minimization prone to overfitting? [/INST] Empirical risk minimization is prone to overfitting because models with high capacity can simply memorize the training set. </s>\n",
      "<s> [INST] What are surrogate loss functions? [/INST] Surrogate loss functions are used when the desired loss function cannot be efficiently optimized. They act as proxies for the desired loss function and have certain advantages. </s>\n",
      "<s> [INST] What is the difference between optimization in general and optimization for training algorithms? [/INST] The difference between optimization in general and optimization for training algorithms is that training algorithms do not usually halt at a local minimum. Instead, they minimize a surrogate loss function but halt when a convergence criterion based on the true underlying loss function is satisfied. </s>\n",
      "<s> [INST] What separates machine learning algorithms from general optimization algorithms? [/INST] The objective function in machine learning algorithms usually decomposes as a sum over the training examples. </s>\n",
      "<s> [INST] How are optimization algorithms for machine learning typically computed? [/INST] Optimization algorithms for machine learning typically compute each update to the parameters based on an expected value of the cost function estimated using only a subset of the terms of the full cost function. </s>\n",
      "<s> [INST] What is the most commonly used property of the objective function used by optimization algorithms? [/INST] The most commonly used property of the objective function is the gradient. </s>\n",
      "<s> [INST] Why is computing the expectation exactly expensive? [/INST] Computing the expectation exactly is expensive because it requires evaluating the model on every example in the entire dataset. </s>\n",
      "<s> [INST] What is the standard error of the mean? [/INST] The standard error of the mean is the true standard deviation of the value of the samples divided by the square root of the number of samples. </s>\n",
      "<s> [INST] What is the benefit of using approximate estimates of the gradient in optimization algorithms? [/INST] Using approximate estimates of the gradient allows optimization algorithms to converge faster in terms of total computation. </s>\n",
      "<s> [INST] What is redundancy in the training set and how does it relate to gradient estimation? [/INST] Redundancy in the training set refers to the presence of examples that make similar contributions to the gradient. Estimating the gradient using a small number of samples can be computationally more efficient in such cases. </s>\n",
      "<s> [INST] What is the difference between batch gradient descent and minibatch stochastic gradient descent? [/INST] Batch gradient descent processes all of the training examples simultaneously in a large batch, while minibatch stochastic gradient descent uses only a subset of the training examples at a time. </s>\n",
      "<s> [INST] What are the common terms used to describe the size of a minibatch? [/INST] The term 'batch size' is commonly used to describe the size of a minibatch. </s>\n",
      "<s> [INST] What are stochastic or online methods in optimization algorithms for deep learning? [/INST] Stochastic or online methods in optimization algorithms for deep learning use only a single example at a time, sometimes drawn from a stream of continually created examples. </s>\n",
      "<s> [INST] What are minibatch or minibatch stochastic methods commonly referred to as in optimization algorithms for deep learning? [/INST] Minibatch or minibatch stochastic methods are commonly referred to as stochastic methods in optimization algorithms for deep learning. </s>\n",
      "<s> [INST] What is the motivation for minibatch stochastic gradient descent? [/INST] The motivation for minibatch stochastic gradient descent is that it follows the gradient of the true generalization error as long as no examples are repeated. </s>\n",
      "<s> [INST] How does shuffling the dataset affect the algorithm? [/INST] Shuffling the dataset once and storing it in shuffled fashion does not have a significant detrimental effect on the algorithm, while failing to shuffle the examples can seriously reduce the effectiveness of the algorithm. </s>\n",
      "<s> [INST] What is the advantage of using asynchronous parallel distributed approaches? [/INST] The advantage of using asynchronous parallel distributed approaches is that multiple updates can be computed in parallel over different examples, increasing efficiency. </s>\n",
      "<s> [INST] What is the equivalence between minibatch stochastic gradient descent and online learning? [/INST] The equivalence between minibatch stochastic gradient descent and online learning is that both involve drawing examples or minibatches from a stream of data, with every experience being a fair sample from the data generating distribution. </s>\n",
      "<s> [INST] How can an unbiased estimator of the exact gradient be obtained? [/INST] An unbiased estimator of the exact gradient of the generalization error can be obtained by sampling a minibatch of examples from the data generating distribution and computing the gradient of the loss with respect to the parameters for that minibatch. </s>\n",
      "<s> [INST] Why is it usually best to make multiple passes through the training set in minibatch stochastic gradient descent? [/INST] It is usually best to make multiple passes through the training set in minibatch stochastic gradient descent because although only the first epoch follows the unbiased gradient of the generalization error, the additional epochs provide enough benefit due to decreased training error to offset the harm they cause by increasing the gap between training error and test error. </s>\n",
      "<s> [INST] What are the challenges in neural network optimization? [/INST] The challenges in neural network optimization include ill-conditioning, local minima, and optimization in general. </s>\n",
      "<s> [INST] What is ill-conditioning in optimization? [/INST] Ill-conditioning in optimization refers to the problem of the Hessian matrix being poorly conditioned, which can cause optimization algorithms to get 'stuck' and increase the cost function even with small steps. </s>\n",
      "<s> [INST] How does ill-conditioning manifest in neural network training? [/INST] Ill-conditioning in neural network training can cause the gradient descent step to add a significant value to the cost function, making learning slow despite the presence of a strong gradient. </s>\n",
      "<s> [INST] What is the relationship between gradient norm and ill-conditioning? [/INST] The gradient norm may not significantly decrease throughout learning while the gHg term grows by more than an order of magnitude, indicating the presence of ill-conditioning. </s>\n",
      "<s> [INST] Can Newton's method be directly applied to neural networks? [/INST] Newton's method is not directly applicable to neural networks and requires significant modifications before it can be used for optimization. </s>\n",
      "<s> [INST] What is the issue of local minima in convex optimization? [/INST] In convex optimization, local minima guarantee global minima, but the presence of a flat region at the bottom can complicate the search for the global minimum. </s>\n",
      "<s> [INST] What is the identifiability problem in models with latent variables? [/INST] The identifiability problem in models with latent variables refers to the fact that multiple arrangements of the latent variables can lead to equivalent models. </s>\n",
      "<s> [INST] What is weight space symmetry? [/INST] Weight space symmetry refers to the property of neural networks where exchanging latent variables with each other creates equivalent models. For example, swapping the incoming and outgoing weight vectors for certain units in a neural network leads to an equivalent model. </s>\n",
      "<s> [INST] Why can non-identifiability lead to a large number of local minima in a neural network? [/INST] Non-identifiability in a neural network can result in a large number of local minima because different arrangements of the latent variables and scaling of weights can lead to equivalent models with different parameter settings. </s>\n",
      "<s> [INST] Are all local minima in a non-convex cost function problematic? [/INST] No, not all local minima in a non-convex cost function are problematic. The non-identifiability of models with latent variables can result in multiple local minima that are equivalent in terms of their cost function value. </s>\n",
      "<s> [INST] How can one test if local minima are the problem in neural network optimization? [/INST] To test if local minima are the problem in neural network optimization, one can plot the norm of the gradient over time. If the norm of the gradient does not decrease significantly, local minima are not the issue. However, in high dimensional spaces, it can be challenging to establish if local minima are the cause of optimization difficulties. </s>\n",
      "<s> [INST] What is a saddle point in high-dimensional non-convex functions? [/INST] In high-dimensional non-convex functions, a saddle point is a point with zero gradient where some points have greater cost and others have a lower cost compared to the saddle point. </s>\n",
      "<s> [INST] What is the difference between a saddle point and a local minimum? [/INST] A local minimum is a point with zero gradient where all points around it have a higher cost. A saddle point, on the other hand, has points around it with both higher and lower costs. </s>\n",
      "<s> [INST] Why are local minima more common in low-dimensional spaces compared to higher dimensional spaces? [/INST] In low-dimensional spaces, local minima are common. In higher dimensional spaces, local minima are rare and saddle points are more common. </s>\n",
      "<s> [INST] What is the intuition behind the ratio of saddle points to local minima growing exponentially with the dimensionality of a function? [/INST] The intuition behind the exponential growth of the ratio of saddle points to local minima with dimensionality is that in n-dimensional space, it becomes exponentially unlikely for all n coin tosses to result in heads, where each coin toss represents an eigenvalue's sign. </s>\n",
      "<s> [INST] What happens to the likelihood of positive eigenvalues in regions of lower cost? [/INST] In regions of lower cost, the likelihood of positive eigenvalues in the Hessian matrix becomes more likely. </s>\n",
      "<s> [INST] Do shallow autoencoders with no nonlinearities have local minima with higher cost than the global minimum? [/INST] Shallow autoencoders with no nonlinearities have global minima and saddle points, but no local minima with higher cost than the global minimum. </s>\n",
      "<s> [INST] What did Choromanska et al. show experimentally about loss functions in real neural networks? [/INST] Choromanska et al. showed experimentally that real neural networks have loss functions that contain many high-cost saddle points. </s>\n",
      "<s> [INST] What is the implication of saddle points for first-order optimization algorithms that use only gradient information? [/INST] For first-order optimization algorithms that use only gradient information, the situation is unclear. The gradient can become very small near a saddle point, but gradient descent seems to be able to escape saddle points in many cases. </s>\n",
      "<s> [INST] What is the primary obstacle revealed by the cost function visualization? [/INST] The primary obstacle revealed by the cost function visualization is a saddle point of high cost near where the parameters are initialized. </s>\n",
      "<s> [INST] Why have second-order methods not succeeded in replacing gradient descent for neural network training? [/INST] Second-order methods have not succeeded in replacing gradient descent for neural network training because of the proliferation of saddle points in high dimensional spaces. </s>\n",
      "<s> [INST] What are the different kinds of points with zero gradient? [/INST] The different kinds of points with zero gradient are minima, saddle points, and maxima. </s>\n",
      "<s> [INST] What problems do wide, flat regions with zero gradient pose for optimization algorithms? [/INST] Wide, flat regions with zero gradient pose major problems for all numerical optimization algorithms because in a general optimization problem, such a region could correspond to a high value of the objective function. </s>\n",
      "<s> [INST] What is a cliff structure in neural networks? [/INST] A cliff structure in neural networks refers to extremely steep regions that result from the multiplication of several large weights together. </s>\n",
      "<s> [INST] What happens in the gradient update step on an extremely steep cliff structure? [/INST] On an extremely steep cliff structure, the gradient update step can move the parameters extremely far, usually jumping off of the cliff structure altogether. </s>\n",
      "<s> [INST] What are some techniques for approximating the gradient of the intractable log-likelihood of a Boltzmann machine? [/INST] Contrastive divergence is a technique for approximating the gradient of the intractable log-likelihood of a Boltzmann machine. </s>\n",
      "<s> [INST] How can imperfections in the gradient estimate be accounted for in neural network optimization algorithms? [/INST] Neural network optimization algorithms can account for imperfections in the gradient estimate by using surrogate loss functions that are easier to approximate. </s>\n",
      "<s> [INST] What problems can arise when there is poor correspondence between local and global structure in optimization? [/INST] Problems can arise when the direction that results in the most improvement locally does not point toward distant regions of much lower cost. </s>\n",
      "<s> [INST] According to Goodfellow et al., what contributes to the runtime of training in neural networks? [/INST] According to Goodfellow et al., the length of the trajectory needed to arrive at the solution contributes to the runtime of training in neural networks. </s>\n",
      "<s> [INST] Why do neural networks often not arrive at a region of small gradient during training? [/INST] Neural networks often do not arrive at a region of small gradient because critical points, such as global minimum points, may not exist, and the loss function may asymptotically approach some value as the model becomes more confident. </s>\n",
      "<s> [INST] Give an example of a failure of local optimization in finding a good cost function value. [/INST] An example of a failure of local optimization in finding a good cost function value is when the local surface does not point toward the global solution, even in the absence of saddle points or local minima. </s>\n",
      "<s> [INST] What is the main cause of difficulty in the given example of failure of local optimization? [/INST] The main cause of difficulty in the given example is being initialized on the wrong side of the 'mountain' and not being able to traverse it. </s>\n",
      "<s> [INST] What are some research directions focused on improving optimization in problems with difficult global structure? [/INST] Some research directions focused on improving optimization in problems with difficult global structure involve finding good initial points instead of developing algorithms that use non-local moves. </s>\n",
      "<s> [INST] What is the basis for gradient descent and other learning algorithms for training neural networks? [/INST] Gradient descent and other learning algorithms for training neural networks are based on making small, local moves. </s>\n",
      "<s> [INST] What are some existing research directions aimed at finding good initial points for problems with difficult global structure? [/INST] Some existing research directions aim to find good initial points for problems with difficult global structure, rather than developing algorithms that use non-local moves. </s>\n",
      "<s> [INST] What are some issues that can arise in optimization algorithms for neural networks? [/INST] Some issues that can arise in optimization algorithms for neural networks include approximate objective function estimation, bias or variance in estimating the correct direction, poor conditioning or discontinuous gradients, lack of local information guidance, and paths that move away from solutions. </s>\n",
      "<s> [INST] Why is it difficult to understand which problems make neural network optimization difficult? [/INST] It is difficult to understand which problems make neural network optimization difficult because there are multiple factors that can contribute, such as flat regions, critical points, and unknown relevance of different problems. Research is actively being conducted in this area. </s>\n",
      "<s> [INST] What are the theoretical limits of optimization for neural networks? [/INST] Theoretical results show that there are limits on the performance of optimization algorithms for neural networks. Some results apply to cases where neural network units output discrete values, while others show that finding a solution for a given network size can be intractable. However, in practice, larger networks can often find solutions easily. It is also difficult to analyze whether an optimization algorithm can achieve the goal of reducing the value sufficiently for good generalization error. </s>\n",
      "<s> [INST] What is stochastic gradient descent (SGD) and why is it commonly used in machine learning? [/INST] Stochastic gradient descent (SGD) is an optimization algorithm commonly used in machine learning, including deep learning. It involves following the gradient of randomly selected minibatches downhill. SGD and its variants are popular because they can accelerate the gradient descent algorithm and are effective for large datasets. </s>\n",
      "<s> [INST] What is the purpose of Algorithm 8.1? [/INST] The purpose of Algorithm 8.1 is to show how to follow an estimate of the gradient downhill in stochastic gradient descent (SGD). </s>\n",
      "<s> [INST] Why is it necessary to gradually decrease the learning rate in SGD? [/INST] It is necessary to gradually decrease the learning rate in SGD because the SGD gradient estimator introduces noise that does not vanish even at a minimum. </s>\n",
      "<s> [INST] What is the condition for convergence of SGD? [/INST] A sufficient condition for convergence of SGD is that the sum of the learning rates approaches infinity, and the squared sum of the learning rates approaches a finite value less than infinity. </s>\n",
      "<s> [INST] How is the learning rate decayed in practice? [/INST] In practice, the learning rate is commonly decayed linearly until iteration τ using the formula: learning_rate = (1 - α) * initial_learning_rate + α * learning_rate_τ, where α = k / τ. After iteration τ, the learning rate is kept constant. </s>\n",
      "<s> [INST] How is the learning rate chosen? [/INST] The learning rate is usually chosen by monitoring learning curves that plot the objective function as a function of time. It is an iterative process and the optimal initial learning rate is typically higher than the best-performing learning rate after the first few iterations. </s>\n",
      "<s> [INST] What is the main advantage of SGD? [/INST] The main advantage of SGD is that its computation time per update does not grow with the number of training examples, allowing convergence even with very large datasets. </s>\n",
      "<s> [INST] What is measured to study the convergence rate of an optimization algorithm? [/INST] To study the convergence rate of an optimization algorithm, the excess error, which is the amount that the current cost function exceeds the minimum possible cost, is commonly measured. </s>\n",
      "<s> [INST] What is the excess error in SGD? [/INST] The excess error in SGD refers to the amount by which the current cost function exceeds the minimum possible cost. In convex problems, the excess error is O(1) after k iterations, and in strongly convex problems, it is O(1/sqrt(k)). </s>\n",
      "<s> [INST] What is the purpose of the momentum algorithm in optimization? [/INST] The purpose of the momentum algorithm in optimization is to accelerate learning, especially in the face of high curvature, small but consistent gradients, or noisy gradients. </s>\n",
      "<s> [INST] How does the momentum algorithm work? [/INST] The momentum algorithm works by accumulating an exponentially decaying moving average of past gradients and continues to move in their direction. </s>\n",
      "<s> [INST] What role does the hyperparameter alpha play in the momentum algorithm? [/INST] The hyperparameter alpha determines how quickly the contributions of previous gradients exponentially decay in the momentum algorithm. </s>\n",
      "<s> [INST] What is the update rule for the velocity in the momentum algorithm? [/INST] The update rule for the velocity in the momentum algorithm is given by: v <- alpha * v - learning_rate * gradient. </s>\n",
      "<s> [INST] What problems does momentum aim to solve? [/INST] Momentum aims to solve two problems: poor conditioning of the Hessian matrix and variance in the stochastic gradient. </s>\n",
      "<s> [INST] What is the difference between the step size in traditional gradient descent and the step size in the momentum algorithm? [/INST] The step size in traditional gradient descent is simply the norm of the gradient multiplied by the learning rate, while the step size in the momentum algorithm depends on how large and how aligned a sequence of gradients are. </s>\n",
      "<s> [INST] What is the purpose of using a lower power of velocity in turbulent drag? [/INST] Using a lower power of velocity in turbulent drag prevents the particle from moving away from its initial position forever. </s>\n",
      "<s> [INST] How does viscous drag prevent motion when the gradient does not justify moving? [/INST] Viscous drag is weak enough to allow motion if the gradient justifies moving, but strong enough to prevent motion if the gradient does not justify moving. </s>\n",
      "<s> [INST] What is the difference between Nesterov momentum and standard momentum? [/INST] The difference between Nesterov momentum and standard momentum is where the gradient is evaluated. With Nesterov momentum, the gradient is evaluated after the current velocity is applied. </s>\n",
      "<s> [INST] What is the rate of convergence of Nesterov momentum in the convex batch gradient case? [/INST] In the convex batch gradient case, Nesterov momentum improves the rate of convergence of the excess error from O(1/k) to O(1/k^2). </s>\n",
      "<s> [INST] Does Nesterov momentum improve the rate of convergence in the stochastic gradient case? [/INST] No, Nesterov momentum does not improve the rate of convergence in the stochastic gradient case. </s>\n",
      "<s> [INST] Why is the choice of initialization important in training deep learning models? [/INST] The choice of initialization is important in training deep learning models because it can determine whether the algorithm converges, how quickly learning converges, and whether it converges to a point with high or low cost. </s>\n",
      "<s> [INST] What are some of the challenges in designing improved initialization strategies for neural networks? [/INST] The challenges in designing improved initialization strategies for neural networks include the lack of understanding of neural network optimization and the need to achieve certain properties when the network is initialized. </s>\n",
      "<s> [INST] What does the initial point need to accomplish from the perspective of optimization and generalization? [/INST] The initial point needs to balance between optimization and generalization. Some initial points may be good for optimization but not for generalization. </s>\n",
      "<s> [INST] What is the only property known with certainty about the initial parameters? [/INST] The only known property is that the initial parameters need to break symmetry between different units. </s>\n",
      "<s> [INST] Why is it important for hidden units with the same activation function to have different initial parameters? [/INST] If hidden units with the same activation function are connected to the same inputs and have the same initial parameters, a deterministic learning algorithm will update them in the same way, potentially losing important information. </s>\n",
      "<s> [INST] Why is it necessary to initialize each unit to compute a different function from all the other units? [/INST] Initializing each unit to compute a different function from all the other units helps prevent the loss of input patterns during forward propagation and gradient patterns during back-propagation. </s>\n",
      "<s> [INST] What is the goal of random initialization of the parameters? [/INST] The goal of random initialization is to have each unit compute a different function from the other units. </s>\n",
      "<s> [INST] How can we ensure that each unit computes a very different function from each other unit? [/INST] Gram-Schmidt orthogonalization can be used to ensure each unit computes a very different function from each other unit, but it incurs a noticeable computational cost. Random initialization avoids assigning the same function to multiple units. </s>\n",
      "<s> [INST] How are biases and extra parameters typically initialized in a model? [/INST] Biases for each unit are set to heuristically chosen constants, and only the weights are initialized randomly. Extra parameters are also set to heuristically chosen constants. </s>\n",
      "<s> [INST] What is the common practice for initializing all the weights in a model? [/INST] All the weights in the model are initialized to values drawn randomly from a Gaussian or uniform distribution. </s>\n",
      "<s> [INST] What effect does the scale of the initial weight distribution have on the optimization procedure and network generalization? [/INST] The choice of Gaussian or uniform distribution for weight initialization does not have a significant impact. However, the scale of the initial distribution affects the optimization procedure and the network's ability to generalize. </s>\n",
      "<s> [INST] What problems can arise from using initial weights that are too large? [/INST] Larger initial weights help break symmetry and avoid redundant units, but they may also result in exploding values during forward or back-propagation. Additionally, extreme values from large weights can cause the activation function to saturate, leading to a complete loss of gradient. </s>\n",
      "<s> [INST] What is the issue with scaling rules that set all initial weights to have the same standard deviation? [/INST] The issue with scaling rules that set all initial weights to have the same standard deviation is that every individual weight becomes extremely small when the layers become large. </s>\n",
      "<s> [INST] What is sparse initialization and how does it help? [/INST] Sparse initialization is an alternative initialization scheme where each unit is initialized with exactly k non-zero weights. It helps achieve more diversity among the units at initialization time without making the magnitude of individual weight elements shrink with the number of inputs. </s>\n",
      "<s> [INST] What is a good rule of thumb for choosing the initial scales of weights? [/INST] A good rule of thumb for choosing the initial scales of weights is to look at the range or standard deviation of activations or gradients on a single minibatch of data. If the weights are too small, the range of activations across the minibatch will shrink as the activations propagate forward through the network. </s>\n",
      "<s> [INST] Why is it important to coordinate the approach for setting biases with the approach for setting weights? [/INST] It is important to coordinate the approach for setting biases with the approach for setting weights because setting biases to zero is compatible with most weight initialization schemes. However, in some cases, biases may need to be set to non-zero values to achieve the right marginal statistics of the output, especially for output units. </s>\n",
      "<s> [INST] What is the purpose of AdaGrad algorithm? [/INST] The purpose of the AdaGrad algorithm is to adapt the learning rates of all model parameters individually by scaling them inversely proportional to the square root of the sum of their historical squared values. This allows for a more rapid decrease in the learning rate for parameters with larger partial derivatives of the loss. </s>\n",
      "<s> [INST] Why does AdaGrad perform well for some but not all deep learning models? [/INST] AdaGrad performs well for some deep learning models because it allows for greater progress in more gently sloped directions of parameter space. However, it may result in a premature and excessive decrease in the effective learning rate for training deep neural network models. </s>\n",
      "<s> [INST] How does RMSProp modify AdaGrad? [/INST] RMSProp modifies AdaGrad by changing the gradient accumulation into an exponentially weighted moving average. This allows it to perform better in a non-convex setting, as it discards history from the extreme past and converges rapidly after finding a locally convex region. </s>\n",
      "<s> [INST] What is the advantage of using RMSProp in a non-convex setting? [/INST] The advantage of using RMSProp in a non-convex setting is that it can converge rapidly after finding a locally convex bowl, as if it were an instance of the AdaGrad algorithm initialized within that bowl. This is achieved by using an exponentially decaying average to discard excessive history from the extreme past. </s>\n",
      "<s> [INST] What is the concept of Adam algorithm? [/INST] Adam (derived from 'adaptive moments') is an adaptive learning rate optimization algorithm. It combines elements of RMSProp and momentum, incorporating momentum directly as an estimate of the first order moment of the gradient with exponential weighting. </s>\n",
      "<s> [INST] What are the important distinctions of Adam compared to other algorithms? [/INST] The important distinctions of Adam compared to other algorithms are: (1) In Adam, momentum is incorporated directly as an estimate of the first order moment of the gradient, while RMSProp applies momentum to the rescaled gradients. (2) Adam includes bias correction terms to reduce the bias of moment estimates. (3) Adam uses adaptive learning rates for each parameter, allowing for individual scaling based on historical squared gradients. </s>\n",
      "<s> [INST] What is the RMSProp algorithm used for? [/INST] The RMSProp algorithm is used for optimizing deep models by adapting the learning rate for each model parameter. </s>\n",
      "<s> [INST] What is the difference between RMSProp and Adam? [/INST] Unlike Adam, the RMSProp algorithm lacks the correction factor and may have higher bias early in training. </s>\n",
      "<s> [INST] Which optimization algorithms are currently popular? [/INST] The popular optimization algorithms currently in use include SGD, SGD with momentum, RMSProp, RMSProp with momentum, AdaDelta, and Adam. </s>\n",
      "<s> [INST] What factors influence the choice of optimization algorithm? [/INST] The choice of which optimization algorithm to use depends largely on the user's familiarity with the algorithm and ease of hyperparameter tuning. </s>\n",
      "<s> [INST] What is Newton's method and how is it applied to neural network training? [/INST] Newton's method is a second-order method that makes use of second derivatives to improve optimization. It is applied to neural network training by iteratively updating the parameters based on the gradient and the Hessian matrix of the objective function. </s>\n",
      "<s> [INST] What is the limitation of using Newton's method for training large neural networks? [/INST] The application of Newton's method for training large neural networks is limited by the significant computational burden it imposes. </s>\n",
      "<s> [INST] What is the computational complexity of inverting a k x k matrix? [/INST] The computational complexity of inverting a k x k matrix is O(k^3). </s>\n",
      "<s> [INST] Why can only networks with a small number of parameters be practically trained via Newton's method? [/INST] Only networks with a small number of parameters can be practically trained via Newton's method because the inverse Hessian has to be computed at every training iteration. </s>\n",
      "<s> [INST] What is the motivation behind the conjugate gradients method? [/INST] The motivation behind the conjugate gradients method is to gain some of the advantages of Newton's method while avoiding the computational hurdles. </s>\n",
      "<s> [INST] How does the method of conjugate gradients address the problem of zig-zag progress? [/INST] The method of conjugate gradients seeks to find a search direction that is conjugate to the previous line search direction, meaning it will not undo progress made in that direction. </s>\n",
      "<s> [INST] What are two popular methods for computing the βt? [/INST] Two popular methods for computing the βt are the Fletcher-Reeves method and the Polak-Ribière method. </s>\n",
      "<s> [INST] What is the conjugate gradient method used for? [/INST] The conjugate gradient method ensures that the gradient along the previous direction does not increase in magnitude, allowing us to stay at the minimum along the previous directions. </s>\n",
      "<s> [INST] What modifications are made to the conjugate gradients algorithm for training neural networks? [/INST] For training neural networks, the nonlinear conjugate gradients algorithm includes occasional resets and line searches along the unaltered gradient. It is often beneficial to initialize the optimization with a few iterations of stochastic gradient descent. </s>\n",
      "<s> [INST] What is the BFGS algorithm and how does it approximate Newton's update? [/INST] The Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm is an optimization method that approximates Newton's update without the computational burden. It uses a matrix, Mt, to iteratively refine the approximation of the inverse Hessian, H^-1. </s>\n",
      "<s> [INST] What determines the direction of descent in the BFGS algorithm? [/INST] The direction of descent in the BFGS algorithm is determined by the inverse Hessian approximation multiplied by the gradient. </s>\n",
      "<s> [INST] How does the BFGS algorithm differ from the method of conjugate gradients? [/INST] Unlike the method of conjugate gradients, the success of the BFGS algorithm is not heavily dependent on finding the true minimum along the line in the line search. </s>\n",
      "<s> [INST] What is the disadvantage of using the BFGS algorithm in modern deep learning models? [/INST] The disadvantage of using the BFGS algorithm in modern deep learning models is that it requires a large amount of memory to store the inverse Hessian matrix. </s>\n",
      "<s> [INST] What is the L-BFGS algorithm? [/INST] The L-BFGS algorithm is a variant of the BFGS algorithm that avoids storing the complete inverse Hessian approximation. </s>\n",
      "<s> [INST] Why is batch normalization important in optimizing deep neural networks? [/INST] Batch normalization is important in optimizing deep neural networks because it helps to address the challenge of training very deep models by adapting the reparametrization of the network. </s>\n",
      "<s> [INST] What problem can arise when updating all layers simultaneously in a deep neural network? [/INST] When updating all layers simultaneously in a deep neural network, unexpected results can occur because the updates are computed under the assumption that the other layers remain constant. </s>\n",
      "<s> [INST] What is the first-order Taylor series approximation used in the example provided? [/INST] The first-order Taylor series approximation used in the example predicts that the value of the output will decrease by the learning rate multiplied by the gradient. </s>\n",
      "<s> [INST] What is an example of a second-order term that arises from the update in the example? [/INST] An example of a second-order term that arises from the update is the square of the learning rate multiplied by the product of the gradients of the weights. </s>\n",
      "<s> [INST] What is the purpose of the L-BFGS algorithm? [/INST] The L-BFGS algorithm is used to decrease the memory costs of the BFGS algorithm by avoiding storing the complete inverse Hessian approximation. </s>\n",
      "<s> [INST] What is the purpose of batch normalization in a neural network? [/INST] The purpose of batch normalization in a neural network is to standardize the mean and variance of each unit in order to stabilize learning. </s>\n",
      "<s> [INST] What does batch normalization normalize? [/INST] Batch normalization normalizes the mean and standard deviation of each unit in a neural network. </s>\n",
      "<s> [INST] Why are the lower layers in a deep neural network with batch normalization often useless? [/INST] The lower layers in a deep neural network with batch normalization are often useless because the normalization of mean and variance removes their harmful and beneficial effects. </s>\n",
      "<s> [INST] What are the learned parameters in batch normalization used for? [/INST] The learned parameters in batch normalization, γ and β, are used to introduce a new parametrization that allows the mean and standard deviation of a unit to be set to any arbitrary value, thus preserving the expressive power of the network. </s>\n",
      "<s> [INST] Should batch normalization be applied to the input or the transformed value in a neural network layer? [/INST] Batch normalization should be applied to the transformed value, XW, in a neural network layer. </s>\n",
      "<s> [INST] What is the ulum context described in the text? [/INST] The ulum context is a random mix of easy and difficult examples presented to the learner, with the proportion of difficult examples gradually increasing. </s>\n",
      "<s> [INST] What was observed when using a deterministic curriculum? [/INST] No improvement over the baseline was observed when using a deterministic curriculum. </s>\n",
      "<s> [INST] What does the text mention about the optimization methods discussed in this chapter? [/INST] The text mentions that the optimization methods discussed in this chapter are often directly applicable to specialized architectures with little or no modification. </s>\n",
      "<s> [INST] What does the weight decay term do to the learning rule? [/INST] The weight decay term multiplicatively shrinks the weight vector by a constant factor before performing the usual gradient update. </s>\n",
      "<s> [INST] What is the quadratic approximation used in the analysis? [/INST] The quadratic approximation used is given by ˆJ(θ) = J(w∗) + 1/2(w-w∗)ᵀH(w-w∗), where H is the Hessian matrix of J with respect to w evaluated at w∗. </s>\n",
      "<s> [INST] How is the gradient of the regularized version of J used to find the minimum? [/INST] The minimum of the regularized version of ˆJ occurs where its gradient, ˜w=∇ˆJ(w∗) + αw∗, is equal to zero. </s>\n",
      "<s> [INST] What happens to the regularized solution as alpha grows? [/INST] As alpha grows, the regularized solution ˜w approaches w∗. </s>\n",
      "<s> [INST] What is the effect of weight decay on the components of w∗? [/INST] The components of w∗ that are aligned with the eigenvectors of the Hessian matrix are rescaled by a factor of the corresponding eigenvalues plus alpha. </s>\n",
      "<s> [INST] What does the diagram in Figure 7.1 illustrate? [/INST] The diagram in Figure 7.1 illustrates the effect of L2 (or weight decay) regularization on the value of the optimal w. It shows how the regularizer and the objective function reach an equilibrium at the point ˜w, where the regularizer pulls one component close to zero and has a small effect on the other component. </s>\n",
      "<s> [INST] What is the gradient given by in the context of the Taylor series? [/INST] The gradient in the context of the Taylor series is given by w ∇ J (∗) </s>\n",
      "<s> [INST] Under what assumption is the Hessian matrix diagonal? [/INST] The Hessian matrix is assumed to be diagonal when there is no correlation between the input features. </s>\n",
      "<s> [INST] What are the two possible outcomes when w∗i > 0? [/INST] The two possible outcomes when w∗i > 0 are: 1) wi = 0 if w∗i ≤ α Hi,i, and 2) wi = sign(w∗i) * (|w∗i| - α Hi,i) otherwise. </s>\n",
      "<s> [INST] How does L1 regularization differ from L2 regularization in terms of sparsity? [/INST] L1 regularization results in sparsity, where some parameters have an optimal value of zero. L2 regularization does not cause sparsity. </s>\n",
      "<s> [INST] What is the LASSO model? [/INST] The LASSO model integrates an L1 penalty with a linear model and a least squares cost function, causing a subset of the weights to become zero. </s>\n",
      "<s> [INST] What is the equivalent prior term for L1 regularization in Bayesian inference? [/INST] The equivalent prior term for L1 regularization in Bayesian inference is the log-prior term that is maximized by MAP Bayesian inference when the prior is an isotropic Laplace distribution. </s>\n",
      "<s> [INST] What is the purpose of constraining the norm of each column of the weight matrix in a neural net layer? [/INST] The purpose of constraining the norm of each column of the weight matrix in a neural net layer is to prevent any one hidden unit from having very large weights and to maintain stability in the parameter space. </s>\n",
      "<s> [INST] What are some linear models in machine learning that require regularization? [/INST] Linear models such as linear regression and PCA in machine learning require regularization. </s>\n",
      "<s> [INST] When would a matrix XX be singular? How is this problem addressed? [/INST] A matrix XX is singular when the data generating distribution has no variance in some direction or when there are fewer examples (rows of X) than input features (columns of X). This problem is addressed by using regularization techniques that involve inverting the matrix I+α instead. </s>\n",
      "<s> [INST] Why is regularization important for underdetermined problems in machine learning? [/INST] Regularization is important for underdetermined problems in machine learning because it guarantees the convergence of iterative methods and prevents numerical overflow. It helps in stabilizing the problem and finding a solution. </s>\n",
      "<s> [INST] What is the Moore-Penrose pseudoinverse used for in linear algebra? [/INST] The Moore-Penrose pseudoinverse is used in linear algebra for solving underdetermined linear equations and stabilizing underdetermined problems using regularization. It is equivalent to performing linear regression with a shrinking regularization coefficient. </s>\n",
      "<s> [INST] What are some examples of operations that are generally applicable in machine learning algorithms? [/INST] Examples of operations that are generally applicable in machine learning algorithms include adding Gaussian noise to the input. </s>\n",
      "<s> [INST] What is the importance of noise applied to the hidden units? [/INST] Noise applied to the hidden units is important for regularization as it can be more powerful than simply shrinking the parameters. </s>\n",
      "<s> [INST] How can noise be used to regularize models? [/INST] Noise can be used to regularize models by injecting it into the inputs, hidden units, or weights. </s>\n",
      "<s> [INST] How can noise applied to the weights be interpreted? [/INST] Noise applied to the weights can be interpreted as a stochastic implementation of Bayesian inference over the weights or as a form of regularization that encourages stability of the function to be learned. </s>\n",
      "<s> [INST] What is the objective function for minimizing the squared error of the output of a network with injected weight noise? [/INST] The objective function for minimizing the squared error of the output of a network with injected weight noise is ∼JW = Ep(x W, y)[(ˆyW(x) - y)²]. </s>\n",
      "<s> [INST] What are the two kinds of parts in the model? [/INST] The two kinds of parts in the model are task-specific parameters and generic parameters. </s>\n",
      "<s> [INST] What are task-specific parameters? [/INST] Task-specific parameters are the upper layers of the neural network that only benefit from the examples of their task to achieve good generalization. </s>\n",
      "<s> [INST] What are generic parameters? [/INST] Generic parameters are shared across all the tasks and benefit from the pooled data of all the tasks. </s>\n",
      "<s> [INST] What is the assumption in multi-task learning? [/INST] The assumption in multi-task learning is that there exists a common pool of factors that explain the variations in the input, while each task is associated with a subset of these factors. </s>\n",
      "<s> [INST] What is the purpose of shared parameters in multi-task learning? [/INST] Shared parameters in multi-task learning improve generalization and generalization error bounds by utilizing statistical strength from the increased number of examples for the shared parameters compared to single-task models. </s>\n",
      "<s> [INST] What is early stopping in deep learning? [/INST] Early stopping in deep learning refers to the observation that training error decreases steadily over time, but validation set error begins to rise again. It involves returning to the parameter setting at the point with the lowest validation set error. </s>\n",
      "<s> [INST] How can a model with better validation set error be obtained? [/INST] A model with better validation set error can be obtained by returning to the parameter setting at the point with the lowest validation set error. </s>\n",
      "<s> [INST] When does the training algorithm terminate? [/INST] The training algorithm terminates when no parameters have improved over the threshold. </s>\n",
      "<s> [INST] What is early stopping? [/INST] Early stopping is a regularization strategy in deep learning that involves stopping the training process when the validation error starts to worsen. </s>\n",
      "<s> [INST] Why is early stopping popular in deep learning? [/INST] Early stopping is popular in deep learning due to its effectiveness and simplicity as a regularization strategy. </s>\n",
      "<s> [INST] How is early stopping related to hyperparameter selection? [/INST] Early stopping can be seen as a hyperparameter selection algorithm, where the number of training steps is treated as a hyperparameter. </s>\n",
      "<s> [INST] What is the cost of using early stopping? [/INST] The cost of using early stopping includes the need to periodically evaluate the validation set error during training and the storage of the best parameters. However, these costs are generally considered negligible. </s>\n",
      "<s> [INST] How does early stopping compare to weight decay as a regularization strategy? [/INST] Early stopping is a less intrusive form of regularization compared to weight decay, as it requires minimal changes to the training procedure and does not risk trapping the network in bad local minima. </s>\n",
      "<s> [INST] Can early stopping be used with other regularization strategies? [/INST] Early stopping can be used alone or in conjunction with other regularization strategies. </s>\n",
      "<s> [INST] What requirement does early stopping have in terms of training data? [/INST] Early stopping requires a validation set, which means that some training data is not used to train the model. </s>\n",
      "<s> [INST] What are the two strategies discussed for the second training procedure? [/INST] The two strategies discussed for the second training procedure are reinitializing the model and retraining on all the data or continuing training using the parameters obtained from the first round. </s>\n",
      "<s> [INST] What are the subtleties associated with retraining for the same number of passes through the dataset? [/INST] The subtleties associated with retraining for the same number of passes through the dataset include the fact that each pass through the dataset in the second round of training will require more parameter updates due to the larger training set. </s>\n",
      "<s> [INST] How does the second strategy for using all the data differ from the first strategy? [/INST] The second strategy for using all the data involves continuing training with the parameters obtained from the first round, without any guide on when to stop in terms of a number of steps. </s>\n",
      "<s> [INST] What is the advantage of using early stopping as a regularization strategy? [/INST] The advantage of using early stopping as a regularization strategy is that it reduces the computational cost of the training procedure, as it limits the number of training iterations and provides regularization without the need for penalty terms or their gradients. </s>\n",
      "<s> [INST] How does early stopping restrict the optimization procedure according to Bishop (1995a)? [/INST] According to Bishop (1995a), early stopping restricts the optimization procedure to a relatively small volume of parameter space around the initial parameter value. The product of the number of iterations and the learning rate is viewed as a measure of effective capacity. </s>\n",
      "<s> [INST] What is the purpose of parameter sharing in CNNs? [/INST] The purpose of parameter sharing in CNNs is to reduce the number of unique model parameters and increase network sizes without requiring more training data. </s>\n",
      "<s> [INST] How does weight decay act as a penalty on the model parameters? [/INST] Weight decay acts as a penalty on the model parameters by directly placing a penalty on them. </s>\n",
      "<s> [INST] What does representational sparsity mean? [/INST] Representational sparsity refers to a representation where many of the elements are zero or close to zero. </s>\n",
      "<s> [INST] What is the difference between parameter sparsity and representational sparsity? [/INST] Parameter sparsity refers to a sparse parametrization where many of the parameters become zero or close to zero, while representational sparsity refers to a representation where many of the elements are zero or close to zero. </s>\n",
      "<s> [INST] What is norm penalty regularization of representations? [/INST] Norm penalty regularization of representations is performed by adding a norm penalty on the representation to the loss function. </s>\n",
      "<s> [INST] What does an L1 penalty on the elements of the representation induce? [/INST] An L1 penalty on the elements of the representation induces representational sparsity. </s>\n",
      "<s> [INST] What are some examples of penalties that result in a sparse representation? [/INST] Examples of penalties that result in a sparse representation include penalties derived from a Student-t prior, KL divergence penalties, and penalties based on regularizing the average activation across examples. </s>\n",
      "<s> [INST] How can representational sparsity be achieved with a hard constraint on activation values? [/INST] Representational sparsity can be achieved with a hard constraint on activation values by solving a constrained optimization problem where the number of non-zero entries in the representation is limited. </s>\n",
      "<s> [INST] What is bagging? [/INST] Bagging is a technique for reducing generalization error by combining several models. </s>\n",
      "<s> [INST] How does model averaging work? [/INST] Model averaging works because different models usually do not make all the same errors on the test set. </s>\n",
      "<s> [INST] What is the expected squared error of the ensemble predictor? [/INST] The expected squared error of the ensemble predictor is 1/k^2 * (k * v + 1 - k * (1/k * sum(η^2_i) + 1/k * sum(η_i * η_j))) where η_i and η_j are the errors made by each model. </s>\n",
      "<s> [INST] How does the ensemble perform when errors are perfectly uncorrelated? [/INST] When errors are perfectly uncorrelated, the expected squared error of the ensemble decreases linearly with the ensemble size. </s>\n",
      "<s> [INST] How are different ensemble methods constructed? [/INST] Different ensemble methods construct the ensemble of models in different ways. </s>\n",
      "<s> [INST] What is the purpose of bagging? [/INST] The purpose of bagging is to reuse the same kind of model, training algorithm, and objective function several times. </s>\n",
      "<s> [INST] Describe how bagging works with an example. [/INST] Bagging involves constructing k different datasets by sampling with replacement from the original dataset, training each model on its corresponding dataset, and then averaging their output to create a robust ensemble. </s>\n",
      "<s> [INST] What is the purpose of dropout training? [/INST] The purpose of dropout training is to minimize the cost of a model defined by parameters and a mask vector, through the inclusion/exclusion of units. </s>\n",
      "<s> [INST] How does dropout training differ from bagging training? [/INST] Unlike bagging training, dropout training involves models that share parameters and inherit different subsets of parameters from the parent neural network. </s>\n",
      "<s> [INST] What is the relationship between dropout training and parameter sharing? [/INST] Dropout training utilizes parameter sharing to represent an exponential number of models with a tractable amount of memory. </s>\n",
      "<s> [INST] What is the difference between dropout training and bagging training in terms of the number of models trained? [/INST] In bagging training, each model is trained to convergence on its respective training set, while in dropout training, only a tiny fraction of possible sub-networks are trained for a single step, with the remaining sub-networks arriving at good parameter settings through parameter sharing. </s>\n",
      "<s> [INST] How does dropout training handle the training set for each sub-network? [/INST] For each sub-network, dropout training samples a subset of the original training set with replacement. </s>\n",
      "<s> [INST] What is the process of inference in bagged ensemble models? [/INST] Inference in bagged ensemble models refers to the process of accumulating votes from all the ensemble members to make a prediction. </s>\n",
      "<s> [INST] What is the weight scaling rule? [/INST] The weight scaling rule is a formula used in regularization for deep learning. </s>\n",
      "<s> [INST] Is the weight scaling rule exact in all settings? [/INST] No, the weight scaling rule is only exact in certain settings and is an approximation in deep models with nonlinearities. </s>\n",
      "<s> [INST] What is the advantage of dropout regularization? [/INST] The advantage of dropout regularization is that it is computationally cheap and can be applied to nearly any model that uses a distributed representation and can be trained with stochastic gradient descent. </s>\n",
      "<s> [INST] What type of models does dropout work well with? [/INST] Dropout works well with models such as feedforward neural networks, probabilistic models (such as restricted Boltzmann machines), and recurrent neural networks. </s>\n",
      "<s> [INST] Does using dropout increase the size of the model? [/INST] Yes, using dropout requires increasing the size of the model to offset its reduction in effective capacity. </s>\n",
      "<s> [INST] What is the cost of using dropout in a complete system? [/INST] Using dropout in a complete system can be significant, as it requires a larger model and many more iterations of the training algorithm. </s>\n",
      "<s> [INST] What is the purpose of regularization in deep learning? [/INST] The purpose of regularization in deep learning is to prevent overfitting and improve generalization performance. </s>\n",
      "<s> [INST] When does regularization confer little reduction in generalization error? [/INST] Regularization confers little reduction in generalization error when working with large datasets or when additional unlabeled data is available. </s>\n",
      "<s> [INST] What is the advantage of unsupervised feature learning over dropout? [/INST] Unsupervised feature learning can be more effective than dropout when there is additional unlabeled data available. </s>\n",
      "<s> [INST] What is the relationship between dropout and L2 weight decay? [/INST] Dropout is equivalent to L2 weight decay when applied to linear regression, but for deep models, dropout is not equivalent to weight decay. </s>\n",
      "<s> [INST] What is fast dropout and how does it affect convergence time? [/INST] Fast dropout is an approximation of dropout that reduces the stochasticity in the computation of the gradient, leading to faster convergence time. </s>\n",
      "<s> [INST] What is dropout boosting and how does it differ from traditional dropout? [/INST] Dropout boosting is a method that uses the same mask noise as traditional dropout but lacks its regularizing effect. It trains the entire ensemble to maximize the log-likelihood on the training set. </s>\n",
      "<s> [INST] What is the main idea behind DropConnect? [/INST] DropConnect is a special case of dropout where each product between a single scalar weight and a single hidden unit state is considered a unit that can be dropped. </s>\n",
      "<s> [INST] What is the most widely used implicit ensemble method? [/INST] The most widely used implicit ensemble method is dropout. </s>\n",
      "<s> [INST] How does dropout implement a form of bagging with parameter sharing? [/INST] By training a network with stochastic behavior and averaging predictions over multiple stochastic decisions, dropout implements a form of bagging with parameter sharing. </s>\n",
      "<s> [INST] What should be considered when choosing modification families for dropout? [/INST] When choosing modification families for dropout, it is important to use random modifications that neural networks can learn to resist and model families that allow for fast approximation. </s>\n",
      "<s> [INST] What is the tangent distance algorithm? [/INST] The tangent distance algorithm is a non-parametric nearest-neighbor algorithm that assumes the data lies near a low-dimensional manifold. </s>\n",
      "<s> [INST] How does the tangent distance algorithm calculate distances between points? [/INST] The tangent distance algorithm approximates the distance between points by measuring the distance between the tangent planes of the manifolds to which the points belong. </s>\n",
      "<s> [INST] What is the tangent prop algorithm? [/INST] The tangent prop algorithm is a neural network classifier trained with an extra penalty to achieve local invariance to known factors of variation. </s>\n",
      "<s> [INST] How does the tangent prop algorithm achieve local invariance? [/INST] The tangent prop algorithm achieves local invariance by requiring the directional derivative of the neural network output to be small in the directions of the known manifold tangent vectors. </s>\n",
      "<s> [INST] What is the relationship between tangent propagation and dataset augmentation? [/INST] Both tangent propagation and dataset augmentation involve encoding prior knowledge of the task by specifying transformations that should not alter the network output. However, dataset augmentation explicitly trains the network on distinct inputs created by applying transformations, while tangent propagation analytically regularizes the model to resist perturbations in the specified transformation directions. </s>\n",
      "<s> [INST] What are the drawbacks of the analytical approach used in tangent propagation? [/INST] The analytical approach used in tangent propagation only regularizes the model to resist infinitesimal perturbations. Dataset augmentation confers resistance to larger perturbations. Additionally, the approach poses difficulties for models based on rectified linear units, as they cannot shrink their derivatives by saturating at a high value with large weights. </s>\n",
      "<s> [INST] Why does dataset augmentation work well with rectified linear units? [/INST] Dataset augmentation works well with rectified linear units because different subsets of rectified units can activate for different transformed versions of each original input. </s>\n",
      "<s> [INST] What are some regularization techniques for neural networks mentioned in the text? [/INST] Regularization techniques mentioned in the text include double backprop, adversarial training, and dataset augmentation using manually specified transformations. </s>\n",
      "<s> [INST] What is the main idea behind tangent propagation? [/INST] The main idea behind tangent propagation is that the model should be invariant to certain specified directions of change in the input. </s>\n",
      "<s> [INST] How does the manifold tangent classiﬁer estimate tangent vectors? [/INST] The manifold tangent classiﬁer estimates tangent vectors by training an autoencoder to fit the training data. </s>\n",
      "<s> [INST] What are the two steps in the algorithm proposed with the manifold tangent classiﬁer? [/INST] The algorithm proposed with the manifold tangent classiﬁer involves using an autoencoder for unsupervised learning to learn the manifold structure, and then using these tangent vectors to regularize a neural net classifier. </s>\n",
      "<s> [INST] What are the central themes of machine learning mentioned in the text? [/INST] The central themes of machine learning mentioned in the text are regularization and optimization. </s>\n",
      "<s> [INST] What are deep feedforward networks often called? [/INST] Deep feedforward networks are often called feedforward neural networks or multilayer perceptrons (MLPs). </s>\n",
      "<s> [INST] What is the goal of a feedforward network? [/INST] The goal of a feedforward network is to approximate some function f * by learning the value of the parameters θ that result in the best function approximation. </s>\n",
      "<s> [INST] What is the difference between feedforward networks and recurrent neural networks? [/INST] Feedforward neural networks do not have feedback connections, while recurrent neural networks do. </s>\n",
      "<s> [INST] What are feedforward networks used for? [/INST] Feedforward networks are used in many important commercial applications, including object recognition and natural language processing. </s>\n",
      "<s> [INST] How are feedforward neural networks represented? [/INST] Feedforward neural networks are represented by composing together many different functions, forming a directed acyclic graph. </s>\n",
      "<s> [INST] What is the final layer of a feedforward network called? [/INST] The final layer of a feedforward network is called the output layer. </s>\n",
      "<s> [INST] What does the training data specify in a feedforward network? [/INST] The training data specifies what the output layer must do at each point x, but does not specify the behavior of the hidden layers. </s>\n",
      "<s> [INST] Why are the layers between the input and output layers called hidden layers? [/INST] The layers between the input and output layers are called hidden layers because the training data does not show the desired output for each of these layers. </s>\n",
      "<s> [INST] Why are feedforward networks called neural? [/INST] Feedforward networks are called neural because they are loosely inspired by neuroscience and each hidden layer can be interpreted as playing a role analogous to a neuron. </s>\n",
      "<s> [INST] What is the inspiration for using many layers of vector-valued representation in neural networks? [/INST] The inspiration for using many layers of vector-valued representation in neural networks comes from neuroscience and the functions that biological neurons compute. </s>\n",
      "<s> [INST] What is the XOR function? [/INST] The XOR function returns 1 when exactly one of the binary values is equal to 1, otherwise it returns 0. </s>\n",
      "<s> [INST] What is the goal of the learning algorithm in this example? [/INST] The goal of the learning algorithm is to adapt the parameters of the model to make it as similar as possible to the target XOR function. </s>\n",
      "<s> [INST] Why is the mean squared error (MSE) loss function used in this example? [/INST] The mean squared error (MSE) loss function is used to simplify the math in this example, although it may not be appropriate for modeling binary data in practical applications. </s>\n",
      "<s> [INST] What is the form of the linear model used in this example? [/INST] The linear model used in this example is defined as f(x; θ) = x * w + b, where θ consists of the weights w and the bias b. </s>\n",
      "<s> [INST] Why is a linear model unable to represent the XOR function? [/INST] A linear model is unable to represent the XOR function because it cannot capture the nonlinear relationship between the input values and the output. </s>\n",
      "<s> [INST] What is the architecture of the feedforward network used in this example? [/INST] The feedforward network used in this example has one hidden layer with two hidden units. The hidden layer computes the values of the hidden units, which are then used as input for the output layer. </s>\n",
      "<s> [INST] Why must we use a nonlinear function to describe the features in the feedforward network? [/INST] A nonlinear function must be used to describe the features in the feedforward network because if a linear function was used, the whole network would remain a linear function of its input. By using a nonlinear function, the network can learn a different feature space where a linear model can represent the solution. </s>\n",
      "<s> [INST] What is the purpose of an activation function in a neural network? [/INST] The purpose of an activation function in a neural network is to introduce nonlinearity into the model, allowing it to learn and represent complex relationships in the data. </s>\n",
      "<s> [INST] What is the default activation function recommended for use with most feedforward neural networks? [/INST] The default activation function recommended for use with most feedforward neural networks is the rectified linear unit or ReLU. </s>\n",
      "<s> [INST] How can the XOR problem be solved using a feedforward neural network? [/INST] The XOR problem can be solved using a feedforward neural network by learning a representation in a transformed space where a linear model can solve the problem. The transformed space is obtained by applying a nonlinear activation function to the output of a linear transformation. </s>\n",
      "<s> [INST] What is the issue with using mean squared error and mean absolute error with gradient-based optimization? [/INST] Mean squared error and mean absolute error often lead to poor results in gradient-based optimization because output units that saturate produce very small gradients when combined with these cost functions. </s>\n",
      "<s> [INST] Why is the cross-entropy cost function more popular than mean squared error or mean absolute error? [/INST] The cross-entropy cost function is more popular than mean squared error or mean absolute error because it overcomes the issue of small gradients produced by saturating output units and is not limited to estimating an entire distribution. </s>\n",
      "<s> [INST] What determines the choice of cost function in a neural network? [/INST] The choice of cost function is tightly coupled with the choice of output unit in a neural network. </s>\n",
      "<s> [INST] Can output units also be used as hidden units in a neural network? [/INST] Yes, output units can also be used as hidden units in a neural network. </s>\n",
      "<s> [INST] What is the role of the output layer in a feedforward network? [/INST] The role of the output layer in a feedforward network is to provide additional transformation from the hidden features to complete the task that the network must perform. </s>\n",
      "<s> [INST] What is the purpose of linear output units in a neural network? [/INST] Linear output units in a neural network are used to produce the mean of a conditional Gaussian distribution. </s>\n",
      "<s> [INST] What distribution is often modeled using linear output units? [/INST] Linear output units are often used to model a conditional Gaussian distribution. </s>\n",
      "<s> [INST] What is the advantage of using linear units in gradient-based optimization? [/INST] Using linear units in gradient-based optimization is advantageous because they do not saturate and pose little difficulty for optimization algorithms. </s>\n",
      "<s> [INST] Why is it difficult to satisfy constraints on the covariance matrix with a linear output layer? [/INST] It is difficult to satisfy constraints on the covariance matrix with a linear output layer because the covariance must be a positive definite matrix for all inputs. </s>\n",
      "<s> [INST] What type of output units are used for predicting binary variables? [/INST] For predicting binary variables, sigmoid units are used as output units. </s>\n",
      "<s> [INST] What constraint does the output of a neural net need to satisfy when modeling a Bernoulli distribution? [/INST] The output of a neural net needs to satisfy the constraint of lying in the interval [0, 1] when modeling a Bernoulli distribution. </s>\n",
      "<s> [INST] Why is using a linear thresholding approach problematic for training a Bernoulli distribution model? [/INST] Using a linear thresholding approach for training a Bernoulli distribution model is problematic because it leads to gradients of 0 whenever the model's output strays outside the unit interval, making it difficult for the learning algorithm to improve the parameters. </s>\n",
      "<s> [INST] What approach is better for training a Bernoulli distribution model with gradient descent? [/INST] A better approach for training a Bernoulli distribution model with gradient descent is to use sigmoid output units combined with maximum likelihood. </s>\n",
      "<s> [INST] What type of output units are used in this approach? [/INST] Sigmoid output units are used in the approach of training a Bernoulli distribution model with gradient descent. </s>\n",
      "<s> [INST] What does the sigmoid output units combined with maximum likelihood ensure? [/INST] Sigmoid output units combined with maximum likelihood ensure that there is always a strong gradient whenever the model has the wrong answer, facilitating effective training with gradient descent. </s>\n",
      "<s> [INST] What is the purpose of using sigmoid output units in a Bernoulli distribution model? [/INST] The purpose of using sigmoid output units in a Bernoulli distribution model is to ensure a strong gradient is present whenever the model predicts the wrong answer, improving the training process. </s>\n",
      "<s> [INST] What is the definition of the sigmoid output unit? [/INST] The sigmoid output unit is defined as ^y = σ(w^h + b) </s>\n",
      "<s> [INST] How is the sigmoid activation function used in the output unit? [/INST] The sigmoid activation function is used to convert the linear output z into a probability </s>\n",
      "<s> [INST] How is a probability distribution over y defined using the sigmoid output? [/INST] A probability distribution over y is defined by exponentiating the unnormalized log probabilities and normalizing them using a sigmoidal transformation of z </s>\n",
      "<s> [INST] What can cause difficulties for learning if the softmax function saturates? [/INST] If the softmax function saturates, it can cause difficulties for learning if the loss function is not designed to compensate for it. </s>\n",
      "<s> [INST] How can we derive a numerically stable variant of the softmax function? [/INST] A numerically stable variant of the softmax can be derived by using the formula softmax(z) = softmax(z - max(z)). This allows for evaluation of the softmax function with small numerical errors even with extremely large or extremely negative input values (z). </s>\n",
      "<s> [INST] What is the purpose of overparametrizing the distribution in the softmax function? [/INST] Overparametrizing the distribution in the softmax function is not necessary, as only one parameter is needed to ensure that the outputs sum to 1. However, it is simpler to implement the overparametrized version. </s>\n",
      "<s> [INST] What does the softmax function do from a neuroscientific point of view? [/INST] From a neuroscientific point of view, the softmax function can be seen as a way to create competition between the participating units. It causes an increase in the value of one unit to correspond to a decrease in the value of others, similar to lateral inhibition between neurons in the cortex. </s>\n",
      "<s> [INST] What is the difference between the softmax function and the arg max function? [/INST] The softmax function is more closely related to the arg max function than the max function. The arg max function, represented as a one-hot vector, is not continuous or differentiable. The softmax function provides a softened version of the arg max. </s>\n",
      "<s> [INST] Why is the softmax function called 'softmax'? [/INST] The softmax function is called 'softmax' because it is a softened version of the arg max function. The term 'soft' refers to the fact that the softmax function is continuous and differentiable, unlike the arg max function. </s>\n",
      "<s> [INST] What is the principle of maximum likelihood when designing a cost function for an output layer? [/INST] The principle of maximum likelihood suggests using log(p(y|x;θ)) as the cost function for designing an output layer. </s>\n",
      "<s> [INST] What can the neural network represent? [/INST] The neural network represents a function f(x;θ) where the outputs of this function provide parameters for a distribution over y. </s>\n",
      "<s> [INST] What is the loss function used to interpret the outputs of the neural network? [/INST] The loss function interprets the outputs as the negative log-likelihood of the distribution p(y|ω(x)). </s>\n",
      "<s> [INST] How can we learn the variance of a conditional Gaussian distribution with the neural network? [/INST] We can learn the variance of a conditional Gaussian distribution by including the variance as one of the properties of the function f(x;θ) controlled by ω. </s>\n",
      "<s> [INST] What is a heteroscedastic model? [/INST] A heteroscedastic model is a model where the amount of variance in y can vary for different values of x. </s>\n",
      "<s> [INST] What is the typical way to formulate the Gaussian distribution when predicting variance? [/INST] The typical way to formulate the Gaussian distribution when predicting variance is to use precision instead of variance. </s>\n",
      "<s> [INST] Why is using precision instead of variance preferred in the formulation of the Gaussian distribution? [/INST] Using precision instead of variance is preferred because it allows for well-behaved gradient operations in the log-likelihood calculation. </s>\n",
      "<s> [INST] What condition needs to be enforced on the output of the model when using a diagonal precision matrix? [/INST] When using a diagonal precision matrix, the only condition that needs to be enforced on the output of the model is positivity. </s>\n",
      "<s> [INST] How can we ensure the covariance matrix of the Gaussian is positive definite? [/INST] To ensure the covariance matrix of the Gaussian is positive definite, we need to ensure the precision matrix is positive definite. This can be achieved by using a diagonal matrix or a scalar multiplied by the diagonal matrix. </s>\n",
      "<s> [INST] What is the activation function used in rectified linear units? [/INST] The activation function used in rectified linear units is max(0, z). </s>\n",
      "<s> [INST] Why are rectified linear units easy to optimize? [/INST] Rectified linear units are easy to optimize because they are similar to linear units and their derivatives remain large and consistent. </s>\n",
      "<s> [INST] What is the purpose of setting all elements of b to a small positive value in the initialization of the parameters? [/INST] Setting all elements of b to a small positive value ensures that rectified linear units will be initially active for most inputs in the training set and allow the derivatives to pass through. </s>\n",
      "<s> [INST] What are some generalizations of rectified linear units? [/INST] Some generalizations of rectified linear units include leaky ReLU, parametric ReLU or PReLU, and absolute value rectification. </s>\n",
      "<s> [INST] How do maxout units differ from rectified linear units? [/INST] Maxout units are a further generalization of rectified linear units where the input is divided into groups and the maximum element of each group is outputted. </s>\n",
      "<s> [INST] What is the purpose of a maxout unit? [/INST] The purpose of a maxout unit is to learn a piecewise linear function with multiple directions in the input space. </s>\n",
      "<s> [INST] What are the benefits of using maxout units? [/INST] The benefits of using maxout units include the ability to learn the activation function itself, resistance to catastrophic forgetting, and potential reduction in the number of parameters needed. </s>\n",
      "<s> [INST] Why do maxout units typically require more regularization than rectified linear units? [/INST] Maxout units typically require more regularization than rectified linear units because each maxout unit is parametrized by multiple weight vectors. </s>\n",
      "<s> [INST] What is catastrophic forgetting? [/INST] Catastrophic forgetting is a phenomenon in which neural networks forget how to perform tasks that they were trained on in the past. Maxout units with redundancy can help resist catastrophic forgetting. </s>\n",
      "<s> [INST] Why are rectified linear units and their generalizations based on linear behavior? [/INST] Rectified linear units and their generalizations are based on linear behavior because models with linear behavior are easier to optimize. </s>\n",
      "<s> [INST] What are the problems with using sigmoidal activation functions in neural networks? [/INST] The problems with using sigmoidal activation functions in neural networks include their widespread saturation, which makes gradient-based learning difficult. </s>\n",
      "<s> [INST] Which activation function typically performs better than the logistic sigmoid? [/INST] The hyperbolic tangent activation function typically performs better than the logistic sigmoid. </s>\n",
      "<s> [INST] When are sigmoidal activation functions still applicable in neural networks? [/INST] Sigmoidal activation functions are still applicable in neural networks as output units when an appropriate cost function can undo the saturation of the sigmoid in the output layer. </s>\n",
      "<s> [INST] What is the main theorem stated in this text? [/INST] The main theorem stated in the text is that the number of linear regions carved out by a deep rectifier network is exponential in the depth. </s>\n",
      "<s> [INST] What belief does choosing a deep model encode? [/INST] Choosing a deep model encodes a belief that the function to be learned involves composition of several simpler functions, or that it is a computer program consisting of multiple steps. </s>\n",
      "<s> [INST] What has empirical evidence shown about deep architectures? [/INST] Empirical evidence has shown that greater depth in neural networks tends to result in better generalization for a wide variety of tasks. </s>\n",
      "<s> [INST] What are some other architectural considerations in neural networks? [/INST] Some other architectural considerations in neural networks include specialized architectures for specific tasks, such as convolutional networks for computer vision, and recurrent neural networks for sequence processing. </s>\n",
      "<s> [INST] What is the purpose of skip connections in deep feedforward networks? [/INST] Skip connections in deep feedforward networks make it easier for the gradient to flow from output layers to layers nearer the input. </s>\n",
      "<s> [INST] What does increasing the depth of a neural network result in, according to empirical results? [/INST] Empirical results show that increasing the depth of a neural network tends to improve its generalization ability, as demonstrated in the context of transcribing multi-digit numbers from photographs of addresses. </s>\n",
      "<s> [INST] What are some strategies for reducing the number of connections in neural networks? [/INST] Some strategies for reducing the number of connections in neural networks include using specialized patterns of sparse connections, such as in convolutional networks, and connecting each unit in the input layer to only a small subset of units in the output layer. </s>\n",
      "<s> [INST] What are convolutional networks particularly effective for? [/INST] Convolutional networks are particularly effective for computer vision problems. </s>\n",
      "<s> [INST] What is the back-propagation algorithm used for? [/INST] The back-propagation algorithm allows the information from the cost to flow backwards through the network in order to compute the gradient. </s>\n",
      "<s> [INST] What is the back-propagation method used for in neural networks? [/INST] The back-propagation method is used to compute the gradient in neural networks. </s>\n",
      "<s> [INST] What does back-propagation compute? [/INST] Back-propagation computes the gradient. </s>\n",
      "<s> [INST] Is back-propagation specific to multi-layer neural networks? [/INST] No, back-propagation is not specific to multi-layer neural networks. It can compute derivatives of any function. </s>\n",
      "<s> [INST] What is the gradient most often required in learning algorithms? [/INST] The gradient of the cost function with respect to the parameters is most often required. </s>\n",
      "<s> [INST] What can the back-propagation algorithm be used for besides computing the gradient of the cost function? [/INST] The back-propagation algorithm can be used to compute derivatives in other machine learning tasks or to analyze the learned model. </s>\n",
      "<s> [INST] What is a computational graph in the context of neural networks? [/INST] A computational graph is a formalization of computation as a graph, where each node represents a variable and each edge represents an operation. </s>\n",
      "<s> [INST] What is an operation in a computational graph? [/INST] An operation is a simple function of one or more variables in a computational graph. </s>\n",
      "<s> [INST] What does the chain rule of calculus state?  [/INST] The chain rule of calculus states that the derivative of a composition of functions is equal to the product of the derivatives of the individual functions. </s>\n",
      "<s> [INST] What is the purpose of back-propagation in computational graphs? [/INST] The purpose of back-propagation in computational graphs is to compute the gradient of a scalar quantity with respect to the input nodes. </s>\n",
      "<s> [INST] How is the forward propagation computation performed in a computational graph? [/INST] In forward propagation computation, each node in the graph computes a numerical value by applying a function to the set of arguments comprising the values of previous nodes. </s>\n",
      "<s> [INST] What is the relationship between the forward and backward graphs in back-propagation? [/INST] The backward graph in back-propagation proceeds in the reverse order of computation in the forward graph and computes the derivative associated with each forward graph node. </s>\n",
      "<s> [INST] How does the amount of computation required for back-propagation scale with the number of edges in the graph? [/INST] The amount of computation required for back-propagation scales linearly with the number of edges in the graph. </s>\n",
      "<s> [INST] What is the main advantage of the back-propagation algorithm in terms of reducing common subexpressions? [/INST] The main advantage of the back-propagation algorithm is that it reduces the number of common subexpressions, resulting in fewer computations. </s>\n",
      "<s> [INST] What is the computational cost of the back-propagation algorithm? [/INST] The computational cost of the back-propagation algorithm is proportional to the number of edges in the graph. </s>\n",
      "<s> [INST] What is the purpose of the grad_table data structure? [/INST] The grad_table data structure stores the derivatives that have been computed. </s>\n",
      "<s> [INST] How does back-propagation algorithm avoid repeated subexpressions? [/INST] Back-propagation algorithm avoids repeated subexpressions by storing and reusing intermediate values computed during the forward propagation. </s>\n",
      "<s> [INST] What is the alternative approach to storing subexpressions in the back-propagation algorithm? [/INST] The alternative approach to storing subexpressions in the back-propagation algorithm is to recompute them each time they are needed. </s>\n",
      "<s> [INST] What are the modern software implementations based on? [/INST] Modern software implementations are based on the generalized form of back-propagation that can accommodate any computational graph. </s>\n",
      "<s> [INST] What is the purpose of the loss function in a deep neural network? [/INST] The purpose of the loss function in a deep neural network is to quantify the difference between the predicted output and the target output. </s>\n",
      "<s> [INST] What does the regularizer do in the computation of the total cost? [/INST] The regularizer in the computation of the total cost is added to the loss to incorporate a penalty for complex models, helping prevent overfitting. </s>\n",
      "<s> [INST] How are the gradients of the cost function computed with respect to the parameters? [/INST] The gradients of the cost function with respect to the parameters (weights and biases) are computed using the backpropagation algorithm. Starting from the output layer and going backwards, the algorithm computes the gradients on the activations for each layer, which are then used to compute the gradients on weights and biases. </s>\n",
      "<s> [INST] What are symbol-to-symbol derivatives? [/INST] Symbol-to-symbol derivatives refer to the approach of computing derivatives without accessing specific numeric values. Instead, a computational graph is constructed to describe the computation of derivatives symbolically. </s>\n",
      "<s> [INST] How can the gradients on weights and biases be used? [/INST] The gradients on weights and biases can be used for stochastic gradient updates or other gradient-based optimization methods to update the model parameters. </s>\n",
      "<s> [INST] What is the purpose of the back-propagation algorithm? [/INST] The purpose of the back-propagation algorithm is to compute the gradients of variables in a computational graph. </s>\n",
      "<s> [INST] What is the role of the op.bprop method in the back-propagation algorithm? [/INST] The op.bprop method is responsible for implementing the backward propagation rules for each operation in the graph. </s>\n",
      "<s> [INST] How are repeated subexpressions handled in the back-propagation algorithm? [/INST] In order to avoid computing the same subexpression in the chain rule multiple times, the back-propagation algorithm prunes the graph to contain only nodes that are ancestors of the target variable and descendants of the variable to be differentiated. </s>\n",
      "<s> [INST] What is the computational cost of the back-propagation algorithm? [/INST] The computational cost of the back-propagation algorithm is analyzed in terms of the number of operations executed in the graph, which is never more than O(n^2). </s>\n",
      "<s> [INST] What is the back-propagation algorithm? [/INST] The back-propagation algorithm is an algorithm used to train neural networks by computing the gradient of the cost function with respect to the network's parameters. </s>\n",
      "<s> [INST] What is the purpose of the back-propagation algorithm? [/INST] The purpose of the back-propagation algorithm is to compute the gradient of the cost function in order to update the network's parameters and minimize the cost. </s>\n",
      "<s> [INST] What is the computational cost of back-propagation? [/INST] The computational cost of back-propagation is O(n) for most neural network cost functions, which is much more efficient than the naive approach that would require exponentially many nodes. </s>\n",
      "<s> [INST] How does back-propagation avoid recomputation of common subexpressions? [/INST] Back-propagation avoids recomputation of common subexpressions by using a table-filling dynamic programming strategy, where intermediate results are stored in a table and filled in order to avoid repeating computations. </s>\n",
      "<s> [INST] What is the example used to explain the back-propagation algorithm in this text? [/INST] The example used to explain the back-propagation algorithm is training a multilayer perceptron with a single hidden layer using minibatch stochastic gradient descent. </s>\n",
      "<s> [INST] How is the cost function defined in the example? [/INST] The cost function in the example consists of the cross-entropy between the targets and the predicted probability distribution, as well as a weight decay term with coefficient lambda. </s>\n",
      "<s> [INST] What does the weight decay term in the total cost represent? [/INST] The weight decay term in the total cost represents a regularization term that penalizes large weights in the network, helping to prevent overfitting. </s>\n",
      "<s> [INST] What does the computational graph illustrate? [/INST] The computational graph illustrates the flow of computations and dependencies in the neural network, showing how the inputs are transformed through layers of hidden features to produce predictions and compute the cost. </s>\n",
      "<s> [INST] What is the benefit of the back-propagation algorithm? [/INST] The benefit of the back-propagation algorithm is that it can automatically generate gradients that would be tedious to derive manually. </s>\n",
      "<s> [INST] What are the two different paths to compute the gradients on the weights? [/INST] The two different paths to compute the gradients on the weights are through the cross-entropy cost and through the weight decay cost. </s>\n",
      "<s> [INST] What is the computational cost dominated by in the MLP? [/INST] The computational cost in the MLP is dominated by the cost of matrix multiplication. </s>\n",
      "<s> [INST] What is the memory cost of the algorithm? [/INST] The memory cost of the algorithm is O(mnh), where m is the number of examples in the minibatch and nh is the number of hidden units. </s>\n",
      "<s> [INST] What is forward mode accumulation in automatic differentiation? [/INST] Forward mode accumulation is another form of automatic differentiation that is sometimes preferable to use over backward mode. It helps obtain real-time computation of gradients in recurrent networks and avoids the need to store the values and gradients for the whole graph. </s>\n",
      "<s> [INST] How does forward mode computation avoid storing the values and gradients for the whole graph? [/INST] Forward mode computation avoids storing the values and gradients for the whole graph by trading off computational efficiency for memory. It starts multiplying from the left and involves a series of matrix-matrix products. </s>\n",
      "<s> [INST] What is the relationship between forward mode and backward mode in automatic differentiation? [/INST] The relationship between forward mode and backward mode in automatic differentiation is analogous to left-multiplying versus right-multiplying a sequence of matrices. Backward mode corresponds to starting the multiplications from the end and going backwards, while forward mode corresponds to starting the multiplications from the left and going forwards. </s>\n",
      "<s> [INST] Why is it cheaper to run multiplications left-to-right in forward mode? [/INST] If the matrix A has fewer rows than D has columns, it is cheaper to run the multiplications left-to-right in forward mode. </s>\n",
      "<s> [INST] What is the drawback of using specialized libraries for computational graphs in deep learning? [/INST] The drawback of using specialized libraries for computational graphs in deep learning is that it requires the library developer to define the bprop methods for every operation and limits the user to only those operations that have been defined. </s>\n",
      "<s> [INST] Why is back-propagation a practical method for computing gradients in deep learning? [/INST] Back-propagation is a practical method for computing gradients in deep learning because it allows customized back-propagation rules to be developed for each operation, enabling the developer to improve speed or stability in non-obvious ways that an automatic procedure may not replicate. </s>\n",
      "<s> [INST] What are Krylov methods in the context of computing derivatives? [/INST] Krylov methods are iterative techniques used in deep learning for performing operations like approximately inverting a matrix or finding approximations to its eigenvectors or eigenvalues, without using any operation other than matrix-vector products. </s>\n",
      "<s> [INST] How is the Hessian matrix typically handled in deep learning applications? [/INST] In deep learning applications, instead of explicitly computing a single second derivative, the focus is on properties of the Hessian matrix, which is typically too large to represent. Krylov methods are used as an alternative. </s>\n",
      "<s> [INST] What technique is used to compute the product between the Hessian matrix and an arbitrary vector? [/INST] The technique used to compute the product between the Hessian matrix and an arbitrary vector is to compute the gradient of the product of the function f and the vector v, denoted as Hv, using the formula: Hv = ∇(∇xf)(v). </s>\n",
      "<s> [INST] What were some algorithmic changes that improved the performance of neural networks? [/INST] The algorithmic changes that improved the performance of neural networks were the replacement of mean squared error with cross-entropy family of loss functions and the replacement of sigmoid hidden units with rectified linear units. </s>\n",
      "<s> [INST] Why were cross-entropy losses used in place of mean squared error? [/INST] Cross-entropy losses were used in place of mean squared error because they greatly improved the performance of models with sigmoid and softmax outputs, which had issues with saturation and slow learning when using mean squared error. </s>\n",
      "<s> [INST] What were the major changes made to hidden units in feedforward networks? [/INST] The major changes made to hidden units in feedforward networks were the replacement of sigmoid hidden units with piecewise linear hidden units, such as rectified linear units. </s>\n",
      "<s> [INST] Why were rectified linear units avoided in the early 2000s? [/INST] Rectified linear units were avoided in the early 2000s due to a belief that activation functions with non-differentiable points should be avoided. </s>\n",
      "<s> [INST] Why did the use of rectified linear units begin to change around 2009? [/INST] The use of rectified linear units began to change around 2009 when it was observed that using rectifying non-linearities was the most important factor in improving the performance of a recognition system. </s>\n",
      "<s> [INST] What were some properties of biological neurons that influenced the development of rectified linear units? [/INST] Rectified linear units were influenced by biological neurons as they aimed to capture properties such as complete inactivity for some inputs, proportionality of output to input for some inputs, and sparse activations. </s>\n",
      "<s> [INST] What was the perception of feedforward networks from 2006-2012? [/INST] From 2006-2012, feedforward networks were believed to not perform well unless assisted by other models, such as probabilistic models. </s>\n",
      "<s> [INST] How is gradient-based learning in feedforward networks viewed today? [/INST] Today, gradient-based learning in feedforward networks is viewed as a powerful technology that can be used to develop probabilistic models and is no longer seen as an unreliable technology that requires support from other techniques. </s>\n",
      "<s> [INST] What is the purpose of chapter 5 in the book? [/INST] The purpose of chapter 5 is to provide a brief course in the most important general principles of machine learning. </s>\n",
      "<s> [INST] What is the difference between deep learning and machine learning? [/INST] Deep learning is a specific kind of machine learning that requires a solid understanding of the basic principles of machine learning. </s>\n",
      "<s> [INST] What are hyperparameters in machine learning? [/INST] Hyperparameters in machine learning are settings external to the learning algorithm itself that need to be determined. </s>\n",
      "<s> [INST] What are the two central approaches to statistics? [/INST] The two central approaches to statistics are frequentist estimators and Bayesian inference. </s>\n",
      "<s> [INST] What are the categories of machine learning algorithms? [/INST] Machine learning algorithms can be divided into the categories of supervised learning and unsupervised learning. </s>\n",
      "<s> [INST] What is stochastic gradient descent? [/INST] Stochastic gradient descent is an optimization algorithm commonly used in deep learning algorithms. </s>\n",
      "<s> [INST] What are some factors that have limited traditional machine learning? [/INST] Factors such as the ability to generalize have limited traditional machine learning. </s>\n",
      "<s> [INST] What is the definition of a learning algorithm according to Mitchell? [/INST] According to Mitchell, a learning algorithm is an algorithm that learns from experience to improve its performance at tasks. </s>\n",
      "<s> [INST] What is the task in machine learning? [/INST] The task in machine learning refers to what the machine learning system should process in order to perform a specific task. </s>\n",
      "<s> [INST] What are the three types of tasks mentioned in the text? [/INST] The three types of tasks mentioned in the text are imputation of missing values, denoising, and density estimation or probability mass function estimation. </s>\n",
      "<s> [INST] What is the purpose of density estimation? [/INST] The purpose of density estimation is to learn a function that captures the structure of the data and represents it as a probability density function or probability mass function. </s>\n",
      "<s> [INST] How is accuracy measured in tasks like classification? [/INST] Accuracy in tasks like classification is measured by the proportion of examples for which the model produces the correct output. </s>\n",
      "<s> [INST] Why is it important to evaluate a machine learning algorithm on data it has not seen before? [/INST] It is important to evaluate a machine learning algorithm on data it has not seen before because it determines how well it will work in the real world. </s>\n",
      "<s> [INST] What is the common approach to measuring performance in density estimation tasks? [/INST] The common approach to measuring performance in density estimation tasks is to report the average log-probability the model assigns to some examples. </s>\n",
      "<s> [INST] What are the challenges in choosing a performance measure for a system? [/INST] The challenges in choosing a performance measure for a system include deciding what should be measured and making design choices that depend on the application. </s>\n",
      "<s> [INST] What are some design choices when measuring the accuracy of a transcription system? [/INST] When performing a transcription task, one can measure the accuracy of the system at transcribing entire sequences or use a more fine-grained performance measure that gives partial credit. </s>\n",
      "<s> [INST] Why is measuring the actual probability value assigned to a specific point in space intractable in some probabilistic models? [/INST] Measuring the actual probability value assigned to a specific point in space is intractable in many probabilistic models because these models represent probability distributions only implicitly. </s>\n",
      "<s> [INST] What is the difference between unsupervised learning and supervised learning? [/INST] Unsupervised learning involves learning useful properties of the structure of a dataset, while supervised learning involves learning to predict a target value or vector based on a dataset with associated labels. </s>\n",
      "<s> [INST] How does the chain rule of probability relate to machine learning? [/INST] The chain rule of probability allows us to compute the joint probability distribution of a vector by multiplying the conditional probabilities of each element given its predecessors. This relation is used in machine learning to estimate the probability distribution and make predictions. </s>\n",
      "<s> [INST] What is the purpose of the decomposition described in equation (5.1)? [/INST] The purpose of the decomposition described in equation (5.1) is to model the probability distribution p(x) by splitting it into n supervised learning problems. </s>\n",
      "<s> [INST] How does equation (5.2) relate supervised and unsupervised learning? [/INST] Equation (5.2) relates supervised and unsupervised learning by using traditional supervised learning technologies to learn the joint distribution p(x, y) and infer the conditional probability p(y | x). </s>\n",
      "<s> [INST] What are some examples of tasks categorized as supervised learning? [/INST] Examples of tasks categorized as supervised learning include regression, classification, and structured output problems. </s>\n",
      "<s> [INST] What is semi-supervised learning? [/INST] Semi-supervised learning is a learning paradigm where some examples include a supervision target, but others do not. </s>\n",
      "<s> [INST] What is multi-instance learning? [/INST] Multi-instance learning is a learning paradigm where an entire collection of examples is labeled as containing or not containing an example of a class, but the individual members of the collection are not labeled. </s>\n",
      "<s> [INST] What is the difference between supervised and unsupervised learning? [/INST] Supervised learning and unsupervised learning categorize the things done with machine learning algorithms, with supervised learning focusing on tasks like regression and classification, while unsupervised learning is associated with density estimation and support of other tasks. </s>\n",
      "<s> [INST] What is a design matrix? [/INST] A design matrix is a matrix that describes a dataset, with each row representing a different example and each column representing a different feature. </s>\n",
      "<s> [INST] When is it not possible to describe a dataset as a design matrix? [/INST] It is not possible to describe a dataset as a design matrix when the examples in the dataset cannot be represented as vectors of the same size. For example, if a collection of photographs has varying widths and heights, the vector sizes will differ. </s>\n",
      "<s> [INST] What is the role of labels in supervised learning? [/INST] Labels in supervised learning provide target information for each example in the dataset, allowing the learning algorithm to associate the features with the corresponding labels and learn a mapping between them. </s>\n",
      "<s> [INST] What is the Vapnik-Chervonenkis (VC) dimension? [/INST] The Vapnik-Chervonenkis (VC) dimension measures the capacity of a binary classifier by defining the largest possible value of m for which there exists a training set of m different x points that the classifier can label arbitrarily. </s>\n",
      "<s> [INST] Why are the bounds provided by statistical learning theory rarely used in practice for deep learning algorithms? [/INST] The bounds provided by statistical learning theory are rarely used in practice for deep learning algorithms because they are often loose, and it is difficult to determine the capacity of deep learning algorithms. Additionally, there is little theoretical understanding of the non-convex optimization problems involved in deep learning. </s>\n",
      "<s> [INST] Why is determining the capacity of a deep learning model difficult? [/INST] Determining the capacity of a deep learning model is difficult because the effective capacity is limited by the capabilities of the optimization algorithm, and there is little theoretical understanding of the non-convex optimization problems involved in deep learning. </s>\n",
      "<s> [INST] What is the relationship between model capacity and error? [/INST] Typically, training error decreases as model capacity increases until it asymptotes to the minimum possible error value. However, generalization error has a U-shaped curve as a function of model capacity, indicating that there is an optimal capacity point where the gap between training and generalization error is minimized. </s>\n",
      "<s> [INST] What are non-parametric models and how are they different from parametric models? [/INST] Non-parametric models have no fixed size limitation for the parameter vector and their complexity can be a function of the training set size. In contrast, parametric models like linear regression have a fixed-length vector of weights that describe the function being learned. </s>\n",
      "<s> [INST] What is an example of a practical non-parametric model? [/INST] An example of a practical non-parametric model is nearest neighbor regression. Instead of using a fixed-length vector of weights, this model stores the training set and classifies a test point by finding the nearest entry in the training set and returning the associated regression target. </s>\n",
      "<s> [INST] Why is it not appropriate to learn hyperparameters on the training set? [/INST] It is not appropriate to learn hyperparameters on the training set because it would always result in choosing the maximum possible model capacity, leading to overfitting. </s>\n",
      "<s> [INST] What is the purpose of a validation set? [/INST] The purpose of a validation set is to estimate the generalization error during or after training and update the hyperparameters accordingly. </s>\n",
      "<s> [INST] How is the validation set constructed? [/INST] The validation set is constructed by splitting the training data into two disjoint subsets, with one subset used for parameter learning and the other subset used for guiding the selection of hyperparameters. </s>\n",
      "<s> [INST] What is the typical split of training and validation data? [/INST] Typically, around 80% of the training data is used for training and 20% is used for validation. </s>\n",
      "<s> [INST] How does the use of a test set help estimate the generalization error? [/INST] The test set is used to estimate the generalization error after hyperparameter optimization is complete. </s>\n",
      "<s> [INST] Why can evaluations using the same test set become optimistic? [/INST] Evaluations using the same test set can become optimistic over time because different algorithms attempt to beat the reported state-of-the-art performance, leading to biased evaluations. </s>\n",
      "<s> [INST] What is the problem with using a small test set? [/INST] The problem with using a small test set is that it results in statistical uncertainty around the estimated average test error, making it difficult to claim one algorithm works better than another. </s>\n",
      "<s> [INST] What is cross-validation? [/INST] Cross-validation is a procedure used when the dataset is too small to have a fixed training and test set. It allows for the estimation of the mean test error using all of the examples in the dataset. </s>\n",
      "<s> [INST] How does cross-validation work? [/INST] In k-fold cross-validation, the dataset is divided into k non-overlapping subsets. On each trial, one of the subsets is used as the test set and the rest of the data is used as the training set. The test error is then averaged across the k trials. </s>\n",
      "<s> [INST] What is the purpose of k-fold cross-validation? [/INST] The purpose of k-fold cross-validation is to estimate the test error by using all of the examples in the dataset, at the cost of increased computational cost. </s>\n",
      "<s> [INST] What is the definition of bias of an estimator? [/INST] The bias of an estimator is defined as the difference between the expected value of the estimator and the true underlying value. </s>\n",
      "<s> [INST] What does it mean for an estimator to be unbiased? [/INST] An estimator is unbiased if its bias is equal to zero, that is, its expected value is equal to the true underlying value. </s>\n",
      "<s> [INST] What is an asymptotically unbiased estimator? [/INST] An estimator is asymptotically unbiased if its bias approaches zero as the number of samples approaches infinity. </s>\n",
      "<s> [INST] What is the estimator for the θ parameter of a Bernoulli distribution? [/INST] The estimator for the θ parameter of a Bernoulli distribution is the mean of the training samples. </s>\n",
      "<s> [INST] Is the estimator for the θ parameter of a Bernoulli distribution biased? [/INST] No, the estimator for the θ parameter of a Bernoulli distribution is unbiased. </s>\n",
      "<s> [INST] What is the estimator for the mean of a Gaussian distribution? [/INST] The estimator for the mean of a Gaussian distribution is the sample mean. </s>\n",
      "<s> [INST] Is the estimator for the mean of a Gaussian distribution biased? [/INST] Yes, the estimator for the mean of a Gaussian distribution is unbiased. </s>\n",
      "<s> [INST] What are the two estimators of the variance parameter of a Gaussian distribution? [/INST] The two estimators of the variance parameter of a Gaussian distribution are the sample variance and the unbiased sample variance. </s>\n",
      "<s> [INST] What is the variance of an estimator? [/INST] The variance of an estimator is the measure of how much we would expect the estimate to vary as we independently resample the dataset from the underlying data generating process. </s>\n",
      "<s> [INST] What is the standard error of an estimator? [/INST] The standard error of an estimator is the square root of its variance. </s>\n",
      "<s> [INST] Why is it desirable for an estimator to have low variance? [/INST] It is desirable for an estimator to have low variance because it means that we would expect the estimate to have less variability as we resample the dataset, leading to more consistent results. </s>\n",
      "<s> [INST] How is the standard error of the mean useful in machine learning experiments? [/INST] The standard error of the mean is useful in machine learning experiments for estimating the generalization error. It helps in computing the probability that the true expectation falls within a chosen interval. </s>\n",
      "<s> [INST] What is the 95% confidence interval used for? [/INST] The 95% confidence interval is used to determine the range within which the true expectation is likely to fall with 95% confidence. It is centered on the mean estimate and is computed using the standard error. </s>\n",
      "<s> [INST] How can you determine if algorithm A is better than algorithm B in machine learning experiments? [/INST] In machine learning experiments, algorithm A is considered better than algorithm B if the upper bound of the 95% confidence interval for the error of A is less than the lower bound of the 95% confidence interval for the error of B. </s>\n",
      "<s> [INST] What is the Bernoulli distribution? [/INST] The Bernoulli distribution is a discrete probability distribution that models the outcomes of a random variable that can take on one of two possible values, usually represented as 0 and 1. </s>\n",
      "<s> [INST] What is the maximum likelihood principle? [/INST] The maximum likelihood principle is a common principle used to derive specific functions that are good estimators for different models. It is based on finding the parameter values that maximize the likelihood of the observed data. </s>\n",
      "<s> [INST] How is the maximum likelihood estimator defined? [/INST] The maximum likelihood estimator for θ is defined as θML = arg max θ ∏ pmodel(x(i); θ) set of m examples X drawn independently from the true data generating distribution pdata(x). </s>\n",
      "<s> [INST] Why is taking the logarithm of the likelihood convenient? [/INST] Taking the logarithm of the likelihood transforms the product over many probabilities into a sum, which is more convenient for optimization. It does not change the arg max of the likelihood. </s>\n",
      "<s> [INST] How can the criterion of maximum likelihood be expressed as an expectation? [/INST] By rescaling the cost function and dividing by the number of examples, the maximum likelihood criterion can be expressed as an expectation with respect to the empirical distribution defined by the training data. </s>\n",
      "<s> [INST] What is the KL divergence and how is it related to maximum likelihood? [/INST] The KL (Kullback-Leibler) divergence measures the dissimilarity between two distributions. In maximum likelihood, minimizing the KL divergence is equivalent to minimizing the cross-entropy between the empirical distribution and the model distribution. </s>\n",
      "<s> [INST] How is maximum likelihood related to minimizing cross-entropy? [/INST] Maximum likelihood can be interpreted as minimizing the dissimilarity between the empirical distribution and the model distribution, where the dissimilarity is measured by the KL divergence. Cross-entropy is a form of loss that measures the similarity between two distributions, and minimizing it is equivalent to minimizing the KL divergence. </s>\n",
      "<s> [INST] What is the difference between maximizing likelihood and minimizing KL divergence? [/INST] Maximizing likelihood and minimizing KL divergence refer to the same optimization problem, but they have different objective functions. Maximizing likelihood is equivalent to minimizing the negative log-likelihood, while minimizing KL divergence has a known minimum value of zero. </s>\n",
      "<s> [INST] What is the negative log-likelihood (NLL) and how is it related to maximum likelihood? [/INST] The negative log-likelihood (NLL) is the negative logarithm of the likelihood function. In maximum likelihood, minimizing the NLL is equivalent to maximizing the likelihood. </s>\n",
      "<s> [INST] What is the conditional log-likelihood? [/INST] The conditional log-likelihood is a variation of the likelihood function that considers only the probability of the observed outputs given the inputs, rather than the joint probability of the inputs and outputs. </s>\n",
      "<s> [INST] How can the maximum likelihood estimator be generalized to the case of regression problems? [/INST] The maximum likelihood estimator can be generalized to regression problems by considering the conditional log-likelihood, which measures the likelihood of the observed outputs given the inputs and the model parameters. </s>\n",
      "<s> [INST] What is the measure used to determine how close the estimated parameter values are to the true parameter values? [/INST] The measure used is the expected mean squared error. </s>\n",
      "<s> [INST] What does the Cramér-Rao lower bound show? [/INST] The Cramér-Rao lower bound shows that no consistent estimator has a lower mean squared error than the maximum likelihood estimator. </s>\n",
      "<s> [INST] What is maximum likelihood estimation? [/INST] Maximum likelihood estimation is considered the preferred estimator to use for machine learning due to its consistency and efficiency. </s>\n",
      "<s> [INST] How can regularization strategies be used in machine learning? [/INST] Regularization strategies such as weight decay can be used to obtain a biased version of maximum likelihood that has less variance when training data is limited. </s>\n",
      "<s> [INST] What is the difference between frequentist statistics and Bayesian statistics? [/INST] Frequentist statistics consider the true parameter value as fixed but unknown, while Bayesian statistics treat the true parameter as a random variable. </s>\n",
      "<s> [INST] What is the prior probability distribution in Bayesian statistics? [/INST] The prior probability distribution represents the knowledge or uncertainty about the true parameter value before observing any data. </s>\n",
      "<s> [INST] What is Bayes' rule? [/INST] Bayes' rule combines the likelihood of the data with the prior probability distribution to calculate the posterior probability distribution. </s>\n",
      "<s> [INST] How does Bayesian estimation differ from maximum likelihood estimation? [/INST] Unlike maximum likelihood estimation, Bayesian estimation makes predictions using a full distribution over the parameters instead of a point estimate. </s>\n",
      "<s> [INST] What is the Bayesian approach to dealing with uncertainty in a point estimate? [/INST] The Bayesian approach incorporates uncertainty in a point estimate by integrating over it. </s>\n",
      "<s> [INST] What is the criticism of the Bayesian approach regarding the prior distribution? [/INST] Critics argue that the prior in Bayesian methods introduces subjective human judgment to the predictions. </s>\n",
      "<s> [INST] How does the Bayesian approach to estimation differ from the maximum likelihood approach? [/INST] The Bayesian approach incorporates the prior distribution, which shifts probability mass density towards preferred regions of the parameter space. </s>\n",
      "<s> [INST] In Bayesian linear regression, what is the prediction equation? [/INST] In Bayesian linear regression, the prediction is parametrized by the vector w and the equation is ˆy = wx. </s>\n",
      "<s> [INST] What is the prior distribution commonly used for real-valued parameters in Bayesian linear regression? [/INST] A Gaussian distribution is commonly used as the prior distribution for real-valued parameters in Bayesian linear regression. </s>\n",
      "<s> [INST] How is the posterior distribution over model parameters determined in Bayesian linear regression? [/INST] The posterior distribution over model parameters in Bayesian linear regression is determined by multiplying the likelihood function and the prior distribution. </s>\n",
      "<s> [INST] How is the posterior distribution in Bayesian linear regression expressed? [/INST] The posterior distribution in Bayesian linear regression is expressed as a Gaussian distribution. </s>\n",
      "<s> [INST] What does supervised learning involve? [/INST] Supervised learning involves estimating the output (y) based on input (x) using a training set. </s>\n",
      "<s> [INST] What does linear regression correspond to? [/INST] Linear regression corresponds to the family (p(y|x, θ) = θ'x + ε). </s>\n",
      "<s> [INST] How can logistic regression be used for classification? [/INST] Logistic regression can be used for classification by using the logistic sigmoid function to interpret the output of the linear function as a probability. </s>\n",
      "<s> [INST] How is the optimal weight in logistic regression found? [/INST] The optimal weights in logistic regression are found by maximizing the log-likelihood, which can be done by minimizing the negative log-likelihood using gradient descent. </s>\n",
      "<s> [INST] What is the support vector machine (SVM) model? [/INST] The support vector machine (SVM) model is a linear function (w'x + b) that predicts the class identity without providing probabilities. </s>\n",
      "<s> [INST] What is the kernel trick in SVM? [/INST] The kernel trick in SVM allows the replacement of dot products with kernel evaluations, which enables the use of nonlinear functions for predictions. </s>\n",
      "<s> [INST] What is the purpose of the kernel trick in machine learning? [/INST] The purpose of the kernel trick is to allow us to learn nonlinear models as a function of x using convex optimization techniques that are guaranteed to converge efficiently. </s>\n",
      "<s> [INST] What are the advantages of using the Gaussian kernel in kernel-based methods? [/INST] The Gaussian kernel is commonly used because it can perform template matching, where training examples become templates for classes. It assigns a large weight to associated training labels based on the similarity of examples. </s>\n",
      "<s> [INST] What is the role of support vectors in support vector machines? [/INST] Support vectors play a crucial role in support vector machines. They are the training examples that have non-zero αi values and are used to evaluate the decision function. These examples are selected to mitigate the linear computational cost. </s>\n",
      "<s> [INST] What is a drawback of kernel machines in terms of computational cost? [/INST] A drawback of kernel machines is that evaluating the decision function has a linear cost in the number of training examples. However, this can be mitigated by learning an α vector with mostly zeros and only evaluating the kernel function for support vectors. </s>\n",
      "<s> [INST] What are some limitations of generic kernels in kernel machines? [/INST] Generic kernels in kernel machines struggle to generalize well, leading to performance limitations. This is explained further in the text. </s>\n",
      "<s> [INST] What is the purpose of the k-nearest neighbors algorithm? [/INST] The purpose of the k-nearest neighbors algorithm is to perform classification or regression. </s>\n",
      "<s> [INST] How does the k-nearest neighbors algorithm work at test time? [/INST] At test time, the k-nearest neighbors algorithm finds the k nearest neighbors to a new test input and returns the average of the corresponding output values in the training set. </s>\n",
      "<s> [INST] How does the k-nearest neighbors algorithm handle classification? [/INST] In classification, the k-nearest neighbors algorithm can average over one-hot code vectors to obtain a probability distribution over classes. </s>\n",
      "<s> [INST] What does it mean for the k-nearest neighbors algorithm to have high capacity? [/INST] Having high capacity means that the k-nearest neighbors algorithm can achieve high accuracy given a large training set. </s>\n",
      "<s> [INST] What is a weakness of the k-nearest neighbors algorithm? [/INST] A weakness of the k-nearest neighbors algorithm is that it cannot learn which features are more discriminative than others. </s>\n",
      "<s> [INST] What is the problem when using k-nearest neighbors on a regression task with multiple features? [/INST] In a regression task with multiple features, the k-nearest neighbors algorithm may be unable to detect a simple pattern if the relevant feature is overshadowed by other features. </s>\n",
      "<s> [INST] What is the difference between internal nodes and leaf nodes in a decision tree? [/INST] Internal nodes in a decision tree are used to categorize examples and divide the input space, while leaf nodes represent distinct regions and their parameters define the function within that region. </s>\n",
      "<s> [INST] What is the difference between stochastic gradient descent and gradient descent? [/INST] Stochastic gradient descent is an extension of the gradient descent algorithm introduced in section 4.3. </s>\n",
      "<s> [INST] Why is large training set more computationally expensive? [/INST] Large training sets are more computationally expensive because the computational cost of computing the gradient for each example is O(m). </s>\n",
      "<s> [INST] How is the gradient estimate formed in stochastic gradient descent? [/INST] The estimate of the gradient in stochastic gradient descent is formed using a minibatch of examples drawn uniformly from the training set. </s>\n",
      "<s> [INST] What is the learning rate in stochastic gradient descent? [/INST] The learning rate in stochastic gradient descent is denoted by  𝜂 (eta). </s>\n",
      "<s> [INST] What was the perception of gradient descent in the past? [/INST] In the past, gradient descent was regarded as slow or unreliable, especially when applied to non-convex optimization problems. </s>\n",
      "<s> [INST] What does stochastic gradient descent optimize? [/INST] Stochastic gradient descent optimizes the cost function. </s>\n",
      "<s> [INST] What is the cost per SGD update dependent on? [/INST] The cost per SGD update does not depend on the training set size m. </s>\n",
      "<s> [INST] What happens to the number of updates required to reach convergence as the training set size increases? [/INST] The number of updates required to reach convergence usually increases with training set size. </s>\n",
      "<s> [INST] What is the asymptotic cost of training a model with SGD as the training set size approaches infinity? [/INST] The asymptotic cost of training a model with SGD is O(1/m). </s>\n",
      "<s> [INST] Why was deep learning initially interesting in academia? [/INST] Deep learning was initially interesting in academia because it was able to generalize to new examples better than competing algorithms when trained on medium-sized datasets. </s>\n",
      "<s> [INST] What is the recipe for constructing a learning algorithm? [/INST] The recipe for constructing a learning algorithm involves combining a dataset, a cost function, an optimization procedure, and a model. </s>\n",
      "<s> [INST] What are the limitations of local template matching? [/INST] Local template matching fails in cases where deep models succeed. </s>\n",
      "<s> [INST] What is the disadvantage of decision trees in smoothness-based learning? [/INST] Decision trees break the input space into as many regions as there are leaves. </s>\n",
      "<s> [INST] How many training examples are generally needed to fit a tree accurately? [/INST] If a tree with at least n leaves is required to represent the target function accurately, then at least n training examples are needed. </s>\n",
      "<s> [INST] What is the relationship between the number of regions in input space and the number of examples needed? [/INST] All of these methods require O(k) examples to distinguish O(k) regions in input space. </s>\n",
      "<s> [INST] How does the smoothness assumption affect learning algorithms? [/INST] The smoothness assumption helps learning algorithms observe high points on most peaks and low points on most valleys of the true underlying function. </s>\n",
      "<s> [INST] What happens when the function to be learned is complicated in high dimensions? [/INST] In high dimensions, even a smooth function can change smoothly but in a different way along each dimension. </s>\n",
      "<s> [INST] Is it possible to generalize well in a complicated function with few examples? [/INST] Generalizing well in a complicated function with few examples can be challenging. </s>\n",
      "<s> [INST] What is the key insight in representing a complicated function efficiently? [/INST] The key insight is that a very large number of regions can be defined with a small number of examples by introducing dependencies between the regions via additional assumptions about the underlying data generating distribution. </s>\n",
      "<s> [INST] What are some assumptions made by deep learning algorithms? [/INST] Deep learning algorithms make implicit or explicit assumptions about the composition of factors or features at multiple levels in a hierarchy. </s>\n",
      "<s> [INST] How do deep learning algorithms counter the curse of dimensionality? [/INST] Deep learning algorithms use deep and distributed representations which confer exponential advantages to counter the exponential challenges posed by the curse of dimensionality. </s>\n",
      "<s> [INST] What is a manifold in the context of machine learning? [/INST] In the context of machine learning, a manifold is a connected region of points with a neighborhood around each point. It can be loosely used to designate a connected set of points that can be approximated well by considering a small number of dimensions embedded in a higher-dimensional space. </s>\n",
      "<s> [INST] How do manifold learning algorithms surmount the obstacle of learning functions across all of Rn? [/INST] Manifold learning algorithms overcome the obstacle of learning functions across all of Rn by allowing the dimensionality of the manifold to vary from one point to another, which often happens when a manifold intersects itself. </s>\n",
      "<s> [INST] What assumption is made in manifold learning? [/INST] The assumption made in manifold learning is that the data lies along a low-dimensional manifold. </s>\n",
      "<s> [INST] Why is the assumption that the data lies along a low-dimensional manifold useful in AI tasks? [/INST] The assumption that the data lies along a low-dimensional manifold is useful in AI tasks involving processing images, sounds, or text because it is approximately correct for these domains. </s>\n",
      "<s> [INST] What is the first observation in favor of the manifold hypothesis? [/INST] The first observation in favor of the manifold hypothesis is that the probability distribution over real-life images, text strings, and sounds is highly concentrated, while uniformly sampled points resemble noise rather than structured inputs. </s>\n",
      "<s> [INST] What is the second argument in favor of the manifold hypothesis? [/INST] The second argument in favor of the manifold hypothesis is that there are neighborhoods and transformations that connect similar examples, allowing traversal of the manifold. For example, in the case of images, we can imagine transformations like dimming or brightening lights, moving or rotating objects, and altering colors on object surfaces. </s>\n",
      "<s> [INST] What experiments support the manifold hypothesis? [/INST] Experiments by Cayton, Narayanan et al., Mitter, Schölkopf, Brand, Belkin and Niyogi, Donoho and Grimes, and Weinberger and Saul support the manifold hypothesis for a large class of datasets in AI. </s>\n",
      "<s> [INST] Why is it natural to represent data in terms of coordinates on the manifold? [/INST] When the data lies on a low-dimensional manifold, it is more natural for machine learning algorithms to represent the data in terms of coordinates on the manifold rather than in terms of coordinates in Rn. </s>\n",
      "<s> [INST] What are some common operations that require numerical computation in machine learning algorithms? [/INST] Some common operations that require numerical computation in machine learning algorithms include optimization, solving systems of linear equations, and evaluating mathematical functions. </s>\n",
      "<s> [INST] What is rounding error and why is it problematic? [/INST] Rounding error refers to the approximation error incurred when representing real numbers in a finite number of bit patterns. It can be problematic because it accumulates across many operations and can cause algorithms to fail if not managed properly. </s>\n",
      "<s> [INST] What is underflow and why is it damaging? [/INST] Underflow occurs when numbers near zero are rounded to zero. It is damaging because many functions behave differently when their argument is zero rather than a small positive number. Division by zero or taking the logarithm of zero can lead to undefined results. </s>\n",
      "<s> [INST] What is overflow and why is it damaging? [/INST] Overflow occurs when numbers with large magnitude are approximated and usually result in infinite or not-a-number values. It is damaging because it can lead to undefined results and affect the stability of numerical calculations. </s>\n",
      "<s> [INST] Why do we need to stabilize the softmax function against underflow and overflow? [/INST] The softmax function needs to be stabilized against underflow and overflow because its denominator can become zero or undefined when the input vector has large magnitudes. This can lead to division by zero or undefined results in the softmax calculation. </s>\n",
      "<s> [INST] How can underflow and overflow be resolved when evaluating the softmax function? [/INST] Underflow and overflow when evaluating the softmax function can be resolved by subtracting the maximum value from the input vector before applying the softmax function. This ensures that the exponential terms do not result in underflow or overflow, making the calculation numerically stable. </s>\n",
      "<s> [INST] What are some popular approaches to choosing the learning rate? [/INST] Some popular approaches to choosing the learning rate are setting it to a small constant or solving for the step size that makes the directional derivative vanish. </s>\n",
      "<s> [INST] What is line search? [/INST] Line search is a strategy where different values of the learning rate are evaluated and the one resulting in the smallest objective function value is chosen. </s>\n",
      "<s> [INST] What does steepest descent convergence depend on? [/INST] Steepest descent convergence depends on every element of the gradient being zero (or very close to zero). </s>\n",
      "<s> [INST] How can gradient descent be generalized to discrete spaces? [/INST] Gradient descent can be generalized to discrete spaces by ascending an objective function of discrete parameters, also known as hill climbing. </s>\n",
      "<s> [INST] What is a Jacobian matrix? [/INST] A Jacobian matrix is a matrix containing all the partial derivatives of a function whose input and output are both vectors. </s>\n",
      "<s> [INST] What does the second derivative of a function measure? [/INST] The second derivative of a function measures how the first derivative will change as the input varies. </s>\n",
      "<s> [INST] What does the second derivative tell us about curvature? [/INST] The second derivative determines the curvature of a function. If the second derivative is negative, the function curves downward and the cost function will decrease by more than the gradient alone would predict. If the second derivative is positive, the function curves upward and the cost function can decrease by less than the gradient predicts. </s>\n",
      "<s> [INST] What is the Hessian matrix and how is it defined? [/INST] The Hessian matrix is a matrix containing all the second derivatives of a function with multiple input dimensions. It is defined as the Jacobian of the gradient. </s>\n",
      "<s> [INST] What is Newton's method based on? [/INST] Newton's method is based on using a second-order Taylor series expansion to approximate a function near some point. </s>\n",
      "<s> [INST] When is Newton's method most effective? [/INST] Newton's method is most effective when the nearby critical point is a minimum. </s>\n",
      "<s> [INST] What is a first-order optimization algorithm? [/INST] A first-order optimization algorithm is an optimization algorithm that uses only the gradient. </s>\n",
      "<s> [INST] What property does Lipschitz continuity guarantee? [/INST] Lipschitz continuity guarantees that a small change in the input will result in a small change in the output. </s>\n",
      "<s> [INST] What guarantees do convex optimization algorithms provide? [/INST] Convex optimization algorithms provide more guarantees by making strong restrictions and are only applicable to convex functions. </s>\n",
      "<s> [INST] What is the importance of convex optimization in deep learning? [/INST] The importance of convex optimization in deep learning is greatly diminished, as most problems in deep learning are difficult to express in terms of convex optimization. </s>\n",
      "<s> [INST] What is constrained optimization? [/INST] Constrained optimization is finding the maximal or minimal value of a function while considering constraints on the input values. </s>\n",
      "<s> [INST] How are feasible points defined in constrained optimization? [/INST] Feasible points in constrained optimization are the points that lie within the set of constraints. </s>\n",
      "<s> [INST] What is one approach for constrained optimization? [/INST] One approach for constrained optimization is modifying gradient descent to take the constraints into account. </s>\n",
      "<s> [INST] How can the efficiency of the approach be improved? [/INST] The efficiency of the approach can be improved by projecting the gradient into the tangent space of the feasible region. </s>\n",
      "<s> [INST] What is the Karush-Kuhn-Tucker (KKT) approach to constrained optimization? [/INST] The Karush-Kuhn-Tucker (KKT) approach is a general solution to constrained optimization. </s>\n",
      "<s> [INST] How is the generalized Lagrangian defined in the KKT approach? [/INST] The generalized Lagrangian is defined as the sum of the objective function, the products of Lagrange multipliers and equality constraints, and the products of Lagrange multipliers and inequality constraints. </s>\n",
      "<s> [INST] How can a constrained minimization problem be solved using unconstrained optimization? [/INST] A constrained minimization problem can be solved using unconstrained optimization of the generalized Lagrangian. </s>\n",
      "<s> [INST] What properties guarantee that the optimal solution of a constrained problem is within the feasible points? [/INST] The properties guarantee that the optimal solution of a constrained problem is within the feasible points, and only feasible points can be optimal. </s>\n",
      "<s> [INST] What is the formulation for constrained maximization? [/INST] The formulation for constrained maximization involves minimizing the negative of the objective function while considering the Lagrange multipliers and inequality constraints. </s>\n",
      "<s> [INST] How are active constraints defined in the KKT approach? [/INST] Active constraints in the KKT approach are the constraints that are satisfied with equality at the optimal solution. </s>\n",
      "<s> [INST] What are Karush-Kuhn-Tucker (KKT) conditions? [/INST] Karush-Kuhn-Tucker (KKT) conditions are necessary conditions for a point to be optimal in constrained optimization problems. </s>\n",
      "<s> [INST] What does it mean for a constraint to be inactive? [/INST] An inactive constraint does not affect the solution to the problem and can be removed without changing the local optimality of the solution. </s>\n",
      "<s> [INST] What is the purpose of the Lagrangian in optimization? [/INST] The Lagrangian is introduced in optimization to handle constraints by combining the objective function and constraints into a single function. </s>\n",
      "<s> [INST] How can the gradient of the Lagrangian be used in optimization? [/INST] The gradient of the Lagrangian can be used to find the stationary points in optimization by setting it to zero and solving the resulting equations. </s>\n",
      "<s> [INST] How can the Moore-Penrose pseudoinverse be used to solve the least squares problem? [/INST] The Moore-Penrose pseudoinverse can be used to find the smallest-norm solution to the unconstrained least squares problem. </s>\n",
      "<s> [INST] What happens when we increase λ in the Lagrangian? [/INST] When we increase λ in the Lagrangian, the coefficient on the xx penalty increases, resulting in a solution with smaller norm for x. </s>\n",
      "<s> [INST] Why do we solve the linear equation for x? [/INST] We solve the linear equation for x to obtain a solution with the correct norm. </s>\n",
      "<s> [INST] How do we determine if x has the correct norm? [/INST] We determine if x has the correct norm by checking if the derivative on λ is 0. </s>\n",
      "<s> [INST] What is the purpose of the mathematical preliminaries in developing machine learning algorithms? [/INST] The purpose of the mathematical preliminaries is to provide the necessary foundation for building and analyzing machine learning algorithms. </s>\n",
      "<s> [INST] What is the next step after the mathematical preliminaries? [/INST] The next step after the mathematical preliminaries is to build and analyze some full-fledged learning systems. </s>\n",
      "<s> [INST] Is the text a transcription of slides in the PDF file format? [/INST] The given context does not provide information about the format of the text transcription. </s>\n",
      "<s> [INST] What is one perspective on deep learning? [/INST] One perspective on deep learning is that it involves learning the right representation of the data. </s>\n",
      "<s> [INST] What does each layer of the representation in deep learning represent? [/INST] Each layer of the representation in deep learning represents a new set of instructions or features extracted from the input. </s>\n",
      "<s> [INST] What is the advantage of networks with greater depth? [/INST] Networks with greater depth can execute more instructions in sequence, providing more power. </s>\n",
      "<s> [INST] How does deep learning break down the complicated mapping from raw sensory input data? [/INST] Deep learning breaks the complicated mapping from raw sensory input data by breaking it into a series of nested simple mappings, each described by a different layer of the model. </s>\n",
      "<s> [INST] Why are the hidden layers in deep learning called 'hidden'? [/INST] The hidden layers in deep learning are called 'hidden' because their values are not given in the data; instead, the model determines which concepts are useful for explaining relationships in the observed data. </s>\n",
      "<s> [INST] What do the images in the figure represent? [/INST] The images in the figure represent visualizations of the features represented by each hidden unit in the deep learning model. </s>\n",
      "<s> [INST] What is the purpose of state information in the representation of deep learning? [/INST] State information in the representation of deep learning helps the model organize its processing, similar to a counter or pointer in a traditional computer program. </s>\n",
      "<s> [INST] What are the two main ways of measuring the depth of a model? [/INST] The two main ways of measuring the depth of a model are based on the number of sequential instructions that must be executed to evaluate the architecture, and the definition of what constitutes a possible computational step. </s>\n",
      "<s> [INST] 1. What is the depth of a model in the context of flowcharts? [/INST] 1. The depth of a model in the context of flowcharts refers to the longest path through the flowchart that describes how to compute each output given the inputs. </s>\n",
      "<s> [INST] 2. What is another approach to measuring the depth of a model? [/INST] 2. Another approach to measuring the depth of a model is to consider the depth of the graph describing how concepts are related to each other. </s>\n",
      "<s> [INST] 3. Can the depth of a model vary depending on the choice of language? [/INST] 3. Yes, the depth of a model can vary depending on the choice of language used to represent the model. </s>\n",
      "<s> [INST] 4. How is the depth of a model in deep probabilistic models defined? [/INST] 4. In deep probabilistic models, the depth of the model is determined by the depth of the graph describing how concepts are related to each other. </s>\n",
      "<s> [INST] 5. Why can the depth of the flowchart of computations be much deeper than the graph of concepts? [/INST] 5. The depth of the flowchart of computations can be much deeper than the graph of concepts because the system's understanding of simpler concepts can be refined given information about more complex concepts. </s>\n",
      "<s> [INST] 6. Can the depth of an architecture have a single correct value? [/INST] 6. The depth of an architecture does not have a single correct value. </s>\n",
      "<s> [INST] 7. What is deep learning? [/INST] 7. Deep learning is an approach to AI and a type of machine learning that involves a greater amount of composition of learned functions or learned concepts than traditional machine learning. </s>\n",
      "<s> [INST] 8. Who are the main target audiences of this book on deep learning? [/INST] 8. The main target audiences of this book on deep learning are university students (undergraduate or graduate) learning about machine learning, and software engineers who want to rapidly acquire a machine learning and statistics background. </s>\n",
      "<s> [INST] 9. What is the relationship between deep learning, representation learning, machine learning, and AI? [/INST] 9. Deep learning is a kind of representation learning, which is a kind of machine learning. Machine learning is used for many, but not all, approaches to AI. </s>\n",
      "<s> [INST] What are the different parts of an AI system? [/INST] The different parts of an AI system include input, simple features, features, hand-designed features, hand-designed program, additional layers of more abstract features, mapping from features, output, rule-based systems, classic machine learning, and deep learning. </s>\n",
      "<s> [INST] What are some of the disciplines that AI is applied to? [/INST] AI is applied to many disciplines including computer vision, speech and audio processing, natural language processing, robotics, bioinformatics and chemistry, video games, search engines, online advertising, and finance. </s>\n",
      "<s> [INST] How is the book organized? [/INST] The book is organized into three parts. Part I introduces basic mathematical tools and machine learning concepts. Part II describes the most established deep learning algorithms and modern practices. Part III discusses more speculative ideas that are believed to be important for future research in deep learning. </s>\n",
      "<s> [INST] What background knowledge is assumed for the readers? [/INST] The readers are assumed to have a computer science background and familiarity with programming, computational performance issues, complexity theory, introductory level calculus, and some of the terminology of graph theory. </s>\n",
      "<s> [INST] What are some historical trends in deep learning? [/INST] Some historical trends in deep learning include: deep learning has had a long and rich history, deep learning has become more useful as the amount of available training data has increased, deep learning models have grown in size over time, and deep learning has solved increasingly complicated applications with increasing accuracy over time. </s>\n",
      "<s> [INST] What are the three waves of development of deep learning? [/INST] The three waves of development of deep learning are deep learning known as cybernetics in the 1940s–1960s, deep learning known as connectionism in the 1980s–1990s, and the current resurgence under the name deep learning beginning in 2006. </s>\n",
      "<s> [INST] What is the neural perspective on deep learning? [/INST] The neural perspective on deep learning is that deep learning models are engineered systems inspired by the biological brain. </s>\n",
      "<s> [INST] What is the motivation behind the neural perspective on deep learning? [/INST] The motivation behind the neural perspective on deep learning is that the brain provides proof that intelligent behavior is possible and reverse engineering the brain's computational principles can lead to building intelligence. Additionally, understanding the brain and principles behind human intelligence is also a scientific interest. </s>\n",
      "<s> [INST] What is the modern term 'deep learning' based on? [/INST] The modern term 'deep learning' is based on the principle of learning multiple levels of composition, which can be applied in machine learning frameworks that are not necessarily neurally inspired. </s>\n",
      "<s> [INST] What were the earliest predecessors of modern deep learning? [/INST] The earliest predecessors of modern deep learning were simple linear models motivated from a neuroscientific perspective. </s>\n",
      "<s> [INST] What was the first wave of neural networks research called? [/INST] The first wave of neural networks research was called cybernetics. </s>\n",
      "<s> [INST] What is the McCulloch-Pitts Neuron? [/INST] The McCulloch-Pitts Neuron was an early model of brain function. </s>\n",
      "<s> [INST] What were the two categories of inputs the McCulloch-Pitts Neuron could recognize? [/INST] The McCulloch-Pitts Neuron could recognize two categories of inputs by testing whether the output of the model is positive or negative. </s>\n",
      "<s> [INST] What inspires modern deep learning? [/INST] Modern deep learning draws inspiration from many fields, especially applied math fundamentals like linear algebra, probability, information theory, and numerical optimization. </s>\n",
      "<s> [INST] What is computational neuroscience primarily concerned with? [/INST] Computational neuroscience is primarily concerned with building more accurate models of how the brain actually works. </s>\n",
      "<s> [INST] What is connectionism in the context of cognitive science? [/INST] Connectionism in the context of cognitive science refers to the idea that a large number of simple computational units can achieve intelligent behavior when networked together. </s>\n",
      "<s> [INST] What is the concept of distributed representation? [/INST] The concept of distributed representation is the idea that each input to a system should be represented by many features, and each feature should be involved in the representation of many possible inputs. </s>\n",
      "<s> [INST] What is the concept of distributed representation? [/INST] The concept of distributed representation refers to the representation of objects based on features obtained from multiple categories and not just one specific category. </s>\n",
      "<s> [INST] How did the connectionist movement use back-propagation? [/INST] The connectionist movement successfully used back-propagation to train deep neural networks with internal representations. Back-propagation algorithm is the dominant approach to training deep models. </s>\n",
      "<s> [INST] What did researchers in the 1990s accomplish in sequence modeling? [/INST] In the 1990s, researchers identified some fundamental difficulties in modeling long sequences and introduced the long short-term memory (LSTM) network to resolve these difficulties. The LSTM is widely used for many sequence modeling tasks. </s>\n",
      "<s> [INST] What caused a decline in the popularity of neural networks until 2007? [/INST] The decline in the popularity of neural networks until 2007 can be attributed to the unrealistic claims made by ventures based on neural networks and the advances made by other fields of machine learning, particularly kernel machines and graphical models. </s>\n",
      "<s> [INST] What led to a breakthrough in neural networks research in 2007? [/INST] The breakthrough in neural networks research in 2007 was the successful training of deep belief networks using a strategy called greedy layer-wise pre-training. This strategy was popularized by Geoﬀrey Hinton. </s>\n",
      "<s> [INST] What is the significance of the term 'deep learning'? [/INST] The term 'deep learning' was popularized during the third wave of neural networks research to emphasize the ability to train deeper neural networks than before and highlight the theoretical importance of depth. </s>\n",
      "<s> [INST] What was the focus of deep learning research during the third wave? [/INST] During the third wave of deep learning research, the focus shifted dramatically. It began with a focus on improving generalization on test examples and outperforming other AI systems. However, the specific focus changed over time and was not mentioned in the text. </s>\n",
      "<s> [INST] What is the trend in model size over time? [/INST] The trend in model size over time has been an increase due to the availability of faster CPUs. </s>\n",
      "<s> [INST] What is the significance of larger datasets in deep learning? [/INST] Larger datasets have allowed for more advanced deep learning techniques and have changed what is possible with deep learning. </s>\n",
      "<s> [INST] What is the MNIST dataset used for? [/INST] The MNIST dataset is used for handwriting digit recognition. </s>\n",
      "<s> [INST] What has been an important trend in the history of deep learning? [/INST] The increasing availability of faster network connectivity and better software infrastructure for distributed computing is an important trend in the history of deep learning. </s>\n",
      "<s> [INST] How has deep learning improved since the 1980s? [/INST] Deep learning has consistently improved in its ability to provide accurate recognition or prediction since the 1980s. </s>\n",
      "<s> [INST] What is the largest contest in object recognition called? [/INST] The largest contest in object recognition is the ImageNet Large Scale Visual Recognition Challenge (ILSVRC). </s>\n",
      "<s> [INST] What was the top-5 error rate in the ImageNet Large Scale Visual Recognition Challenge in 2012? [/INST] The top-5 error rate in the ImageNet Large Scale Visual Recognition Challenge in 2012 was 15.3%. </s>\n",
      "<s> [INST] What is the latest top-5 error rate in the ImageNet Large Scale Visual Recognition Challenge? [/INST] The latest top-5 error rate in the ImageNet Large Scale Visual Recognition Challenge is 3.6%. </s>\n",
      "<s> [INST] What impact did deep learning have on speech recognition? [/INST] Deep learning resulted in a sudden drop of error rates in speech recognition. </s>\n",
      "<s> [INST] What are some other areas where deep learning has had successes? [/INST] Deep learning has also had successes in pedestrian detection, image segmentation, and traﬃc sign classiﬁcation. </s>\n",
      "<s> [INST] What are some applications of deep learning? [/INST] Some applications of deep learning include image recognition, speech recognition, natural language processing, machine translation, and reinforcement learning. </s>\n",
      "<s> [INST] Which companies use deep learning? [/INST] Companies such as Google, Microsoft, Facebook, IBM, Baidu, Apple, Adobe, Netflix, NVIDIA, and NEC use deep learning. </s>\n",
      "<s> [INST] How has deep learning contributed to other sciences? [/INST] Deep learning has contributed to other sciences by providing tools for processing massive amounts of data and making predictions. It has been used for predicting molecular interactions, designing new drugs, searching for subatomic particles, and analyzing microscope images. </s>\n",
      "<s> [INST] What factors have contributed to the growth of deep learning? [/INST] The growth of deep learning has been influenced by more powerful computers, larger datasets, and techniques for training deeper networks. </s>\n",
      "<s> [INST] What are some challenges and opportunities for deep learning in the future? [/INST] The years ahead present challenges and opportunities for improving deep learning further and exploring new frontiers. </s>\n",
      "<s> [INST] What is the growth rate of artificial neural networks since the introduction of hidden units? [/INST] Artificial neural networks have doubled in size roughly every 2.4 years since the introduction of hidden units. </s>\n",
      "<s> [INST] What are some early developments in neural networks? [/INST] Some early developments in neural networks include the perceptron, adaptive linear element, neocognitron, early back-propagation network, and recurrent neural network for speech recognition. </s>\n",
      "<s> [INST] What is the purpose of the mentioned deep networks? [/INST] The purpose of the mentioned deep networks is speech recognition, belief network modeling, image classification, and unsupervised learning. </s>\n",
      "<s> [INST] What is shown in Figure 1.12? [/INST] Figure 1.12 shows the consistent winning streak of deep networks in the ImageNet Large Scale Visual Recognition Challenge. </s>\n",
      "<s> [INST] What is the significance of winning the ImageNet challenge? [/INST] Winning the ImageNet challenge is significant as it reflects the ability of deep networks to outperform other methods in image classification tasks. </s>\n",
      "<s> [INST] What is the trend of error rates in the ImageNet challenge over the years? [/INST] The trend of error rates in the ImageNet challenge has been consistently decreasing over the years, indicating improving performance of deep networks. </s>\n",
      "<s> [INST] What is linear algebra? [/INST] Linear algebra is a branch of mathematics widely used in science and engineering. </s>\n",
      "<s> [INST] Why is linear algebra important for understanding deep learning? [/INST] Linear algebra is important for understanding deep learning because many machine learning algorithms, especially deep learning algorithms, rely on linear algebra. </s>\n",
      "<s> [INST] What are the types of mathematical objects studied in linear algebra? [/INST] The types of mathematical objects studied in linear algebra are scalars, vectors, matrices, and tensors. </s>\n",
      "<s> [INST] What is a scalar? Give an example. [/INST] A scalar is a single number. For example, the slope of a line or the number of units can be scalar values. </s>\n",
      "<s> [INST] What is a vector? How are the elements of a vector identified? [/INST] A vector is an array of numbers where each number is identified by its index. The elements of a vector are identified using subscripts. For example, x1 represents the first element, x2 represents the second element, and so on. </s>\n",
      "<s> [INST] What is a matrix? How are the elements of a matrix identified? [/INST] A matrix is a 2-D array of numbers where each element is identified by two indices. Matrices are usually denoted by upper-case letters. The elements of a matrix are identified using subscripts with separating commas. For example, A1,1 represents the upper left entry of the matrix. </s>\n",
      "<s> [INST] What does the transpose of a matrix represent? [/INST] The transpose of a matrix is a mirror image of the matrix across its main diagonal. </s>\n",
      "<s> [INST] How do you denote the transpose of a matrix A? [/INST] The transpose of a matrix A is denoted as A᾿. </s>\n",
      "<s> [INST] What is the definition of the transpose of a matrix A? [/INST] The transpose of a matrix A is defined as (A᾿)ᷘ,ᷖ = Aᷗ,ᷘ. </s>\n",
      "<s> [INST] How do you represent an element at coordinates (i, j, k) of a tensor A? [/INST] An element at coordinates (i, j, k) of a tensor A is represented as Ai,j,k. </s>\n",
      "<s> [INST] What is the transpose of a vector? [/INST] The transpose of a vector is a matrix with only one row. </s>\n",
      "<s> [INST] What is the element-wise product of matrices A and B? [/INST] The element-wise product of matrices A and B is denoted as A ⊗ B. </s>\n",
      "<s> [INST] What is the dot product between two vectors x and y? [/INST] The dot product between two vectors x and y is represented as x᾿ y. </s>\n",
      "<s> [INST] What properties do matrix products have? [/INST] Matrix products are distributive and associative, but matrix multiplication is not commutative. </s>\n",
      "<s> [INST] Is matrix multiplication commutative? [/INST] No, matrix multiplication is not commutative. </s>\n",
      "<s> [INST] What is the transpose of a matrix product AB? [/INST] The transpose of a matrix product AB is denoted as (AB)᾿ = B᾿A᾿. </s>\n",
      "<s> [INST] What is the span of a set of vectors? [/INST] The span of a set of vectors is the set of all points obtainable by linear combination of the original vectors. </s>\n",
      "<s> [INST] What is the column space of a matrix? [/INST] The column space of a matrix is the set of all points that can be obtained by linear combination of the columns of the matrix. </s>\n",
      "<s> [INST] What condition should be met for the system Ax=b to have a solution for all values of b? [/INST] For the system Ax=b to have a solution for all values of b in Rm, the column space of A must be all of Rm. </s>\n",
      "<s> [INST] What is linear dependence? [/INST] Linear dependence is when a vector in a set is a linear combination of the other vectors in the set. </s>\n",
      "<s> [INST] What condition must be met for a matrix to have an inverse? [/INST] For a matrix to have an inverse, it must be square and have linearly independent columns. </s>\n",
      "<s> [INST] What is a norm? [/INST] A norm is a function that measures the size of a vector. </s>\n",
      "<s> [INST] What is the L2 norm? [/INST] The L2 norm is the Euclidean norm, which is the Euclidean distance from the origin to the point identified by the vector x. </s>\n",
      "<s> [INST] What is the squared L2 norm? [/INST] The squared L2 norm is the square of the L2 norm of a vector, denoted as ||x||2^2 = x^T * x. </s>\n",
      "<s> [INST] Why is the squared L2 norm more convenient to work with mathematically and computationally? [/INST] The squared L2 norm is more convenient to work with mathematically and computationally because the derivatives of the squared L2 norm depend only on the corresponding element of the vector, whereas the derivatives of the L2 norm depend on the entire vector. </s>\n",
      "<s> [INST] When is the L1 norm commonly used in machine learning? [/INST] The L1 norm is commonly used in machine learning when the difference between zero and nonzero elements is very important. </s>\n",
      "<s> [INST] What is the L1 norm often used as a substitute for? [/INST] The L1 norm is often used as a substitute for the number of nonzero entries in a vector. </s>\n",
      "<s> [INST] What is the L∞ norm also known as? [/INST] The L∞ norm is also known as the max norm. </s>\n",
      "<s> [INST] What is the Frobenius norm? [/INST] The Frobenius norm of a matrix is analogous to the L2 norm of a vector and is calculated as the square root of the sum of the squares of all the elements in the matrix. </s>\n",
      "<s> [INST] What is a diagonal matrix? [/INST] A diagonal matrix consists mostly of zeros and has non-zero entries only along the main diagonal. </s>\n",
      "<s> [INST] Why are diagonal matrices of interest? [/INST] Diagonal matrices are of interest because multiplying by a diagonal matrix is computationally efficient, and inverting a square diagonal matrix is also efficient. </s>\n",
      "<s> [INST] What is a symmetric matrix? [/INST] A symmetric matrix is any matrix that is equal to its own transpose, denoted as A = A^T. </s>\n",
      "<s> [INST] What is the purpose of decomposing a matrix into eigenvalues and eigenvectors? [/INST] The purpose of decomposing a matrix into eigenvalues and eigenvectors is to analyze certain properties of the matrix and understand its behavior. It is analogous to decomposing an integer into its prime factors to understand its behavior. </s>\n",
      "<s> [INST] Can every matrix be decomposed into eigenvalues and eigenvectors? [/INST] Not every matrix can be decomposed into eigenvalues and eigenvectors. In some cases, the decomposition may involve complex numbers instead of real numbers. </s>\n",
      "<s> [INST] What class of matrices can be decomposed using only real-valued eigenvectors and eigenvalues? [/INST] Every real symmetric matrix can be decomposed into a expression using real-valued eigenvectors and eigenvalues. </s>\n",
      "<s> [INST] What does it mean if two or more eigenvectors share the same eigenvalue? [/INST] If two or more eigenvectors share the same eigenvalue, any set of orthogonal vectors lying in their span are also eigenvectors with that eigenvalue. </s>\n",
      "<s> [INST] What useful facts about a matrix does the eigendecomposition provide? [/INST] The eigendecomposition of a matrix provides useful facts such as determining if the matrix is singular (any eigenvalue is zero), optimizing quadratic expressions, finding maximum and minimum eigenvalues, and classifying the matrix as positive definite, positive semidefinite, negative definite, or negative semidefinite. </s>\n",
      "<s> [INST] What is a positive definite matrix? [/INST] A positive definite matrix is a matrix whose eigenvalues are all positive. </s>\n",
      "<s> [INST] Why is the singular value decomposition (SVD) more generally applicable than the eigendecomposition? [/INST] The singular value decomposition (SVD) is more generally applicable than the eigendecomposition because every real matrix has an SVD, while the same is not true for the eigenvalue decomposition. The SVD can be used even for non-square matrices. </s>\n",
      "<s> [INST] What is the purpose of the singular value decomposition (SVD)? [/INST] The purpose of the singular value decomposition (SVD) is to factorize a matrix into singular vectors and singular values. It helps in discovering information about the matrix similar to eigendecomposition. </s>\n",
      "<s> [INST] What are the special structures of the matrices U, V, and D in the SVD? [/INST] The matrices U and V in the SVD are orthogonal matrices, while the matrix D is a diagonal matrix. </s>\n",
      "<s> [INST] What are the left-singular vectors of matrix A? [/INST] The left-singular vectors of matrix A are the eigenvectors of AA^T. </s>\n",
      "<s> [INST] What are the right-singular vectors of matrix A? [/INST] The right-singular vectors of matrix A are the eigenvectors of A^TA. </s>\n",
      "<s> [INST] What are the non-zero singular values of matrix A? [/INST] The non-zero singular values of matrix A are the square roots of the eigenvalues of A^TA. </s>\n",
      "<s> [INST] What is the Moore-Penrose pseudoinverse of matrix A? [/INST] The Moore-Penrose pseudoinverse of matrix A is obtained using the formula A^+ = VD^+U^T, where U, D, and V are the singular value decomposition of matrix A. </s>\n",
      "<s> [INST] How is the pseudoinverse of a diagonal matrix obtained? [/INST] The pseudoinverse of a diagonal matrix D is obtained by taking the reciprocal of its non-zero elements and then taking the transpose of the resulting matrix. </s>\n",
      "<s> [INST] What does the trace operator give? [/INST] The trace operator gives the sum of all the diagonal entries of a matrix. </s>\n",
      "<s> [INST] What is the determinant of a square matrix? [/INST] The determinant of a square matrix is a function that maps the matrix to a real scalar and is equal to the product of all the eigenvalues of the matrix. </s>\n",
      "<s> [INST] What does the determinant of a matrix represent? [/INST] The determinant of a matrix represents how much multiplication by the matrix expands or contracts space. If the determinant is 0, space is contracted completely along at least one dimension, losing all volume. If the determinant is 1, the transformation preserves volume. </s>\n",
      "<s> [INST] What is the purpose of lossy compression in principal components analysis (PCA)? [/INST] The purpose of lossy compression in PCA is to store the points in a way that requires less memory but may lose some precision. </s>\n",
      "<s> [INST] How can we encode the points in PCA? [/INST] The points can be encoded by representing them in a lower-dimensional space. </s>\n",
      "<s> [INST] What is the decoding function in PCA? [/INST] The decoding function in PCA uses matrix multiplication to map the code back into the original space. </s>\n",
      "<s> [INST] What constraint does PCA apply to the columns of the decoding matrix? [/INST] PCA constrains the columns of the decoding matrix to be orthogonal to each other. </s>\n",
      "<s> [INST] How is the optimal code point determined in PCA? [/INST] The optimal code point in PCA is determined by minimizing the distance between the input point and its reconstruction using the L2 norm. </s>\n",
      "<s> [INST] What is the equation for the L2 norm in PCA? [/INST] The equation for the L2 norm in PCA is ||x - g(c)||^2. </s>\n",
      "<s> [INST] How is the optimization problem of finding the optimal code point solved? [/INST] The optimization problem of finding the optimal code point in PCA is solved using vector calculus. </s>\n",
      "<s> [INST] What are the encoder and decoder functions in PCA? [/INST] The encoder function in PCA is f(x) = Dx, where D is the encoding matrix. </s>\n",
      "<s> [INST] How is the PCA reconstruction operation defined? [/INST] The decoder function in PCA is g(c) = DD^T x, where D is the decoding matrix. </s>\n",
      "<s> [INST] How do we choose the encoding matrix in PCA? [/INST] The PCA reconstruction operation is defined as r(x) = g(f(x)) = DD^T x. </s>\n",
      "<s> [INST] What is probability theory used for in artificial intelligence? [/INST] Probability theory is used in artificial intelligence to represent uncertain statements, quantify uncertainty, and derive new uncertain statements. </s>\n",
      "<s> [INST] How does probability theory help in analyzing proposed AI systems? [/INST] Probability theory helps in analyzing proposed AI systems by providing laws of probability that guide reasoning and by using probability and statistics to analyze behavior. </s>\n",
      "<s> [INST] What is information theory used for? [/INST] Information theory is used to quantify the amount of uncertainty in a probability distribution. </s>\n",
      "<s> [INST] Why does machine learning make heavy use of probability theory? [/INST] Machine learning makes heavy use of probability theory because it deals with uncertain and stochastic quantities, which can arise from inherent stochasticity, incomplete observability, and incomplete modeling. </s>\n",
      "<s> [INST] What are the three possible sources of uncertainty? [/INST] The three possible sources of uncertainty are inherent stochasticity in the system being modeled, incomplete observability, and incomplete modeling. </s>\n",
      "<s> [INST] What is the purpose of discretization in the context of the robot's prediction? [/INST] The purpose of discretization in the context of the robot's prediction is to make the robot uncertain about the precise position of objects. </s>\n",
      "<s> [INST] Why is it more practical to use a simple but uncertain rule? [/INST] It is more practical to use a simple but uncertain rule because it is cheaper to develop and broadly useful compared to a complex but certain rule. </s>\n",
      "<s> [INST] How does probability theory help in representing and reasoning about uncertainty in AI applications? [/INST] Probability theory helps in representing and reasoning about uncertainty in AI applications by providing tools to quantify degrees of belief and compute probabilities based on evidence. </s>\n",
      "<s> [INST] What is the difference between frequentist probability and Bayesian probability? [/INST] Frequentist probability is related to the rates at which events occur, while Bayesian probability represents qualitative levels of certainty. </s>\n",
      "<s> [INST] How is probability theory related to logic? [/INST] Probability theory can be seen as the extension of logic to deal with uncertainty by providing formal rules for determining the likelihood of propositions being true. </s>\n",
      "<s> [INST] What is a random variable? [/INST] A random variable is a variable that can take on different values randomly. </s>\n",
      "<s> [INST] What is a random variable? [/INST] A random variable is a description of the possible states that can occur. </s>\n",
      "<s> [INST] What is the difference between a discrete random variable and a continuous random variable? [/INST] A discrete random variable has a finite or countably infinite number of states, while a continuous random variable is associated with a real value. </s>\n",
      "<s> [INST] How can a probability distribution be described? [/INST] A probability distribution describes how likely a random variable or set of random variables is to take on each of its possible states. </s>\n",
      "<s> [INST] What is a probability mass function? [/INST] A probability mass function (PMF) is a way to describe a probability distribution over discrete variables. </s>\n",
      "<s> [INST] How is the probability of a specific state denoted in a probability mass function? [/INST] The probability of a specific state in a probability mass function is denoted as P(x). </s>\n",
      "<s> [INST] What is a joint probability distribution? [/INST] A joint probability distribution is a probability distribution over multiple variables. </s>\n",
      "<s> [INST] What properties must a probability mass function satisfy? [/INST] A probability mass function must satisfy the following properties: the domain must be the set of all possible states, the probability of any state must be greater than or equal to 0, the sum of all probabilities must be 1, and no state can have a greater chance of occurring than 1. </s>\n",
      "<s> [INST] How can a uniform distribution be placed on a discrete random variable? [/INST] A uniform distribution can be placed on a discrete random variable by setting the probability mass function to 1/k for all states. </s>\n",
      "<s> [INST] What is a probability density function? [/INST] A probability density function (PDF) is used to describe a probability distribution for continuous random variables. </s>\n",
      "<s> [INST] What properties must a probability density function satisfy? [/INST] A probability density function must satisfy the following properties: the domain must be the set of all possible states, the density at any state must be greater than or equal to 0, and the integral of the density function over all states must be 1. </s>\n",
      "<s> [INST] What does the integral of the density function represent? [/INST] The integral of the density function represents the actual probability mass of a set of points. </s>\n",
      "<s> [INST] How can we ensure that there is no probability mass outside an interval? [/INST] To ensure that there is no probability mass outside an interval, we set the density function to 0 for all values outside the interval. </s>\n",
      "<s> [INST] What is the formula for calculating marginal probability for discrete variables? [/INST] The formula for calculating marginal probability for discrete variables is P(x) = ∑ P(x, y) for all values of y. </s>\n",
      "<s> [INST] How is the conditional probability computed? [/INST] The conditional probability is computed using the formula P(y = y | x = x) = P(x = x, y = y) / P(x = x). </s>\n",
      "<s> [INST] What is the chain rule of conditional probabilities? [/INST] The chain rule of conditional probabilities states that any joint probability distribution can be decomposed into conditional distributions over one variable. The formula is P(x1, ..., xn) = P(x1) * P(x2 | x1) * ... * P(xn | x1, ..., xn-1). </s>\n",
      "<s> [INST] When are two random variables considered independent? [/INST] Two random variables x and y are considered independent if their probability distribution can be expressed as the product of two factors, one involving only x and one involving only y. </s>\n",
      "<s> [INST] What is the expectation or expected value of a function f(x) with respect to a probability distribution P(x)? [/INST] The expectation or expected value of a function f(x) with respect to a probability distribution P(x) is the average or mean value that f takes on when x is drawn from the distribution. </s>\n",
      "<s> [INST] How is the expectation computed for discrete variables? [/INST] For discrete variables, the expectation is computed by summing the product of each value of x and its probability: Ex[f(x)] = summation(x * P(x) * f(x)). </s>\n",
      "<s> [INST] How is the expectation computed for continuous variables? [/INST] For continuous variables, the expectation is computed by integrating the product of each value of x and its probability: Ex[f(x)] = integral(x * p(x) * f(x) dx). </s>\n",
      "<s> [INST] What does the variance measure? [/INST] The variance gives a measure of how much the values of a function of a random variable x vary as we sample different values of x from its probability distribution. </s>\n",
      "<s> [INST] What does the covariance measure? [/INST] The covariance gives a measure of how much two values are linearly related to each other, as well as the scale of these variables. </s>\n",
      "<s> [INST] What does a high absolute value of covariance mean? [/INST] A high absolute value of covariance means that the values change very much and are both far from their respective means at the same time. </s>\n",
      "<s> [INST] What is the relationship between covariance and dependence? [/INST] Covariance and dependence are related but distinct concepts. Two variables that are independent have zero covariance, but two variables with non-zero covariance can still be dependent. Independence is a stronger requirement than zero covariance. </s>\n",
      "<s> [INST] Give an example of two dependent variables with zero covariance. [/INST] For example, if we sample a real number x uniformly from the interval [1, 1], and then sample a random variable y such that y = s*x, where s is chosen to be 1 or -1 with equal probability, x and y would be dependent but have zero covariance. </s>\n",
      "<s> [INST] What is the Bernoulli Distribution? [/INST] The Bernoulli Distribution is a distribution over a single binary random variable that gives the probability of the random variable being equal to 1. It is controlled by a parameter φ. </s>\n",
      "<s> [INST] How is the variance of a random variable defined? [/INST] The variance of a random variable is defined as the diagonal elements of the covariance matrix. For a variable xi, Cov(xi, xi) gives the variance. </s>\n",
      "<s> [INST] What is the multinoulli distribution? [/INST] The multinoulli distribution is a distribution over a single discrete variable with k different states, where k is finite. </s>\n",
      "<s> [INST] Why are the Bernoulli and multinoulli distributions sufficient to describe any distribution over their domain? [/INST] The Bernoulli and multinoulli distributions are sufficient to describe any distribution over their domain because they are able to model discrete variables for which it is feasible to enumerate all states. </s>\n",
      "<s> [INST] What is the Gaussian distribution? [/INST] The Gaussian distribution, also known as the normal distribution, is the most commonly used distribution over real numbers. </s>\n",
      "<s> [INST] What parameters control the Gaussian distribution? [/INST] The Gaussian distribution is controlled by two parameters: mean (µ) and standard deviation (σ). </s>\n",
      "<s> [INST] Why is the normal distribution a good default choice when there is no prior knowledge about the form of a distribution over real numbers? [/INST] The normal distribution is a good default choice when there is no prior knowledge about the form of a distribution over real numbers because it exhibits a classic 'bell curve' shape and is a sensible choice for many applications. </s>\n",
      "<s> [INST] What does the central limit theorem show? [/INST] The central limit theorem shows that the sum of many independent random variables is approximately normally distributed. </s>\n",
      "<s> [INST] What is the significance of the normal distribution? [/INST] The normal distribution can model many complicated systems that can be decomposed into parts with more structured behavior. </s>\n",
      "<s> [INST] What is the multivariate normal distribution? [/INST] The multivariate normal distribution is a generalization of the normal distribution to multiple dimensions. </s>\n",
      "<s> [INST] What is the exponential distribution used for? [/INST] The exponential distribution is used to have a sharp point at x=0 in a probability distribution. </s>\n",
      "<s> [INST] What is the Laplace distribution used for? [/INST] The Laplace distribution is used to place a sharp peak of probability mass at an arbitrary point. </s>\n",
      "<s> [INST] What is the Dirac distribution used for? [/INST] The Dirac distribution is used to specify that all of the mass in a probability distribution clusters around a single point. </s>\n",
      "<s> [INST] What is an empirical distribution? [/INST] An empirical distribution is a distribution that is based on a given dataset or collection of samples. </s>\n",
      "<s> [INST] Why is the Dirac delta function used in empirical distributions? [/INST] The Dirac delta function is used in empirical distributions over continuous variables to define in infinitely narrow and high peak of probability mass at a specific point. </s>\n",
      "<s> [INST] What is the difference between the Dirac delta distribution and the empirical distribution? [/INST] The Dirac delta distribution is a mathematical object defined using the Dirac delta function, while the empirical distribution is a distribution based on a dataset. </s>\n",
      "<s> [INST] What is the empirical distribution? [/INST] The empirical distribution is formed from a dataset of training examples and specifies the distribution that we sample from when training a model on this dataset. </s>\n",
      "<s> [INST] What is a mixture distribution? [/INST] A mixture distribution combines several component distributions and the choice of which component distribution generates the sample is determined by sampling a component identity from a multinoulli distribution. </s>\n",
      "<s> [INST] What is a latent variable? [/INST] A latent variable is a random variable that cannot be observed directly. In the context of the mixture model, the component identity variable is an example of a latent variable. </s>\n",
      "<s> [INST] What is a Gaussian mixture model? [/INST] A Gaussian mixture model is a type of mixture model where the components are Gaussians. It consists of separately parametrized means and covariances for each component, as well as prior probabilities given to each component. </s>\n",
      "<s> [INST] What is the logistic sigmoid function used for? [/INST] The logistic sigmoid function is commonly used to produce the parameter of a Bernoulli distribution in deep learning models. It is defined as the function 1 / (1 + exp(-x)). </s>\n",
      "<s> [INST] What is the sigmoid function? [/INST] The sigmoid function is a mathematical function that maps any real number to a value between 0 and 1. </s>\n",
      "<s> [INST] What is the range of values for the phi parameter in the sigmoid function? [/INST] The range of values for the phi parameter in the sigmoid function is (0,1). </s>\n",
      "<s> [INST] What is the softplus function used for? [/INST] The softplus function is used for producing the beta or sigma parameter of a normal distribution. It also arises commonly when manipulating expressions involving sigmoids. </s>\n",
      "<s> [INST] What are the properties of the softplus function? [/INST] Some properties of the softplus function are: sigma(x) = exp(x)/(1 + exp(x)), x(σ(x)) = (1 - σ(x))σ(x), σ(-x) = 1 - σ(x), log(1 + exp(x)) = σ(x), log(exp(x) - 1) = x </s>\n",
      "<s> [INST] What is Bayes' rule used for? [/INST] Bayes' rule is used to compute the conditional probability of an event based on prior knowledge of related events. It allows us to update our beliefs or knowledge about an event given new evidence. </s>\n",
      "<s> [INST] What is the basic intuition behind information theory? [/INST] The basic intuition behind information theory is that learning about an unlikely event is more informative than learning about a likely event. </s>\n",
      "<s> [INST] How is self-information defined in information theory? [/INST] Self-information of an event x is defined as -log(P(x)) in nats. </s>\n",
      "<s> [INST] What is the Shannon entropy used to quantify? [/INST] The Shannon entropy quantifies the amount of uncertainty in an entire probability distribution. It represents the expected amount of information in an event drawn from that distribution. </s>\n",
      "<s> [INST] How is the KL divergence used to measure the difference between two distributions? [/INST] The KL divergence measures the extra amount of information needed to send a message containing symbols drawn from distribution P when using a code designed for distribution Q. </s>\n",
      "<s> [INST] What is the KL divergence and how is it conceptualized? [/INST] The KL divergence measures the difference between two probability distributions and is often conceptualized as measuring some sort of distance between them. </s>\n",
      "<s> [INST] Why is the KL divergence not a true distance measure? [/INST] The KL divergence is not a true distance measure because it is not symmetric. DKL(PQ) is not equal to DKL(QP). </s>\n",
      "<s> [INST] What is Shannon entropy? How does it relate to the probability distribution? [/INST] Shannon entropy is a measure of the uncertainty or randomness in a probability distribution. When the distribution is close to deterministic, the entropy is low, and when it is close to uniform, the entropy is high. </s>\n",
      "<s> [INST] What is cross-entropy and how is it related to KL divergence? [/INST] Cross-entropy is similar to KL divergence but lacks a certain term. Minimizing the cross-entropy is equivalent to minimizing the KL divergence. </s>\n",
      "<s> [INST] What is the significance of the asymmetry in KL divergence? [/INST] The asymmetry in KL divergence means that there are important consequences in choosing the direction to use. It depends on whether the application requires a high probability where the true distribution has high probability or low probability where the true distribution has low probability. </s>\n",
      "<s> [INST] How can the KL divergence be minimized? [/INST] The KL divergence can be minimized by choosing the distribution q that is closest to the distribution p. </s>\n",
      "<s> [INST] What is the effect of minimizing DKL(pq)? [/INST] Minimizing DKL(pq) results in selecting a q that has high probability where p has high probability, potentially blurring multiple modes together. Minimizing DKL(qp) results in selecting a q that has low probability where p has low probability, potentially emphasizing a single mode. </s>\n",
      "<s> [INST] What is the effect of minimizing DKL(qp)? [/INST] Minimizing DKL(qp) can result in choosing a single mode to avoid putting probability mass in low-probability areas between modes of p. </s>\n",
      "<s> [INST] What are factorizations used for in probability distributions? [/INST] Factorizations are used as a language to describe probability distributions. </s>\n",
      "<s> [INST] Are directed and undirected graphical models mutually exclusive? [/INST] No, directed and undirected graphical models are not mutually exclusive. </s>\n",
      "<s> [INST] Is being directed or undirected a property of a probability distribution? [/INST] No, being directed or undirected is a property of a particular description of a probability distribution. </s>\n",
      "<s> [INST] What does the undirected graphical model in Figure 3.8 represent? [/INST] The undirected graphical model in Figure 3.8 represents a probability distribution that can be factored as (a b c d e) = 1/Z * φ(1)(a b c) * φ(2)(b d) * φ(3)(c e). </s>\n",
      "<s> [INST] How are a and c related in the undirected graphical model of Figure 3.8? [/INST] In the undirected graphical model of Figure 3.8, a and c interact directly. </s>\n"
     ]
    }
   ],
   "source": [
    "# Generate instruction set\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "row_result = {\"prompt\":[],\"questions\":[],\"answers\":[]}\n",
    "\n",
    "for i, r in enumerate(responses):\n",
    "    try:\n",
    "        tmp = json.loads(r)\n",
    "       \n",
    "\n",
    "        for j in range(len(tmp[\"questions\"])):\n",
    "            if type(tmp[\"questions\"][j])!=dict:\n",
    "                instr = f\"<s> [INST] {tmp['questions'][j]} [/INST] {tmp['answers'][j]} </s>\"\n",
    "                row_result[\"questions\"].append(tmp['questions'][j])\n",
    "                row_result[\"answers\"].append(tmp['answers'][j])\n",
    "                row_result[\"prompt\"].append(instr)\n",
    "            else:\n",
    "                if \"question\" in tmp['questions'][j] and \"answer\" in tmp['question'][j]:\n",
    "                    instr = f\"<s> [INST] {tmp['questions'][j]['question']} [/INST] {tmp['questions'][j]['answer']} </s>\"\n",
    "                    row_result[\"questions\"].append(tmp['questions'][j])\n",
    "                    row_result[\"answers\"].append(tmp['answers'][j])\n",
    "                    row_result[\"prompt\"].append(instr)\n",
    "        \n",
    "                # print(instr)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "instructions = pd.DataFrame(row_result)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "for index, row in instructions.iterrows():\n",
    "    print(row[\"prompt\"])\n",
    "\n",
    "\n",
    "# print(instructions.iloc[0][\"Instructions\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "915"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection closed.\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/314521\n"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           source  page  \\\n",
      "0     DeepLearning-chapter-10.pdf     0   \n",
      "1     DeepLearning-chapter-10.pdf     0   \n",
      "2     DeepLearning-chapter-10.pdf     0   \n",
      "3     DeepLearning-chapter-10.pdf     0   \n",
      "4     DeepLearning-chapter-10.pdf     0   \n",
      "...                           ...   ...   \n",
      "9566          01_introduction.pdf    78   \n",
      "9567          01_introduction.pdf    79   \n",
      "9568          01_introduction.pdf    79   \n",
      "9569          01_introduction.pdf    80   \n",
      "9570          01_introduction.pdf    80   \n",
      "\n",
      "                                                content  \n",
      "0     [Chapter 10  Sequence Modeling: Recurrent and ...  \n",
      "1     [Chapter 10  Sequence Modeling: Recurrent and ...  \n",
      "2     [Chapter 10  Sequence Modeling: Recurrent and ...  \n",
      "3     [Chapter 10  Sequence Modeling: Recurrent and ...  \n",
      "4     [Chapter 10  Sequence Modeling: Recurrent and ...  \n",
      "...                                                 ...  \n",
      "9566  [Cross-Entropy - Example  ▶ Six tosses of a co...  \n",
      "9567  [References  ▶ Ian Goodfellow et al., Deep Lea...  \n",
      "9568  [References  ▶ Ian Goodfellow et al., Deep Lea...  \n",
      "9569  [Questions?  Acknowledgements Some of the pict...  \n",
      "9570  [Questions?  Acknowledgements Some of the pict...  \n",
      "\n",
      "[9571 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# emb = {\"source\":[],\"page\":[],\"content\":[]}\n",
    "# for e in embeddings:\n",
    "#     for idx,t in enumerate(embeddings[e][\"text\"]):\n",
    "#         emb[\"source\"].append(e)\n",
    "#         emb[\"page\"].append(idx+1)\n",
    "#         emb[\"content\"].append(t)\n",
    "\n",
    "\n",
    "embedding_df = pd.DataFrame(emb)\n",
    "\n",
    "print(embedding_df)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/314521/fs/314440/fg/515417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 9571/9571 | Elapsed Time: 00:39 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: embeddings_3_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai/p/314521/jobs/named/embeddings_3_offline_fg_materialization/executions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<hsfs.core.job.Job at 0x7fa287daa0d0>, None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs = project.get_feature_store()\n",
    "emb_fg = fs.get_or_create_feature_group(\n",
    "    name=\"embeddings\",\n",
    "    version=3,\n",
    "    primary_key=list(embedding_df), \n",
    "    description=\"Content of each page of each file\")\n",
    "emb_fg.insert(embedding_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 915/915 | Elapsed Time: 00:07 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: instructionset_4_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai/p/314521/jobs/named/instructionset_4_offline_fg_materialization/executions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<hsfs.core.job.Job at 0x7fa6f7aa9e50>, None)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs = project.get_feature_store()\n",
    "instructions_fg = fs.get_or_create_feature_group(\n",
    "    name=\"instructionset\",\n",
    "    version=4,\n",
    "    primary_key=list(instructions), \n",
    "    description=\"Instruction Set for fine tuning of llms\")\n",
    "instructions_fg.insert(instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(embedding_df['source'].isin(['02_serverless_ml.pdf','Python']))\n",
    "\n",
    "fs = project.get_feature_store()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
