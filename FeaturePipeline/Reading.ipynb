{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "- https://pythonhosted.org/PyDrive/quickstart.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "\n",
    "\n",
    "# gauth = GoogleAuth()\n",
    "# gauth.CommandLineAuth()\n",
    "\n",
    "# drive = GoogleDrive(gauth)\n",
    "gauth = GoogleAuth()\n",
    "# Try to load saved client credentials\n",
    "gauth.LoadCredentialsFile(\"credentials.json\")\n",
    "if gauth.credentials is None:\n",
    "    # Authenticate if they're not there\n",
    "    gauth.LocalWebserverAuth()\n",
    "elif gauth.access_token_expired:\n",
    "    # Refresh them if expired\n",
    "    gauth.Refresh()\n",
    "else:\n",
    "    # Initialize the saved creds\n",
    "    gauth.Authorize()\n",
    "# Save the current credentials to a file\n",
    "gauth.SaveCredentialsFile(\"credentials.json\")\n",
    "\n",
    "drive = GoogleDrive(gauth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file from GDrive in the folder \"slides\" (with id folder_id)\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "folder_id = os.environ[\"FOLDER_ID\"]\n",
    "\n",
    "file_list = drive.ListFile({'q': f\"'{folder_id}' in parents and trashed=false\"}).GetList() \n",
    "new_files = []\n",
    "for file in file_list:\n",
    "    if not os.path.isfile(f\"{file['title']}\"): \n",
    "        # print(\"title: %s, id: %s\" % (file[\"title\"],file[\"id\"]))\n",
    "        file.GetContentFile(file[\"title\"])\n",
    "        new_files.append(file)\n",
    "\n",
    "# new_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'02_serverless_ml.pdf': {'text': ['Serverless Machine Learning\\nJim Dowling\\njdowling@kth.se\\n2022-11-04', 'Enterprise AI Value Chain\\n1 / 54', 'Modern Enterprise Data and ML Infrastructure\\n2 / 54', 'Monolithic ML Pipeline\\n3 / 54', 'Problems with Monolithic ML Pipelines\\n▶They are often not modular - their components are not modular and cannot be\\nindependently scaled or deployed on different hardware (e.g., CPUs for feature engi-\\nneering, GPUs for model training).\\n▶They are difficult to test - production software needs automated tests to ensure\\nfeatures and models are of high quality.\\n▶They tightly couple the execution of feature engineering, model training, and infer-\\nence steps - running them in the same pipeline program at the same time.\\n▶They do not promote reuse of features/models/code. The code for computing fea-\\ntures (feature logic) cannot be easily disentangled from its pipeline jungle.\\n4 / 54', 'Modularity enables more Robust and Scalable Systems\\nModular water pipes in a Google Datacenter. Instead of one giant water pipe (our\\nmonolithic notebook), separate water pipes reduce the blast radius if one fails. Color\\ncoding makes it easier to debug problems in a damaged water pipe.\\n5 / 54', 'Pipelines as Modular Programs\\n▶Modularity involves structuring your code such that its functionality is separated into\\nindependent classes and/or functions that can be more easily reused and tested.\\n▶Modules should be placed in accessible classes or functions, keeping them small and\\neasy to understand and document.\\n▶Modules enable code to be more easily reused in different pipelines.\\n▶Modules enable code to be more easily independently tested, enabling the easier and\\nearlier discovery of bugs.\\n6 / 54', 'Supervised ML Pipeline Stages\\ntrain (features ,labels )−>model\\nmodel (features )−>predictions\\n7 / 54', 'ML Pipeline Stages in a Serverless Machine Learning System\\n8 / 54', 'ML Pipeline Stages - Data Sources\\n9 / 54', 'Connect to Data Sources and Read Raw Data\\n▶Discover data sources, securely connect to heterogeneous data sources\\n▶Manage dependencies such as connectors and drivers\\n▶Manage connection information securely: network endpoint, database/table names,\\nauthentication credentials such as API keys or credentials (username/password)\\n10 / 54', 'Heterogeneous Data Sources\\n11 / 54', 'File Formats for different Data Sources\\n12 / 54', 'ML Pipeline Stages - Feature Pipelines\\n13 / 54', 'Feature Pipelines\\n14 / 54', 'Feature Pipelines\\n▶A feature pipeline is a program that orchestrates the execution of feature engineering\\nsteps on input data to create feature values.\\nExamples of feature engineering steps:\\n▶Clean, validate, data\\n▶Data de-duplication, pseudononymization, data wrangling\\n▶Feature extraction, aggregations, dimensionality reduction, feature binning, feature\\ncrosses\\n15 / 54', 'Tabular Data\\n16 / 54', 'Tabular Data as Features, Labels, Entity (or Primary) Keys,\\nEvent Time\\n17 / 54', 'Tabular Data in Pandas\\n18 / 54', 'Exploratory Data Analysis in Pandas\\n19 / 54', 'Aggregations in Pandas\\n20 / 54', 'Rolling Windows in Pandas\\n21 / 54', 'Feature binning\\n22 / 54', 'Feature Crosses\\n▶A feature cross is a synthetic feature formed by multiplying (crossing) two or more\\nfeatures. By multiplying features together, you encode nonlinearity in the feature\\nspace.\\n▶For example, imagine we are looking for credit card fraud activity within a geographic\\nregion (e.g., a city district), how would we capture that as a feature?\\n▶We could cross to a geographic area (binned latitude and binned longitude - a grid\\nidentifying a city district) with the level of credit card activity within that geographic\\narea.\\n23 / 54', 'Embeddings as Features\\n▶An embedding is a lower dimension representation of a sparse input that retains some\\nof the semantics of the input.\\n▶An embedding store (vector database) stores semantically similar inputs close to-\\ngether in the embedding space. You can implement “similarity search” by finding\\nembeddings close in embedding space. You can even apply arithmetic on embeddings\\nto discover semantic relationships.\\n24 / 54', 'ML Pipeline Stages - Feature Store\\n25 / 54', 'Store Features\\nThere are two general ways people manage features and labels for both training and\\nserving:\\n▶(1) Compute features on-demand as part of the model training or batch inference\\npipeline.\\n▶(2) Use a feature store to store the features so that they can be reused across\\ndifferent models for both training and inference. For online models that require\\nfeatures with either historical or contextual information , feature stores are typically\\nused.\\n26 / 54', 'ML Pipeline Stages - Training Pipelines\\n27 / 54', 'Feature Types\\nReference: https://www.hopsworks.ai/post/feature-types-for-machine-learning\\n28 / 54', 'Feature Types Taxonomy\\n29 / 54', 'Model Training Pipelines\\n30 / 54', 'Model-Dependent Transformations\\nReference: https://developers.google.com/machine-learning/data-\\nprep/transform/introduction\\n31 / 54', 'Transformations in Pandas\\n32 / 54', 'Different types of Transformations\\n33 / 54', 'Model Training with Train and Test Sets\\n34 / 54', 'Model Training with Train and Test Sets in Scikti-Learn\\n35 / 54', 'Model Training is an Iterative Process\\n36 / 54', 'Model-Centric Iteration to Improve Model Performance\\nPossible steps to improve your model performance:\\n▶Try out a different supervised ML learning algorithm (e.g., random forest, feedforward\\ndeep neural network, Gradient-boosted decision tree)\\n▶Try out new combinations of hyperparameters (e.g., number of training epochs, the\\nlearning rate, number of layers in a deep neural network, adjust regularizations such\\nas Dropout or BatchNorm)\\n▶Evaluate your model on a validation set (keeping a separate holdout test set for final\\nmodel performance evaluation)\\n37 / 54', 'Data-Centric Iteration to Improve Model Performance\\nSteps to improve your model\\n▶Add or remove features to or from your model (feature selection)\\n▶Add more training data\\n▶Remove poor quality training samples\\n▶Improve the quality of existing training samples (e.g., using Cleanlab or Snorkel)\\n▶Rank the importance of the training samples (Active Learning)\\n38 / 54', 'Train, Validation, and Test Sets\\n▶Random splits of the training data when the data is not time-series data\\n▶Time-series splits of the training data when the data is time-series data\\n39 / 54', 'Model Training is an Iterative Process\\n40 / 54', 'ML Pipeline Stages - Inference Pipelines\\n41 / 54', 'Batch Inference Pipeline\\n42 / 54', 'Online Inference Pipeline\\n43 / 54', 'Serverless ML with Python\\n▶Write Feature, Training, and Inference Pipelines in Python\\n▶Orchestrate the execution of Pipelines using Serverless Compute Platforms\\n▶Store features and models in a serverless feature/model store\\n▶Run a User Interface (UI), written in Python, on serverless infrastructure\\n44 / 54', 'Serverless Compute Platforms\\n45 / 54', 'Serverless Feature Stores and Model Registry/Serving\\nFeature Stores\\n▶Hopsworks\\nModel Registry and Serving\\n▶Hopsworks\\n▶AWS Sagemaker\\n▶Databricks\\n▶Google Vertex\\n46 / 54', 'Serverless User Interfaces\\n▶Hugging Faces Spaces\\n▶Streamlit Cloud\\n47 / 54', 'Iris Flower Dataset\\nhttps://github.com/ID2223KTH/id2223kth.github.io/tree/master/src/serverless-ml-\\nintro\\n▶4 input features: sepal length, sepal width, petal length, petal width\\n▶label (target): Iris Flower Type (one of Setosa, Versicolor, Virginica)\\n▶Only 150 samples in the dataset\\n48 / 54', 'Serverless Iris with Modal, Hopsworks, and Hugging Face\\n49 / 54', 'Iris Flowers: Feature Pipeline with Modal and Hopsworks\\nimport os\\nimport modal\\nstub = modal.Stub()\\nhopsworks_image = modal.Image.debian_slim().pip_install([\"hopsworks\"])\\n@stub.function(image=hopsworks_image, schedule=modal.Period(days=1), \\\\\\nsecret=modal.Secret.from_name(\"jim-hopsworks-ai\"))\\ndef f():\\nimport hopsworks\\nimport pandas as pd\\nproject = hopsworks.login()\\nfs = project.get_feature_store()\\niris_df = pd.read_csv(\"https://repo.hops.works/master/hopsworks-tutorials/data/iris.csv\")\\niris_fg = fs.get_or_create_feature_group( name=\"iris_modal\", version=1,\\nprimary_key=[\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\"],\\ndescription=\"Iris flower dataset\")\\niris_fg.insert(iris_df)\\nif __name__ == \"__main__\":\\nwith stub.run():\\nf()\\n50 / 54', 'Training Pipeline with Modal and Hopsworks\\n@stub.function(image=hopsworks_image, schedule=modal.Period(days=1),\\\\\\nsecret=modal.Secret.from_name(\"jim-hopsworks-ai\"))\\ndef f():\\n# lots of imports\\nproject = hopsworks.login()\\nfs = project.get_feature_store()\\ntry:\\nfeature_view = fs.get_feature_view(name=\"iris_modal\", version=1)\\nexcept:\\niris_fg = fs.get_feature_group(name=\"iris_modal\", version=1)\\nquery = iris_fg.select_all()\\nfeature_view = fs.create_feature_view(name=\"iris_modal\",\\nversion=1,\\ndescription=\"Read from Iris flower dataset\",\\nlabels=[\"variety\"],\\nquery=query)\\nX_train, X_test, y_train, y_test = feature_view.train_test_split(0.2)\\nmodel = KNeighborsClassifier(n_neighbors=2)\\nmodel.fit(X_train, y_train.values.ravel())\\n51 / 54', 'Training Pipeline (ctd)\\ny_pred = model.predict(X_test)\\nmetrics = classification_report(y_test, y_pred, output_dict=True)\\nresults = confusion_matrix(y_test, y_pred)\\ndf_cm = pd.DataFrame(results, [’True Setosa’, ’True Versicolor’, ’True Virginica’],\\n[’Pred Setosa’, ’Pred Versicolor’, ’Pred Virginica’])\\ncm = sns.heatmap(df_cm, annot=True)\\nfig = cm.get_figure()\\njoblib.dump(model, \"iris_model/iris_model.pkl\")\\nfig.savefig(\"iris_model/confusion_matrix.png\")\\ninput_schema = Schema(X_train)\\noutput_schema = Schema(y_train)\\nmodel_schema = ModelSchema(input_schema, output_schema)\\nmr = project.get_model_registry()\\niris_model = mr.python.create_model(\\nname=\"iris_modal\",\\nmetrics={\"accuracy\" : metrics[’accuracy’]},\\nmodel_schema=model_schema,\\ndescription=\"Iris Flower Predictor\")\\niris_model.save(\"iris_model\")\\n52 / 54', 'Interactive Inference Pipeline with Hugging Face/Hopsworks\\nmodel = mr.get_model(\"iris_modal\", version=1)\\nmodel_dir = model.download()\\nmodel = joblib.load(model_dir + \"/iris_model.pkl\")\\ndef iris(sepal_length, sepal_width, petal_length, petal_width):\\ninput_list = []\\ninput_list.append(sepal_length)\\ninput_list.append(sepal_width)\\ninput_list.append(petal_length)\\ninput_list.append(petal_width)\\nres = model.predict(np.asarray(input_list).reshape(1, -1))\\nflower_url = \"https://raw.githubusercontent.com/.../assets/\" + res[0] + \".png\"\\nreturn Image.open(requests.get(flower_url, stream=True).raw)\\ndemo = gr.Interface(\\nfn=iris, title=\"Iris Flower Predictive Analytics\", allow_flagging=\"never\",\\ndescription=\"Experiment with sepal/petal lengths/widths to predict which flower it is.\",\\ninputs=[ gr.inputs.Number(default=1.0, label=\"sepal length (cm)\"),\\ngr.inputs.Number(default=1.0, label=\"sepal width (cm)\"),\\ngr.inputs.Number(default=1.0, label=\"petal length (cm)\"),\\ngr.inputs.Number(default=1.0, label=\"petal width (cm)\"),],\\noutputs=gr.Image(type=\"pil\"))\\ndemo.launch()\\n53 / 54', 'Questions?\\nAcknowledgements\\nSome of the images are used with permission from Hopsworks AB.\\n54 / 54']}, '01_introduction.pdf': {'text': ['Introduction\\nJim Dowling\\njdowling@kth.se\\nCourse Assistants: Fabien Schmidt, Javier Ron, Ahmad Al-Shishtawy\\n2023-10-30\\nSlides by Amir H. Payberah and Jim Dowling', 'Course Information\\n1 / 79', 'Course Objective\\n▶This course has a system-based focus.\\n▶Learn the theory of machine learning and deep learning.\\n▶Learn the practical aspects of building machine learning and deep learning algorithms\\nusing data parallel programming platforms, such as TensorFlow.\\n2 / 79', '————————————————\\n3 / 79', 'Topics Covered in the Course\\n▶Part 1: large scale deep learning\\n•TensorFlow\\n•Deep Neural Networks (DNN)\\n•Different DNN architectures, e.g., CNNs, RNNs, Autoencoders, GAN\\n•Distributed learning\\n▶Part 2: large scale machine learning\\n•Serverless Machine Learning\\n•MLOps (machine learning operations)\\n•Feature Stores for Machine Learning\\n•Distributed Training of DNNs\\n3 / 79', 'Intended Learning Outcomes (ILOs)\\n▶ILO1: explain the principles of ML/DL algorithms and apply their techniques to solve\\nproblems.\\n▶ILO2: demonstrate an ability to design DNN architectures and explain challenges to\\nscaling their training and inference with increasing compute and data.\\n▶ILO3: explain the principles of scaling machine learning systems.\\n▶ILO4: implement scalable ML/DL systems.\\n4 / 79', 'The Course Assessment\\n▶Task1: the Examination on 8th January 2023(A-F)\\n▶Task2: the lab assignments (A-F)\\n▶Task3: the final project (P/F)\\n5 / 79', 'How Each ILO is Assessed?\\nTask1 Task2 Task3\\nILO1 x\\nILO2 x x x\\nILO3 x x\\nILO4 x x\\n6 / 79', 'Task1: The Exam (A-F)\\n▶Questions about the lectures and the labs.\\n▶Some sample questions for the exam will be distributed in mid December.\\n▶The examination is graded (A-F).\\n7 / 79', 'Task2: The Lab Assignments (A-F)\\n▶Two lab assignments: source code and oral presentation.\\n▶E: source code\\n▶D: source code + half questions (basic)\\n▶C: source code + all questions (basic)\\n▶B: source code + half questions (basic and advanced)\\n▶A: source code + all questions (basic and advanced)\\n8 / 79', 'Task3: The Final Project (A-F)\\n▶One final project: source code and oral presentation.\\n▶Proposed by students and confirmed by the teacher:\\n▶Source code and documentation as a README.md by the students of the project\\n▶5 minute Presentation of the project by the students.\\n▶20% of total grade bonus for projects graded as excellent.\\n9 / 79', 'The Final Grade\\n▶The final grade is the weighted average of the Exam (0.3), two labs (0.15 each). A\\nbonus of .09 points will be added for projects graded ’excellent’.\\n▶To compute it, map A-E to 5-1, and take the average.\\n▶The floating values are rounded up, if they are more than half, otherwise they are\\nrounded down.\\n•E.g., 3.6 will be rounded to 4, and 4.5 will be rounded to 4.\\n▶A late submission will reduce your grade level by one. That is, A will become B, B\\nwill become C, and so on.\\n▶To pass the course, you need to take at least E in all the assignments.\\n10 / 79', 'How do you submit the Assignments?\\n▶Through the Canvas site.\\n▶Students will work in groups of two on Task 2 and Task 3.\\n11 / 79', 'The Course Material\\n▶Hands-on machine learning with Scikit-Learn and TensorFlow, 2nd Edition, A. Geron,\\nO’Reilly Media, 2019\\n▶Deep learning, I. Goodfellow et al., Cambridge: MIT press, 2016\\n12 / 79', 'The Course Web Page\\nhttps://id2223kth.github.io\\n13 / 79', 'Supervised Machine Learning 101\\n14 / 79', 'Features in Machine Learning\\n15 / 79', 'Supervised Machine Learning (ML)\\n16 / 79', 'A Linear Model can classify the Fruit (Classifier)\\n17 / 79', 'The Decision Boundary\\n18 / 79', 'Source code for a small sample of data with a decision tree\\nclassifier\\n19 / 79', 'But wait, apples can also be red!\\n20 / 79', 'It’s harder to separate Green Apples, Oranges, and Red Apples\\nwith just 2 colors (red and green)\\n21 / 79', 'From two dimensions to millions of dimensions\\n▶Can we just add more features? Yes. If you add weight and smoothness, you can\\nseparate apples and oranges. We could plot our fruit in 3d or even 4d and find a\\nplane that separates apples and oranges\\n▶You can add many more features (dimensions). With the caveat that too many\\ndimensions can lead to overfitting. Overfitting means your model is not good at\\ngeneralizing to correctly predict new fruit examples (it would work well for the training\\ndata, but not unseen (new) examples)\\n▶In image classification, each pixel is a feature. That’s millions of features for a single\\nHD image. Deep learning can be used to train models with millions of features.\\n22 / 79', 'Not all properties with predictive power should be features\\n23 / 79', 'Lots of features: deep learning for apple classification works\\n24 / 79', 'A model can also be trained to predict a number (regression)\\n25 / 79', 'Predicting Surf Height at a beach – Classification or\\nRegression?\\n26 / 79', 'What is Supervised Machine Learning, then?\\n▶With our Apple/Orange classifier, we used (features, label) examples to train a model\\nto find a decision boundary.\\n▶Then when a new fruit arrived, we could use the model to predict if the new piece\\nof fruit is an apple or an orange.\\n▶We can generalize to say that supervised machine learning is concerned with:\\n▶extracting a pattern from labeled data (features) to a model\\n▶using that model to make predictions for new unlabeled data (features)\\n27 / 79', 'Tradtional ML courses vs ID2223\\n▶Static Datasets, where Features for ML are correct and unbiased\\n▶The goal is to optimize your model with a model evaluation metric (accuracy) to\\ncommunicate the value of your model\\n▶Data never stops coming and it comes from heterogeneous data sources\\n▶Communicate the value of your model as a Prediction Service - one that can be scaled\\nand deployed using MLOps (Machine Learning Operations) best practices (versioning,\\nautomated testing/deployment)\\n28 / 79', 'Feature Engineering is often treated like this “helpful” guide to\\ndrawing a Barn Owl\\n29 / 79', 'Extract the features from the input data. We will study feature\\nengineering at scale.\\nRaw Data Extracted Feature Method\\nHotel room bookings Weekly vacancy level Aggregation\\nUser’s web session history Session history embedding Dimensionality Reduction\\nUser’s date of birth Binning\\nHourly spot electricity prices Scale into range [0, 1] Normalization\\nUser’s home country Binary number of country One Hot Encoding\\n30 / 79', 'Many Enterprises have walled gardens between teams building\\nproduction ML Systems\\n31 / 79', 'With infrastructure support (Serverless Machine Learning),\\ndevelopers need less infrastructural skills to deploy ML Systems\\n32 / 79', 'Monolithic ML Pipeline\\n33 / 79', 'Refactor the Monolithic ML Pipeline to Scale your ML Systems\\n34 / 79', 'Machine Learning and Deep Learning\\n35 / 79', 'Learning Algorithms\\n▶A ML algorithm is an algorithm that is able to learn from data.\\n▶What is learning?\\n▶A computer program is said to learn from experience E with respect to some class of\\ntasks T and performance measure P, if its performance at tasks in T, as measured\\nby P, improves with experience E. (Tom M. Mitchell)\\n36 / 79', 'Learning Algorithms - Example 1\\n▶A spam filter that can learn to flag spam given examples of spam emails and examples\\nof regular emails.\\n▶Task T: flag spam for new emails\\n▶Experience E: the training data\\n▶Performance measure P: the ratio of correctly classified emails\\n[https://bit.ly/2oiplYM]\\n37 / 79', 'Learning Algorithms - Example 2\\n▶Given dataset of prices of 500 houses, how can we learn to predict the prices of other\\nhouses, as a function of the size of their living areas?\\n▶Task T: predict the price\\n▶Experience E: the dataset of living areas and prices\\n▶Performance measure P: the difference between the predicted price and the real price\\n[https://bit.ly/2MyiJUy]\\n38 / 79', 'Types of Machine Learning Algorithms\\n▶Supervised learning\\n•Input data is labeled, e.g., spam/not-spam or a stock price at a time.\\n•Regression vs. classification\\n▶Unsupervised learning\\n•Input data is unlabeled.\\n•Find hidden structures in data.\\n39 / 79', 'AI Generations - Deep Learning\\n▶For many tasks, it is difficult to know what features should be extracted\\n▶Use machine learning to discover the mapping from representation to output\\n[https://bit.ly/2woLEzs]\\n40 / 79', 'Image Classification with Deep Learning\\n▶For image classification, where each pixel is a feature, Deep Learning can do the\\nfeature extraction as part of the learning algorithm.\\n41 / 79', 'Sheepdog or Mop\\n42 / 79', 'Chihuahua or Muffin\\n43 / 79', 'Barn Owl or Apple\\n44 / 79', 'Training Deep Neural Networks\\n▶Computationally intensive\\n▶Time consuming\\n[https://cloud.google.com/tpu/docs/images/inceptionv3onc--oview.png]\\n45 / 79', 'Why?\\n▶Massive amount of training dataset\\n▶Large number of parameters\\n46 / 79', 'Accuracy vs. Data/Model Size\\n[Jeff Dean at AI Frontiers: Trends and Developments in Deep Learning Research]\\n47 / 79', 'Accuracy vs. Data/Model Size\\n[Jeff Dean at AI Frontiers: Trends and Developments in Deep Learning Research]\\n48 / 79', 'Accuracy vs. Data/Model Size\\n[Jeff Dean at AI Frontiers: Trends and Developments in Deep Learning Research]\\n49 / 79', 'Why Does Deep Learning Work Now?\\n▶Huge quantity of data\\n▶Tremendous increase in computing power\\n▶Better training algorithms\\n50 / 79', 'Linear Algebra Review\\n51 / 79', 'Vector\\n▶A vector is an array of numbers.\\n▶Notation:\\n•Denoted by bold lowercase letters, e.g., x.\\n•xidenotes the ith entry.\\nx=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0x1\\nx2\\n...\\nxn\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n52 / 79', 'Matrix and Tensor\\n▶A matrix is a 2-D array of numbers.\\n▶A tensor is an array with more than two axes.\\n▶Notation:\\n•Denoted by bold uppercase letters, e.g., A.\\n•aijdenotes the entry in ith row and jth column.\\n•IfAism×n, it has mrows and ncolumns.\\nA=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0a1,1a1,2a1,3. . . a1,n\\na2,1a2,2a2,3. . . a2,n\\n...............\\nam,1am,2am,3. . . am,n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n53 / 79', 'Matrix Addition and Subtraction\\n▶The matrices must have the same dimensions.\\nA=\\x14a b\\nc d\\x15\\n+\\x14e f\\ng h\\x15\\n=\\x14a+e b +f\\nc+g d +h\\x15\\n54 / 79', 'Matrix Product\\n▶The matrix product of matrices AandBis a third matrix C, where C=AB.\\n▶IfAis of shape m×nandBis of shape n×p, then Cis of shape m×p.\\ncij=X\\nkaikbkj\\n▶Properties\\n•Associative: ( AB)C=A(BC)\\n•Not commutative: AB̸=BA\\n[https://en.wikipedia.org/wiki/Matrix multiplication]\\n55 / 79', 'Matrix Transpose\\n▶Swap the rows and columns of a matrix.\\nA=\\uf8ee\\n\\uf8f0a b\\nc d\\ne f\\uf8f9\\n\\uf8fb⇒A⊺=\\x14a c e\\nb d f\\x15\\n▶Properties\\n•Aij=A⊺\\nji\\n•IfAism×n, then A⊺isn×m\\n•(A+B)⊺=A⊺+B⊺\\n•(AB)⊺=B⊺A⊺\\n56 / 79', 'Inverse of a Matrix\\n▶IfAis a square matrix, its inverse is called A−1.\\nAA−1=A−1A=I\\n▶Where I, the identity matrix, is a diagonal matrix with all 1’s on the diagonal.\\nI2=\\x141 0\\n0 1\\x15\\nI3=\\uf8ee\\n\\uf8f01 0 0\\n0 1 0\\n0 0 1\\uf8f9\\n\\uf8fb\\n57 / 79', 'LpNorm for Vectors\\n▶We can measure the size of vectors using a norm function.\\n▶Norms are functions mapping vectors to non-negative values.\\n▶L1norm\\n||x||1=X\\ni|xi|\\n▶L2norm\\n||x||2= (X\\ni|xi|2)1\\n2=q\\nx2\\n1+x2\\n2+···+x2n\\n▶Lpnorm\\n||x||p= (X\\ni|xi|p)1\\np\\n58 / 79', 'Probability Review\\n59 / 79', 'Random Variables\\n▶Random variable: a variable that can take on different values randomly.\\n▶Random variables may be discrete or continuous.\\n•Discrete random variable: finite or countably infinite number of states\\n•Continuous random variable: real value\\n▶Notation:\\n•Denoted by an upper case letter, e.g., X\\n•Values of a random variable Xare denoted by lower case letters, e.g., xandy.\\n60 / 79', 'Probability Distributions\\n▶Probability distribution: how likely a random variable is to take on each of its possible\\nstates.\\n•E.g., the random variable Xdenotes the outcome of a coin toss.\\n•The probability distribution of Xwould take the value 0.5forX=head , and 0.5for\\nY=tail (assuming the coin is fair).\\n▶The way we describe probability distributions depends on whether the variables are\\ndiscrete or continuous.\\n61 / 79', 'Discrete Variables\\n▶Probability mass function (PMF): the probability distribution of a discrete random\\nvariable X.\\n▶Notation: denoted by a lowercase p.\\n•E.g., p(x) =1indicates that X=xis certain\\n•E.g., p(x) =0indicates that X=xis impossible\\n▶Properties:\\n•The domain Dofpmust be the set of all possible states of X\\n•∀x∈D(X),0≤p(x)≤1\\n•P\\nx∈D(X)p(x) =1\\n62 / 79', 'Independence\\n▶Two random variables XandYare independent, if their probability distribution can\\nbe expressed as their products.\\n∀x∈D(X),y∈D(Y),p(X=x,Y=y) =p(X=x)p(Y=y)\\n▶E.g., if a coin is tossed and a single 6-sided die is rolled, then the probability of\\nlanding on the head side of the coin and rolling a 3 on the die is:\\np(X=head ,Y=3) =p(X=head )p(Y=3) =1\\n2×1\\n6=1\\n12\\n63 / 79', 'Conditional Probability\\n▶Conditional probability: the probability of an event given that another event has\\noccurred.\\np(Y=y|X=x) =p(Y=y,X=x)\\np(X=x)\\n▶E.g., if 60% of the class passed both labs and 80% of the class passed the first labs,\\nthen what percent of those who passed the first lab also passed the second lab?\\n•E.g., XandYrandom variables for the first and the second labs, respectively.\\np(Y=lab2|X=lab1 ) =p(Y=lab2 ,X=lab1 )\\np(X=lab1 )=0.6\\n0.8=3\\n4\\n64 / 79', 'Expectation\\n▶The expected value of a random variable Xwith respect to a probability distribution\\np(X) is the average value that Xtakes on when it is drawn from p(X).\\nEx∼p[X] =X\\nxp(x)x\\n▶E.g., If X:{1,2,3}, and p(X=1) =0.3,p(X=2) =0.5,p(X=3) =0.2\\n•E[X] =0.3×1+0.5×2+0.2×3=1.9\\n65 / 79', 'Variance and Standard Deviation\\n▶The variance gives a measure of how much the values of a random variable Xvary\\nas we sample it from its probability distribution p(X).\\nVar(X) =E[(X−E[X])2]\\nVar(X) =X\\nxp(x)(x−E[X])2\\n▶E.g., If X:{1,2,3}, and p(X=1) =0.3,p(X=2) =0.5,p(X=3) =0.2\\n•E[X] =0.3×1+0.5×2+0.2×3=1.9\\n•Var(X) =0.3(1−1.9)2+0.5(2−1.9)2+0.2(3−1.9)2=0.49\\n▶The standard deviation, shown by σ, is the square root of the variance.\\n66 / 79', 'Covariance (1/2)\\n▶The covariance gives some sense of how much two values are linearly related to each\\nother.\\nCov(X,Y) =E[(X−E[X])(Y−E[Y])]\\nCov(X,Y) =X X\\n(x,y)p(x,y)(x−E[X])(y−E[Y])\\n67 / 79', 'Covariance (2/2)\\nY\\np(X, Y) 1 2 3 p(X)\\n1 1/4 1/4 0 1/2\\nX 2 0 1/4 1/4 1/2\\np(Y) 1/4 1/2 1/4 1\\nE[X] =1\\n2×1+1\\n2×2=3\\n2E[Y] =1\\n4×1+1\\n2×2+1\\n4×3=2\\nCov(X,Y) =X X\\n(x,y)p(x,y)(x−E[X])(y−E[Y])\\n=1\\n4(1−3\\n2)(1−2) +1\\n4(1−3\\n2)(2−2) +0(1−3\\n2)(3−2)\\n+0(2−3\\n2)(1−2) +1\\n4(2−3\\n2)(2−2) +1\\n4(2−3\\n2)(3−2) =1\\n4\\n68 / 79', 'Correlation Coefficient\\n▶The Correlation coefficient is a quantity that measures the strength of the association\\n(or dependence) between two random variables, e.g., XandY.\\nρ(X,Y) =Cov(X,Y)\\nσ(X)σ(Y)\\n69 / 79', 'Probability and Likelihood (1/2)\\n▶LetX:{x(1),x(2),···,x(m)}be a discrete random variable drawn independently from\\na distribution probability pdepending on a parameter θ.\\n•For six tosses of a coin, X:{h,t,t,t,h,t},h: head, and t: tail.\\n•Suppose you have a coin with probability θto land heads and ( 1−θ) to land tails.\\n▶p(X|θ=2\\n3) is the probability of Xgiven θ=2\\n3.\\n▶p(X=h|θ) is the likelihood of θgiven X=h.\\n▶Likelihood ( L): a function of the parameters ( θ) of a probability model, given specific\\nobserved data, e.g., X=h.\\nL(θ|X) =p(X|θ)\\n70 / 79', 'Probability and Likelihood (2/2)\\n▶The likelihood differs from that of a probability.\\n▶A probability p(X|θ) refers to the occurrence of future events.\\n▶A likelihood L(θ|X) refers to past events with known outcomes.\\n71 / 79', 'Maximum Likelihood Estimator\\n▶If samples in Xare independent we have:\\nL(θ|X) =p(X|θ) =p(x(1),x(2),···,x(m)|θ)\\n=p(x(1)|θ)p(x(2)|θ)···p(x(m)|θ) =mY\\ni=1p(x(i)|θ)\\n▶The maximum likelihood estimator (MLE): what is the most likely value of θgiven\\nthe training set?\\n^θMLE= arg max\\nθL(θ|X) = arg max\\nθmY\\ni=1p(x(i)|θ)\\n72 / 79', 'Maximum Likelihood Estimator - Example\\n▶Six tosses of a coin, with the following model:\\n•Possible outcomes: h with probability of θ, and t with probability ( 1−θ).\\n•Results of coin tosses are independent of one another.\\n▶Data: X:{h,t,t,t,h,t}\\n▶The likelihood is\\nL(θ|X) =p(X|θ)\\n=p(X=h|θ)p(X=t|θ)p(X=t|θ)p(X=t|θ)p(X=h|θ)p(X=t|θ)\\n=θ(1−θ)(1−θ)(1−θ)θ(1−θ)\\n=θ2(1−θ)4\\n▶^θis the value of θthat maximizes the likelihood:\\n^θMLE= arg max\\nθL(θ|X) =2\\n2+4\\n73 / 79', 'Log-Likelihood\\n▶The MLE product is prone to numerical underflow.\\n^θMLE= arg max\\nθL(θ|X) = arg max\\nθmY\\ni=1p(x(i)|θ)\\n▶To overcome this problem we can use the logarithm of the likelihood.\\n•It does not change its arg max, but transforms a product into a sum.\\n^θMLE= arg max\\nθmX\\ni=1logp (x(i)|θ)\\n74 / 79', 'Negative Log-Likelihood\\n▶Likelihood: L(θ|X) =Qm\\ni=1p(x(i)|θ)\\n▶Log-Likelihood: logL (θ|X) =logQm\\ni=1p(x(i)|θ) =Pm\\ni=1logp (x(i)|θ)\\n▶Negative Log-Likelihood: −logL (θ|X) =−Pm\\ni=1logp (x(i)|θ)\\n▶Negative log-likelihood is also called the cross-entropy\\n75 / 79', 'Cross-Entropy\\n▶Coss-entropy: quantify the difference (error) between two probability distributions.\\n▶How close is the predicted distribution to the true distribution?\\nH(p,q) =−X\\nxp(x)log(q(x))\\n▶Where pis the true distribution, and qthe predicted distribution.\\n76 / 79', 'Cross-Entropy - Example\\n▶Six tosses of a coin: X:{h,t,t,t,h,t}\\n▶The true distribution p:p(h) =2\\n6andp(t) =4\\n6\\n▶The predicted distribution q: h with probability of θ, and t with probability ( 1−θ).\\n▶Cross entropy: H(p,q) =−P\\nxp(x)log(q(x))\\n=−p(h)log(q(h))−p(t)log(q(t)) =−2\\n6log(θ)−4\\n6log(1−θ)\\n▶Likelihood: θ2(1−θ)4\\n▶Negative log likelihood: −log(θ2(1−θ)4) =−2log (θ)−4log (1−θ)\\n77 / 79', 'References\\n▶Ian Goodfellow et al., Deep Learning (Ch. 1, 2, 3)\\n78 / 79', 'Questions?\\nAcknowledgements\\nSome of the pictures were copied from the book Hands-On Machine Learning\\nwith Scikit-Learn and TensorFlow, Aurelien Geron, O’Reilly Media, 2017.\\n79 / 79']}}\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "\n",
    "transcriptions = [] \n",
    "embeddings = {}\n",
    "\n",
    "for file in new_files:\n",
    "    pdfReader = PyPDF2.PdfReader(file[\"title\"])\n",
    "\n",
    "    count = len(pdfReader.pages)\n",
    "    output = \"\"\n",
    "    embeddings[file[\"title\"]] = {\"text\":[]}\n",
    "    for i in range(count):\n",
    "        pageObj = pdfReader.pages[i]\n",
    "       \n",
    "        extr = pageObj.extract_text()\n",
    "        embeddings[file[\"title\"]][\"text\"].append(extr)\n",
    "        output += \"\\n\" + extr\n",
    "        \n",
    "    transcriptions.append(output)\n",
    "print(embeddings)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nServerless Machine Learning\\nJim Dowling\\njdowling@kth.se\\n2022-11-04\\nEnterprise AI Value Chain\\n1 / 54\\nModern Enterprise Data and ML Infrastructure\\n2 / 54\\nMonolithic ML Pipeline\\n3 / 54\\nProblems with Monolithic ML Pipelines\\n▶They are often not modular - their components are not modular and cannot be\\nindependently scaled or deployed on different hardware (e.g., CPUs for feature engi-\\nneering, GPUs for model training).\\n▶They are difficult to test - production software needs automated tests to ensure\\nfeatures and models are of high quality.\\n▶They tightly couple the execution of feature engineering, model training, and infer-\\nence steps - running them in the same pipeline program at the same time.\\n▶They do not promote reuse of features/models/code. The code for computing fea-\\ntures (feature logic) cannot be easily disentangled from its pipeline jungle.\\n4 / 54\\nModularity enables more Robust and Scalable Systems\\nModular water pipes in a Google Datacenter. Instead of one giant water pipe (our\\nmonolithic notebook), separate water pipes reduce the blast radius if one fails. Color\\ncoding makes it easier to debug problems in a damaged water pipe.\\n5 / 54\\nPipelines as Modular Programs\\n▶Modularity involves structuring your code such that its functionality is separated into\\nindependent classes and/or functions that can be more easily reused and tested.\\n▶Modules should be placed in accessible classes or functions, keeping them small and\\neasy to understand and document.\\n▶Modules enable code to be more easily reused in different pipelines.\\n▶Modules enable code to be more easily independently tested, enabling the easier and\\nearlier discovery of bugs.\\n6 / 54\\nSupervised ML Pipeline Stages\\ntrain (features ,labels )−>model\\nmodel (features )−>predictions\\n7 / 54\\nML Pipeline Stages in a Serverless Machine Learning System\\n8 / 54\\nML Pipeline Stages - Data Sources\\n9 / 54\\nConnect to Data Sources and Read Raw Data\\n▶Discover data sources, securely connect to heterogeneous data sources\\n▶Manage dependencies such as connectors and drivers\\n▶Manage connection information securely: network endpoint, database/table names,\\nauthentication credentials such as API keys or credentials (username/password)\\n10 / 54\\nHeterogeneous Data Sources\\n11 / 54\\nFile Formats for different Data Sources\\n12 / 54\\nML Pipeline Stages - Feature Pipelines\\n13 / 54\\nFeature Pipelines\\n14 / 54\\nFeature Pipelines\\n▶A feature pipeline is a program that orchestrates the execution of feature engineering\\nsteps on input data to create feature values.\\nExamples of feature engineering steps:\\n▶Clean, validate, data\\n▶Data de-duplication, pseudononymization, data wrangling\\n▶Feature extraction, aggregations, dimensionality reduction, feature binning, feature\\ncrosses\\n15 / 54\\nTabular Data\\n16 / 54\\nTabular Data as Features, Labels, Entity (or Primary) Keys,\\nEvent Time\\n17 / 54\\nTabular Data in Pandas\\n18 / 54\\nExploratory Data Analysis in Pandas\\n19 / 54\\nAggregations in Pandas\\n20 / 54\\nRolling Windows in Pandas\\n21 / 54\\nFeature binning\\n22 / 54\\nFeature Crosses\\n▶A feature cross is a synthetic feature formed by multiplying (crossing) two or more\\nfeatures. By multiplying features together, you encode nonlinearity in the feature\\nspace.\\n▶For example, imagine we are looking for credit card fraud activity within a geographic\\nregion (e.g., a city district), how would we capture that as a feature?\\n▶We could cross to a geographic area (binned latitude and binned longitude - a grid\\nidentifying a city district) with the level of credit card activity within that geographic\\narea.\\n23 / 54\\nEmbeddings as Features\\n▶An embedding is a lower dimension representation of a sparse input that retains some\\nof the semantics of the input.\\n▶An embedding store (vector database) stores semantically similar inputs close to-\\ngether in the embedding space. You can implement “similarity search” by finding\\nembeddings close in embedding space. You can even apply arithmetic on embeddings\\nto discover semantic relationships.\\n24 / 54\\nML Pipeline Stages - Feature Store\\n25 / 54\\nStore Features\\nThere are two general ways people manage features and labels for both training and\\nserving:\\n▶(1) Compute features on-demand as part of the model training or batch inference\\npipeline.\\n▶(2) Use a feature store to store the features so that they can be reused across\\ndifferent models for both training and inference. For online models that require\\nfeatures with either historical or contextual information , feature stores are typically\\nused.\\n26 / 54\\nML Pipeline Stages - Training Pipelines\\n27 / 54\\nFeature Types\\nReference: https://www.hopsworks.ai/post/feature-types-for-machine-learning\\n28 / 54\\nFeature Types Taxonomy\\n29 / 54\\nModel Training Pipelines\\n30 / 54\\nModel-Dependent Transformations\\nReference: https://developers.google.com/machine-learning/data-\\nprep/transform/introduction\\n31 / 54\\nTransformations in Pandas\\n32 / 54\\nDifferent types of Transformations\\n33 / 54\\nModel Training with Train and Test Sets\\n34 / 54\\nModel Training with Train and Test Sets in Scikti-Learn\\n35 / 54\\nModel Training is an Iterative Process\\n36 / 54\\nModel-Centric Iteration to Improve Model Performance\\nPossible steps to improve your model performance:\\n▶Try out a different supervised ML learning algorithm (e.g., random forest, feedforward\\ndeep neural network, Gradient-boosted decision tree)\\n▶Try out new combinations of hyperparameters (e.g., number of training epochs, the\\nlearning rate, number of layers in a deep neural network, adjust regularizations such\\nas Dropout or BatchNorm)\\n▶Evaluate your model on a validation set (keeping a separate holdout test set for final\\nmodel performance evaluation)\\n37 / 54\\nData-Centric Iteration to Improve Model Performance\\nSteps to improve your model\\n▶Add or remove features to or from your model (feature selection)\\n▶Add more training data\\n▶Remove poor quality training samples\\n▶Improve the quality of existing training samples (e.g., using Cleanlab or Snorkel)\\n▶Rank the importance of the training samples (Active Learning)\\n38 / 54\\nTrain, Validation, and Test Sets\\n▶Random splits of the training data when the data is not time-series data\\n▶Time-series splits of the training data when the data is time-series data\\n39 / 54\\nModel Training is an Iterative Process\\n40 / 54\\nML Pipeline Stages - Inference Pipelines\\n41 / 54\\nBatch Inference Pipeline\\n42 / 54\\nOnline Inference Pipeline\\n43 / 54\\nServerless ML with Python\\n▶Write Feature, Training, and Inference Pipelines in Python\\n▶Orchestrate the execution of Pipelines using Serverless Compute Platforms\\n▶Store features and models in a serverless feature/model store\\n▶Run a User Interface (UI), written in Python, on serverless infrastructure\\n44 / 54\\nServerless Compute Platforms\\n45 / 54\\nServerless Feature Stores and Model Registry/Serving\\nFeature Stores\\n▶Hopsworks\\nModel Registry and Serving\\n▶Hopsworks\\n▶AWS Sagemaker\\n▶Databricks\\n▶Google Vertex\\n46 / 54\\nServerless User Interfaces\\n▶Hugging Faces Spaces\\n▶Streamlit Cloud\\n47 / 54\\nIris Flower Dataset\\nhttps://github.com/ID2223KTH/id2223kth.github.io/tree/master/src/serverless-ml-\\nintro\\n▶4 input features: sepal length, sepal width, petal length, petal width\\n▶label (target): Iris Flower Type (one of Setosa, Versicolor, Virginica)\\n▶Only 150 samples in the dataset\\n48 / 54\\nServerless Iris with Modal, Hopsworks, and Hugging Face\\n49 / 54\\nIris Flowers: Feature Pipeline with Modal and Hopsworks\\nimport os\\nimport modal\\nstub = modal.Stub()\\nhopsworks_image = modal.Image.debian_slim().pip_install([\"hopsworks\"])\\n@stub.function(image=hopsworks_image, schedule=modal.Period(days=1), \\\\\\nsecret=modal.Secret.from_name(\"jim-hopsworks-ai\"))\\ndef f():\\nimport hopsworks\\nimport pandas as pd\\nproject = hopsworks.login()\\nfs = project.get_feature_store()\\niris_df = pd.read_csv(\"https://repo.hops.works/master/hopsworks-tutorials/data/iris.csv\")\\niris_fg = fs.get_or_create_feature_group( name=\"iris_modal\", version=1,\\nprimary_key=[\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\"],\\ndescription=\"Iris flower dataset\")\\niris_fg.insert(iris_df)\\nif __name__ == \"__main__\":\\nwith stub.run():\\nf()\\n50 / 54\\nTraining Pipeline with Modal and Hopsworks\\n@stub.function(image=hopsworks_image, schedule=modal.Period(days=1),\\\\\\nsecret=modal.Secret.from_name(\"jim-hopsworks-ai\"))\\ndef f():\\n# lots of imports\\nproject = hopsworks.login()\\nfs = project.get_feature_store()\\ntry:\\nfeature_view = fs.get_feature_view(name=\"iris_modal\", version=1)\\nexcept:\\niris_fg = fs.get_feature_group(name=\"iris_modal\", version=1)\\nquery = iris_fg.select_all()\\nfeature_view = fs.create_feature_view(name=\"iris_modal\",\\nversion=1,\\ndescription=\"Read from Iris flower dataset\",\\nlabels=[\"variety\"],\\nquery=query)\\nX_train, X_test, y_train, y_test = feature_view.train_test_split(0.2)\\nmodel = KNeighborsClassifier(n_neighbors=2)\\nmodel.fit(X_train, y_train.values.ravel())\\n51 / 54\\nTraining Pipeline (ctd)\\ny_pred = model.predict(X_test)\\nmetrics = classification_report(y_test, y_pred, output_dict=True)\\nresults = confusion_matrix(y_test, y_pred)\\ndf_cm = pd.DataFrame(results, [’True Setosa’, ’True Versicolor’, ’True Virginica’],\\n[’Pred Setosa’, ’Pred Versicolor’, ’Pred Virginica’])\\ncm = sns.heatmap(df_cm, annot=True)\\nfig = cm.get_figure()\\njoblib.dump(model, \"iris_model/iris_model.pkl\")\\nfig.savefig(\"iris_model/confusion_matrix.png\")\\ninput_schema = Schema(X_train)\\noutput_schema = Schema(y_train)\\nmodel_schema = ModelSchema(input_schema, output_schema)\\nmr = project.get_model_registry()\\niris_model = mr.python.create_model(\\nname=\"iris_modal\",\\nmetrics={\"accuracy\" : metrics[’accuracy’]},\\nmodel_schema=model_schema,\\ndescription=\"Iris Flower Predictor\")\\niris_model.save(\"iris_model\")\\n52 / 54\\nInteractive Inference Pipeline with Hugging Face/Hopsworks\\nmodel = mr.get_model(\"iris_modal\", version=1)\\nmodel_dir = model.download()\\nmodel = joblib.load(model_dir + \"/iris_model.pkl\")\\ndef iris(sepal_length, sepal_width, petal_length, petal_width):\\ninput_list = []\\ninput_list.append(sepal_length)\\ninput_list.append(sepal_width)\\ninput_list.append(petal_length)\\ninput_list.append(petal_width)\\nres = model.predict(np.asarray(input_list).reshape(1, -1))\\nflower_url = \"https://raw.githubusercontent.com/.../assets/\" + res[0] + \".png\"\\nreturn Image.open(requests.get(flower_url, stream=True).raw)\\ndemo = gr.Interface(\\nfn=iris, title=\"Iris Flower Predictive Analytics\", allow_flagging=\"never\",\\ndescription=\"Experiment with sepal/petal lengths/widths to predict which flower it is.\",\\ninputs=[ gr.inputs.Number(default=1.0, label=\"sepal length (cm)\"),\\ngr.inputs.Number(default=1.0, label=\"sepal width (cm)\"),\\ngr.inputs.Number(default=1.0, label=\"petal length (cm)\"),\\ngr.inputs.Number(default=1.0, label=\"petal width (cm)\"),],\\noutputs=gr.Image(type=\"pil\"))\\ndemo.launch()\\n53 / 54\\nQuestions?\\nAcknowledgements\\nSome of the images are used with permission from Hopsworks AB.\\n54 / 54', '\\nIntroduction\\nJim Dowling\\njdowling@kth.se\\nCourse Assistants: Fabien Schmidt, Javier Ron, Ahmad Al-Shishtawy\\n2023-10-30\\nSlides by Amir H. Payberah and Jim Dowling\\nCourse Information\\n1 / 79\\nCourse Objective\\n▶This course has a system-based focus.\\n▶Learn the theory of machine learning and deep learning.\\n▶Learn the practical aspects of building machine learning and deep learning algorithms\\nusing data parallel programming platforms, such as TensorFlow.\\n2 / 79\\n————————————————\\n3 / 79\\nTopics Covered in the Course\\n▶Part 1: large scale deep learning\\n•TensorFlow\\n•Deep Neural Networks (DNN)\\n•Different DNN architectures, e.g., CNNs, RNNs, Autoencoders, GAN\\n•Distributed learning\\n▶Part 2: large scale machine learning\\n•Serverless Machine Learning\\n•MLOps (machine learning operations)\\n•Feature Stores for Machine Learning\\n•Distributed Training of DNNs\\n3 / 79\\nIntended Learning Outcomes (ILOs)\\n▶ILO1: explain the principles of ML/DL algorithms and apply their techniques to solve\\nproblems.\\n▶ILO2: demonstrate an ability to design DNN architectures and explain challenges to\\nscaling their training and inference with increasing compute and data.\\n▶ILO3: explain the principles of scaling machine learning systems.\\n▶ILO4: implement scalable ML/DL systems.\\n4 / 79\\nThe Course Assessment\\n▶Task1: the Examination on 8th January 2023(A-F)\\n▶Task2: the lab assignments (A-F)\\n▶Task3: the final project (P/F)\\n5 / 79\\nHow Each ILO is Assessed?\\nTask1 Task2 Task3\\nILO1 x\\nILO2 x x x\\nILO3 x x\\nILO4 x x\\n6 / 79\\nTask1: The Exam (A-F)\\n▶Questions about the lectures and the labs.\\n▶Some sample questions for the exam will be distributed in mid December.\\n▶The examination is graded (A-F).\\n7 / 79\\nTask2: The Lab Assignments (A-F)\\n▶Two lab assignments: source code and oral presentation.\\n▶E: source code\\n▶D: source code + half questions (basic)\\n▶C: source code + all questions (basic)\\n▶B: source code + half questions (basic and advanced)\\n▶A: source code + all questions (basic and advanced)\\n8 / 79\\nTask3: The Final Project (A-F)\\n▶One final project: source code and oral presentation.\\n▶Proposed by students and confirmed by the teacher:\\n▶Source code and documentation as a README.md by the students of the project\\n▶5 minute Presentation of the project by the students.\\n▶20% of total grade bonus for projects graded as excellent.\\n9 / 79\\nThe Final Grade\\n▶The final grade is the weighted average of the Exam (0.3), two labs (0.15 each). A\\nbonus of .09 points will be added for projects graded ’excellent’.\\n▶To compute it, map A-E to 5-1, and take the average.\\n▶The floating values are rounded up, if they are more than half, otherwise they are\\nrounded down.\\n•E.g., 3.6 will be rounded to 4, and 4.5 will be rounded to 4.\\n▶A late submission will reduce your grade level by one. That is, A will become B, B\\nwill become C, and so on.\\n▶To pass the course, you need to take at least E in all the assignments.\\n10 / 79\\nHow do you submit the Assignments?\\n▶Through the Canvas site.\\n▶Students will work in groups of two on Task 2 and Task 3.\\n11 / 79\\nThe Course Material\\n▶Hands-on machine learning with Scikit-Learn and TensorFlow, 2nd Edition, A. Geron,\\nO’Reilly Media, 2019\\n▶Deep learning, I. Goodfellow et al., Cambridge: MIT press, 2016\\n12 / 79\\nThe Course Web Page\\nhttps://id2223kth.github.io\\n13 / 79\\nSupervised Machine Learning 101\\n14 / 79\\nFeatures in Machine Learning\\n15 / 79\\nSupervised Machine Learning (ML)\\n16 / 79\\nA Linear Model can classify the Fruit (Classifier)\\n17 / 79\\nThe Decision Boundary\\n18 / 79\\nSource code for a small sample of data with a decision tree\\nclassifier\\n19 / 79\\nBut wait, apples can also be red!\\n20 / 79\\nIt’s harder to separate Green Apples, Oranges, and Red Apples\\nwith just 2 colors (red and green)\\n21 / 79\\nFrom two dimensions to millions of dimensions\\n▶Can we just add more features? Yes. If you add weight and smoothness, you can\\nseparate apples and oranges. We could plot our fruit in 3d or even 4d and find a\\nplane that separates apples and oranges\\n▶You can add many more features (dimensions). With the caveat that too many\\ndimensions can lead to overfitting. Overfitting means your model is not good at\\ngeneralizing to correctly predict new fruit examples (it would work well for the training\\ndata, but not unseen (new) examples)\\n▶In image classification, each pixel is a feature. That’s millions of features for a single\\nHD image. Deep learning can be used to train models with millions of features.\\n22 / 79\\nNot all properties with predictive power should be features\\n23 / 79\\nLots of features: deep learning for apple classification works\\n24 / 79\\nA model can also be trained to predict a number (regression)\\n25 / 79\\nPredicting Surf Height at a beach – Classification or\\nRegression?\\n26 / 79\\nWhat is Supervised Machine Learning, then?\\n▶With our Apple/Orange classifier, we used (features, label) examples to train a model\\nto find a decision boundary.\\n▶Then when a new fruit arrived, we could use the model to predict if the new piece\\nof fruit is an apple or an orange.\\n▶We can generalize to say that supervised machine learning is concerned with:\\n▶extracting a pattern from labeled data (features) to a model\\n▶using that model to make predictions for new unlabeled data (features)\\n27 / 79\\nTradtional ML courses vs ID2223\\n▶Static Datasets, where Features for ML are correct and unbiased\\n▶The goal is to optimize your model with a model evaluation metric (accuracy) to\\ncommunicate the value of your model\\n▶Data never stops coming and it comes from heterogeneous data sources\\n▶Communicate the value of your model as a Prediction Service - one that can be scaled\\nand deployed using MLOps (Machine Learning Operations) best practices (versioning,\\nautomated testing/deployment)\\n28 / 79\\nFeature Engineering is often treated like this “helpful” guide to\\ndrawing a Barn Owl\\n29 / 79\\nExtract the features from the input data. We will study feature\\nengineering at scale.\\nRaw Data Extracted Feature Method\\nHotel room bookings Weekly vacancy level Aggregation\\nUser’s web session history Session history embedding Dimensionality Reduction\\nUser’s date of birth Binning\\nHourly spot electricity prices Scale into range [0, 1] Normalization\\nUser’s home country Binary number of country One Hot Encoding\\n30 / 79\\nMany Enterprises have walled gardens between teams building\\nproduction ML Systems\\n31 / 79\\nWith infrastructure support (Serverless Machine Learning),\\ndevelopers need less infrastructural skills to deploy ML Systems\\n32 / 79\\nMonolithic ML Pipeline\\n33 / 79\\nRefactor the Monolithic ML Pipeline to Scale your ML Systems\\n34 / 79\\nMachine Learning and Deep Learning\\n35 / 79\\nLearning Algorithms\\n▶A ML algorithm is an algorithm that is able to learn from data.\\n▶What is learning?\\n▶A computer program is said to learn from experience E with respect to some class of\\ntasks T and performance measure P, if its performance at tasks in T, as measured\\nby P, improves with experience E. (Tom M. Mitchell)\\n36 / 79\\nLearning Algorithms - Example 1\\n▶A spam filter that can learn to flag spam given examples of spam emails and examples\\nof regular emails.\\n▶Task T: flag spam for new emails\\n▶Experience E: the training data\\n▶Performance measure P: the ratio of correctly classified emails\\n[https://bit.ly/2oiplYM]\\n37 / 79\\nLearning Algorithms - Example 2\\n▶Given dataset of prices of 500 houses, how can we learn to predict the prices of other\\nhouses, as a function of the size of their living areas?\\n▶Task T: predict the price\\n▶Experience E: the dataset of living areas and prices\\n▶Performance measure P: the difference between the predicted price and the real price\\n[https://bit.ly/2MyiJUy]\\n38 / 79\\nTypes of Machine Learning Algorithms\\n▶Supervised learning\\n•Input data is labeled, e.g., spam/not-spam or a stock price at a time.\\n•Regression vs. classification\\n▶Unsupervised learning\\n•Input data is unlabeled.\\n•Find hidden structures in data.\\n39 / 79\\nAI Generations - Deep Learning\\n▶For many tasks, it is difficult to know what features should be extracted\\n▶Use machine learning to discover the mapping from representation to output\\n[https://bit.ly/2woLEzs]\\n40 / 79\\nImage Classification with Deep Learning\\n▶For image classification, where each pixel is a feature, Deep Learning can do the\\nfeature extraction as part of the learning algorithm.\\n41 / 79\\nSheepdog or Mop\\n42 / 79\\nChihuahua or Muffin\\n43 / 79\\nBarn Owl or Apple\\n44 / 79\\nTraining Deep Neural Networks\\n▶Computationally intensive\\n▶Time consuming\\n[https://cloud.google.com/tpu/docs/images/inceptionv3onc--oview.png]\\n45 / 79\\nWhy?\\n▶Massive amount of training dataset\\n▶Large number of parameters\\n46 / 79\\nAccuracy vs. Data/Model Size\\n[Jeff Dean at AI Frontiers: Trends and Developments in Deep Learning Research]\\n47 / 79\\nAccuracy vs. Data/Model Size\\n[Jeff Dean at AI Frontiers: Trends and Developments in Deep Learning Research]\\n48 / 79\\nAccuracy vs. Data/Model Size\\n[Jeff Dean at AI Frontiers: Trends and Developments in Deep Learning Research]\\n49 / 79\\nWhy Does Deep Learning Work Now?\\n▶Huge quantity of data\\n▶Tremendous increase in computing power\\n▶Better training algorithms\\n50 / 79\\nLinear Algebra Review\\n51 / 79\\nVector\\n▶A vector is an array of numbers.\\n▶Notation:\\n•Denoted by bold lowercase letters, e.g., x.\\n•xidenotes the ith entry.\\nx=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0x1\\nx2\\n...\\nxn\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n52 / 79\\nMatrix and Tensor\\n▶A matrix is a 2-D array of numbers.\\n▶A tensor is an array with more than two axes.\\n▶Notation:\\n•Denoted by bold uppercase letters, e.g., A.\\n•aijdenotes the entry in ith row and jth column.\\n•IfAism×n, it has mrows and ncolumns.\\nA=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0a1,1a1,2a1,3. . . a1,n\\na2,1a2,2a2,3. . . a2,n\\n...............\\nam,1am,2am,3. . . am,n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n53 / 79\\nMatrix Addition and Subtraction\\n▶The matrices must have the same dimensions.\\nA=\\x14a b\\nc d\\x15\\n+\\x14e f\\ng h\\x15\\n=\\x14a+e b +f\\nc+g d +h\\x15\\n54 / 79\\nMatrix Product\\n▶The matrix product of matrices AandBis a third matrix C, where C=AB.\\n▶IfAis of shape m×nandBis of shape n×p, then Cis of shape m×p.\\ncij=X\\nkaikbkj\\n▶Properties\\n•Associative: ( AB)C=A(BC)\\n•Not commutative: AB̸=BA\\n[https://en.wikipedia.org/wiki/Matrix multiplication]\\n55 / 79\\nMatrix Transpose\\n▶Swap the rows and columns of a matrix.\\nA=\\uf8ee\\n\\uf8f0a b\\nc d\\ne f\\uf8f9\\n\\uf8fb⇒A⊺=\\x14a c e\\nb d f\\x15\\n▶Properties\\n•Aij=A⊺\\nji\\n•IfAism×n, then A⊺isn×m\\n•(A+B)⊺=A⊺+B⊺\\n•(AB)⊺=B⊺A⊺\\n56 / 79\\nInverse of a Matrix\\n▶IfAis a square matrix, its inverse is called A−1.\\nAA−1=A−1A=I\\n▶Where I, the identity matrix, is a diagonal matrix with all 1’s on the diagonal.\\nI2=\\x141 0\\n0 1\\x15\\nI3=\\uf8ee\\n\\uf8f01 0 0\\n0 1 0\\n0 0 1\\uf8f9\\n\\uf8fb\\n57 / 79\\nLpNorm for Vectors\\n▶We can measure the size of vectors using a norm function.\\n▶Norms are functions mapping vectors to non-negative values.\\n▶L1norm\\n||x||1=X\\ni|xi|\\n▶L2norm\\n||x||2= (X\\ni|xi|2)1\\n2=q\\nx2\\n1+x2\\n2+···+x2n\\n▶Lpnorm\\n||x||p= (X\\ni|xi|p)1\\np\\n58 / 79\\nProbability Review\\n59 / 79\\nRandom Variables\\n▶Random variable: a variable that can take on different values randomly.\\n▶Random variables may be discrete or continuous.\\n•Discrete random variable: finite or countably infinite number of states\\n•Continuous random variable: real value\\n▶Notation:\\n•Denoted by an upper case letter, e.g., X\\n•Values of a random variable Xare denoted by lower case letters, e.g., xandy.\\n60 / 79\\nProbability Distributions\\n▶Probability distribution: how likely a random variable is to take on each of its possible\\nstates.\\n•E.g., the random variable Xdenotes the outcome of a coin toss.\\n•The probability distribution of Xwould take the value 0.5forX=head , and 0.5for\\nY=tail (assuming the coin is fair).\\n▶The way we describe probability distributions depends on whether the variables are\\ndiscrete or continuous.\\n61 / 79\\nDiscrete Variables\\n▶Probability mass function (PMF): the probability distribution of a discrete random\\nvariable X.\\n▶Notation: denoted by a lowercase p.\\n•E.g., p(x) =1indicates that X=xis certain\\n•E.g., p(x) =0indicates that X=xis impossible\\n▶Properties:\\n•The domain Dofpmust be the set of all possible states of X\\n•∀x∈D(X),0≤p(x)≤1\\n•P\\nx∈D(X)p(x) =1\\n62 / 79\\nIndependence\\n▶Two random variables XandYare independent, if their probability distribution can\\nbe expressed as their products.\\n∀x∈D(X),y∈D(Y),p(X=x,Y=y) =p(X=x)p(Y=y)\\n▶E.g., if a coin is tossed and a single 6-sided die is rolled, then the probability of\\nlanding on the head side of the coin and rolling a 3 on the die is:\\np(X=head ,Y=3) =p(X=head )p(Y=3) =1\\n2×1\\n6=1\\n12\\n63 / 79\\nConditional Probability\\n▶Conditional probability: the probability of an event given that another event has\\noccurred.\\np(Y=y|X=x) =p(Y=y,X=x)\\np(X=x)\\n▶E.g., if 60% of the class passed both labs and 80% of the class passed the first labs,\\nthen what percent of those who passed the first lab also passed the second lab?\\n•E.g., XandYrandom variables for the first and the second labs, respectively.\\np(Y=lab2|X=lab1 ) =p(Y=lab2 ,X=lab1 )\\np(X=lab1 )=0.6\\n0.8=3\\n4\\n64 / 79\\nExpectation\\n▶The expected value of a random variable Xwith respect to a probability distribution\\np(X) is the average value that Xtakes on when it is drawn from p(X).\\nEx∼p[X] =X\\nxp(x)x\\n▶E.g., If X:{1,2,3}, and p(X=1) =0.3,p(X=2) =0.5,p(X=3) =0.2\\n•E[X] =0.3×1+0.5×2+0.2×3=1.9\\n65 / 79\\nVariance and Standard Deviation\\n▶The variance gives a measure of how much the values of a random variable Xvary\\nas we sample it from its probability distribution p(X).\\nVar(X) =E[(X−E[X])2]\\nVar(X) =X\\nxp(x)(x−E[X])2\\n▶E.g., If X:{1,2,3}, and p(X=1) =0.3,p(X=2) =0.5,p(X=3) =0.2\\n•E[X] =0.3×1+0.5×2+0.2×3=1.9\\n•Var(X) =0.3(1−1.9)2+0.5(2−1.9)2+0.2(3−1.9)2=0.49\\n▶The standard deviation, shown by σ, is the square root of the variance.\\n66 / 79\\nCovariance (1/2)\\n▶The covariance gives some sense of how much two values are linearly related to each\\nother.\\nCov(X,Y) =E[(X−E[X])(Y−E[Y])]\\nCov(X,Y) =X X\\n(x,y)p(x,y)(x−E[X])(y−E[Y])\\n67 / 79\\nCovariance (2/2)\\nY\\np(X, Y) 1 2 3 p(X)\\n1 1/4 1/4 0 1/2\\nX 2 0 1/4 1/4 1/2\\np(Y) 1/4 1/2 1/4 1\\nE[X] =1\\n2×1+1\\n2×2=3\\n2E[Y] =1\\n4×1+1\\n2×2+1\\n4×3=2\\nCov(X,Y) =X X\\n(x,y)p(x,y)(x−E[X])(y−E[Y])\\n=1\\n4(1−3\\n2)(1−2) +1\\n4(1−3\\n2)(2−2) +0(1−3\\n2)(3−2)\\n+0(2−3\\n2)(1−2) +1\\n4(2−3\\n2)(2−2) +1\\n4(2−3\\n2)(3−2) =1\\n4\\n68 / 79\\nCorrelation Coefficient\\n▶The Correlation coefficient is a quantity that measures the strength of the association\\n(or dependence) between two random variables, e.g., XandY.\\nρ(X,Y) =Cov(X,Y)\\nσ(X)σ(Y)\\n69 / 79\\nProbability and Likelihood (1/2)\\n▶LetX:{x(1),x(2),···,x(m)}be a discrete random variable drawn independently from\\na distribution probability pdepending on a parameter θ.\\n•For six tosses of a coin, X:{h,t,t,t,h,t},h: head, and t: tail.\\n•Suppose you have a coin with probability θto land heads and ( 1−θ) to land tails.\\n▶p(X|θ=2\\n3) is the probability of Xgiven θ=2\\n3.\\n▶p(X=h|θ) is the likelihood of θgiven X=h.\\n▶Likelihood ( L): a function of the parameters ( θ) of a probability model, given specific\\nobserved data, e.g., X=h.\\nL(θ|X) =p(X|θ)\\n70 / 79\\nProbability and Likelihood (2/2)\\n▶The likelihood differs from that of a probability.\\n▶A probability p(X|θ) refers to the occurrence of future events.\\n▶A likelihood L(θ|X) refers to past events with known outcomes.\\n71 / 79\\nMaximum Likelihood Estimator\\n▶If samples in Xare independent we have:\\nL(θ|X) =p(X|θ) =p(x(1),x(2),···,x(m)|θ)\\n=p(x(1)|θ)p(x(2)|θ)···p(x(m)|θ) =mY\\ni=1p(x(i)|θ)\\n▶The maximum likelihood estimator (MLE): what is the most likely value of θgiven\\nthe training set?\\n^θMLE= arg max\\nθL(θ|X) = arg max\\nθmY\\ni=1p(x(i)|θ)\\n72 / 79\\nMaximum Likelihood Estimator - Example\\n▶Six tosses of a coin, with the following model:\\n•Possible outcomes: h with probability of θ, and t with probability ( 1−θ).\\n•Results of coin tosses are independent of one another.\\n▶Data: X:{h,t,t,t,h,t}\\n▶The likelihood is\\nL(θ|X) =p(X|θ)\\n=p(X=h|θ)p(X=t|θ)p(X=t|θ)p(X=t|θ)p(X=h|θ)p(X=t|θ)\\n=θ(1−θ)(1−θ)(1−θ)θ(1−θ)\\n=θ2(1−θ)4\\n▶^θis the value of θthat maximizes the likelihood:\\n^θMLE= arg max\\nθL(θ|X) =2\\n2+4\\n73 / 79\\nLog-Likelihood\\n▶The MLE product is prone to numerical underflow.\\n^θMLE= arg max\\nθL(θ|X) = arg max\\nθmY\\ni=1p(x(i)|θ)\\n▶To overcome this problem we can use the logarithm of the likelihood.\\n•It does not change its arg max, but transforms a product into a sum.\\n^θMLE= arg max\\nθmX\\ni=1logp (x(i)|θ)\\n74 / 79\\nNegative Log-Likelihood\\n▶Likelihood: L(θ|X) =Qm\\ni=1p(x(i)|θ)\\n▶Log-Likelihood: logL (θ|X) =logQm\\ni=1p(x(i)|θ) =Pm\\ni=1logp (x(i)|θ)\\n▶Negative Log-Likelihood: −logL (θ|X) =−Pm\\ni=1logp (x(i)|θ)\\n▶Negative log-likelihood is also called the cross-entropy\\n75 / 79\\nCross-Entropy\\n▶Coss-entropy: quantify the difference (error) between two probability distributions.\\n▶How close is the predicted distribution to the true distribution?\\nH(p,q) =−X\\nxp(x)log(q(x))\\n▶Where pis the true distribution, and qthe predicted distribution.\\n76 / 79\\nCross-Entropy - Example\\n▶Six tosses of a coin: X:{h,t,t,t,h,t}\\n▶The true distribution p:p(h) =2\\n6andp(t) =4\\n6\\n▶The predicted distribution q: h with probability of θ, and t with probability ( 1−θ).\\n▶Cross entropy: H(p,q) =−P\\nxp(x)log(q(x))\\n=−p(h)log(q(h))−p(t)log(q(t)) =−2\\n6log(θ)−4\\n6log(1−θ)\\n▶Likelihood: θ2(1−θ)4\\n▶Negative log likelihood: −log(θ2(1−θ)4) =−2log (θ)−4log (1−θ)\\n77 / 79\\nReferences\\n▶Ian Goodfellow et al., Deep Learning (Ch. 1, 2, 3)\\n78 / 79\\nQuestions?\\nAcknowledgements\\nSome of the pictures were copied from the book Hands-On Machine Learning\\nwith Scikit-Learn and TensorFlow, Aurelien Geron, O’Reilly Media, 2017.\\n79 / 79']\n"
     ]
    }
   ],
   "source": [
    "print(transcriptions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-19 20:44:14,539 INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What are some problems with monolithic ML pipelines?\",\n",
      "    \"What is the purpose of modular code in ML pipelines?\",\n",
      "    \"What are some examples of feature engineering steps in a feature pipeline?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Some problems with monolithic ML pipelines include lack of modularity, difficulty in testing, tight coupling of pipeline steps, and limited code reuse.\",\n",
      "    \"The purpose of modular code in ML pipelines is to make the functionality separated into independent classes or functions that can be easily reused, tested, and understood.\",\n",
      "    \"Some examples of feature engineering steps in a feature pipeline are cleaning and validating data, data deduplication and wrangling, feature extraction and aggregations, feature binning, and feature crosses.\"\n",
      "  ]\n",
      "}\n",
      "2023-12-19 20:44:17,524 INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What are the two options for computing features in a model training or batch inference pipeline?\",\n",
      "    \"What is the purpose of using a feature store in online models that require historical or contextual information?\",\n",
      "    \"What are the possible steps to improve model performance in a model-centric iteration process?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The two options for computing features in a model training or batch inference pipeline are on-demand computation and using a feature store.\",\n",
      "    \"The purpose of using a feature store in online models that require historical or contextual information is to store and reuse features across different models for both training and inference.\",\n",
      "    \"Possible steps to improve model performance in a model-centric iteration process include trying out different supervised ML learning algorithms, exploring different combinations of hyperparameters, and evaluating the model on a validation set.\"\n",
      "  ]\n",
      "}\n",
      "2023-12-19 20:44:21,723 INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the purpose of the code in the given context?\",\n",
      "    \"How does the code create a confusion matrix for the model's predictions?\",\n",
      "    \"What is the purpose of the Interactive Inference Pipeline?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The purpose of the code in the given context is to train a machine learning model using the Iris flower dataset, evaluate its performance, and create an inference pipeline for predicting the type of iris flower based on input attributes.\",\n",
      "    \"The code creates a confusion matrix for the model's predictions by using the `classification_report` function to generate a dictionary of metrics, including precision, recall, and f1-score, and then using the `confusion_matrix` function to create a pandas DataFrame visualization of the results using seaborn heatmap.\",\n",
      "    \"The purpose of the Interactive Inference Pipeline is to provide a user-friendly interface for making real-time predictions on the type of iris flower based on input attributes. It loads the trained model, accepts input values for sepal and petal lengths/widths, uses the model to make a prediction, downloads the corresponding image of the predicted flower from a predefined URL, and displays the image as output.\"\n",
      "  ]\n",
      "}\n",
      "2023-12-19 20:44:25,714 INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the main objective of the course?\",\n",
      "    \"What are the topics covered in the course?\",\n",
      "    \"How are the assignments assessed?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The main objective of the course is to learn the theory and practical aspects of building machine learning and deep learning algorithms using data parallel programming platforms.\",\n",
      "    \"The topics covered in the course include large scale deep learning with topics like TensorFlow, Deep Neural Networks, different DNN architectures, and distributed learning. The course also covers large scale machine learning with topics like Serverless Machine Learning, MLOps, Feature Stores for Machine Learning, and Distributed Training of DNNs.\",\n",
      "    \"The assignments in the course are assessed through an exam, lab assignments, and a final project. The exam is graded and consists of questions related to the lectures and labs. The lab assignments are graded based on the source code and oral presentation, with different grading criteria for different levels of completion. The final project is proposed by students, confirmed by the teacher, and graded based on the source code and oral presentation. Excellent projects receive a 20% bonus.\"\n",
      "  ]\n",
      "}\n",
      "2023-12-19 20:44:28,071 INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is deep learning used for in image classification?\",\n",
      "    \"What is supervised machine learning concerned with?\",\n",
      "    \"What is feature engineering?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Deep learning can be used to train models with millions of features in image classification.\",\n",
      "    \"Supervised machine learning is concerned with extracting a pattern from labeled data to a model and using that model to make predictions for new unlabeled data.\",\n",
      "    \"Feature engineering is the process of extracting useful features from raw data to improve the performance of machine learning models.\"\n",
      "  ]\n",
      "}\n",
      "2023-12-19 20:44:38,438 INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What are some challenges of training deep neural networks?\",\n",
      "    \"What are the reasons why deep learning works now?\",\n",
      "    \"What is the definition of a vector?\",\n",
      "    \"What is a matrix and tensor?\",\n",
      "    \"How can you calculate the product of matrices?\",\n",
      "    \"What is the significance of matrix transpose?\",\n",
      "    \"What is the LpNorm for vectors?\",\n",
      "    \"What is a probability distribution?\",\n",
      "    \"What is the probability mass function for discrete variables?\",\n",
      "    \"What does it mean for two random variables to be independent?\",\n",
      "    \"What is conditional probability?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Some challenges of training deep neural networks include computational intensity and time consumption due to the massive amount of training dataset and large number of parameters.\",\n",
      "    \"Deep learning works now due to the significant increase in data quantity, tremendous increase in computing power, and better training algorithms.\",\n",
      "    \"A vector is an array of numbers denoted by bold lowercase letters. Each entry in the vector is represented by xi.\",\n",
      "    \"A matrix is a 2-D array of numbers denoted by bold uppercase letters. An entry in the matrix is represented by aij, where i is the row and j is the column. A tensor is an array with more than two axes.\",\n",
      "    \"The product of matrices A and B is a third matrix C, denoted as C = AB. If A is of shape m×n and B is of shape n×p, then C is of shape m×p. The product of matrices is associative, but not commutative.\",\n",
      "    \"Matrix transpose involves swapping the rows and columns of a matrix. It can be represented as A⊺ and has properties such as Aij = A⊺ji, changing the dimensions from m×n to n×m, and distributing over addition and multiplication operations.\",\n",
      "    \"LpNorm is a function used to measure the size of vectors. L1norm calculates the sum of absolute values of vector elements, L2norm calculates the square root of the sum of squared vector elements, and Lpnorm is a generalized form for any positive value of p.\",\n",
      "    \"A probability distribution describes how likely a random variable is to take on each of its possible states. It can be represented as a probability mass function (PMF) for discrete variables or a probability density function (PDF) for continuous variables.\",\n",
      "    \"The probability mass function (PMF) is the probability distribution of a discrete random variable. It assigns probabilities to each possible state of the variable, denoted as p(x). The properties of PMF include having a domain that covers all possible states of the variable, probabilities ranging from 0 to 1, and the sum of probabilities equaling 1.\",\n",
      "    \"Two random variables are independent if their probability distribution can be expressed as the product of their individual distributions. Mathematically, for any x in the domain of X and y in the domain of Y, p(X=x, Y=y) = p(X=x) * p(Y=y).\",\n",
      "    \"Conditional probability refers to the probability of an event occurring given that another event has already occurred. It is denoted as p(Y=y|X=x) and can be calculated by dividing the joint probability of X and Y by the probability of X.\"\n",
      "  ]\n",
      "}\n",
      "2023-12-19 20:44:41,894 INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the expected value of a random variable with respect to a probability distribution?\",\n",
      "    \"What does the variance measure?\",\n",
      "    \"What is the maximum likelihood estimator?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The expected value of a random variable X with respect to a probability distribution p(X) is the average value that X takes on when it is drawn from p(X). It can be calculated by multiplying each value of X by its probability and summing the results.\",\n",
      "    \"The variance measures how much the values of a random variable X vary as we sample it from its probability distribution. It is calculated by taking the expected value of the squared difference between each value of X and the expected value of X.\",\n",
      "    \"The maximum likelihood estimator (MLE) is a method used to estimate the most likely value of a parameter in a probability model given a training set. It is obtained by maximizing the likelihood function, which is the product of the probabilities of the observed data given the parameter.\"\n",
      "  ]\n",
      "}\n",
      "2023-12-19 20:44:44,369 INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "{\n",
      "  \"questions\": [\n",
      "    \"What is the true distribution for the coin toss example?\",\n",
      "    \"What is the predicted distribution for the coin toss example?\",\n",
      "    \"How is the cross entropy calculated for this example?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The true distribution for the coin toss example is p(h) = 2/6 and p(t) = 4/6.\",\n",
      "    \"The predicted distribution for the coin toss example is h with a probability of θ, and t with probability (1-θ).\",\n",
      "    \"The cross entropy is calculated by using the formula H(p,q) = -p(h)log(q(h)) - p(t)log(q(t)), where p is the true distribution and q is the predicted distribution.\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from openai import OpenAI\n",
    "from math import ceil\n",
    "\n",
    "\n",
    "api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "client = OpenAI(api_key=api_key)\n",
    "responses = []\n",
    "\n",
    "for t in transcriptions:\n",
    "    context = t\n",
    "    i = 0\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(ceil(len(t)/4097)):\n",
    "        chunks.append(t[i*4097:i*4097+4097])\n",
    "\n",
    "    for c in chunks:\n",
    "        context = c\n",
    "        question = \"The text above is the result of the transcription of slides in the PDF file format. Remove chapter names and slides numbers and rephrase the sentences. Once you do that generate 2 to 3 meaningful questions on the text and the respective answers. Plese reply in the JSON format {'questions':<questions generated>,'answers':<answers generated>}. DO NOT write anything else than the requested JSON and remember to write the full elaborated content and not just one part.\"\n",
    "        #question = \"The text above is the result of the transcription of slides in the PDF file format. Remove chapter names and slides numbers and rephrase the sentences. Once you do that generate 3 meaningful questions based on the new text and the respective answers. As for the reply, follow the following template FOR EACH pair of question and the respective answer: '[INST] <question> [/INST] <answer>'  and so on, let's call this template a 'block'.  NEVER use newlines other than separating blocks and NEVER write anything that is not formatted as the proposed template. DO NOT write anything else than the requested blocks and make sure everything is formatted correctly.\"\n",
    "        # response = openai.Completion.create(\n",
    "        # engine=\"gpt-3.5-turbo\",\n",
    "        prompt=f\"\\nContext: {context}\\nQuestion: {question}\"\n",
    "        # )\n",
    "        # answer = response.choices[0].text.strip()\n",
    "        # print(answer)\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        print(response.choices[0].message.content)\n",
    "        responses.append(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] What are some problems with monolithic ML pipelines? [/INST] Some problems with monolithic ML pipelines include lack of modularity, difficulty in testing, tight coupling of pipeline steps, and limited code reuse. </s>\n",
      "<s> [INST] What is the purpose of modular code in ML pipelines? [/INST] The purpose of modular code in ML pipelines is to make the functionality separated into independent classes or functions that can be easily reused, tested, and understood. </s>\n",
      "<s> [INST] What are some examples of feature engineering steps in a feature pipeline? [/INST] Some examples of feature engineering steps in a feature pipeline are cleaning and validating data, data deduplication and wrangling, feature extraction and aggregations, feature binning, and feature crosses. </s>\n",
      "<s> [INST] What are the two options for computing features in a model training or batch inference pipeline? [/INST] The two options for computing features in a model training or batch inference pipeline are on-demand computation and using a feature store. </s>\n",
      "<s> [INST] What is the purpose of using a feature store in online models that require historical or contextual information? [/INST] The purpose of using a feature store in online models that require historical or contextual information is to store and reuse features across different models for both training and inference. </s>\n",
      "<s> [INST] What are the possible steps to improve model performance in a model-centric iteration process? [/INST] Possible steps to improve model performance in a model-centric iteration process include trying out different supervised ML learning algorithms, exploring different combinations of hyperparameters, and evaluating the model on a validation set. </s>\n",
      "<s> [INST] What is the purpose of the code in the given context? [/INST] The purpose of the code in the given context is to train a machine learning model using the Iris flower dataset, evaluate its performance, and create an inference pipeline for predicting the type of iris flower based on input attributes. </s>\n",
      "<s> [INST] How does the code create a confusion matrix for the model's predictions? [/INST] The code creates a confusion matrix for the model's predictions by using the `classification_report` function to generate a dictionary of metrics, including precision, recall, and f1-score, and then using the `confusion_matrix` function to create a pandas DataFrame visualization of the results using seaborn heatmap. </s>\n",
      "<s> [INST] What is the purpose of the Interactive Inference Pipeline? [/INST] The purpose of the Interactive Inference Pipeline is to provide a user-friendly interface for making real-time predictions on the type of iris flower based on input attributes. It loads the trained model, accepts input values for sepal and petal lengths/widths, uses the model to make a prediction, downloads the corresponding image of the predicted flower from a predefined URL, and displays the image as output. </s>\n",
      "<s> [INST] What is the main objective of the course? [/INST] The main objective of the course is to learn the theory and practical aspects of building machine learning and deep learning algorithms using data parallel programming platforms. </s>\n",
      "<s> [INST] What are the topics covered in the course? [/INST] The topics covered in the course include large scale deep learning with topics like TensorFlow, Deep Neural Networks, different DNN architectures, and distributed learning. The course also covers large scale machine learning with topics like Serverless Machine Learning, MLOps, Feature Stores for Machine Learning, and Distributed Training of DNNs. </s>\n",
      "<s> [INST] How are the assignments assessed? [/INST] The assignments in the course are assessed through an exam, lab assignments, and a final project. The exam is graded and consists of questions related to the lectures and labs. The lab assignments are graded based on the source code and oral presentation, with different grading criteria for different levels of completion. The final project is proposed by students, confirmed by the teacher, and graded based on the source code and oral presentation. Excellent projects receive a 20% bonus. </s>\n",
      "<s> [INST] What is deep learning used for in image classification? [/INST] Deep learning can be used to train models with millions of features in image classification. </s>\n",
      "<s> [INST] What is supervised machine learning concerned with? [/INST] Supervised machine learning is concerned with extracting a pattern from labeled data to a model and using that model to make predictions for new unlabeled data. </s>\n",
      "<s> [INST] What is feature engineering? [/INST] Feature engineering is the process of extracting useful features from raw data to improve the performance of machine learning models. </s>\n",
      "<s> [INST] What are some challenges of training deep neural networks? [/INST] Some challenges of training deep neural networks include computational intensity and time consumption due to the massive amount of training dataset and large number of parameters. </s>\n",
      "<s> [INST] What are the reasons why deep learning works now? [/INST] Deep learning works now due to the significant increase in data quantity, tremendous increase in computing power, and better training algorithms. </s>\n",
      "<s> [INST] What is the definition of a vector? [/INST] A vector is an array of numbers denoted by bold lowercase letters. Each entry in the vector is represented by xi. </s>\n",
      "<s> [INST] What is a matrix and tensor? [/INST] A matrix is a 2-D array of numbers denoted by bold uppercase letters. An entry in the matrix is represented by aij, where i is the row and j is the column. A tensor is an array with more than two axes. </s>\n",
      "<s> [INST] How can you calculate the product of matrices? [/INST] The product of matrices A and B is a third matrix C, denoted as C = AB. If A is of shape m×n and B is of shape n×p, then C is of shape m×p. The product of matrices is associative, but not commutative. </s>\n",
      "<s> [INST] What is the significance of matrix transpose? [/INST] Matrix transpose involves swapping the rows and columns of a matrix. It can be represented as A⊺ and has properties such as Aij = A⊺ji, changing the dimensions from m×n to n×m, and distributing over addition and multiplication operations. </s>\n",
      "<s> [INST] What is the LpNorm for vectors? [/INST] LpNorm is a function used to measure the size of vectors. L1norm calculates the sum of absolute values of vector elements, L2norm calculates the square root of the sum of squared vector elements, and Lpnorm is a generalized form for any positive value of p. </s>\n",
      "<s> [INST] What is a probability distribution? [/INST] A probability distribution describes how likely a random variable is to take on each of its possible states. It can be represented as a probability mass function (PMF) for discrete variables or a probability density function (PDF) for continuous variables. </s>\n",
      "<s> [INST] What is the probability mass function for discrete variables? [/INST] The probability mass function (PMF) is the probability distribution of a discrete random variable. It assigns probabilities to each possible state of the variable, denoted as p(x). The properties of PMF include having a domain that covers all possible states of the variable, probabilities ranging from 0 to 1, and the sum of probabilities equaling 1. </s>\n",
      "<s> [INST] What does it mean for two random variables to be independent? [/INST] Two random variables are independent if their probability distribution can be expressed as the product of their individual distributions. Mathematically, for any x in the domain of X and y in the domain of Y, p(X=x, Y=y) = p(X=x) * p(Y=y). </s>\n",
      "<s> [INST] What is conditional probability? [/INST] Conditional probability refers to the probability of an event occurring given that another event has already occurred. It is denoted as p(Y=y|X=x) and can be calculated by dividing the joint probability of X and Y by the probability of X. </s>\n",
      "<s> [INST] What is the expected value of a random variable with respect to a probability distribution? [/INST] The expected value of a random variable X with respect to a probability distribution p(X) is the average value that X takes on when it is drawn from p(X). It can be calculated by multiplying each value of X by its probability and summing the results. </s>\n",
      "<s> [INST] What does the variance measure? [/INST] The variance measures how much the values of a random variable X vary as we sample it from its probability distribution. It is calculated by taking the expected value of the squared difference between each value of X and the expected value of X. </s>\n",
      "<s> [INST] What is the maximum likelihood estimator? [/INST] The maximum likelihood estimator (MLE) is a method used to estimate the most likely value of a parameter in a probability model given a training set. It is obtained by maximizing the likelihood function, which is the product of the probabilities of the observed data given the parameter. </s>\n",
      "<s> [INST] What is the true distribution for the coin toss example? [/INST] The true distribution for the coin toss example is p(h) = 2/6 and p(t) = 4/6. </s>\n",
      "<s> [INST] What is the predicted distribution for the coin toss example? [/INST] The predicted distribution for the coin toss example is h with a probability of θ, and t with probability (1-θ). </s>\n",
      "<s> [INST] How is the cross entropy calculated for this example? [/INST] The cross entropy is calculated by using the formula H(p,q) = -p(h)log(q(h)) - p(t)log(q(t)), where p is the true distribution and q is the predicted distribution. </s>\n"
     ]
    }
   ],
   "source": [
    "# Generate instruction set\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "row_result = {\"prompt\":[],\"questions\":[],\"answers\":[]}\n",
    "\n",
    "for i, r in enumerate(responses):\n",
    "    try:\n",
    "        tmp = json.loads(r)\n",
    "        for j in range(len(tmp[\"questions\"])):\n",
    "            instr = f\"<s> [INST] {tmp['questions'][j]} [/INST] {tmp['answers'][j]} </s>\"\n",
    "            row_result[\"questions\"].append(tmp['questions'][j])\n",
    "            row_result[\"answers\"].append(tmp['answers'][j])\n",
    "            row_result[\"prompt\"].append(instr)\n",
    "            # print(instr)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "instructions = pd.DataFrame(row_result)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "for index, row in instructions.iterrows():\n",
    "    print(row[\"prompt\"])\n",
    "\n",
    "# print(instructions.iloc[0][\"Instructions\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/314521\n"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   source  page  \\\n",
      "0    02_serverless_ml.pdf     0   \n",
      "1    02_serverless_ml.pdf     1   \n",
      "2    02_serverless_ml.pdf     2   \n",
      "3    02_serverless_ml.pdf     3   \n",
      "4    02_serverless_ml.pdf     4   \n",
      "..                    ...   ...   \n",
      "131   01_introduction.pdf    76   \n",
      "132   01_introduction.pdf    77   \n",
      "133   01_introduction.pdf    78   \n",
      "134   01_introduction.pdf    79   \n",
      "135   01_introduction.pdf    80   \n",
      "\n",
      "                                               content  \n",
      "0    Serverless Machine Learning\\nJim Dowling\\njdow...  \n",
      "1                    Enterprise AI Value Chain\\n1 / 54  \n",
      "2    Modern Enterprise Data and ML Infrastructure\\n...  \n",
      "3                       Monolithic ML Pipeline\\n3 / 54  \n",
      "4    Problems with Monolithic ML Pipelines\\n▶They a...  \n",
      "..                                                 ...  \n",
      "131  Negative Log-Likelihood\\n▶Likelihood: L(θ|X) =...  \n",
      "132  Cross-Entropy\\n▶Coss-entropy: quantify the dif...  \n",
      "133  Cross-Entropy - Example\\n▶Six tosses of a coin...  \n",
      "134  References\\n▶Ian Goodfellow et al., Deep Learn...  \n",
      "135  Questions?\\nAcknowledgements\\nSome of the pict...  \n",
      "\n",
      "[136 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "emb = {\"source\":[],\"page\":[],\"content\":[]}\n",
    "for e in embeddings:\n",
    "    for idx,t in enumerate(embeddings[e][\"text\"]):\n",
    "        emb[\"source\"].append(e)\n",
    "        emb[\"page\"].append(idx)\n",
    "        emb[\"content\"].append(t)\n",
    "\n",
    "\n",
    "embedding_df = pd.DataFrame(emb)\n",
    "\n",
    "print(embedding_df)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/314521/fs/314440/fg/349524\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8963609ab4864a3683571c20a2bfbf8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Uploading Dataframe: 0.00% |          | Rows 0/136 | Elapsed Time: 00:00 | Remaining Time: ?"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: embeddings_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai/p/314521/jobs/named/embeddings_1_offline_fg_materialization/executions\n"
     ]
    },
    {
     "data": {
      "text/plain": "(<hsfs.core.job.Job at 0x133fdf810>, None)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs = project.get_feature_store()\n",
    "emb_fg = fs.get_or_create_feature_group(\n",
    "    name=\"embeddings\",\n",
    "    version=1,\n",
    "    primary_key=list(embedding_df), \n",
    "    description=\"Content of each page of each file\")\n",
    "emb_fg.insert(embedding_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/314521/fs/314440/fg/336255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 32/32 | Elapsed Time: 00:05 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: instructionset_4_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai/p/314521/jobs/named/instructionset_4_offline_fg_materialization/executions\n"
     ]
    },
    {
     "data": {
      "text/plain": "(<hsfs.core.job.Job at 0x130aac890>, None)"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs = project.get_feature_store()\n",
    "instructions_fg = fs.get_or_create_feature_group(\n",
    "    name=\"instructionset\",\n",
    "    version=4,\n",
    "    primary_key=list(instructions), \n",
    "    description=\"Instruction Set for fine tuning of llms\")\n",
    "instructions_fg.insert(instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(embedding_df['source'].isin(['02_serverless_ml.pdf','Python']))\n",
    "\n",
    "fs = project.get_feature_store()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.6 64-bit",
   "name": "python3116jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}