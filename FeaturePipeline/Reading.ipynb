{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?client_id=745312273193-mloneconglkdeh457jl05jm6bg6b1hdn.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&access_type=offline&response_type=code\n",
      "\n",
      "Authentication successful.\n"
     ]
    }
   ],
   "source": [
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "\n",
    "gauth = GoogleAuth()\n",
    "gauth.LocalWebserverAuth()\n",
    "\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leggi File da GDrive nella cartella \"slides\" (con id 1u2jMMh-hxSb93sL-BGsX9tYVeuA2wdcq). \n",
    "import os\n",
    "\n",
    "file_list = drive.ListFile({'q': \"'1u2jMMh-hxSb93sL-BGsX9tYVeuA2wdcq' in parents and trashed=false\"}).GetList() \n",
    "new_files = []\n",
    "for file in file_list:\n",
    "    if not os.path.isfile(f\"{file['title']}\"): \n",
    "        #print(\"title: %s, id: %s\" % (file[\"title\"],file[\"id\"]))\n",
    "        file.GetContentFile(file[\"title\"])\n",
    "        new_files.append(file)\n",
    "\n",
    "#new_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "transcriptions=[]\n",
    "for file in new_files:\n",
    "    pdfReader = PyPDF2.PdfReader(file[\"title\"])\n",
    "\n",
    "    count = len(pdfReader.pages)\n",
    "    output = \"\"\n",
    "    for i in range(count):\n",
    "        pageObj = pdfReader.pages[i]\n",
    "        output+= \"\\n\" + pageObj.extract_text()\n",
    "    transcriptions.append(output)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nServerless Machine Learning\\nJim Dowling\\njdowling@kth.se\\n2022-11-04\\nEnterprise AI Value Chain\\n1 / 54\\nModern Enterprise Data and ML Infrastructure\\n2 / 54\\nMonolithic ML Pipeline\\n3 / 54\\nProblems with Monolithic ML Pipelines\\n▶They are often not modular - their components are not modular and cannot be\\nindependently scaled or deployed on different hardware (e.g., CPUs for feature engi-\\nneering, GPUs for model training).\\n▶They are difficult to test - production software needs automated tests to ensure\\nfeatures and models are of high quality.\\n▶They tightly couple the execution of feature engineering, model training, and infer-\\nence steps - running them in the same pipeline program at the same time.\\n▶They do not promote reuse of features/models/code. The code for computing fea-\\ntures (feature logic) cannot be easily disentangled from its pipeline jungle.\\n4 / 54\\nModularity enables more Robust and Scalable Systems\\nModular water pipes in a Google Datacenter. Instead of one giant water pipe (our\\nmonolithic notebook), separate water pipes reduce the blast radius if one fails. Color\\ncoding makes it easier to debug problems in a damaged water pipe.\\n5 / 54\\nPipelines as Modular Programs\\n▶Modularity involves structuring your code such that its functionality is separated into\\nindependent classes and/or functions that can be more easily reused and tested.\\n▶Modules should be placed in accessible classes or functions, keeping them small and\\neasy to understand and document.\\n▶Modules enable code to be more easily reused in different pipelines.\\n▶Modules enable code to be more easily independently tested, enabling the easier and\\nearlier discovery of bugs.\\n6 / 54\\nSupervised ML Pipeline Stages\\ntrain (features ,labels )−>model\\nmodel (features )−>predictions\\n7 / 54\\nML Pipeline Stages in a Serverless Machine Learning System\\n8 / 54\\nML Pipeline Stages - Data Sources\\n9 / 54\\nConnect to Data Sources and Read Raw Data\\n▶Discover data sources, securely connect to heterogeneous data sources\\n▶Manage dependencies such as connectors and drivers\\n▶Manage connection information securely: network endpoint, database/table names,\\nauthentication credentials such as API keys or credentials (username/password)\\n10 / 54\\nHeterogeneous Data Sources\\n11 / 54\\nFile Formats for different Data Sources\\n12 / 54\\nML Pipeline Stages - Feature Pipelines\\n13 / 54\\nFeature Pipelines\\n14 / 54\\nFeature Pipelines\\n▶A feature pipeline is a program that orchestrates the execution of feature engineering\\nsteps on input data to create feature values.\\nExamples of feature engineering steps:\\n▶Clean, validate, data\\n▶Data de-duplication, pseudononymization, data wrangling\\n▶Feature extraction, aggregations, dimensionality reduction, feature binning, feature\\ncrosses\\n15 / 54\\nTabular Data\\n16 / 54\\nTabular Data as Features, Labels, Entity (or Primary) Keys,\\nEvent Time\\n17 / 54\\nTabular Data in Pandas\\n18 / 54\\nExploratory Data Analysis in Pandas\\n19 / 54\\nAggregations in Pandas\\n20 / 54\\nRolling Windows in Pandas\\n21 / 54\\nFeature binning\\n22 / 54\\nFeature Crosses\\n▶A feature cross is a synthetic feature formed by multiplying (crossing) two or more\\nfeatures. By multiplying features together, you encode nonlinearity in the feature\\nspace.\\n▶For example, imagine we are looking for credit card fraud activity within a geographic\\nregion (e.g., a city district), how would we capture that as a feature?\\n▶We could cross to a geographic area (binned latitude and binned longitude - a grid\\nidentifying a city district) with the level of credit card activity within that geographic\\narea.\\n23 / 54\\nEmbeddings as Features\\n▶An embedding is a lower dimension representation of a sparse input that retains some\\nof the semantics of the input.\\n▶An embedding store (vector database) stores semantically similar inputs close to-\\ngether in the embedding space. You can implement “similarity search” by finding\\nembeddings close in embedding space. You can even apply arithmetic on embeddings\\nto discover semantic relationships.\\n24 / 54\\nML Pipeline Stages - Feature Store\\n25 / 54\\nStore Features\\nThere are two general ways people manage features and labels for both training and\\nserving:\\n▶(1) Compute features on-demand as part of the model training or batch inference\\npipeline.\\n▶(2) Use a feature store to store the features so that they can be reused across\\ndifferent models for both training and inference. For online models that require\\nfeatures with either historical or contextual information , feature stores are typically\\nused.\\n26 / 54\\nML Pipeline Stages - Training Pipelines\\n27 / 54\\nFeature Types\\nReference: https://www.hopsworks.ai/post/feature-types-for-machine-learning\\n28 / 54\\nFeature Types Taxonomy\\n29 / 54\\nModel Training Pipelines\\n30 / 54\\nModel-Dependent Transformations\\nReference: https://developers.google.com/machine-learning/data-\\nprep/transform/introduction\\n31 / 54\\nTransformations in Pandas\\n32 / 54\\nDifferent types of Transformations\\n33 / 54\\nModel Training with Train and Test Sets\\n34 / 54\\nModel Training with Train and Test Sets in Scikti-Learn\\n35 / 54\\nModel Training is an Iterative Process\\n36 / 54\\nModel-Centric Iteration to Improve Model Performance\\nPossible steps to improve your model performance:\\n▶Try out a different supervised ML learning algorithm (e.g., random forest, feedforward\\ndeep neural network, Gradient-boosted decision tree)\\n▶Try out new combinations of hyperparameters (e.g., number of training epochs, the\\nlearning rate, number of layers in a deep neural network, adjust regularizations such\\nas Dropout or BatchNorm)\\n▶Evaluate your model on a validation set (keeping a separate holdout test set for final\\nmodel performance evaluation)\\n37 / 54\\nData-Centric Iteration to Improve Model Performance\\nSteps to improve your model\\n▶Add or remove features to or from your model (feature selection)\\n▶Add more training data\\n▶Remove poor quality training samples\\n▶Improve the quality of existing training samples (e.g., using Cleanlab or Snorkel)\\n▶Rank the importance of the training samples (Active Learning)\\n38 / 54\\nTrain, Validation, and Test Sets\\n▶Random splits of the training data when the data is not time-series data\\n▶Time-series splits of the training data when the data is time-series data\\n39 / 54\\nModel Training is an Iterative Process\\n40 / 54\\nML Pipeline Stages - Inference Pipelines\\n41 / 54\\nBatch Inference Pipeline\\n42 / 54\\nOnline Inference Pipeline\\n43 / 54\\nServerless ML with Python\\n▶Write Feature, Training, and Inference Pipelines in Python\\n▶Orchestrate the execution of Pipelines using Serverless Compute Platforms\\n▶Store features and models in a serverless feature/model store\\n▶Run a User Interface (UI), written in Python, on serverless infrastructure\\n44 / 54\\nServerless Compute Platforms\\n45 / 54\\nServerless Feature Stores and Model Registry/Serving\\nFeature Stores\\n▶Hopsworks\\nModel Registry and Serving\\n▶Hopsworks\\n▶AWS Sagemaker\\n▶Databricks\\n▶Google Vertex\\n46 / 54\\nServerless User Interfaces\\n▶Hugging Faces Spaces\\n▶Streamlit Cloud\\n47 / 54\\nIris Flower Dataset\\nhttps://github.com/ID2223KTH/id2223kth.github.io/tree/master/src/serverless-ml-\\nintro\\n▶4 input features: sepal length, sepal width, petal length, petal width\\n▶label (target): Iris Flower Type (one of Setosa, Versicolor, Virginica)\\n▶Only 150 samples in the dataset\\n48 / 54\\nServerless Iris with Modal, Hopsworks, and Hugging Face\\n49 / 54\\nIris Flowers: Feature Pipeline with Modal and Hopsworks\\nimport os\\nimport modal\\nstub = modal.Stub()\\nhopsworks_image = modal.Image.debian_slim().pip_install([\"hopsworks\"])\\n@stub.function(image=hopsworks_image, schedule=modal.Period(days=1), \\\\\\nsecret=modal.Secret.from_name(\"jim-hopsworks-ai\"))\\ndef f():\\nimport hopsworks\\nimport pandas as pd\\nproject = hopsworks.login()\\nfs = project.get_feature_store()\\niris_df = pd.read_csv(\"https://repo.hops.works/master/hopsworks-tutorials/data/iris.csv\")\\niris_fg = fs.get_or_create_feature_group( name=\"iris_modal\", version=1,\\nprimary_key=[\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\"],\\ndescription=\"Iris flower dataset\")\\niris_fg.insert(iris_df)\\nif __name__ == \"__main__\":\\nwith stub.run():\\nf()\\n50 / 54\\nTraining Pipeline with Modal and Hopsworks\\n@stub.function(image=hopsworks_image, schedule=modal.Period(days=1),\\\\\\nsecret=modal.Secret.from_name(\"jim-hopsworks-ai\"))\\ndef f():\\n# lots of imports\\nproject = hopsworks.login()\\nfs = project.get_feature_store()\\ntry:\\nfeature_view = fs.get_feature_view(name=\"iris_modal\", version=1)\\nexcept:\\niris_fg = fs.get_feature_group(name=\"iris_modal\", version=1)\\nquery = iris_fg.select_all()\\nfeature_view = fs.create_feature_view(name=\"iris_modal\",\\nversion=1,\\ndescription=\"Read from Iris flower dataset\",\\nlabels=[\"variety\"],\\nquery=query)\\nX_train, X_test, y_train, y_test = feature_view.train_test_split(0.2)\\nmodel = KNeighborsClassifier(n_neighbors=2)\\nmodel.fit(X_train, y_train.values.ravel())\\n51 / 54\\nTraining Pipeline (ctd)\\ny_pred = model.predict(X_test)\\nmetrics = classification_report(y_test, y_pred, output_dict=True)\\nresults = confusion_matrix(y_test, y_pred)\\ndf_cm = pd.DataFrame(results, [’True Setosa’, ’True Versicolor’, ’True Virginica’],\\n[’Pred Setosa’, ’Pred Versicolor’, ’Pred Virginica’])\\ncm = sns.heatmap(df_cm, annot=True)\\nfig = cm.get_figure()\\njoblib.dump(model, \"iris_model/iris_model.pkl\")\\nfig.savefig(\"iris_model/confusion_matrix.png\")\\ninput_schema = Schema(X_train)\\noutput_schema = Schema(y_train)\\nmodel_schema = ModelSchema(input_schema, output_schema)\\nmr = project.get_model_registry()\\niris_model = mr.python.create_model(\\nname=\"iris_modal\",\\nmetrics={\"accuracy\" : metrics[’accuracy’]},\\nmodel_schema=model_schema,\\ndescription=\"Iris Flower Predictor\")\\niris_model.save(\"iris_model\")\\n52 / 54\\nInteractive Inference Pipeline with Hugging Face/Hopsworks\\nmodel = mr.get_model(\"iris_modal\", version=1)\\nmodel_dir = model.download()\\nmodel = joblib.load(model_dir + \"/iris_model.pkl\")\\ndef iris(sepal_length, sepal_width, petal_length, petal_width):\\ninput_list = []\\ninput_list.append(sepal_length)\\ninput_list.append(sepal_width)\\ninput_list.append(petal_length)\\ninput_list.append(petal_width)\\nres = model.predict(np.asarray(input_list).reshape(1, -1))\\nflower_url = \"https://raw.githubusercontent.com/.../assets/\" + res[0] + \".png\"\\nreturn Image.open(requests.get(flower_url, stream=True).raw)\\ndemo = gr.Interface(\\nfn=iris, title=\"Iris Flower Predictive Analytics\", allow_flagging=\"never\",\\ndescription=\"Experiment with sepal/petal lengths/widths to predict which flower it is.\",\\ninputs=[ gr.inputs.Number(default=1.0, label=\"sepal length (cm)\"),\\ngr.inputs.Number(default=1.0, label=\"sepal width (cm)\"),\\ngr.inputs.Number(default=1.0, label=\"petal length (cm)\"),\\ngr.inputs.Number(default=1.0, label=\"petal width (cm)\"),],\\noutputs=gr.Image(type=\"pil\"))\\ndemo.launch()\\n53 / 54\\nQuestions?\\nAcknowledgements\\nSome of the images are used with permission from Hopsworks AB.\\n54 / 54', '\\nIntroduction\\nJim Dowling\\njdowling@kth.se\\nCourse Assistants: Fabien Schmidt, Javier Ron, Ahmad Al-Shishtawy\\n2023-10-30\\nSlides by Amir H. Payberah and Jim Dowling\\nCourse Information\\n1 / 79\\nCourse Objective\\n▶This course has a system-based focus.\\n▶Learn the theory of machine learning and deep learning.\\n▶Learn the practical aspects of building machine learning and deep learning algorithms\\nusing data parallel programming platforms, such as TensorFlow.\\n2 / 79\\n————————————————\\n3 / 79\\nTopics Covered in the Course\\n▶Part 1: large scale deep learning\\n•TensorFlow\\n•Deep Neural Networks (DNN)\\n•Different DNN architectures, e.g., CNNs, RNNs, Autoencoders, GAN\\n•Distributed learning\\n▶Part 2: large scale machine learning\\n•Serverless Machine Learning\\n•MLOps (machine learning operations)\\n•Feature Stores for Machine Learning\\n•Distributed Training of DNNs\\n3 / 79\\nIntended Learning Outcomes (ILOs)\\n▶ILO1: explain the principles of ML/DL algorithms and apply their techniques to solve\\nproblems.\\n▶ILO2: demonstrate an ability to design DNN architectures and explain challenges to\\nscaling their training and inference with increasing compute and data.\\n▶ILO3: explain the principles of scaling machine learning systems.\\n▶ILO4: implement scalable ML/DL systems.\\n4 / 79\\nThe Course Assessment\\n▶Task1: the Examination on 8th January 2023(A-F)\\n▶Task2: the lab assignments (A-F)\\n▶Task3: the final project (P/F)\\n5 / 79\\nHow Each ILO is Assessed?\\nTask1 Task2 Task3\\nILO1 x\\nILO2 x x x\\nILO3 x x\\nILO4 x x\\n6 / 79\\nTask1: The Exam (A-F)\\n▶Questions about the lectures and the labs.\\n▶Some sample questions for the exam will be distributed in mid December.\\n▶The examination is graded (A-F).\\n7 / 79\\nTask2: The Lab Assignments (A-F)\\n▶Two lab assignments: source code and oral presentation.\\n▶E: source code\\n▶D: source code + half questions (basic)\\n▶C: source code + all questions (basic)\\n▶B: source code + half questions (basic and advanced)\\n▶A: source code + all questions (basic and advanced)\\n8 / 79\\nTask3: The Final Project (A-F)\\n▶One final project: source code and oral presentation.\\n▶Proposed by students and confirmed by the teacher:\\n▶Source code and documentation as a README.md by the students of the project\\n▶5 minute Presentation of the project by the students.\\n▶20% of total grade bonus for projects graded as excellent.\\n9 / 79\\nThe Final Grade\\n▶The final grade is the weighted average of the Exam (0.3), two labs (0.15 each). A\\nbonus of .09 points will be added for projects graded ’excellent’.\\n▶To compute it, map A-E to 5-1, and take the average.\\n▶The floating values are rounded up, if they are more than half, otherwise they are\\nrounded down.\\n•E.g., 3.6 will be rounded to 4, and 4.5 will be rounded to 4.\\n▶A late submission will reduce your grade level by one. That is, A will become B, B\\nwill become C, and so on.\\n▶To pass the course, you need to take at least E in all the assignments.\\n10 / 79\\nHow do you submit the Assignments?\\n▶Through the Canvas site.\\n▶Students will work in groups of two on Task 2 and Task 3.\\n11 / 79\\nThe Course Material\\n▶Hands-on machine learning with Scikit-Learn and TensorFlow, 2nd Edition, A. Geron,\\nO’Reilly Media, 2019\\n▶Deep learning, I. Goodfellow et al., Cambridge: MIT press, 2016\\n12 / 79\\nThe Course Web Page\\nhttps://id2223kth.github.io\\n13 / 79\\nSupervised Machine Learning 101\\n14 / 79\\nFeatures in Machine Learning\\n15 / 79\\nSupervised Machine Learning (ML)\\n16 / 79\\nA Linear Model can classify the Fruit (Classifier)\\n17 / 79\\nThe Decision Boundary\\n18 / 79\\nSource code for a small sample of data with a decision tree\\nclassifier\\n19 / 79\\nBut wait, apples can also be red!\\n20 / 79\\nIt’s harder to separate Green Apples, Oranges, and Red Apples\\nwith just 2 colors (red and green)\\n21 / 79\\nFrom two dimensions to millions of dimensions\\n▶Can we just add more features? Yes. If you add weight and smoothness, you can\\nseparate apples and oranges. We could plot our fruit in 3d or even 4d and find a\\nplane that separates apples and oranges\\n▶You can add many more features (dimensions). With the caveat that too many\\ndimensions can lead to overfitting. Overfitting means your model is not good at\\ngeneralizing to correctly predict new fruit examples (it would work well for the training\\ndata, but not unseen (new) examples)\\n▶In image classification, each pixel is a feature. That’s millions of features for a single\\nHD image. Deep learning can be used to train models with millions of features.\\n22 / 79\\nNot all properties with predictive power should be features\\n23 / 79\\nLots of features: deep learning for apple classification works\\n24 / 79\\nA model can also be trained to predict a number (regression)\\n25 / 79\\nPredicting Surf Height at a beach – Classification or\\nRegression?\\n26 / 79\\nWhat is Supervised Machine Learning, then?\\n▶With our Apple/Orange classifier, we used (features, label) examples to train a model\\nto find a decision boundary.\\n▶Then when a new fruit arrived, we could use the model to predict if the new piece\\nof fruit is an apple or an orange.\\n▶We can generalize to say that supervised machine learning is concerned with:\\n▶extracting a pattern from labeled data (features) to a model\\n▶using that model to make predictions for new unlabeled data (features)\\n27 / 79\\nTradtional ML courses vs ID2223\\n▶Static Datasets, where Features for ML are correct and unbiased\\n▶The goal is to optimize your model with a model evaluation metric (accuracy) to\\ncommunicate the value of your model\\n▶Data never stops coming and it comes from heterogeneous data sources\\n▶Communicate the value of your model as a Prediction Service - one that can be scaled\\nand deployed using MLOps (Machine Learning Operations) best practices (versioning,\\nautomated testing/deployment)\\n28 / 79\\nFeature Engineering is often treated like this “helpful” guide to\\ndrawing a Barn Owl\\n29 / 79\\nExtract the features from the input data. We will study feature\\nengineering at scale.\\nRaw Data Extracted Feature Method\\nHotel room bookings Weekly vacancy level Aggregation\\nUser’s web session history Session history embedding Dimensionality Reduction\\nUser’s date of birth Binning\\nHourly spot electricity prices Scale into range [0, 1] Normalization\\nUser’s home country Binary number of country One Hot Encoding\\n30 / 79\\nMany Enterprises have walled gardens between teams building\\nproduction ML Systems\\n31 / 79\\nWith infrastructure support (Serverless Machine Learning),\\ndevelopers need less infrastructural skills to deploy ML Systems\\n32 / 79\\nMonolithic ML Pipeline\\n33 / 79\\nRefactor the Monolithic ML Pipeline to Scale your ML Systems\\n34 / 79\\nMachine Learning and Deep Learning\\n35 / 79\\nLearning Algorithms\\n▶A ML algorithm is an algorithm that is able to learn from data.\\n▶What is learning?\\n▶A computer program is said to learn from experience E with respect to some class of\\ntasks T and performance measure P, if its performance at tasks in T, as measured\\nby P, improves with experience E. (Tom M. Mitchell)\\n36 / 79\\nLearning Algorithms - Example 1\\n▶A spam filter that can learn to flag spam given examples of spam emails and examples\\nof regular emails.\\n▶Task T: flag spam for new emails\\n▶Experience E: the training data\\n▶Performance measure P: the ratio of correctly classified emails\\n[https://bit.ly/2oiplYM]\\n37 / 79\\nLearning Algorithms - Example 2\\n▶Given dataset of prices of 500 houses, how can we learn to predict the prices of other\\nhouses, as a function of the size of their living areas?\\n▶Task T: predict the price\\n▶Experience E: the dataset of living areas and prices\\n▶Performance measure P: the difference between the predicted price and the real price\\n[https://bit.ly/2MyiJUy]\\n38 / 79\\nTypes of Machine Learning Algorithms\\n▶Supervised learning\\n•Input data is labeled, e.g., spam/not-spam or a stock price at a time.\\n•Regression vs. classification\\n▶Unsupervised learning\\n•Input data is unlabeled.\\n•Find hidden structures in data.\\n39 / 79\\nAI Generations - Deep Learning\\n▶For many tasks, it is difficult to know what features should be extracted\\n▶Use machine learning to discover the mapping from representation to output\\n[https://bit.ly/2woLEzs]\\n40 / 79\\nImage Classification with Deep Learning\\n▶For image classification, where each pixel is a feature, Deep Learning can do the\\nfeature extraction as part of the learning algorithm.\\n41 / 79\\nSheepdog or Mop\\n42 / 79\\nChihuahua or Muffin\\n43 / 79\\nBarn Owl or Apple\\n44 / 79\\nTraining Deep Neural Networks\\n▶Computationally intensive\\n▶Time consuming\\n[https://cloud.google.com/tpu/docs/images/inceptionv3onc--oview.png]\\n45 / 79\\nWhy?\\n▶Massive amount of training dataset\\n▶Large number of parameters\\n46 / 79\\nAccuracy vs. Data/Model Size\\n[Jeff Dean at AI Frontiers: Trends and Developments in Deep Learning Research]\\n47 / 79\\nAccuracy vs. Data/Model Size\\n[Jeff Dean at AI Frontiers: Trends and Developments in Deep Learning Research]\\n48 / 79\\nAccuracy vs. Data/Model Size\\n[Jeff Dean at AI Frontiers: Trends and Developments in Deep Learning Research]\\n49 / 79\\nWhy Does Deep Learning Work Now?\\n▶Huge quantity of data\\n▶Tremendous increase in computing power\\n▶Better training algorithms\\n50 / 79\\nLinear Algebra Review\\n51 / 79\\nVector\\n▶A vector is an array of numbers.\\n▶Notation:\\n•Denoted by bold lowercase letters, e.g., x.\\n•xidenotes the ith entry.\\nx=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0x1\\nx2\\n...\\nxn\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n52 / 79\\nMatrix and Tensor\\n▶A matrix is a 2-D array of numbers.\\n▶A tensor is an array with more than two axes.\\n▶Notation:\\n•Denoted by bold uppercase letters, e.g., A.\\n•aijdenotes the entry in ith row and jth column.\\n•IfAism×n, it has mrows and ncolumns.\\nA=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0a1,1a1,2a1,3. . . a1,n\\na2,1a2,2a2,3. . . a2,n\\n...............\\nam,1am,2am,3. . . am,n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n53 / 79\\nMatrix Addition and Subtraction\\n▶The matrices must have the same dimensions.\\nA=\\x14a b\\nc d\\x15\\n+\\x14e f\\ng h\\x15\\n=\\x14a+e b +f\\nc+g d +h\\x15\\n54 / 79\\nMatrix Product\\n▶The matrix product of matrices AandBis a third matrix C, where C=AB.\\n▶IfAis of shape m×nandBis of shape n×p, then Cis of shape m×p.\\ncij=X\\nkaikbkj\\n▶Properties\\n•Associative: ( AB)C=A(BC)\\n•Not commutative: AB̸=BA\\n[https://en.wikipedia.org/wiki/Matrix multiplication]\\n55 / 79\\nMatrix Transpose\\n▶Swap the rows and columns of a matrix.\\nA=\\uf8ee\\n\\uf8f0a b\\nc d\\ne f\\uf8f9\\n\\uf8fb⇒A⊺=\\x14a c e\\nb d f\\x15\\n▶Properties\\n•Aij=A⊺\\nji\\n•IfAism×n, then A⊺isn×m\\n•(A+B)⊺=A⊺+B⊺\\n•(AB)⊺=B⊺A⊺\\n56 / 79\\nInverse of a Matrix\\n▶IfAis a square matrix, its inverse is called A−1.\\nAA−1=A−1A=I\\n▶Where I, the identity matrix, is a diagonal matrix with all 1’s on the diagonal.\\nI2=\\x141 0\\n0 1\\x15\\nI3=\\uf8ee\\n\\uf8f01 0 0\\n0 1 0\\n0 0 1\\uf8f9\\n\\uf8fb\\n57 / 79\\nLpNorm for Vectors\\n▶We can measure the size of vectors using a norm function.\\n▶Norms are functions mapping vectors to non-negative values.\\n▶L1norm\\n||x||1=X\\ni|xi|\\n▶L2norm\\n||x||2= (X\\ni|xi|2)1\\n2=q\\nx2\\n1+x2\\n2+···+x2n\\n▶Lpnorm\\n||x||p= (X\\ni|xi|p)1\\np\\n58 / 79\\nProbability Review\\n59 / 79\\nRandom Variables\\n▶Random variable: a variable that can take on different values randomly.\\n▶Random variables may be discrete or continuous.\\n•Discrete random variable: finite or countably infinite number of states\\n•Continuous random variable: real value\\n▶Notation:\\n•Denoted by an upper case letter, e.g., X\\n•Values of a random variable Xare denoted by lower case letters, e.g., xandy.\\n60 / 79\\nProbability Distributions\\n▶Probability distribution: how likely a random variable is to take on each of its possible\\nstates.\\n•E.g., the random variable Xdenotes the outcome of a coin toss.\\n•The probability distribution of Xwould take the value 0.5forX=head , and 0.5for\\nY=tail (assuming the coin is fair).\\n▶The way we describe probability distributions depends on whether the variables are\\ndiscrete or continuous.\\n61 / 79\\nDiscrete Variables\\n▶Probability mass function (PMF): the probability distribution of a discrete random\\nvariable X.\\n▶Notation: denoted by a lowercase p.\\n•E.g., p(x) =1indicates that X=xis certain\\n•E.g., p(x) =0indicates that X=xis impossible\\n▶Properties:\\n•The domain Dofpmust be the set of all possible states of X\\n•∀x∈D(X),0≤p(x)≤1\\n•P\\nx∈D(X)p(x) =1\\n62 / 79\\nIndependence\\n▶Two random variables XandYare independent, if their probability distribution can\\nbe expressed as their products.\\n∀x∈D(X),y∈D(Y),p(X=x,Y=y) =p(X=x)p(Y=y)\\n▶E.g., if a coin is tossed and a single 6-sided die is rolled, then the probability of\\nlanding on the head side of the coin and rolling a 3 on the die is:\\np(X=head ,Y=3) =p(X=head )p(Y=3) =1\\n2×1\\n6=1\\n12\\n63 / 79\\nConditional Probability\\n▶Conditional probability: the probability of an event given that another event has\\noccurred.\\np(Y=y|X=x) =p(Y=y,X=x)\\np(X=x)\\n▶E.g., if 60% of the class passed both labs and 80% of the class passed the first labs,\\nthen what percent of those who passed the first lab also passed the second lab?\\n•E.g., XandYrandom variables for the first and the second labs, respectively.\\np(Y=lab2|X=lab1 ) =p(Y=lab2 ,X=lab1 )\\np(X=lab1 )=0.6\\n0.8=3\\n4\\n64 / 79\\nExpectation\\n▶The expected value of a random variable Xwith respect to a probability distribution\\np(X) is the average value that Xtakes on when it is drawn from p(X).\\nEx∼p[X] =X\\nxp(x)x\\n▶E.g., If X:{1,2,3}, and p(X=1) =0.3,p(X=2) =0.5,p(X=3) =0.2\\n•E[X] =0.3×1+0.5×2+0.2×3=1.9\\n65 / 79\\nVariance and Standard Deviation\\n▶The variance gives a measure of how much the values of a random variable Xvary\\nas we sample it from its probability distribution p(X).\\nVar(X) =E[(X−E[X])2]\\nVar(X) =X\\nxp(x)(x−E[X])2\\n▶E.g., If X:{1,2,3}, and p(X=1) =0.3,p(X=2) =0.5,p(X=3) =0.2\\n•E[X] =0.3×1+0.5×2+0.2×3=1.9\\n•Var(X) =0.3(1−1.9)2+0.5(2−1.9)2+0.2(3−1.9)2=0.49\\n▶The standard deviation, shown by σ, is the square root of the variance.\\n66 / 79\\nCovariance (1/2)\\n▶The covariance gives some sense of how much two values are linearly related to each\\nother.\\nCov(X,Y) =E[(X−E[X])(Y−E[Y])]\\nCov(X,Y) =X X\\n(x,y)p(x,y)(x−E[X])(y−E[Y])\\n67 / 79\\nCovariance (2/2)\\nY\\np(X, Y) 1 2 3 p(X)\\n1 1/4 1/4 0 1/2\\nX 2 0 1/4 1/4 1/2\\np(Y) 1/4 1/2 1/4 1\\nE[X] =1\\n2×1+1\\n2×2=3\\n2E[Y] =1\\n4×1+1\\n2×2+1\\n4×3=2\\nCov(X,Y) =X X\\n(x,y)p(x,y)(x−E[X])(y−E[Y])\\n=1\\n4(1−3\\n2)(1−2) +1\\n4(1−3\\n2)(2−2) +0(1−3\\n2)(3−2)\\n+0(2−3\\n2)(1−2) +1\\n4(2−3\\n2)(2−2) +1\\n4(2−3\\n2)(3−2) =1\\n4\\n68 / 79\\nCorrelation Coefficient\\n▶The Correlation coefficient is a quantity that measures the strength of the association\\n(or dependence) between two random variables, e.g., XandY.\\nρ(X,Y) =Cov(X,Y)\\nσ(X)σ(Y)\\n69 / 79\\nProbability and Likelihood (1/2)\\n▶LetX:{x(1),x(2),···,x(m)}be a discrete random variable drawn independently from\\na distribution probability pdepending on a parameter θ.\\n•For six tosses of a coin, X:{h,t,t,t,h,t},h: head, and t: tail.\\n•Suppose you have a coin with probability θto land heads and ( 1−θ) to land tails.\\n▶p(X|θ=2\\n3) is the probability of Xgiven θ=2\\n3.\\n▶p(X=h|θ) is the likelihood of θgiven X=h.\\n▶Likelihood ( L): a function of the parameters ( θ) of a probability model, given specific\\nobserved data, e.g., X=h.\\nL(θ|X) =p(X|θ)\\n70 / 79\\nProbability and Likelihood (2/2)\\n▶The likelihood differs from that of a probability.\\n▶A probability p(X|θ) refers to the occurrence of future events.\\n▶A likelihood L(θ|X) refers to past events with known outcomes.\\n71 / 79\\nMaximum Likelihood Estimator\\n▶If samples in Xare independent we have:\\nL(θ|X) =p(X|θ) =p(x(1),x(2),···,x(m)|θ)\\n=p(x(1)|θ)p(x(2)|θ)···p(x(m)|θ) =mY\\ni=1p(x(i)|θ)\\n▶The maximum likelihood estimator (MLE): what is the most likely value of θgiven\\nthe training set?\\n^θMLE= arg max\\nθL(θ|X) = arg max\\nθmY\\ni=1p(x(i)|θ)\\n72 / 79\\nMaximum Likelihood Estimator - Example\\n▶Six tosses of a coin, with the following model:\\n•Possible outcomes: h with probability of θ, and t with probability ( 1−θ).\\n•Results of coin tosses are independent of one another.\\n▶Data: X:{h,t,t,t,h,t}\\n▶The likelihood is\\nL(θ|X) =p(X|θ)\\n=p(X=h|θ)p(X=t|θ)p(X=t|θ)p(X=t|θ)p(X=h|θ)p(X=t|θ)\\n=θ(1−θ)(1−θ)(1−θ)θ(1−θ)\\n=θ2(1−θ)4\\n▶^θis the value of θthat maximizes the likelihood:\\n^θMLE= arg max\\nθL(θ|X) =2\\n2+4\\n73 / 79\\nLog-Likelihood\\n▶The MLE product is prone to numerical underflow.\\n^θMLE= arg max\\nθL(θ|X) = arg max\\nθmY\\ni=1p(x(i)|θ)\\n▶To overcome this problem we can use the logarithm of the likelihood.\\n•It does not change its arg max, but transforms a product into a sum.\\n^θMLE= arg max\\nθmX\\ni=1logp (x(i)|θ)\\n74 / 79\\nNegative Log-Likelihood\\n▶Likelihood: L(θ|X) =Qm\\ni=1p(x(i)|θ)\\n▶Log-Likelihood: logL (θ|X) =logQm\\ni=1p(x(i)|θ) =Pm\\ni=1logp (x(i)|θ)\\n▶Negative Log-Likelihood: −logL (θ|X) =−Pm\\ni=1logp (x(i)|θ)\\n▶Negative log-likelihood is also called the cross-entropy\\n75 / 79\\nCross-Entropy\\n▶Coss-entropy: quantify the difference (error) between two probability distributions.\\n▶How close is the predicted distribution to the true distribution?\\nH(p,q) =−X\\nxp(x)log(q(x))\\n▶Where pis the true distribution, and qthe predicted distribution.\\n76 / 79\\nCross-Entropy - Example\\n▶Six tosses of a coin: X:{h,t,t,t,h,t}\\n▶The true distribution p:p(h) =2\\n6andp(t) =4\\n6\\n▶The predicted distribution q: h with probability of θ, and t with probability ( 1−θ).\\n▶Cross entropy: H(p,q) =−P\\nxp(x)log(q(x))\\n=−p(h)log(q(h))−p(t)log(q(t)) =−2\\n6log(θ)−4\\n6log(1−θ)\\n▶Likelihood: θ2(1−θ)4\\n▶Negative log likelihood: −log(θ2(1−θ)4) =−2log (θ)−4log (1−θ)\\n77 / 79\\nReferences\\n▶Ian Goodfellow et al., Deep Learning (Ch. 1, 2, 3)\\n78 / 79\\nQuestions?\\nAcknowledgements\\nSome of the pictures were copied from the book Hands-On Machine Learning\\nwith Scikit-Learn and TensorFlow, Aurelien Geron, O’Reilly Media, 2017.\\n79 / 79']\n"
     ]
    }
   ],
   "source": [
    "print(transcriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"text\": \"Serverless Machine Learning enables the modularization of ML pipelines, addressing the problems associated with monolithic pipelines. Monolithic pipelines lack modularity, making it difficult to scale or deploy components independently. They are also challenging to test and tightly couple feature engineering, model training, and inference steps. In contrast, modular pipelines allow for more robust and scalable systems by separating functionality into independent classes and functions. These modules can be easily reused and tested, promoting code reusability. Additionally, modular pipelines enable the connection to data sources, reading raw data, and managing dependencies and connection information securely. Feature pipelines are another crucial component of ML pipelines, responsible for orchestrating feature engineering steps on input data to create feature values. These steps include data cleaning, validation, deduplication, feature extraction, aggregations, feature binning, and feature crosses. Tabular data is commonly used in feature pipelines, and libraries like Pandas facilitate exploratory data analysis, aggregations, rolling windows, and feature crosses. Embeddings, which are lower dimensional representations of sparse inputs, can also be used as features in ML pipelines. Embedding stores, or vector databases, store semantically similar inputs close together in the embedding space. Finally, a feature store is a component that manages and stores features and labels for training and inference purposes.\",\n",
      "  \"questions\": [\n",
      "    \"What are the problems with monolithic ML pipelines?\",\n",
      "    \"What are the benefits of modular pipelines?\",\n",
      "    \"What are some feature engineering steps in a feature pipeline?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The problems with monolithic ML pipelines include a lack of modularity, making it difficult to independently scale or deploy components, challenges in testing, and tight coupling between feature engineering, model training, and inference steps.\",\n",
      "    \"Modular pipelines offer benefits such as improved scalability, independent component scaling and deployment, easier testing, and promotion of code reusability.\",\n",
      "    \"Some feature engineering steps in a feature pipeline include data cleaning, validation, deduplication, feature extraction, aggregations, feature binning, and feature crosses.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"text\": \"Feature types are an important aspect of machine learning pipelines, and they can be computed on-demand or stored in a feature store for reuse across models. Feature stores are particularly useful for online models that require historical or contextual information. Model training pipelines involve various stages, and model-dependent transformations are often applied to the features. These transformations can be done using tools like Pandas. Model training is an iterative process and can be improved by trying different algorithms, adjusting hyperparameters, and evaluating the model on a validation set. Data-centric iteration can also enhance model performance by adding/removing features, adding more training data, or improving the quality of existing training samples. The use of train, validation, and test sets is crucial in model training and evaluation. Inference pipelines include batch inference and online inference. Serverless ML with Python allows for the writing and execution of feature, training, and inference pipelines, along with the storage of features and models in a serverless feature/model store. Serverless compute platforms and feature stores like Hopsworks, AWS Sagemaker, Databricks, and Google Vertex can be utilized. User interfaces written in Python can also be deployed on serverless infrastructure. The Iris Flower dataset is an example dataset with four input features and a target label representing the Iris Flower Type. It can be used to demonstrate serverless ML with tools like Modal, Hopsworks, and Hugging Face.\",\n",
      "  \"questions\": [\n",
      "    \"What are feature stores and how are they useful in machine learning pipelines?\",\n",
      "    \"How can model training be improved iteratively?\",\n",
      "    \"What are some serverless compute platforms and feature stores that can be used for serverless ML?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Feature stores are used to store features that can be reused across different models for training and inference. They are particularly helpful in online models that require historical or contextual information.\",\n",
      "    \"Model training can be improved iteratively by trying out different supervised ML learning algorithms, adjusting hyperparameters, evaluating the model on a validation set, and making data-centric improvements such as adding/removing features, adding more training data, and improving the quality of existing training samples.\",\n",
      "    \"Some serverless compute platforms and feature stores that can be used for serverless ML include Hopsworks, AWS Sagemaker, Databricks, and Google Vertex.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"text\": \"The code snippet provided demonstrates a training pipeline for an iris flower classification model. It starts by logging into Hopsworks and getting the feature store. If the feature view for the iris dataset is not found, it creates a feature group and a feature view based on the iris dataset. Then, it splits the data into training and testing sets, initializes a K-Nearest Neighbors classifier, fits the model on the training data, and evaluates its performance on the testing data. The classification report, confusion matrix, and the trained model are saved. The code also shows an interactive inference pipeline where the saved model is loaded and used to predict the species of an iris flower based on its sepal and petal dimensions. Lastly, a graphical user interface is created using the Gradio library to allow users to input iris flower dimensions and see the predicted flower species.\",\n",
      "  \"questions\": [\n",
      "    \"What is the purpose of the training pipeline?\",\n",
      "    \"How is the model evaluated in the pipeline?\",\n",
      "    \"What is the purpose of the interactive inference pipeline?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"The purpose of the training pipeline is to train a classification model for iris flowers using the K-Nearest Neighbors algorithm.\",\n",
      "    \"The model is evaluated in the pipeline by predicting the flower species for the testing data and generating a classification report and confusion matrix to assess its performance.\",\n",
      "    \"The purpose of the interactive inference pipeline is to allow users to input the dimensions of an iris flower and get a prediction of its species using the trained model.\"\n",
      "  ]\n",
      "}\n",
      "{'text': 'This course has a system-based focus. The objective is to learn the theory and practical aspects of machine learning and deep learning algorithms. The course covers topics such as TensorFlow, Deep Neural Networks (DNN), different DNN architectures (CNNs, RNNs, Autoencoders, GAN), distributed learning, serverless machine learning, MLOps, feature stores for machine learning, and distributed training of DNNs. The intended learning outcomes include the ability to explain and apply ML/DL algorithms, design DNN architectures, scale machine learning systems, and implement scalable ML/DL systems. The course assessment includes an exam, lab assignments, and a final project, with each task assessing different learning outcomes. The final grade is calculated based on the weighted average of the exam and lab grades, with a bonus for excellent projects. Late submissions result in a grade reduction. Assignments are submitted through the Canvas site, and students work in groups for some tasks. The recommended course material includes books on hands-on machine learning and deep learning. The course webpage provides additional resources.', 'questions': ['What is the focus of the course?', 'What are the topics covered in the course?', 'How is the final grade calculated?'], 'answers': ['The focus of the course is system-based.', 'The topics covered in the course include TensorFlow, DNN, different DNN architectures, distributed learning, serverless machine learning, MLOps, feature stores for machine learning, and distributed training of DNNs.', 'The final grade is calculated based on the weighted average of the exam and lab grades, with a bonus for excellent projects.']}\n",
      "{\n",
      "\"text\": \"Deep learning is capable of training models with millions of features, making it useful for image classification. Supervised machine learning involves extracting patterns from labeled data to create a model that can make predictions for new unlabeled data. Feature engineering is the process of extracting relevant features from input data. Enterprises often have separate teams working on production ML systems, but with infrastructure support like serverless machine learning, developers require less infrastructure skills. Machine learning algorithms learn from experience and improve their performance with time. There are two types of machine learning algorithms: supervised learning (with labeled input data) and unsupervised learning (with unlabeled input data). Deep learning is a type of AI generation that uses machine learning to discover the mapping from representation to output. In image classification, deep learning can perform feature extraction as part of the learning algorithm.\",\n",
      "\"questions\": [\n",
      "\"Can deep learning handle images with millions of features?\",\n",
      "\"What is supervised machine learning and how does it work?\",\n",
      "\"What is the role of feature engineering in machine learning?\"\n",
      "],\n",
      "\"answers\": [\n",
      "\"Yes, deep learning is capable of handling images with millions of features.\",\n",
      "\"Supervised machine learning involves extracting patterns from labeled data to create a model that can make predictions for new unlabeled data.\",\n",
      "\"Feature engineering is the process of extracting relevant features from input data.\"\n",
      "]\n",
      "}\n",
      "{\n",
      "  \"text\": \"Deep neural network training is computationally intensive and time-consuming due to the massive amount of training data and large number of parameters. The increase in computing power, better training algorithms, and availability of huge quantities of data are the reasons why deep learning works now. Linear algebra concepts like vectors, matrices, matrix operations (addition, subtraction, multiplication, transpose, inverse), and vector norms are important in deep learning. Probability theory is also used in deep learning, with random variables, probability distributions, probability mass functions, independence, and conditional probability being key concepts.\",\n",
      "  \"questions\": [\n",
      "    \"Why is deep neural network training computationally intensive and time-consuming?\",\n",
      "    \"What are the reasons why deep learning works now?\",\n",
      "    \"What are some important linear algebra concepts in deep learning?\",\n",
      "    \"How is probability theory used in deep learning?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Deep neural network training is computationally intensive and time-consuming due to the massive amount of training data and large number of parameters.\",\n",
      "    \"Deep learning works now due to the increase in computing power, better training algorithms, and availability of huge quantities of data.\",\n",
      "    \"Important linear algebra concepts in deep learning include vectors, matrices, matrix operations (addition, subtraction, multiplication, transpose, inverse), and vector norms.\",\n",
      "    \"Probability theory is used in deep learning to understand random variables, probability distributions, probability mass functions, independence, and conditional probability.\"\n",
      "  ]\n",
      "}\n",
      "{\"text\": \"Probability and Likelihood are two concepts used in the field of statistics. Probability refers to the chance of an event or outcome occurring in the future, while Likelihood refers to the probability of a past event or outcome given certain observed data. In Maximum Likelihood Estimation, we try to find the most likely value of a parameter in a probability model based on a given set of data. The likelihood function is used to calculate the probability of the data given the parameter value. In order to avoid numerical underflow, the logarithm of the likelihood function is often used. The negative log-likelihood, also known as cross-entropy, is used to quantify the difference between two probability distributions.\", \n",
      "\"questions\": [\"What is the difference between probability and likelihood?\", \"What is the purpose of Maximum Likelihood Estimation?\", \"How is numerical underflow avoided in Maximum Likelihood Estimation?\"],\n",
      "\"answers\": [\"Probability refers to future events, while likelihood refers to past events. \", \"The purpose of Maximum Likelihood Estimation is to find the most likely value of a parameter in a probability model based on a given set of data. \", \"Numerical underflow is avoided in Maximum Likelihood Estimation by using the logarithm of the likelihood function.\"] }\n",
      "{\n",
      "    \"text\": \"Entropy is a mathematical concept used to measure the uncertainty or randomness in a probability distribution. In this example, we have six tosses of a coin with outcomes of heads and tails. The true distribution of the coin tosses is given by p(h) = 2/6 and p(t) = 4/6. The predicted distribution is represented by q, where heads has a probability of θ and tails has a probability of (1−θ). The cross entropy between the true and predicted distributions is calculated using the formula -px(p)log(q(x)). The likelihood of the coin tosses occurring according to the predicted distribution is θ^2(1-θ)^4. The negative log likelihood is -log(θ^2(1-θ)^4) = -2log(θ) - 4log(1-θ).\",\n",
      "    \"questions\": [\n",
      "        \"What is entropy and how is it used?\",\n",
      "        \"What is the true distribution of the coin tosses in the example?\",\n",
      "        \"How is the negative log likelihood calculated in this example?\"\n",
      "    ],\n",
      "    \"answers\": [\n",
      "        \"Entropy is a measure of uncertainty or randomness in a probability distribution. It is used to quantify the amount of information or surprise in an outcome. In this example, entropy is used to compare the true distribution of coin tosses to a predicted distribution.\",\n",
      "        \"The true distribution of the coin tosses is given by p(h) = 2/6 (2 out of 6 tosses resulting in heads) and p(t) = 4/6 (4 out of 6 tosses resulting in tails).\",\n",
      "        \"The negative log likelihood is calculated by taking the logarithm of the predicted distribution's likelihood function. In this example, the negative log likelihood is given by -log(θ^2(1-θ)^4) = -2log(θ) - 4log(1-θ), where θ represents the probability of heads and (1-θ) represents the probability of tails.\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "from math import ceil\n",
    "\n",
    "api_key = \"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\"\n",
    "client = OpenAI(api_key=api_key)\n",
    "responses = []\n",
    "for t in transcriptions:\n",
    "    context = t\n",
    "    i = 0\n",
    "    chunks = []\n",
    "    for i in range(ceil(len(t)/4097)):\n",
    "        chunks.append(t[i*4097:i*4097+4097])\n",
    "    for c in chunks:\n",
    "        context = c\n",
    "        question = \"The text above is the result of the transcription of slides in the PDF file format. Remove chapter names and slides numbers and rephrase the sentences. Once you do that generate 2 to 3 meaningful questions on the text and the respective answers. Plese reply in the JSON format {'text':<full elaborated text>,'questions':<questions generated>,'answers':<answers generated>}. DO NOT write anything else than the requested JSON and remember to write the full elaborated content and not just one part.\"\n",
    "        # response = openai.Completion.create(\n",
    "        # engine=\"gpt-3.5-turbo\",\n",
    "        prompt=f\"\\nContext: {context}\\nQuestion: {question}\"\n",
    "        # )\n",
    "    # answer = response.choices[0].text.strip()\n",
    "    # print(answer)\n",
    "        response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt },\n",
    "            \n",
    "        ]\n",
    "\n",
    "        )\n",
    "        \n",
    "        print(response.choices[0].message.content)\n",
    "        responses.append(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                                                                                                                                                            Instructions\n",
      "0                                                                               The conversation between Human and AI assistance [INST] What are the problems with monolithic ML pipelines? [/INST] The problems with monolithic ML pipelines include a lack of modularity, making it difficult to independently scale or deploy components, challenges in testing, and tight coupling between feature engineering, model training, and inference steps.\n",
      "1                                                                                                                                                                            The conversation between Human and AI assistance [INST] What are the benefits of modular pipelines? [/INST] Modular pipelines offer benefits such as improved scalability, independent component scaling and deployment, easier testing, and promotion of code reusability.\n",
      "2                                                                                                                                          The conversation between Human and AI assistance [INST] What are some feature engineering steps in a feature pipeline? [/INST] Some feature engineering steps in a feature pipeline include data cleaning, validation, deduplication, feature extraction, aggregations, feature binning, and feature crosses.\n",
      "3                                                                                      The conversation between Human and AI assistance [INST] What are feature stores and how are they useful in machine learning pipelines? [/INST] Feature stores are used to store features that can be reused across different models for training and inference. They are particularly helpful in online models that require historical or contextual information.\n",
      "4   The conversation between Human and AI assistance [INST] How can model training be improved iteratively? [/INST] Model training can be improved iteratively by trying out different supervised ML learning algorithms, adjusting hyperparameters, evaluating the model on a validation set, and making data-centric improvements such as adding/removing features, adding more training data, and improving the quality of existing training samples.\n",
      "5                                                                                                                             The conversation between Human and AI assistance [INST] What are some serverless compute platforms and feature stores that can be used for serverless ML? [/INST] Some serverless compute platforms and feature stores that can be used for serverless ML include Hopsworks, AWS Sagemaker, Databricks, and Google Vertex.\n",
      "6                                                                                                                                                                                                        The conversation between Human and AI assistance [INST] What is the purpose of the training pipeline? [/INST] The purpose of the training pipeline is to train a classification model for iris flowers using the K-Nearest Neighbors algorithm.\n",
      "7                                                                                                                                                        The conversation between Human and AI assistance [INST] How is the model evaluated in the pipeline? [/INST] The model is evaluated in the pipeline by predicting the flower species for the testing data and generating a classification report and confusion matrix to assess its performance.\n",
      "8                                                                                                                                                  The conversation between Human and AI assistance [INST] What is the purpose of the interactive inference pipeline? [/INST] The purpose of the interactive inference pipeline is to allow users to input the dimensions of an iris flower and get a prediction of its species using the trained model.\n",
      "9                                                                                                                                                                                                                                                 The conversation between Human and AI assistance [INST] Can deep learning handle images with millions of features? [/INST] Yes, deep learning is capable of handling images with millions of features.\n",
      "10                                                                                                                                                                              The conversation between Human and AI assistance [INST] What is supervised machine learning and how does it work? [/INST] Supervised machine learning involves extracting patterns from labeled data to create a model that can make predictions for new unlabeled data.\n",
      "11                                                                                                                                                                                                                                      The conversation between Human and AI assistance [INST] What is the role of feature engineering in machine learning? [/INST] Feature engineering is the process of extracting relevant features from input data.\n",
      "12                                                                                                                                             The conversation between Human and AI assistance [INST] Why is deep neural network training computationally intensive and time-consuming? [/INST] Deep neural network training is computationally intensive and time-consuming due to the massive amount of training data and large number of parameters.\n",
      "13                                                                                                                                                                                            The conversation between Human and AI assistance [INST] What are the reasons why deep learning works now? [/INST] Deep learning works now due to the increase in computing power, better training algorithms, and availability of huge quantities of data.\n",
      "14                                                                                                                                      The conversation between Human and AI assistance [INST] What are some important linear algebra concepts in deep learning? [/INST] Important linear algebra concepts in deep learning include vectors, matrices, matrix operations (addition, subtraction, multiplication, transpose, inverse), and vector norms.\n",
      "15                                                                                                                                                        The conversation between Human and AI assistance [INST] How is probability theory used in deep learning? [/INST] Probability theory is used in deep learning to understand random variables, probability distributions, probability mass functions, independence, and conditional probability.\n",
      "16                                                                                                                                                                                                                                              The conversation between Human and AI assistance [INST] What is the difference between probability and likelihood? [/INST] Probability refers to future events, while likelihood refers to past events. \n",
      "17                                                                                                                                                                              The conversation between Human and AI assistance [INST] What is the purpose of Maximum Likelihood Estimation? [/INST] The purpose of Maximum Likelihood Estimation is to find the most likely value of a parameter in a probability model based on a given set of data. \n",
      "18                                                                                                                                                                                               The conversation between Human and AI assistance [INST] How is numerical underflow avoided in Maximum Likelihood Estimation? [/INST] Numerical underflow is avoided in Maximum Likelihood Estimation by using the logarithm of the likelihood function.\n",
      "19                                                                        The conversation between Human and AI assistance [INST] What is entropy and how is it used? [/INST] Entropy is a measure of uncertainty or randomness in a probability distribution. It is used to quantify the amount of information or surprise in an outcome. In this example, entropy is used to compare the true distribution of coin tosses to a predicted distribution.\n",
      "20                                                                                                                                                        The conversation between Human and AI assistance [INST] What is the true distribution of the coin tosses in the example? [/INST] The true distribution of the coin tosses is given by p(h) = 2/6 (2 out of 6 tosses resulting in heads) and p(t) = 4/6 (4 out of 6 tosses resulting in tails).\n",
      "21    The conversation between Human and AI assistance [INST] How is the negative log likelihood calculated in this example? [/INST] The negative log likelihood is calculated by taking the logarithm of the predicted distribution's likelihood function. In this example, the negative log likelihood is given by -log(θ^2(1-θ)^4) = -2log(θ) - 4log(1-θ), where θ represents the probability of heads and (1-θ) represents the probability of tails.\n",
      "The conversation between Human and AI assistance [INST] What are the problems with monolithic ML pipelines? [/INST] The problems with monolithic ML pipelines include a lack of modularity, making it difficult to independently scale or deploy components, challenges in testing, and tight coupling between feature engineering, model training, and inference steps.\n",
      "The conversation between Human and AI assistance [INST] What are the benefits of modular pipelines? [/INST] Modular pipelines offer benefits such as improved scalability, independent component scaling and deployment, easier testing, and promotion of code reusability.\n",
      "The conversation between Human and AI assistance [INST] What are some feature engineering steps in a feature pipeline? [/INST] Some feature engineering steps in a feature pipeline include data cleaning, validation, deduplication, feature extraction, aggregations, feature binning, and feature crosses.\n",
      "The conversation between Human and AI assistance [INST] What are feature stores and how are they useful in machine learning pipelines? [/INST] Feature stores are used to store features that can be reused across different models for training and inference. They are particularly helpful in online models that require historical or contextual information.\n",
      "The conversation between Human and AI assistance [INST] How can model training be improved iteratively? [/INST] Model training can be improved iteratively by trying out different supervised ML learning algorithms, adjusting hyperparameters, evaluating the model on a validation set, and making data-centric improvements such as adding/removing features, adding more training data, and improving the quality of existing training samples.\n",
      "The conversation between Human and AI assistance [INST] What are some serverless compute platforms and feature stores that can be used for serverless ML? [/INST] Some serverless compute platforms and feature stores that can be used for serverless ML include Hopsworks, AWS Sagemaker, Databricks, and Google Vertex.\n",
      "The conversation between Human and AI assistance [INST] What is the purpose of the training pipeline? [/INST] The purpose of the training pipeline is to train a classification model for iris flowers using the K-Nearest Neighbors algorithm.\n",
      "The conversation between Human and AI assistance [INST] How is the model evaluated in the pipeline? [/INST] The model is evaluated in the pipeline by predicting the flower species for the testing data and generating a classification report and confusion matrix to assess its performance.\n",
      "The conversation between Human and AI assistance [INST] What is the purpose of the interactive inference pipeline? [/INST] The purpose of the interactive inference pipeline is to allow users to input the dimensions of an iris flower and get a prediction of its species using the trained model.\n",
      "The conversation between Human and AI assistance [INST] Can deep learning handle images with millions of features? [/INST] Yes, deep learning is capable of handling images with millions of features.\n",
      "The conversation between Human and AI assistance [INST] What is supervised machine learning and how does it work? [/INST] Supervised machine learning involves extracting patterns from labeled data to create a model that can make predictions for new unlabeled data.\n",
      "The conversation between Human and AI assistance [INST] What is the role of feature engineering in machine learning? [/INST] Feature engineering is the process of extracting relevant features from input data.\n",
      "The conversation between Human and AI assistance [INST] Why is deep neural network training computationally intensive and time-consuming? [/INST] Deep neural network training is computationally intensive and time-consuming due to the massive amount of training data and large number of parameters.\n",
      "The conversation between Human and AI assistance [INST] What are the reasons why deep learning works now? [/INST] Deep learning works now due to the increase in computing power, better training algorithms, and availability of huge quantities of data.\n",
      "The conversation between Human and AI assistance [INST] What are some important linear algebra concepts in deep learning? [/INST] Important linear algebra concepts in deep learning include vectors, matrices, matrix operations (addition, subtraction, multiplication, transpose, inverse), and vector norms.\n",
      "The conversation between Human and AI assistance [INST] How is probability theory used in deep learning? [/INST] Probability theory is used in deep learning to understand random variables, probability distributions, probability mass functions, independence, and conditional probability.\n",
      "The conversation between Human and AI assistance [INST] What is the difference between probability and likelihood? [/INST] Probability refers to future events, while likelihood refers to past events. \n",
      "The conversation between Human and AI assistance [INST] What is the purpose of Maximum Likelihood Estimation? [/INST] The purpose of Maximum Likelihood Estimation is to find the most likely value of a parameter in a probability model based on a given set of data. \n",
      "The conversation between Human and AI assistance [INST] How is numerical underflow avoided in Maximum Likelihood Estimation? [/INST] Numerical underflow is avoided in Maximum Likelihood Estimation by using the logarithm of the likelihood function.\n",
      "The conversation between Human and AI assistance [INST] What is entropy and how is it used? [/INST] Entropy is a measure of uncertainty or randomness in a probability distribution. It is used to quantify the amount of information or surprise in an outcome. In this example, entropy is used to compare the true distribution of coin tosses to a predicted distribution.\n",
      "The conversation between Human and AI assistance [INST] What is the true distribution of the coin tosses in the example? [/INST] The true distribution of the coin tosses is given by p(h) = 2/6 (2 out of 6 tosses resulting in heads) and p(t) = 4/6 (4 out of 6 tosses resulting in tails).\n",
      "The conversation between Human and AI assistance [INST] How is the negative log likelihood calculated in this example? [/INST] The negative log likelihood is calculated by taking the logarithm of the predicted distribution's likelihood function. In this example, the negative log likelihood is given by -log(θ^2(1-θ)^4) = -2log(θ) - 4log(1-θ), where θ represents the probability of heads and (1-θ) represents the probability of tails.\n"
     ]
    }
   ],
   "source": [
    "# Generate instruction set\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "instructions = pd.DataFrame(columns=[\"Instructions\"])\n",
    "row_result = {\"Instructions\":[]}\n",
    "\n",
    "for i,r in enumerate(responses):\n",
    "    #p\n",
    "    try:\n",
    "        tmp = json.loads(r)\n",
    "        for j in range(len(tmp[\"questions\"])):\n",
    "            instr = f\"The conversation between Human and AI assistance [INST] {tmp['questions'][j]} [/INST] {tmp['answers'][j]}\"\n",
    "            row_result[\"Instructions\"].append(instr)\n",
    "            #print(instr)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "instructions = pd.DataFrame(row_result)\n",
    "print(instructions)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "for index, row in instructions.iterrows():\n",
    "    print(row[\"Instructions\"])\n",
    "\n",
    "\n",
    "\n",
    "#print(instructions.iloc[0][\"Instructions\"])\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.6 64-bit",
   "name": "python3116jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}